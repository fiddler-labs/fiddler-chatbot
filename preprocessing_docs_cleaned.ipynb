{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a8359b3-5822-4ae4-8f4c-2790033cb519",
   "metadata": {},
   "source": [
    "## Follow these steps to reload the vector index, currently running in DataStax\n",
    "\n",
    "* **Gather Data** \n",
    "    * Clear out the old doc content folders if necessary\n",
    "        * Delete old content source: documentation_data/vector_index_feed_v2x.x.csv\n",
    "        * Clear out:\n",
    "          * documentation_data/fiddler-docs\n",
    "          * documentation_data/md-notebooks\n",
    "          * documentation_data/notebooks-ipynb\n",
    "    * Download The [docs](https://github.com/fiddler-labs/fiddler/tree/main/docs) folder from the [Fiddler Repo](https://github.com/fiddler-labs/fiddler/docs)\n",
    "        * Clone or Make a pull request on the repo\n",
    "        * Make sure this is pulled from the branch corresponding to the current release (branch name should be `release/2x.x`)\n",
    "        * Copy the \"docs\" folder contents to fiddler-chatbot/documentation_data/fiddler-docs\n",
    "    * Copy the latest [quickstart notebooks](https://github.com/fiddler-labs/fiddler-examples)\n",
    "        * Make sure to grab the notebook from the latest release\n",
    "        * Place all the .ipynb files from for the lastest release in fiddler-chatbot/documentation_data/notebooks-ipynb\n",
    "* **Run this notebook to generate the vector index feed for this verison**\n",
    "    * Update the `release_num` flag (in set state step) to the release version you are loading the docs for\n",
    "    * Generate the markdown version of .ipynb files and add it to quickstart pages on with the script below\n",
    "    * Crawl the our website for blog and other resources\n",
    "    * You will need to add the caveats from last version the current version\n",
    "    * Chunk the data\n",
    "    * Finally we generate the vector_index_feed_2x.x.csv that we will upload to our vector database\n",
    "\n",
    "* **Last step:** Reloading the vector index table is done via the \"loader_cassandra_vector_index.ipynb\" notebook. Open that notebook and follow the instructions there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31ab6c-89d1-4191-b166-b930e40b95b0",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ca32fdd-957d-41d5-9a1c-8cff797ac19e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip -q install tiktoken\n",
    "!pip -q install openai\n",
    "!pip -q install scipy\n",
    "!pip -q install feedparser\n",
    "!pip -q install bs4\n",
    "!pip -q install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d89bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a221e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7563f685-be29-4064-a416-78cea3659bcc",
   "metadata": {},
   "source": [
    "### Set State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c13e1f17-8e72-43f7-8d99-de0e8e3afaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING_MODEL = \"text-embedding-3-large\"  # OpenAI's latest and most capable embedding model\n",
    "# GPT_MODEL = \"gpt-4-turbo\"\n",
    "release_num = 'v25.10'\n",
    "source_doc_path = './documentation_data'\n",
    "fiddler_doc_path = f'{source_doc_path}/fiddler-docs'\n",
    "quickstarts_doc_path = f'{fiddler_doc_path}/tutorials-and-quick-starts'\n",
    "quickstart_notebooks_path = f'{source_doc_path}/notebooks-ipynb'\n",
    "nbconverted_output_path = f'{source_doc_path}/md-notebooks'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a025d27-522a-4122-9676-9e0c9c313f7d",
   "metadata": {},
   "source": [
    "## Append Notebook markdowns to quick starts\n",
    "\n",
    "- Download the latest [Quickstart guides](https://github.com/fiddler-labs/fiddler-examples/tree/main/quickstart/latest) into *documentation_data/notebooks-ipynb* folder: \n",
    "- Running the NBConvert in the next step will update the md-notebooks folder with markdown version of all the quickstarts from the *notebooks-ipynb* directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7352c6-7c88-4fa7-b8e3-e801e5b2a10e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --output-dir={nbconverted_output_path} {quickstart_notebooks_path}/* --to markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2470619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the script dynamically injects the full text of quickstart notebook tutorials into their corresponding documentation pages, \n",
    "# ensuring the docs are comprehensive and self-contained.\n",
    "\n",
    "for root, dirs, files in os.walk(quickstarts_doc_path): \n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        print(path)\n",
    "        if path[-3:] == '.md':\n",
    "            with open(path,'r') as f:\n",
    "                file_str = f.read()\n",
    "            ipynb_links = re.search(r'\\bFiddler_Quickstart_\\w+', file_str)\n",
    "            print(ipynb_links)\n",
    "            if ipynb_links:\n",
    "                f = f'{nbconverted_output_path}/{ipynb_links.group()}.md'\n",
    "                print(f)\n",
    "                with open(f,'r') as l: \n",
    "                    QS = l.read()\n",
    "                    # print(QS)\n",
    "                with open(path, 'a') as f:\n",
    "                    f.write(QS)\n",
    "                    print(f'Opening {path} to append contents of {ipynb_links.group()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4795caef-4377-4fde-9369-b0b35c12f4ff",
   "metadata": {},
   "source": [
    "The cell will take the markdown version of the notebooks and append it to the quickstart pages in the documentation directory "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b1b58-85bc-492d-b7fe-4d1c45664471",
   "metadata": {},
   "source": [
    "## Creating list of chunked_docs from downloaded documentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd988d-8d2b-46b9-8a3d-b7f18a920280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the path to where your downloaded folder is and choose the version of the docs you want to process\n",
    "source_docs = []\n",
    "for root, dirs, files in os.walk(fiddler_doc_path):\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        if path[-3:] == \".md\":\n",
    "            with open(path, \"r\") as f:\n",
    "                file_str = f.read()\n",
    "                # Embed the URL of the doc in the file so the LLM doesn't have to look it up \n",
    "                doc_url = f'DOC_URL:{path[:-3]}'\n",
    "                doc_content = f'DOC_CONTENT:{file_str}'\n",
    "                source_docs.append(f'{doc_url}\\n{doc_content}')\n",
    "print(source_docs[0])\n",
    "len(source_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0e8cc-ba4f-4be0-9e43-3c263b1f56bf",
   "metadata": {},
   "source": [
    "### Crawl the blog and resources content and append it to chunked_doc list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb2e96-9b61-452d-b40f-4426ff197d74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def crawl_rss_feed(rss_url):\n",
    "    # Parse the RSS feed\n",
    "    feed = feedparser.parse(rss_url)\n",
    "\n",
    "    print(\"Number of Blogs:\", len(feed.entries))\n",
    "    \n",
    "    # Iterate through the entries in the feed\n",
    "    for entry in feed.entries:\n",
    "\n",
    "        # Get the URL of the blog article\n",
    "        article_url = entry.link\n",
    "\n",
    "        # Fetch the content of the article\n",
    "        response = requests.get(article_url)\n",
    "        html_content = response.content.decode('utf-8', 'ignore')\n",
    "\n",
    "        # Use BeautifulSoup to parse the HTML and extract the body\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        div_content = soup.find('div', class_='blog-post_content-wrapper')  # You may need to adjust this based on the HTML structure\n",
    "\n",
    "        # Print or manipulate the content of the div\n",
    "        if div_content:\n",
    "            print(\"Title:\", entry.title)\n",
    "            print(\"Link:\", entry.link)\n",
    "            itemtext=''\n",
    "            for item in div_content.select('p'):\n",
    "                itemtext+=item.text + ' '\n",
    "            source_docs.append(f\"BlogLink:{entry.link}\\nContent: {itemtext}\")\n",
    "        else:\n",
    "            print(\"Div not found.\")\n",
    "\n",
    "# Replace 'your_rss_feed_url' with the actual RSS feed URL\n",
    "rss_feed_url = 'https://www.fiddler.ai/blog/rss.xml'\n",
    "crawl_rss_feed(rss_feed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c7bca-6280-4003-82d1-7d43dcdd6aa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mostly duplicate code, consider refactoring\n",
    "\n",
    "def crawl_rss_feed(rss_url):\n",
    "    # Parse the RSS feed\n",
    "    feed = feedparser.parse(rss_url)\n",
    "\n",
    "    print(\"Number of Resources:\", len(feed.entries))\n",
    "    \n",
    "    # Iterate through the entries in the feed\n",
    "    for entry in feed.entries:\n",
    "\n",
    "        # Get the URL of the blog article\n",
    "        article_url = entry.link\n",
    "\n",
    "        # Fetch the content of the article\n",
    "        response = requests.get(article_url)\n",
    "        html_content = response.content.decode('utf-8', 'ignore')\n",
    "\n",
    "        # Use BeautifulSoup to parse the HTML and extract the body\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        \n",
    "        div_content = soup.find('div', class_='resources-copy')  # You may need to adjust this based on the HTML structure\n",
    "\n",
    "        # Print or manipulate the content of the div\n",
    "        if div_content:\n",
    "            print(\"Title:\", entry.title)\n",
    "            print(\"Link:\", entry.link)\n",
    "            itemtext=''\n",
    "            for item in div_content.select('p'):\n",
    "                itemtext+=item.text + ' '\n",
    "            source_docs.append(f\"ResourceLink:{entry.link}\\nContent:{itemtext}\")\n",
    "        else:\n",
    "            print(\"Div not found.\")\n",
    "\n",
    "# Replace 'your_rss_feed_url' with the actual RSS feed URL\n",
    "rss_feed_url = 'http://www.fiddler.ai/resources/rss.xml'\n",
    "crawl_rss_feed(rss_feed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bc3813-bc83-40be-8605-de3839b50efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(source_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d6f84b-61f3-4943-a686-e36a430f3aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "chunked_doc = [item.strip() for item in source_docs]\n",
    "print(len(chunked_doc))\n",
    "# text_splitter = TokenTextSplitter(chunk_size=400, chunk_overlap=40)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=300,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunked_doc_with_overlap = []\n",
    "\n",
    "for doc in chunked_doc:\n",
    "    texts = text_splitter.split_text(doc)\n",
    "    chunked_doc_with_overlap = chunked_doc_with_overlap + texts\n",
    "\n",
    "print(len(chunked_doc_with_overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad02b4d7-ffe5-4610-95f5-9d6cec911baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(chunked_doc_with_overlap, columns=['text'])\n",
    "df.to_csv(f'documentation_data/vector_index_feed_{release_num}.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59ae7c5-9aa6-4aae-a383-d05c058d5fd8",
   "metadata": {},
   "source": [
    "### You are done! Please check the **vector_index_feed_{release_num}.csv** file and navigate to loader_casandra_vector_index.ipynb to upload these snippets to our vector database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-secure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
