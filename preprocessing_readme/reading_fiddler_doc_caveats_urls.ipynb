{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a221e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90f76ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff7f5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#improvements based on hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eac371a-7dd7-44aa-8e8b-f4a75ec643ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"/Users/murtuzashergadwala/Downloads/Fiddler AI - Blogs (2).csv\"\n",
    "df = pd.read_csv(path)\n",
    "blog_text = df[df['Blog title']==\"Fiddler Introduces End-to-End Workflow for Robust Generative AI\"][\"Content\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f3cb9cd-3cb4-4dce-b2ff-021978a5d937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI has been in the limelight thanks to ‌recent AI products like ChatGPT, DALLE- 2, and Stable Diffusion. These breakthroughs reinforce the notion that companies need to double down on their AI strategy and execute on their roadmap to stay ahead of the competition. However, Large Language Models (LLMs) and other generative AI models pose the risk of providing users with inaccurate or biased results, generating adversarial output that’s harmful to users, and exposing private information used in training. This makes it critical for companies to implement LLMOps practices to ensure generative AI models and LLMs are continuously high-performing, correct, and safe.The Fiddler AI Observability platform helps standardize LLMOps by streamlining LLM workflows from pre-production to production, and creating a continuous feedback loop for improved prompt engineering and LLM fine-tuning.Figure 1: Fiddler AI Observability optimizes LLMs and generative AI for better outcomesPre-production Workflow:Robust evaluation of prompts and models with Fiddler AuditorWe are thrilled to launch Fiddler Auditor today to ensure LLMs perform in a safe and correct fashion. Fiddler Auditor is the first robustness library that leverages LLMs to evaluate robustness of other LLMs. Testing the robustness of LLMs in pre-production is a critical step in LLMOps. It helps identify weaknesses that can result in hallucinations, generate harmful or biased responses, and expose private information. ML and software application teams can now utilize the Auditor to test model robustness by applying perturbations, including adversarial examples, out-of-distribution inputs, and linguistic variations, and obtain a report to analyze the outputs generated by the LLM.A practitioner can evaluate LLMs from OpenAI, Anthropic, and Cohere using the Fiddler Auditor and find areas to improve correctness and performance while minimizing hallucinations. In the example below, we tested OpenAI’s test-davinci-003 model with the following prompt and the best output it should generate when prompted: Then, we entered five perturbations with linguistic variations, and only one of them generated the desired output as seen in the report below. If the LLM were released for public use as is, users would lose trust in it as the model generates hallucinations for simple paraphrasing, and users could potentially be harmed had they acted on the output generated.Figure 2: Evaluate the robustness of LLMs in a reportThe Fiddler Auditor is on GitHub. Don’t forget to give us a star if you enjoy using it! ⭐‍Production Workflow:Continuous monitoring to ensure optimal experienceTransitioning into production requires continuous monitoring to ensure optimal performance. Earlier this year, we announced how vector monitoring in the Fiddler AI Observability platform can monitor LLM-based embeddings generated by OpenAI, Anthropic, Cohere, and embeddings from other LLMs with a minimal integration effort. Our clustering-based multivariate drift detection algorithm is a novel method for measuring data drift in natural language processing (NLP) and computer vision (CV) models.ML teams can track and share LLM metrics like model performance, latency, toxicity, costs, and other LLM-specific metrics in real-time using custom dashboards and charts. Metrics like toxicity are calculated by using methods from HuggingFace. Early warnings from flexible model monitoring alerts cut through the noise and help teams prioritize on business-critical  issues. Figure 3: Track metrics like toxicity in real-time to improve prompt engineering and LLM fine-tuningImproving LLM performance using root cause analysisOrganizations need in-depth visibility into their AI solutions to help improve user satisfaction. Through slice & explain, ML teams can get a 360° view into the performance of their AI solutions, helping them refine prompt context, and gain valuable inputs for fine-tuning models.Fiddler AI Observability: A Unified Platform for ML and Generative AI Figure 4: The Fiddler AI Observability platformWith these new product enhancements, the Fiddler AI Observability platform is a full stack platform for predictive and generative AI models. ML/AI and engineering teams can standardize their practices for both LLMOps and MLOps through model monitoring, explainable AI, analytics, fairness, and safety. We continue our unwavering mission to partner with companies in their AI journey to build trust into AI. Our product and data science teams have been working with companies that are defining ways to operationalize AI beyond predictive models and successfully implement generative AI models to deliver high performance AI, reduce costs, and be responsible with model governance.We look forward to building more capabilities to help companies standardize their LLMOps and MLOps. \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# html_text = \"<p>This is <b>HTML</b> text.</p>\"\n",
    "soup = BeautifulSoup(blog_text, 'html.parser')\n",
    "clean_text = soup.get_text()\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d3a057a-19eb-4182-8d6a-e91930acc202",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_doc = [clean_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd9ec0-1fa6-4244-9fa0-58367bf32cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7346a177-a308-4d02-b56c-e924f69c2006",
   "metadata": {},
   "outputs": [],
   "source": [
    "caveats = \"\"\"Currently, only the following fields in [fdl.ModelInfo()](ref:fdlmodelinfo) can be updated:\n",
    "> \n",
    "> - `custom_explanation_names`\n",
    "> - `preferred_explanation_method`\n",
    "> - `display_name`\n",
    "> - `description` \"\"\"\n",
    "\n",
    "chunked_doc = [caveats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f97a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caveats = \"Once you have added a model on the Fiddler platform using a specific model info object, that is fdl.ModelInfo, you cannot modify aspects such as features, inputs, outputs, model task etc. specified in the model info object. Currently, if you want to change fundamental details about a modelinfo object, then it is advised to create/add a new model with a new modelinfo object.\"\n",
    "# chunked_doc = [Caveats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1212bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caveats = \"Custom metrics is an upcoming feature and it is currently not supported.\"\n",
    "# chunked_doc = [Caveats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2d757bc-5507-4048-beed-ec5dcf89e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "url = \"https://www.fiddler.ai/blog/\"\n",
    "\n",
    "#\n",
    "loader = RecursiveUrlLoader(url=url, max_depth=2, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a659909-42d4-4a75-b5bf-4b4521977adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Fiddler Introduces End-to-End Workflow for Robust Generative AI | Fiddler AI Blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJoin us 8/24 for AI Explained: AI Explained: Graph Neural Networks and Generative AI! Register nowProductPlatform CapabilitiesWhy Fiddler AI ObservabilityKey capabilities and benefitsExplainable AIUnderstand the â\\x80\\x98whyâ\\x80\\x99 and â\\x80\\x98howâ\\x80\\x99 behind your modelsNLP and CV ModelsMonitor and uncover anomalies in unstructured modelsLLMOpsOptimize LLMs for better outcomesÂ\\xa0SecurityEnterprise-grade security and compliance standardsMLOpsDeliver high performing AI solutions at scaleMoreModel MonitoringDetect model drift, assess performance and integrity, and set alertsAnalyticsConnect predictions with context to business alignment and valueFairnessMitigate bias and build a responsible AI cultureImprove your AI models. Request a demoSolutionsCustomer ExperienceDeliver seamless customer experiencesLending and TradingMake fair and transparent lending decisions with confidenceCase Studieshow.fm reduces time to detect model issues from days to minutesRead moreConjura reduces time to detect and resolve model drift from days to hoursRead moreLifetime ValueExtend the customer lifetime valueRisk and ComplianceMinimize risk with model governance and ML complianceGovernmentSafeguarding citizens and national security with trusted AIPricingPricing PlansChoose the plan thatâ\\x80\\x99s right for youPlatform Pricing MethodologyDiscover our simple and transparent pricingPlan ComparisonCompare platform capabilities and support across plansFAQsObtain pricing answers from frequently asked questionsBuild vs BuyKey considerations for buying an AI Observability solutionContact SalesHave questions about pricing, plans, or Fiddler? Contact us to talk to an expertResourcesLearnFeatured ResourcesIntroducing Fiddler AuditorRead blogOperationalize Models at ScaleWatch nowModel Monitoring Best PracticesLearn moreResource LibraryDiscover the latest reports, videos, and researchDocsGet in-depth user guides and technical documentationBlogRead the latest blogs, product updates, and company newsGuidesDownload our latest guides on model monitoring and AI explainabilityConnectEventsFind out about upcoming eventsBecome a PartnerLearn more about our partner programSupportNeed help? Contact the Fiddler AI support teamAmazon SageMaker +Â\\xa0FiddlerEnd-to-end model lifecycle managementCompanyAboutOur mission and who we areCareers We're hiring!Join Fiddler AI to build trustworthy and responsible AI solutionsFeatured newsFiddler AI is on a16z's inaugural Data50 list of the world's top 50 data startupsRead analysisCustomersLearn how customers use FiddlerNewsroomExplore recent news and press releasesRequest demoContact usContact usRequest demoRequest demoBack to blog homeFiddler Introduces End-to-End Workflow for Robust Generative AIProductGenerative AIPublishedMay 23, 2023Last EditedJune 21, 2023Sree KamireddyDirector of Product ManagementFiddler AIKaren HePrincipal Product Marketing ManagerFiddler AI\\n\\n\\n\\n\\n\\nImprove your AI modelsMonitor your models, know the how and why behind decisions, and standardize MLOps best practices.Request demoAI has been in the limelight thanks to â\\x80\\x8crecent AI products like ChatGPT, DALLE- 2, and Stable Diffusion. These breakthroughs reinforce the notion that companies need to double down on their AI strategy and execute on their roadmap to stay ahead of the competition. However, Large Language Models (LLMs) and other generative AI models pose the risk of providing users with inaccurate or biased results, generating adversarial output thatâ\\x80\\x99s harmful to users, and exposing private information used in training. This makes it critical for companies to implement LLMOps practices to ensure generative AI models and LLMs are continuously high-performing, correct, and safe.The Fiddler AI Observability platform helps standardize LLMOps by streamlining LLM workflows from pre-production to production, and creating a continuous feedback loop for improved prompt engineering and LLM fine-tuning.Figure 1: Fiddler AI Observability optimizes LLMs and generative AI for better outcomesPre-production Workflow:Robust evaluation of prompts and models with Fiddler AuditorWe are thrilled to launch Fiddler Auditor today to ensure LLMs perform in a safe and correct fashion.Â\\xa0Fiddler Auditor is the first robustness library that leverages LLMs to evaluate robustness of other LLMs. Testing the robustness of LLMs in pre-production is a critical step in LLMOps. It helps identify weaknesses that can result in hallucinations, generate harmful or biased responses, and expose private information. ML and software application teams can now utilize the Auditor to test model robustness by applying perturbations, including adversarial examples, out-of-distribution inputs, and linguistic variations, and obtain a report to analyze the outputs generated by the LLM.A practitioner can evaluate LLMs from OpenAI, Anthropic, and Cohere using the Fiddler Auditor and find areas to improve correctness and performance while minimizing hallucinations. In the example below, we tested OpenAIâ\\x80\\x99s test-davinci-003 model with the following prompt and the best output it should generate when prompted: Then, we entered five perturbations with linguistic variations, and only one of them generated the desired output as seen in the report below. If the LLM were released for public use as is, users would lose trust in it as the model generates hallucinations for simple paraphrasing, and users could potentially be harmed had they acted on the output generated.Figure 2: Evaluate the robustness of LLMs in a reportThe Fiddler Auditor is on GitHub. Donâ\\x80\\x99t forget to give us a star if you enjoy using it! â\\xad\\x90â\\x80\\x8dProduction Workflow:Continuous monitoring to ensure optimal experienceTransitioning into production requires continuous monitoring to ensure optimal performance. Earlier this year, we announced how vector monitoring in the Fiddler AI Observability platform can monitor LLM-based embeddings generated by OpenAI, Anthropic, Cohere, and embeddings from other LLMs with a minimal integration effort. Our clustering-based multivariate drift detection algorithm is a novel method for measuring data drift in natural language processing (NLP) and computer vision (CV) models.ML teams can track and share LLM metrics like model performance, latency, toxicity, costs, and other LLM-specific metrics in real-time using custom dashboards and charts. Metrics like toxicity are calculated by using methods from HuggingFace. Early warnings from flexible model monitoring alerts cut through the noise and help teams prioritize on business-criticalÂ\\xa0 issues.Â\\xa0Figure 3: Track metrics like toxicity in real-time to improve prompt engineering and LLM fine-tuningImproving LLM performance using root cause analysisOrganizations need in-depth visibility into their AI solutions to help improve user satisfaction. Through slice & explain, ML teams can get a 360Â° view into the performance of their AI solutions, helping them refine prompt context, and gain valuable inputs for fine-tuning models.Fiddler AI Observability: A Unified Platform for ML and Generative AIÂ\\xa0Figure 4: The Fiddler AI Observability platformWith these new product enhancements, the Fiddler AI Observability platform is a full stack platform for predictive and generative AI models. ML/AI and engineering teams can standardize their practices for both LLMOps and MLOps through model monitoring, explainable AI, analytics, fairness, and safety.Â\\xa0We continue our unwavering mission to partner with companies in their AI journey to build trust into AI. Our product and data science teams have been working with companies that are defining ways to operationalize AI beyond predictive models and successfully implement generative AI models to deliver high performance AI, reduce costs, and be responsible with model governance.We look forward to building more capabilities to help companies standardize their LLMOps and MLOps. Subscribe to our newsletterMonthly curated AI content, Fiddler updates, and more.\\n\\n\\nReady to try AI Observability?Request demoContact usProductAI ObservabilityModel MonitoringExplainable AIAnalyticsFairnessLLMOpsNLP and CV Model MonitoringMLOpsBuild vs BuySecurityCompanyAbout UsCareers We're hiring!NewsroomPartnersAmazon SageMaker + FiddlerSolutionsCustomer ExperienceLending and TradingLifetime ValueRisk and ComplianceGovernmentResourcesMediaBlogResource LibraryDocsWhat is Model Monitoring?ConnectEventsJoin our Slack CommunitySubscribe to our NewsletterContact usSupport CenterÂ© 2023 Fiddler AI. All rights reserved.Privacy PolicyTerms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://www.fiddler.ai/blog/fiddler-introduces-end-to-end-workflow-for-robust-generative-ai', 'title': 'Fiddler Introduces End-to-End Workflow for Robust Generative AI | Fiddler AI Blog', 'description': 'Fiddler introduces end-to-end workflow for robust generative AI enabling MLOps teams to monitor, analyze, and optimize predictive AI models, LLMs, and generative AI.', 'language': 'en'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8bb115-f4df-4276-89a5-b7104bfa7fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366c2290-9e47-48f5-bfe0-6be96efdc4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Caveats = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f56749",
   "metadata": {},
   "outputs": [],
   "source": [
    "Caveats = \"Re-uploading in Fiddler essentially means having to delete what was uploaded or ingested earlier, making the updates you want to make, and then following the same steps as before for the specific thing you are looking to upload. So for example, if you want make changes to the baseline dataset you uploaded, you will have to delete the dataset and then make modifications for the datasetinfo object or the dataset itself and then upload the dataset again. As for events, currently there isn't a way for the user to directly delete events. Please contact Fiddler personnell for the same. \"\n",
    "chunked_doc = [Caveats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5917d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "url_pattern = r'\\b((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)\\b'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c50101df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.example.com is a valid URL.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        response = requests.head(url)\n",
    "        return response.status_code == 200 or response.status_code == 302\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False\n",
    "\n",
    "# Example usage:\n",
    "url_to_check = \"https://www.example.com\"\n",
    "if is_valid_url(url_to_check):\n",
    "    print(f\"{url_to_check} is a valid URL.\")\n",
    "else:\n",
    "    print(f\"{url_to_check} is not a valid URL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a8afb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Fiddler's role in the ML lifecycle is to monitor, explain, analyze, and improve ML deployments at enterprise scale. It provides contextual insights at any stage of the ML lifecycle, helps improve predictions, increases transparency and fairness, and optimizes business revenue. Reference: [Fiddler Simple Monitoring Quick Start Guide](https://docs.fiddler.ai/docs/Fiddler_Quickstart_Simple_Monitoring)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "055bfb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = re.findall(url_pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7a4370c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://docs.fiddler.ai/docs/Fiddler_Quickstart_Simple_Monitoring']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "497f2eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Fiddler's role in the ML lifecycle is to monitor, explain, analyze, and improve ML deployments at enterprise scale. It provides contextual insights at any stage of the ML lifecycle, helps improve predictions, increases transparency and fairness, and optimizes business revenue. Reference: [Fiddler Simple Monitoring Quick Start Guide](\",\n",
       " ')']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa2e2ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://docs.fiddler.ai/docs/publish_event'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b168447f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_valid_url(urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9dabcd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [404]>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.head(\"https://docs.fiddler.ai/docs/Fiddler_Quickstart_Simple_Monitoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9d83fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [302]>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.head(urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53341fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_valid_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ded9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_doc = []\n",
    "target_path = \"./fiddler-2023-6-16/v1.8\"\n",
    "for root, dirs, files in os.walk(target_path):\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        if path[-3:] == '.md':\n",
    "            with open(path,'r') as f:\n",
    "                file_str = f.read()\n",
    "                file_str = file_str.replace('\\r', '').replace('\\n', ' ')\n",
    "            chunked_doc.append(file_str)\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06648a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ac15dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_doc = []\n",
    "target_path = \"./fiddler-2023-5-26/quickstart\"\n",
    "for root, dirs, files in os.walk(target_path):\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        if path[-3:] == '.md':\n",
    "            with open(path,'r') as f:\n",
    "                file_str = f.read()\n",
    "                file_str = file_str.replace('\\r', '').replace('\\n', ' ')\n",
    "            chunked_doc.append(path[len(target_path):]+file_str)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b021d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Currently, only the following fields in [fdl.ModelInfo()](ref:fdlmodelinfo) can be updated:\\n> \\n> - `custom_explanation_names`\\n> - `preferred_explanation_method`\\n> - `display_name`\\n> - `description` '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be055ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13dd4831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "for i in chunked_doc:\n",
    "    print(len(encoding.encode(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da9fc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f879fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=[]\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
    "for i in range(len(chunked_doc)):\n",
    "    fdl_doc_token_list = encoding.encode(chunked_doc[i])\n",
    "    if(len(fdl_doc_token_list)<8000):\n",
    "        response = openai.Embedding.create(model=EMBEDDING_MODEL, input=chunked_doc[i])\n",
    "        embeddings.append(response[\"data\"][0][\"embedding\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da3e4789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)==len(chunked_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7446892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"text\": chunked_doc, \"embedding\": embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b589b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"caveats5_llm.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0773b4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--- title: \"fdl.Priority\" slug: \"fdlpriority\" excerpt: \"Priority identifiers used on Alert Rules\" hidden: false createdAt: \"2023-01-31T07:30:12.886Z\" updatedAt: \"2023-01-31T07:30:12.886Z\" --- **This field can be used to prioritize the alert rules by adding an identifier - low, medium, and high to help users better categorize them on the basis of their importance. Following are the Priority Enums:**  | Enums               | Values | | :------------------ | :----- | | fdl.Priority.HIGH   | HIGH   | | fdl.Priority.MEDIUM | MEDIUM | | fdl.Priority.LOW    | LOW    |    ```coffeescript Usage import fiddler as fdl  client.add_alert_rule(     name = \"perf-gt-5prec-1hr-1d-ago\",     project_name = \\'project-a\\',     model_name = \\'model-a\\',     alert_type = fdl.AlertType.PERFORMANCE,      metric = fdl.Metric.PRECISION,     bin_size = fdl.BinSize.ONE_HOUR,      compare_to = fdl.CompareTo.TIME_PERIOD,     compare_period = fdl.ComparePeriod.ONE_DAY,     warning_threshold = 0.05,     critical_threshold = 0.1,     condition = fdl.AlertCondition.GREATER,     priority = fdl.Priority.HIGH, <---     notifications_config = notifications_config ) ``` ```coffeescript Outputs [AlertRule(alert_rule_uuid=\\'9b8711fa-735e-4a72-977c-c4c8b16543ae\\',            organization_name=\\'some_org_name\\',            project_id=\\'project-a\\',            model_id=\\'model-a\\',            name=\\'perf-gt-5prec-1hr-1d-ago\\',            alert_type=AlertType.PERFORMANCE,             metric=Metric.PRECISION,            priority=Priority.HIGH, <----            compare_to=\\'CompareTo.TIME_PERIOD,            compare_period=ComparePeriod.ONE_DAY,            compare_threshold=None,            raw_threshold=None,            warning_threshold=0.05,            critical_threshold=0.1,            condition=AlertCondition.GREATER,            bin_size=BinSize.ONE_HOUR)] ```'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[71][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e43900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c30a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100\n",
    "):\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = openai.Embedding.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50624bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55ce247f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Client Guide/uploading-a-baseline-dataset.md---\\ntitle: \"Uploading a Baseline Dataset\"\\nslug: \"uploading-a-baseline-dataset\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:07:03.211Z\"\\nupdatedAt: \"2022-06-08T15:32:37.948Z\"\\n---\\nTo upload a baseline dataset to Fiddler, you can use the [`client.upload_dataset`](https://api.fiddler.ai/#client-upload_dataset) API. Let\\'s walk through a simple example of how this can be done.\\n\\n***\\n\\nThe first step is to load your baseline dataset into a pandas DataFrame.\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"import pandas as pd\\\\n\\\\ndf = pd.read_csv(\\'example_dataset.csv\\')\",\\n      \"language\": \"python\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:api-header]\\n{\\n  \"title\": \"Creating a DatasetInfo object\"\\n}\\n[/block]\\nThen, you\\'ll need to create a [fdl.DatasetInfo()](ref:fdldatasetinfo) object that can be used to **define the schema for your dataset**.\\n\\nThis schema can be inferred from your DataFrame using the [fdl.DatasetInfo.from_dataframe()](ref:fdldatasetinfofrom_dataframe) function.\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"dataset_info = fdl.DatasetInfo.from_dataframe(df)\",\\n      \"language\": \"python\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:callout]\\n{\\n  \"type\": \"info\",\\n  \"title\": \"Info\",\\n  \"body\": \"In the case that you have **categorical columns in your dataset that are encoded as strings**, you can use the `max_inferred_cardinality` argument.\\\\n    \\\\nThis argument specifies a threshold for unique values in a column. Any column with fewer than `max_inferred_cardinality` unique values will be converted to[fdl.DataType.CATEGORY](ref:fdldatatype)  type.\"\\n}\\n[/block]\\n\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \" dataset_info = fdl.DatasetInfo.from_dataframe(\\\\n        df=df,\\\\n        max_inferred_cardinality=1000\\\\n    )\",\\n      \"language\": \"python\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:api-header]\\n{\\n  \"title\": \"Uploading your dataset\"\\n}\\n[/block]\\nOnce you have your [fdl.DatasetInfo()](ref:fdldatasetinfo) object, you can make any **necessary adjustments** before upload (see [Customizing Your Dataset Schema](doc:customizing-your-dataset-schema) ).\\n\\nWhen you\\'re ready, the dataset can be uploaded using [client.upload_dataset()](ref:clientupload_dataset).\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"PROJECT_ID = \\'example_project\\'\\\\nDATASET_ID = \\'example_dataset\\'\\\\n\\\\nclient.upload_dataset(\\\\n    project_id=PROJECT_ID,\\\\n    dataset_id=DATASET_ID,\\\\n    dataset={\\\\n        \\'baseline\\': df\\\\n    },\\\\n    info=dataset_info\\\\n)\",\\n      \"language\": \"python\"\\n    }\\n  ]\\n}\\n[/block]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Platform Guide/ds.md---\\ntitle: \"ML Algorithms In Fiddler\"\\nslug: \"ds\"\\nhidden: true\\ncreatedAt: \"2022-11-18T22:11:48.747Z\"\\nupdatedAt: \"2022-11-18T22:12:58.704Z\"\\n---\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Introduction/about-the-fiddler-client.md---\\ntitle: \"About the Fiddler Client\"\\nslug: \"about-the-fiddler-client\"\\nhidden: false\\ncreatedAt: \"2022-05-23T15:59:05.747Z\"\\nupdatedAt: \"2022-05-23T15:59:05.747Z\"\\n---\\nThe Fiddler Client contains many useful methods for sending and receiving data to and from the Fiddler platform.\\n\\nFiddler provides a Python Client that allows you to connect to Fiddler directly from a Python notebook or automated pipeline.\\n\\nEach client function is documented with a description, usage information, and code examples.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Client Guide/publishing-production-data.md---\\ntitle: \"Publishing Production Data\"\\nslug: \"publishing-production-data\"\\nhidden: false\\ncreatedAt: \"2022-11-18T23:28:25.348Z\"\\nupdatedAt: \"2022-12-19T19:14:28.171Z\"\\n---\\nThis Section guides you on the various ways you can provide event data to Fiddler and update and retrieve them.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Platform Guide/project-architecture.md---\\ntitle: \"Project Architecture\"\\nslug: \"project-architecture\"\\nhidden: false\\ncreatedAt: \"2022-11-15T18:06:28.079Z\"\\nupdatedAt: \"2023-02-14T23:21:13.699Z\"\\n---\\nSupervised machine learning involves identifying a predictive task, finding data to enable that task, and building a model using that data. \\n\\nFiddler captures this workflow with **project**, **dataset**, and **model** entities.\\n\\n## Project\\n\\nIn Fiddler, a project is essentially a parent folder that hosts one or more **model** (s) for the ML task (e.g. A Project HousePredict for predicting house prices will LinearRegression-HousePredict, RandomForest-HousePredict).\\n\\n## Models\\n\\nA model in Fiddler represents a **placeholder** for a machine-learning model. It\\'s a placeholder because we may not need the **[model artifacts](doc:artifacts-and-surrogates#Model-Artifacts)**. Instead, we may just need adequate [information about the model](ref:fdlmodelinfo) in order to monitor model-specific data. \\n\\n> 📘 Info\\n> \\n> You can [upload your model artifacts](https://dash.readme.com/project/fiddler/v1.6/docs/uploading-model-artifacts) to Fiddler to unlock high-fidelity explainability for your model. However, it is not required. If you do not wish to upload your artifact but want to explore explainability with Fiddler, we can build a [**surrogate model**](doc:artifacts-and-surrogates#surrogate-model) on the backend to be used in place of your artifact.\\n\\n## Datasets\\n\\nA dataset in Fiddler is a data table containing [information about data](ref:fdldatasetinfo) such as **features**, **model outputs**, and a **target** for machine learning models. Optionally, you can also upload **metadata** and “**decision**” columns, which can be used to segment the dataset for analyses, track business decisions, and work as protected attributes in bias-related workflows. \\n\\nIn order to monitor **production data**, a [dataset must be uploaded](ref:clientupload_dataset) to be used as a **baseline** for making comparisons. This baseline dataset should be sampled from your model\\'s **training data**. The sample should be unbiased and should faithfully capture moments of the parent distribution. Further, values appearing in the baseline dataset\\'s columns should be representative of their entire ranges within the complete training dataset.\\n\\n**Datasets are used by Fiddler in the following ways:**\\n\\n1. As a reference for [drift calculations](doc:data-drift-platform) and [data integrity violations ](doc:data-integrity-platform)on the **[Monitor](doc:monitoring-ui)** page\\n2. To train a model to be used as a [surrogate](doc:artifacts-and-surrogates#surrogate-model) when using [`add_model_surrogate`](/reference/clientadd_model_surrogate)\\n3. For computing model performance metrics globally on the **[Evaluate](doc:evaluation-ui)** page, or on slices on the **[Analyze](doc:analytics-ui)** page\\n4. As a reference for explainability algorithms (e.g. partial dependence plots, permutation feature impact, approximate Shapley values, and ICE plots).\\n\\nBased on the above uses, _datasets with sizes much in excess of 10K rows are often unnecessary_ and can lead to excessive upload, precomputation, and query times. That being said, here are some situations where larger datasets may be desirable:\\n\\n- **Auto-modeling for tasks with significant class imbalance; or strong and complex feature interactions, possibly with deeply encoded semantics**\\n  - However, in use cases like these, most users opt to upload carefully-engineered model artifacts tailored to the specific application.\\n- **Deep segmentation analysis**\\n  - If it’s desirable to perform model analyses on very specific subpopulations (e.g. “55-year-old Canadian home-owners who have been customers between 18 and 24 months”), large datasets may be necessary to have sufficient reference representation to drive model analytics.\\n\\n> 📘 Info\\n> \\n> Datasets can be uploaded to Fiddler using the[ Python API client](doc:installation-and-setup).\\n\\n [Check the UI Guide to Visualize Project Architecture on our User Interface](doc:project-structure)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "strings, relatednesses = strings_ranked_by_relatedness(\"Fiddler API documentation on uploading datasets\", df, top_n=5)\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    print(f\"{relatedness=:.3f}\")\n",
    "    display(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e24bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    "):\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
    "    introduction = 'Use the below documentation from the company Fiddler to answer the subsequent question. If the answer cannot be found in the documentation, write \"I could not find an answer.\"'\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    for string in strings:\n",
    "        next_article = string\n",
    "        if (\n",
    "            num_tokens(message + next_article + question, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_article\n",
    "    return message + question\n",
    "\n",
    "\n",
    "def ask(\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096 - 500,\n",
    "    print_message: bool = False,\n",
    "):\n",
    "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions about Fiddler documentation.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9bf5c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key='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'\n",
    "\n",
    "# ask('What API to use to upload dataset on Fiddler platform?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bc63012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I could not find an answer. The documentation provided explains the different types of explainability methods and their characteristics, but does not provide a sequence of steps for setting up model explanations.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"Give me the sequence of steps required to setup model explanations?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4da87547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The fiddler-client Python package can be used via API to get explanations from a model.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"What API to use to get run an explanation from a model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08f8bcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To bring your own requirements.txt while loading a model artifact, you can add a `requirements.txt` file inside your model artifact directory where `package.py` is defined. This is only used starting at version 23.1 with Model Deployment enabled. Add the dependencies to the `requirements.txt` file in the following format:\\n\\n```\\nscikit-learn==1.0.2  \\nnumpy==1.23.0  \\npandas==1.5.0\\n```'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"How can i bring my own requirements.txt while loading a model artifact?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb8efa1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A dataset is a data table containing information about data such as features, model outputs, and a target for machine learning models. It is used as a reference for drift calculations and data integrity violations on the Monitor page, to train a model to be used as a surrogate when using add_model_surrogate, for computing model performance metrics globally on the Evaluate page, or on slices on the Analyze page, and as a reference for explainability algorithms. On the other hand, an event is a prediction log sent to the Fiddler platform for monitoring purposes. It is used to calculate metrics around feature drift, prediction drift, and model performance, and is stored in Fiddler to allow for ad hoc segment analysis.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"What's the difference between a dataset and an event?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f488c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "570e9220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The different types of baselines are PRE_PRODUCTION, STATIC_PRODUCTION, and ROLLING_PRODUCTION.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"what are the different types of baselines?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7b6b0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I could not find an answer.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"Does fiddler support service account?  So we can perform actions via CI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "163e0839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can use the Fiddler Client to publish events in batch or streaming mode. The documentation does not specify a limit on the number of events that can be published.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"What API should I use if I want to publish a large number of events?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3e29add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Fiddler helps build trust in AI models through monitoring, explainability, and fairness capabilities. Fiddler provides powerful visualizations that can explain your model's behavior, which can be queried at an individual prediction level in the Explain tab, at a model level in the Analyze tab, or within the monitoring context in the Monitor tab. Explanations are available in the UI for structured (tabular) and natural language (NLP) models. Fiddler also provides several popular explanation methods to work fast and at scale, including SHAP and Fiddler SHAP, Integrated Gradients, and Tree SHAP. Additionally, Fiddler allows users to bring their own explanation method, which can be customized in the model's `package.py` wrapper script.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"What are the various ways in which Fiddler helps me build trust in my AI models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "75f201df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To use the bin size selector on Fiddler UI, you can customize your chart view using time range, time zone, and bin size chart filters. The bin size selected controls the frequency for which the data is displayed. So selecting Day will show daily data over the date range selected.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"How to use bin size selector on Fiddler UI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2a4a523f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'://files.readme.io/ea63eca-Screen_Shot_2022-05-19_at_3.34.24_PM.png\",\\n        \"Screen Shot 2022-05-19 at 3.34.24 PM.png\",\\n        870,\\n        343,\\n        \"#dadeed\"\\n      ]\\n    }\\n  ]\\n}\\n[/block]\\n3. Enter the Service account name and description. You can use the BigQuery Admin role under Grant this service account access to the project. Click Done. You can now see the new service account under the Credentials screen. Click the pencil icon beside the new service account you have created and click Add Key to add auth key. Please choose JSON and click CREATE. It will download the JSON file with auth key info. (Download path will be used to authenticate)\\n\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/662315e-Screen_Shot_2022-05-19_at_3.39.24_PM.png\",\\n        \"Screen Shot 2022-05-19 at 3.39.24 PM.png\",\\n        986,\\n        581,\\n        \"#c3c1c1\"\\n      ]\\n    }\\n  ]\\n}\\n[/block]\\n## Step 2 - Import data from BigQuery\\nWe will now use the generated key to connect to BigQuery tables from Jupyter Notebook. \\n1. Install the following libraries in the python environment and load them to jupyter-\\n  * Google-cloud\\n  * Google-cloud-bigquery[pandas]\\n  * Google-cloud-storage\\n\\n2. Set the environment variable using the key that was generated in Step 1\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"#Set environment variables for your notebook\\\\nimport os\\\\nos.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'<path to json file>\\'\",\\n      \"language\": \"python\"\\n    }\\n  ]\\n}\\n[/block]\\n3. Import Google cloud client and initiate BigQuery service\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"#Imports google cloud client library and initiates BQ service\\\\nfrom google.cloud import bigquery\\\\nbigquery_client = bigquery.Client()\",\\n      \"language\": \"python\"\\n    }\\n  ]\\n}\\n[/block]\\n4. Specify the query which will be used to import the data from BigQuery\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"#Write Query on BQ\\\\nQUERY = \\\\\"\\\\\"\\\\\"\\\\nSELECT * FROM `fiddler-bq.fiddler_test.churn_prediction_baseline` \\\\n  \\\\\"\\\\\"\\\\\"\",\\n      \"language\": \"python\"\\n    }\\n  ]\\n}\\n[/block]\\n5. Read the data using the query and write the data to a pandas dataframe\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"#Run the query and write result to a pandas data frame\\\\nQuery_Results = bigquery_client.query(QUERY)\\\\nbaseline_df = Query_Results.to_dataframe()\",\\n      \"language\": \"python\"\\n    }\\n  ]\\n}\\n[/block]\\nNow that we have data imported from BigQuery to a jupyter notebook, we can refer to the following notebooks to\\n1. [Upload baseline data and register model ](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/integration-examples/bigquery/Fiddler-BigQuery%20Integration%20-%20Model%20Registration.ipynb)\\n2. [Publish production events ](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/integration-examples/bigquery/Fiddler-BigQuery%20Integration%20-%20Event%20Publishing.ipynb) ---\\ntitle: \"S3 Integration\"\\nslug: \"integration-with-s3\"\\nhidden: false\\ncreatedAt: \"2022-04-19T17:40:36.681Z\"\\nupdatedAt: \"2023-02-02T20:19:53.031Z\"\\n---\\n## Pulling a dataset from S3\\n\\nYou may want to **pull a dataset directly from S3**. This may be used either to upload a baseline dataset, or to publish production traffic to Fiddler.\\n\\nYou can use the following code snippet to do so. Just fill out each of the string variables (`S3_BUCKET`, `S3_FILENAME`, etc.) with the correct information.\\n\\n```python\\nimport boto3\\nimport pandas as pd\\n\\nS3_BUCKET = \\'my_bucket\\'\\nS3_FILENAME = \\'my_baseline.csv\\'\\n\\nAWS_ACCESS_KEY_ID = \\'my_access_key\\'\\nAWS_SECRET_ACCESS_KEY = \\'my_secret_access_key\\'\\nAWS_REGION = \\'my_region\\'\\n\\nsession = boto3.session.Session(\\n    aws_access_key_id=AWS_ACCESS_KEY_ID,\\n    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\\n    region_name=AWS_REGION\\n)\\n\\ns3 = session.client(\\'s3\\')\\n\\ns3_data = s3.get_object(\\n    Bucket=S3_BUCKET,\\n    Key=S3_FILENAME\\n)[\\'Body\\']\\n\\ndf = pd.read_csv(s3_data)\\n```\\n\\n\\n\\n## Uploading the data to Fiddler\\n\\nIf your goal is to **use this data as a baseline dataset** within Fiddler, you can then proceed to upload your dataset (see [Uploading a Baseline Dataset](doc:uploading-a-baseline-dataset)).\\n\\nIf your goal is to **use this data as a batch of production traffic**, you can then proceed to publish the batch to Fiddler (see [Publishing Batches of Events](doc:publishing-batches-of-events) ). \\n\\n## What if I don’t want to hardcode my AWS credentials?\\n\\nIf you don’t want to hardcode your credentials, you can **use an AWS profile** instead. For more information on how to create an AWS profile, click [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html).\\n\\nYou can use the following code snippet to point your `boto3` session to the profile of your choosing.\\n\\n```python\\nimport boto3\\nimport pandas as pd\\n\\nS3_BUCKET = \\'my_bucket\\'\\nS3_FILENAME = \\'my_baseline.csv\\'\\n\\nAWS_PROFILE = \\'my_profile\\'\\n\\nsession = boto3.session.Session(\\n    profile_name=AWS_PROFILE\\n)\\n\\ns3 = session.client(\\'s3\\')\\n\\ns3_data = s3.get_object(\\n    Bucket=S3_BUCKET,\\n    Key=S3_FILENAME\\n)[\\'Body\\']\\n\\ndf = pd.read_csv(s3_data)\\n```\\n\\n\\n\\n## What if I don\\'t want to load the data into memory?\\n\\nIf you would rather **save the data to a disk** instead of loading it in as a pandas DataFrame, you can use the following code snippet instead.\\n\\n```python\\nimport boto3\\nimport pandas as pd\\nimport fiddler as fdl\\n\\nS3_BUCKET = \\'my_bucket\\'\\nS3_FILENAME = \\'my_baseline.csv\\'\\n\\nAWS_ACCESS_KEY_ID = \\'my_access_key\\'\\nAWS_SECRET_ACCESS_KEY = \\'my_secret_access_key\\'\\nAWS_REGION = \\'my_region\\'\\n\\nOUTPUT_FILENAME = \\'s3_data.csv\\'\\n\\nsession = boto3.session.Session(\\n    aws_access_key_id=AWS_ACCESS_KEY_ID,\\n    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\\n    region_name=AWS_REGION\\n)\\n\\ns3 = session.client(\\'s3\\')\\n\\ns3.download_file(\\n    Bucket=S3_BUCKET,\\n    Key=S3_FILENAME,\\n    Filename=OUTPUT_FILENAME\\n)\\n``` ---\\ntitle: \"Databricks Integration\"\\nslug: \"databricks-integration\"\\nhidden: false\\ncreatedAt: \"2023-02-02T20:38:54.971Z\"\\nupdatedAt: \"2023'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'02-03T19:39:05.557Z\"\\n---\\nDatabricks is a web-based platform for working with Spark, that provides automated cluster management and IPython-style notebooks for data engineering and machine learning.\\n\\nThis guide will walk you through the process of getting data from Databricks tables into a _Pandas_ dataframe. Once you have a dataframe ready you can easily upload that data into Fiddler.\\n\\n## Create the Dataframe in your Notebook\\n\\nStart by creating a Notebook in your Databricks workspace. Databricks has a lot of pre-installed libraries like _Spark_ and _Pandas_. Using the Spark library you can interact with all your delta lake assets.\\n\\nTo get your data into a _Pandas_ dataframe use the following code snippet. Just replace `table_name` with your desired table in your Databricks environment.\\n\\n```python\\nspark_dataframe = spark.read.table(table_name)\\nbaseline_df = spark_dataframe.toPandas()\\n```\\n\\n\\n\\n## Upload the Dataframe to Fiddler\\n\\nNow that we have a dataframe, you are ready to upload it to Fiddler. You will need to do the following: \\n\\n1. [Authorize the Fiddler client](https://dash.readme.com/project/fiddler/v1.6/docs/authorizing-the-client)\\n2. [Create a Project ](https://dash.readme.com/project/fiddler/v1.6/docs/project-architecture)\\n3. [Upload the Baseline Dataset ](https://dash.readme.com/project/fiddler/v1.6/docs/uploading-a-baseline-dataset)\\n\\nThe following code snippet combines all the steps mentioned abov\\n\\n```python\\n!pip install -q fiddler-client\\nimport fiddler as fdl\\nimport pandas as pd\\n\\n#set up fiddler client\\nURL = \\'\\' # Make sure to include the full URL (including https://). For example, https://abc.xyz.ai\\nORG_ID = \\'\\' # Found in General section under the settings tab \\nAUTH_TOKEN =\\'\\' # Found in the Credentials section under the settings tab \\n\\n# Initiate Fiddler client\\nclient = fdl.FiddlerApi(\\n    url=URL,\\n    org_id=ORG_ID,\\n    auth_token=AUTH_TOKEN\\n)\\n\\n#create a project\\nPROJECT_ID = \\'project_name\\'\\n\\nclient.create_project(PROJECT_ID)\\n\\n#let Fiddler understand your data\\ndataset_info = fdl.DatasetInfo.from_dataframe(baseline_df, max_inferred_cardinality=100)\\ndataset_info\\n\\n#Upload baseline dataset\\nDATASET_ID = \\'dataset_name\\'\\n\\nclient.upload_dataset(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID,\\n    dataset={\\n        \\'baseline\\': baseline_df\\n    },\\n    info=dataset_info\\n)\\n```\\n\\n\\n\\nYou should be able to find your dataset in Fiddler UI listed under the project you just created. Now you can onboard a model for this dataset. ---\\ntitle: \"Kafka Integration\"\\nslug: \"kafka-integration\"\\nhidden: false\\ncreatedAt: \"2023-05-23T20:07:39.194Z\"\\nupdatedAt: \"2023-05-24T15:22:11.758Z\"\\n---\\nFiddler Kafka connector is a service that connects to a [Kafka topic](https://kafka.apache.org/documentation/#intro_concepts_and_terms) containing production events for a model, and publishes the events to Fiddler.\\n\\n## Pre-requisites\\n\\nWe assume that the user has an account with Fiddler, has already created a project, uploaded a dataset and onboarded a model. We will need the [url_id, org_id,](doc:client-setup) project_id and model_id to configure the Kafka connector.\\n\\n## Installation\\n\\nThe Kafka connector runs on Kubernetes within the customer’s environment. It is packaged as a Helm chart. To install:\\n\\n```shell\\nhelm repo add fiddler https://helm.fiddler.ai/stable/\\n\\nhelm repo update\\n\\nkubectl -n kafka create secret generic fiddler-credentials --from-literal=auth=<API-KEY>\\n\\nhelm install fiddler-kafka fiddler/fiddler-kafka \\\\\\n    --devel \\\\\\n    --namespace kafka \\\\\\n    --set fiddler.url=https://<FIDDLER-URL> \\\\\\n    --set fiddler.org=<ORG> \\\\\\n    --set fiddler.project_id=<PROJECT-ID> \\\\\\n    --set fiddler.model_id=<MODEL-ID> \\\\\\n    --set fiddler.ts_field=timestamp \\\\\\n    --set fiddler.ts_format=INFER \\\\\\n    --set kafka.host=kafka \\\\\\n    --set kafka.port=9092 \\\\\\n    --set kafka.topic=<KAFKA-TOPIC> \\\\\\n    --set kafka.security_protocol=SSL \\\\\\n    --set kafka.ssl_cafile=cafile \\\\\\n    --set kafka.ssl_certfile=certfile \\\\\\n    --set kafka.ssl_keyfile=keyfile \\\\\\n    --set-string kafka.ssl_check_hostname=False\\n\\n```\\n\\nThis creates a deployment that reads events from the Kafka topic and publishes it to the configured model. The deployment can be scaled as needed. However, if the Kafka topic is not partitioned, scaling will not result in any gains.\\n\\n## Limitations\\n\\n1. The connector assumes that there is a single dedicated topic containing production events for a given model. Multiple deployments can be created, one for each model, and scaled independently.\\n2. The connector assumes that events are published as JSON serialized dictionaries of key-value pairs. Support for other formats can be added on request. As an example, a Kafka message should look like the following:\\n\\n```json\\n{\\n    “feature_1”: 20.7,\\n    “feature_2”: 45000,\\n    “feature_3”: true,\\n    “output_column”: 0.79,\\n    “target_column”: 1,\\n    “ts”: 1637344470000,\\n}\\n\\n``` ---\\ntitle: \"Snowflake Integration\"\\nslug: \"snowflake-integration\"\\nhidden: false\\ncreatedAt: \"2022-06-22T14:51:45.373Z\"\\nupdatedAt: \"2022-06-22T14:51:45.373Z\"\\n---\\n## Using Fiddler on your ML data stored in Snowflake\\n\\nIn this article, we will be looking at loading data from Snowflake tables and using the data for the following tasks-\\n1. Uploading baseline data to Fiddler\\n2. Registering a surrogate model to Fiddler\\n3. Publishing production data to Fiddler\\n\\n### Import data from Snowflake\\n\\nIn order to import data from Snowflake to Jupyter notebook, we will use the snowflake library, this can be installed using the following command in your Python environment.\\n\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"pip install snowflake-connector-python\",\\n      \"language\": \"python\"\\n    }\\n  ]\\n}\\n[/block]\\nOnce the library is installed, we would require the following to establish a connection to Snowflake\\n  * Snowflake Warehouse\\n  * Snowflake Role\\n  * Snowflake Account\\n  * Snowflake User\\n  * Snowflake Password\\n\\nThese can be obtained from your Snowflake account under the ‘Admin’ option in the Menu as shown below or by running the queries - \\n  * Warehouse - select CURRENT_WAREHOUSE()\\n  * Role - select CURRENT_ROLE()\\n  * Account - select CURRENT_ACCOUNT()\\n\\n\\'User\\' and \\'Password\\' are the same as one used for logging into your Snowflake account.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/c2f4cf4-Screen_Shot_2022-06-14_at_4.17.36_PM.png'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \"Data Pipeline Integrations\"\\nslug: \"data-pipeline-integrations\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:18:12.053Z\"\\nupdatedAt: \"2022-04-19T20:18:12.053Z\"\\n---\\n ---\\ntitle: \"PagerDuty Integration\"\\nslug: \"pagerduty\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:19:10.407Z\"\\nupdatedAt: \"2023-04-07T01:25:06.491Z\"\\n---\\nFiddler offers powerful alerting tools for monitoring models. By integrating with  \\nPagerDuty services, you gain the ability to trigger PagerDuty events within your monitoring  \\nworkflow.\\n\\n> 📘 \\n> \\n> If your organization has already integrated with PagerDuty, then you may skip to the [Setup: In Fiddler](#setup-in-fiddler) section to learn more about setting up PagerDuty within Fiddler.\\n\\n## Setup: In PagerDuty\\n\\n1. Within your PagerDuty Team, navigate to **Services** → **Service Directory**.\\n\\n![](https://files.readme.io/0ae47bb-pagerduty_1.png \"pagerduty_1.png\")\\n\\n\\n\\n2. Within the Service Directory:\\n   - If you are creating a new service for integration, select **+New Service** and follow the prompts to create your service.\\n   - Click the **name of the service** you want to integrate with.\\n\\n![](https://files.readme.io/956dbdf-pagerduty_2.png \"pagerduty_2.png\")\\n\\n\\n\\n3. Navigate to **Integrations** within your service, and select **Add a new integration to this service**.\\n\\n![](https://files.readme.io/ca2e4c2-pagerduty_3.png \"pagerduty_3.png\")\\n\\n\\n\\n4. Enter an **Integration Name**, and under **Integration Type** select the option **Use our API directly**. Then, select the **Add Integration** button to save your new integration. You will be redirected to the Integrations page for your service.\\n\\n![](https://files.readme.io/0f5d5ae-pagerduty_4.png \"pagerduty_4.png\")\\n\\n\\n\\n5. Copy the **Integration Key** for your new integration.\\n\\n![](https://files.readme.io/e144e08-pagerduty_5.png \"pagerduty_5.png\")\\n\\n\\n\\n## Setup: In Fiddler\\n\\n1. Within **Fiddler**, navigate to the **Settings** page, and then to the **PagerDuty Integration** menu. If your organization **already has a PagerDuty service integrated with Fiddler**, you will be able to find it in the list of services.\\n\\n![](https://files.readme.io/8de1a6b-pagerduty_setup_f_1.png \"pagerduty_setup_f_1.png\")\\n\\n\\n\\n2. If you are looking to integrate with a new service, select the **`+`** box on the top right. Then, enter the name of your service, as well as the Integration Key copied from the end of the [Setup: In PagerDuty](#setup-in-pagerduty) section above. After creation, confirm that your new entry is now in the list of available services.\\n\\n![](https://files.readme.io/9febb10-pagerduty_setup_f_2.png \"pagerduty_setup_f_2.png\")\\n\\n\\n\\n> 🚧 \\n> \\n> Creating, editing, and deleting these services is an **ADMINSTRATOR**-only privilege. Please contact an **ADMINSTRATOR** within your organization to setup any new PagerDuty services\\n\\n## PagerDuty Alerts in Fiddler\\n\\n1. Within the **Projects** page, select the model you wish to use with PagerDuty.\\n\\n![](https://files.readme.io/d9ad82e-pagerduty_fiddler_1.png \"pagerduty_fiddler_1.png\")\\n\\n\\n\\n2. Select **Monitor** → **Alerts** → **Add Alert**.\\n\\n![](https://files.readme.io/b7118f0-pagerduty_fiddler_2.png \"pagerduty_fiddler_2.png\")\\n\\n\\n\\n3. Enter the condition you would like to alert on, and under **PagerDuty Services**, select all services you would like the alert to trigger for. Additionally, select the **Severity** of this alert, and hit **Save**.\\n\\n![](https://files.readme.io/8fbffde-pagerduty_fiddler_3.png \"pagerduty_fiddler_3.png\")\\n\\n\\n\\n4. After creation, the alert will now trigger for the specified PagerDuty services.\\n\\n> 📘 Info\\n> \\n> Check out the [alerts documentation](doc:alerts-platform) for more information on setting up alerts.\\n\\n## FAQ\\n\\n**Can Fiddler integrate with multiple PagerDuty services?**\\n\\n- Yes. So long as the service is present within **Settings** → **PagerDuty Services**, anyone within your organization can select that service to be a recipient for an alert. ---\\ntitle: \"BigQuery Integration\"\\nslug: \"bigquery-integration\"\\nhidden: false\\ncreatedAt: \"2022-05-20T18:53:49.606Z\"\\nupdatedAt: \"2022-05-20T20:56:48.910Z\"\\n---\\n[block:api-header]\\n{\\n  \"title\": \"Using Fiddler on your ML data stored in BigQuery\"\\n}\\n[/block]\\nIn this article, we will be looking at loading data from BigQuery tables and using the data for the following tasks-\\n1. Uploading baseline data to Fiddler\\n2. Registering a surrogate model to Fiddler\\n3. Publishing production data to Fiddler\\n\\n##  Step 1 - Enable BigQuery API\\nBefore looking at how to import data from BigQuery to Fiddler, we will first see how to enable BigQuery API. This can be done as follows - \\n1. In the GCP platform, Go to the navigation menu -> click APIs & Services. Once you are there, click + Enable APIs and Services (Highlighted below). In the search bar, enter BigQuery API and click Enable.\\n\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/75ca647-Screen_Shot_2022-05-19_at_1.26.33_PM.png\",\\n        \"Screen Shot 2022-05-19 at 1.26.33 PM.png\",\\n        700,\\n        459,\\n        \"#e8ecf7\"\\n      ]\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/3dd5deb-Screen_Shot_2022-05-19_at_3.33.43_PM.png\",\\n        \"Screen Shot 2022-05-19 at 3.33.43 PM.png\",\\n        623,\\n        299,\\n        \"#f4f3f5\"\\n      ]\\n    }\\n  ]\\n}\\n[/block]\\n2. In order to make a request to the API enabled in Step#1, you need to create a service account and get an authentication file for your Jupyter Notebook. To do so, navigate to the Credentials tab under APIs and Services console and click Create Credentials tab, and then Service account under dropdown.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' On the left, click **\"Inference\"** and go to **\"Models\"**. Then select the model you want to upload to Fiddler.\\n\\n![](https://files.readme.io/ae27cba-sagemaker_model_select.png \"sagemaker_model_select.png\")\\n\\n\\n\\nCopy the **Model data location** to your clipboard.\\n\\n![](https://files.readme.io/be19325-sagemaker_model_location.png \"sagemaker_model_location.png\")\\n\\n\\n\\n## Downloading your model with Python\\n\\nNow, from a Python environment (Jupyter notebook or standard Python script), paste the **Model data location** you copied into a new variable.\\n\\n```python\\nMODEL_S3_LOCATION = \\'s3://fiddler-sagemaker-integration/fiddler-xgboost-sagemaker-demo/xgboost_model/output/sagemaker-xgboost-2022-06-06-15-49-54-626/output/model.tar.gz\\'\\n```\\n\\n\\n\\nThen extract the bucket name and file key into their own variables.\\n\\n```python\\nMODEL_S3_BUCKET = \\'fiddler-sagemaker-integration\\'\\nMODEL_S3_KEY = \\'fiddler-xgboost-sagemaker-demo/xgboost_model/output/sagemaker-xgboost-2022-06-06-15-49-54-626/output/model.tar.gz\\'\\n```\\n\\n\\n\\nLet\\'s also import a few packages we will be using.\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nimport boto3\\nimport tarfile\\nimport yaml\\nimport xgboost as xgb\\nimport fiddler as fdl\\n```\\n\\n\\n\\nAfter that, initialize an S3 client with AWS using `boto3`.\\n\\n```python\\nAWS_PROFILE = \\'my_profile\\'\\nAWS_REGION = \\'us-west-1\\'\\n\\nsession = boto3.session.Session(\\n    profile_name=AWS_PROFILE,\\n    region_name=AWS_REGION\\n)\\n\\ns3_client = session.client(\\'s3\\')\\n```\\n\\n\\n\\nWe\\'re ready to download! Just run the following code block.\\n\\n```python\\ns3_client.download_file(\\n    Bucket=MODEL_S3_BUCKET,\\n    Key=MODEL_S3_KEY,\\n    Filename=\\'model.tar.gz\\'\\n)\\n\\ntarfile.open(\\'model.tar.gz\\').extractall(\\'model\\')\\n```\\n\\n\\n\\nThis will save the model into a directory called `model`.\\n\\n!!! note  \\n    It\\'s important to **keep track of the name of your saved model file**. Check the `model` directory in your local filesystem to see its name.\\n\\n## Upload your model to Fiddler\\n\\nNow it\\'s time to connect to Fiddler. For more information on how this is done, see [Authorizing the Client](doc:uploading-model-artifacts).\\n\\n```python\\nURL = \\'https://app.fiddler.ai\\'\\nORG_ID = \\'my_org\\'\\nAUTH_TOKEN = \\'xtu4g_lReHyEisNg23xJ8IEex0YZEZeeEbTwAsupT0U\\'\\n\\nfiddler_client = fdl.FiddlerApi(\\n    url=URL,\\n    org_id=ORG_ID,\\n    auth_token=AUTH_TOKEN\\n)\\n```\\n\\n\\n\\nThen, get the dataset info from your baseline dataset by using (client.get_dataset_info)[https://api.fiddler.ai/#client-get_dataset_info].\\n\\nAfter that, construct a model info object and save it as a `.yaml` file into the `model` directory.\\n\\n```python\\nPROJECT_ID = \\'example_project\\'\\nDATASET_ID = \\'example_data\\'\\n\\ndataset_info = fiddler_client.get_dataset_info(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID\\n)\\n\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n    dataset_info=dataset_info,\\n    dataset_id=DATASET_ID,\\n    target=\\'target_column\\',\\n    outputs=[\\'output_column\\']\\n)\\n\\nwith open(\\'model/model.yaml\\', \\'w\\') as yaml_file:\\n    yaml.dump({\\'model\\': model_info.to_dict()}, yaml_file)\\n```\\n\\n\\n\\nThe last step is to write our `package.py`.\\n\\n```python\\n%%writefile model/package.py\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom pathlib import Path\\nimport xgboost as xgb\\n\\nimport fiddler as fdl\\n\\nPACKAGE_PATH = Path(__file__).parent\\n\\nclass ModelPackage:\\n\\n    def __init__(self):\\n        \\n        self.model_path = str(PACKAGE_PATH / \\'xgboost-model\\') # This is the name of your model file within the model directory\\n        self.model = xgb.Booster()\\n        self.model.load_model(self.model_path)\\n        \\n        self.output_columns = [\\'output_column\\']\\n    \\n    def transform_input(self, input_df):\\n        return xgb.DMatrix(input_df)\\n    \\n    def predict(self, input_df):\\n        transformed_input = self.transform_input(input_df)\\n        pred = self.model.predict(transformed_input)\\n        return pd.DataFrame(pred, columns=self.output_columns)\\n    \\ndef get_model():\\n    return ModelPackage()\\n```\\n\\n\\n\\nNow, go ahead and upload!\\n\\n```python\\nMODEL_ID = \\'sagemaker_model\\'\\n\\nfiddler_client.upload_model_package(\\n    artifact_path=\\'model\\',\\n    project_id=PROJECT_ID,\\n    model_id=MODEL_ID\\n)\\n``` ---\\ntitle: \"client.publish_events_batch\"\\nslug: \"clientpublish_events_batch\"\\nexcerpt: \"Publishes a batch of events to Fiddler asynchronously.\"\\nhidden: false\\ncreatedAt: \"2022-05-23T20:30:23.793Z\"\\nupdatedAt: \"2023-03-09T15:58:42.420Z\"\\n---\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"Input Parameter\",\\n    \"h-1\": \"Type\",\\n    \"h-2\": \"Default\",\\n    \"h-3\": \"Description\",\\n    \"0-0\": \"project_id\",\\n    \"0-1\": \"str\",\\n    \"0-2\": \"None\",\\n    \"0-3\": \"The unique identifier for the project.\",\\n    \"1-0\": \"model_id\",\\n    \"1-1\": \"str\",\\n    \"1-2\": \"None\",\\n    \"1-3\": \"A unique identifier for the model.\",\\n    \"2-0\": \"batch_source\",\\n    \"2-1\": \"Union[pd.Dataframe, str]\",\\n    \"2-2\": \"None\",\\n    \"2-3\": \"Either a pandas DataFrame containing a batch of events, or the path to a file containing a batch of events. Supported file types are  \\\\n_ CSV (.csv)  \\\\n_ Parquet (.pq)  \\\\n  \\\\n- Pickled DataFrame (.pkl)\",\\n    \"3-0\": \"id_field\",\\n    \"3-1\": \"Optional [str]\",\\n    \"3-2\": \"None\",\\n    \"3-3\": \"The field containing event IDs for events in the batch.  If not specified, Fiddler will generate its own ID, which can be retrived using the **get_slice** API.\",\\n    \"4-0\": \"update_event\",\\n    \"4-1\": \"Optional [bool]\",\\n    \"4-2\": \"None\",\\n    \"4-3\": \"If True, will only modify an existing event, referenced by _id_field_.  If an ID is provided for which there is no event, no change will take place.\",\\n    \"5-0\": \"timestamp_field\",\\n    \"5-1\": \"Optional [str]\",\\n    \"5-2\": \"None\",\\n    \"5-3\": \"The field containing timestamps for events in the batch. The format of these timestamps is given by _timestamp_format_. If no timestamp is provided for a given row, the current time will be used.\",\\n    \"6-0\": \"timestamp_format\",\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.\\n\\nYou can access your projects from the Projects Page.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/82404e6-Screenshot_2022-12-27_at_1.00.15_PM.png\",\\n        null,\\n        \"Projects Page on Fiddler UI\"\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Projects Page on Fiddler UI\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n## Datasets\\n\\nA dataset in Fiddler is a data table containing features, model outputs, and a target for machine learning models. Optionally, you can also upload metadata and “decision” columns, which can be used to segment the dataset for analyses, track business decisions, and work as protected attributes in bias-related workflows. For more details refer to [Datasets](doc:project-architecture#datasets) in the Platform Guide.\\n\\nOnce you click on a particular project, you will be able to see if there are any datasets associated with the project. For example, the bank_churn project, in the following screenshot, has the bank_churn dataset. [Datasets are uploaded via the Fiddler client](ref:clientupload_dataset). \\n\\n![](https://files.readme.io/3fa7700-Screenshot_2022-12-27_at_1.05.05_PM.png)\\n\\n## Models\\n\\nA model in Fiddler represents a machine learning model. A project will have one or more models for the ML task (e.g. a project to predict house prices might contain LinearRegression-HousePredict and RandomForest-HousePredict). For further details refer to the [Models](doc:project-architecture#models) section in the Platform Guide.\\n\\n![](https://files.readme.io/e151df5-Model_Dashboard.png \"Model_Dashboard.png\")\\n\\n### Model Artifacts\\n\\nAt its most basic level, a model in Fiddler is simply a directory that contains [model artifacts](doc:artifacts-and-surrogates) such as:\\n\\n- The model file (e.g. `*.pkl`)\\n- `package.py`: A wrapper script containing all of the code needed to standardize the execution of the model.\\n\\n![](https://files.readme.io/7170489-Model_Details.png \"Model_Details.png\")\\n\\n![](https://files.readme.io/2b3d52e-Model_Details_1.png \"Model_Details_1.png\")\\n\\n## Project Dashboard\\n\\nYou can collate specific visualizations under the Project Dashboard. After visualizations are created using the Model Analytics tool, you can pin them to the dashboard, which can then be shared with others.\\n\\n![](https://files.readme.io/b7cb9ce-Chart_Dashboard.png \"Chart_Dashboard.png\")\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_ ---\\ntitle: \"Authorization and Access Control\"\\nslug: \"authorization-and-access-control\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:26:44.914Z\"\\nupdatedAt: \"2022-12-09T22:07:03.897Z\"\\n---\\n## Project Roles\\n\\nEach project supports its own set of permissions for its users.\\n\\n![](https://files.readme.io/caf2bc9-project_settings.png \"project_settings.png\")\\n\\n![](https://files.readme.io/97b71c4-project_settings_add.png \"project_settings_add.png\")\\n\\nFor more details refer to [Administration Page](https://dash.readme.com/project/fiddler/v2.0/docs/administration-1) in the Platform Guide.\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_ ---\\ntitle: \"Surrogate Models\"\\nslug: \"surrogate-models\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:25:57.279Z\"\\nupdatedAt: \"2023-02-14T01:19:34.914Z\"\\n---\\nFiddler’s explainability features require a model on the backend that can generate explanations for you.\\n\\nA surrogate model is an approximation of your model using gradient boosted trees (LightGBM), trained with a general, predefined set of hyperparameters. It serves as a way for Fiddler to generate approximate explanations without you having to upload your actual model artifact.\\n\\n***\\n\\n\\n\\nA surrogate model **will be built automatically** for you when you call  [`add_model_surrogate`](/reference/clientadd_model_surrogate).  \\nYou just need to provide a few pieces of information about how your model operates.\\n\\n## What you need to specify\\n\\n- Your model’s task (regression, binary classification, etc.)\\n- Your model’s target column (ground truth labels)\\n- Your model’s output column (model predictions)\\n- Your model’s feature columns\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block] ---\\ntitle: \"Point Explainability\"\\nslug: \"point-explainability\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:25:41.102Z\"\\nupdatedAt: \"2023-02-14T01:19:25.331Z\"\\n---\\nFiddler provides powerful visualizations that can explain your model\\'s behavior. These explanations can be queried at an individual prediction level in the **Explain** tab, at a model level in the **Analyze** tab or within the monitoring context in the **Monitor** tab.\\n\\nExplanations are available in the UI for structured (tabular) and natural language (NLP) models. They are also supported via API using the [fiddler-client](https://pypi.org/project/fiddler-client/) Python package. Explanations are available for both production and baseline queries.\\n\\nFiddler’s explanations are interactive — you can change feature inputs and immediately view an updated prediction and explanation. We have productized several popular **explanation methods** to work fast and at scale:\\n\\n- SHAP and Fiddler SHAP, using Kernel SHAP implementations, are game-theory based methods. They work for all models, because they only require the ability to ask a model for predictions.\\n- Integrated Gradients, which is particularly performant for deep learning models with a large number of inputs. It requires the model’s prediction to be mathematically differentiable, and a prediction gradient must be made available to Fiddler.\\n- Tree SHAP, is not enabled by default but can be used for Tree-based model. This is a faster and model-specific method to approximate Shapley values.\\n\\nThese methods are discussed in more detail below.\\n\\nIn addition to the previous out of the box explanation methods, Fiddler allows to bring your own explanation method. This can be customized in your model’s `package.py` wrapper script.\\n\\n## Tabular Models\\n\\nFor tabular models, Fiddler’s Point Explanation tool shows how any given model prediction can be attributed to its individual input features.\\n\\nThe following is an example of an explanation for a model predicting the likelihood of customer churn:\\n\\n![](https://files.readme.io/b8e4f81-Tabular_Explain.png \"Tabular_Explain.png\")\\n\\nA brief tour of the features above:\\n\\n- **_Explanation Method_**: The explanation method is selected from the **Explanation Type'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 1 - Setting up baseline and publishing production events\\n\\nPlease refer to our [Getting Started guide](https://docs.fiddler.ai/pages/getting-started/product-tour/) for a step-by-step walkthrough of how to upload baseline and production data to the Fiddler platform.\\n\\n#### Step 2 - Monitor Drift\\n\\nWhen we check the monitoring dashboard, we notice a drop in the predicted churn value and a rise in the predicted churn drift value. Our next step is to check if this has resulted in a drop in performance.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/2f20fd2-Churn-image1-monitor-drift.png\",\\n        \"Churn-image1-monitor-drift.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Monitor Drift\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 3 - Monitor Performance Metrics\\n\\nWe use **precision, recall, and F1-score** as accuracy metrics for this example. We’re choosing these metrics as they are suited for classification problems and help us in identifying the number of false positives and false negatives. We notice that although the precision has remained constant, there is a drop in the F1-score and recall, which means that there are a few customers who are likely to churn but the model is not able to predict their outcome correctly. \\n\\nThere could be a number of reasons for drop in performance, some of them are-\\n\\n1. Cases of extreme events (Outliers)\\n2. Data distribution changes\\n3. Model/Concept drift\\n4. Pipeline health issues\\n\\nWhile **Pipeline health issues** could be due to a component in the Data pipeline failing, the first 3 could be due to changes in data. In order to check that we can go to the **Data Integrity** tab to first check if the incoming data is consistent with the baseline data.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/bb02793-churn-image2-monitor-performance-metrics.png\",\\n        \"churn-image2-monitor-performance-metrics.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Monitor Performance Metrics\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 4 - Data Integrity\\n\\nOur next step would be to check if this could be due to any data integrity issues. On navigating to the **Data Integrity** tab under the **Monitor** tab, we see that there has been a range violation. On selecting the bins which have the range violations, we notice it is due to the field `numofproducts`. \\n\\nIt is advised to check all the fields which cause data integrity violations. Since we see a range violation, we can check how much the data has drifted.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/5819966-churn-image3-data-integrity.png\",\\n        \"churn-image3-data-integrity.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"smart\",\\n      \"caption\": \"Data Integrity\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 5 - Check the impact of drift on ‘numofproducts’ features\\n\\nOur next step would be to go back to the **Data Drift** tab to measure the amount of drift in the field `numofproducts`. The drift is calculated using **Jensen Shannon Divergence**, which compares the distributions of the two data sets being compared. \\n\\nWe can select the bin where we see an increase in average value as well as drift. We see a significant increase in the `numofproducts` average value and drift. We can also see there is a difference in the distribution of the baseline and production data which leads to a drift. \\n\\nNext step could be to find out if the change in distribution was only for a subsection of data or was it due to other factors like time (seasonality etc.), fault in data reporting (sensor data), change in the unit in which the metric is reported etc.  \\nSeasonality could be observed by plotting the data across time (provided we have enough data), a fault in data reporting would mean missing values, and change in unit of data would mean change in values for all subsections of data.\\n\\nIn order to investigate if the change was only for a subsection of data, we will go to the **Analyze** tab. We can do this by clicking **Export bin and feature to Analyze**. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/1d1a5b3-churn-image4-impact-of-drift.png\",\\n        \"churn-image4-impact-of-drift.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Impact of Drift\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 6 - Root Cause Analysis in the ‘Analyze’ tab\\n\\nIn the analyze tab, we will have an auto-generated SQL query based on our selection in the **Monitor** tab, we can also write custom SQL queries to investigate the data. \\n\\nWe check the distribution of the field `numofproducts` for our selection. We can do this by selecting **Chart Type - Feature Distribution** on the RHS of the tab. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/9b1f7d1-Churn-image5-analyze-rca-1.png\",\\n        \"Churn-image5-analyze-rca-1.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 1\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe further check the performance of the model for our selection by selecting the **Chart Type - Slice Evaluation**.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/350aef8-Churn-image6-analyze-rca-2.png\",\\n        \"Churn-image6-analyze-rca-2.png\",\\n        1578\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 2\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nIn order to check if the change in the range violation has occurred for a subsection of data, we can plot it against the categorical variable. In our case, we can check distribution of `numofproducts` against `age` and `geography`. For this we can plot a feature correlation plot for two features by querying data and selecting **Chart type - Feature Correlation**.\\n\\nOn plotting the feature correlation plot of `gender` vs `numofprodcuts`, we observe the distribution to be similar.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/4f73274-churn-image6-analyze-rca-2-1.png\",\\n        \"churn-image6-analyze-rca-2-1.png\",\\n        512\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 3\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/aff3dbf-churn-image6-analyze-rca-2-2.png\",\\n        \"churn-image6-analyze-rca-2-2.png\",\\n        464\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' `bank_churn`, `iris_classification`, `lending`). It also contains dataset directories, found within datasets (e.g. `p2p_loans`, `winequality`).\\n\\n**Commands**\\n\\n```\\n```\\n\\n\\n\\n```bsh\\n   $ cd samples\\n   $ ./deploy\\n```\\n\\n\\n\\nFiddler Onebox comes with pre-loaded samples. If samples don\\'t exist, then run the above command to imports all dataset and models to Fiddler\\n\\n```\\n```\\n\\n\\n\\n```\\n```\\n\\n\\n\\n**Accessing the UI**\\n\\nYou can access the Fiddler UI locally with Onebox using <http://localhost:4100>.\\n\\nYou will be prompted to log in. Use the following credentials:\\n\\n- Username: onebox@fiddler.ai\\n- Default password: xai/4u  \\n  \\t\\t\\\\* You can change the password in Settings → General.\\n\\nOnce user logs into fiddler, a home page appears as seen in the below screenshot.\\n\\n![](https://files.readme.io/49ad4c6-Home_Page_1.png \"Home_Page (1).png\")\\n\\nFor the next steps in the onboarding, visit our [Quick Start](doc:quick-start) guide to get started.\\n\\n## Upgrade\\n\\nIf you’re upgrading from an older Onebox solution and you want to transfer all your data and model changes to the new version, follow these steps:\\n\\n1. Copy your local `fiddler` folder to a new folder called `fiddler-backup`.\\n2. Follow the steps in the **Setup** section above to install the latest version of Onebox.\\n3. Copy the `license.key` file from your new `fiddler/repo/onebox/common` folder and put it in a safe place.\\n4. Delete the `repo` and `data` directories from your new `fiddler` directory.\\n5. Copy the `repo` and `data` directories from your `fiddler-backup` directory into your new `fiddler` directory.\\n6. Replace the `license.key` file in your `repo/onebox/common` folder with the copy that you saved in step 3.\\n7. (Optional) If you want to save space, you can delete the fiddler-backup directory.\\n\\n## Systems requirements\\n\\n- Image - Linux (Supported distro: Amazon AMI HVM, Ubuntu, Centos, RHEL, Mac OS)\\n- Compute - 8 cores, 32GB or larger machines\\n- Storage - min. 128GB\\n- Port - 4100 or 443 (HTTPS support on port 443)  \\n  \\t\\t- Internally uses 5100, 6100, 27017, 5432, 4369\\n- Docker - min. Docker ver 19+\\n\\n> 🚧 Note\\n> \\n> If you are using a Mac to run Onebox, make sure to allocate the maximum (or at least 4GB) memory reserved for docker (via  Docker Icon > Preferences > Advanced > Memory)\\n\\n### Supporting packages\\n\\n- Linux 64bits with docker support\\n- `bash` for scripts\\n- `nc` (net cat) for establishing command line tcp connection\\n- `curl`\\n- internet connection for pip install packages\\n\\n### Server Setup\\n\\nPlease run the following commands or equivalent depending on where Fiddler is running. These instructions are for an AWS EC2 instance.\\n\\nOnce the EC2 instance is successfully started, SSH into the machine.\\n\\n```bash\\nssh -i \"../deploy/secret/fiddler-service.pem\" \\\\\\n   ec2-user@ec2-<--ipaddress-->.<region>.compute.amazonaws.com\\n```\\n\\n\\n\\nPerform the yum update, followed by installing docker and adding your user to the docker group:\\n\\n```\\nsudo yum update -y\\nsudo amazon-linux-extras install docker\\nsudo service docker start\\nsudo usermod -a -G docker ec2-user\\n```\\n\\n\\n\\nNext, install `nc`\\n\\n```\\nsudo yum install nc\\n```\\n\\n\\n\\nLog out and SSH into the machine again, then run the following commands:\\n\\n```\\ndocker info\\ndocker images\\n```\\n\\n\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_ ---\\ntitle: \"System Architecture\"\\nslug: \"system-architecture\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:19:53.311Z\"\\nupdatedAt: \"2023-05-18T20:21:38.327Z\"\\n---\\nFiddler deploys into your private cloud\\'s existing Kubernetes clusters, for which the minimum system requirements can be found [here](doc:technical-requirements). Fiddler supports deployment into Kubernetes in AWS, Azure, or GCP. All services of the Fiddler platform are containerized in order to eliminate reliance on other cloud services and to reduce the deployment and maintenance friction of the platform. This includes storage services like object storage and databases as well as system monitoring services like Grafana.\\n\\nUpdates to the Fiddler containers is accomplished through a shared container registry (that Fiddler is provided access to). Updates to the containers are orchestrated using Helm charts.\\n\\nA full-stack deployment of Fiddler is shown in the diagram below.\\n\\n![](https://files.readme.io/7cbfe31-reference_architecture.png)\\n\\nThe Fiddler system components are deployed within a single namespace on a Kubernetes cluster, using the official Fiddler Helm chart.\\n\\n- Fiddler core infrastructure relies on persistent volumes provided within the Kubernetes cluster. We recommend using encrypted storage volumes wherever possible.\\n- Fiddler may be configured to utilize external infrastructure in a self-hosted environment, such as existing PostgresQL servers, but this is not required as all services are containerized by default.\\n- Full-stack \"any-prem\" Fiddler includes observability infrastructure to monitor Fiddler system health and performance. These mainstream observability components may be integrated with external observability systems to support administration in a self-hosted environment.\\n- HTTP traffic to the Fiddler system is handled by an L4 or L7 load balancer or other proxy. TLS termination should usually occur outside the Fiddler system.\\n\\nOnce the platform is running, end users can interface with the Fiddler platform using their browser, the [Fiddler Python client](ref:about-the-fiddler-client), or Fiddler\\'s RESTful APIs. ---\\ntitle: \"On-prem Installation Guide\"\\nslug: \"installation-guide\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:20:10.097Z\"\\nupdatedAt: \"2022-08-19T17:44:14.367Z\"\\n---\\nFiddler can run on most mainstream flavors of Kubernetes, provided that a suitable [storage class](https://kubernetes.io/docs/concepts/storage/storage-classes/) is available to provide POSIX-compliant block storage (see [On-prem Technical Requirements](technical-requirements)).\\n\\nBefore you start\\n----------------\\n\\n- Create a namespace where Fiddler will be deployed, or request that a namespace/project be created for you by the team that administers your Kubernetes cluster.\\n  ```text\\n  [~] kubectl create ns my-fiddler-ns\\n  ```\\n\\n- Identify the name of the storage class(es) that you will use for Fiddler\\'s block storage needs. Consult the team that administers your Kubernetes cluster for guidance if you are not sure which class to use.\\n  ```\\n  [~] kubectl get storageclass\\n  NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\\n  gp2 (default)   kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                 '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\\nupdatedAt: \"2023-02-14T01:17:30.575Z\"\\n---\\nFiddler\\'s Explainability offering covers:\\n\\n- [Point Explainations](doc:point-explainability) \\n- [Global Explainations](doc:global-explainability)\\n- [Surrogate Model](doc:artifacts-and-surrogates#surrogate-model)\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block] ---\\ntitle: \"Supported Browsers\"\\nslug: \"supported-browsers\"\\nexcerpt: \"Platform Guide\"\\nhidden: false\\ncreatedAt: \"2023-01-10T22:16:01.134Z\"\\nupdatedAt: \"2023-01-10T22:16:17.735Z\"\\n---\\nFiddler Product can be accessed through the following supported web browsers:\\n\\n- Google Chrome\\n- Firefox\\n- Safari\\n- Microsoft Edge ---\\ntitle: \"Project Architecture\"\\nslug: \"project-architecture\"\\nhidden: false\\ncreatedAt: \"2022-11-15T18:06:28.079Z\"\\nupdatedAt: \"2023-02-14T23:21:13.699Z\"\\n---\\nSupervised machine learning involves identifying a predictive task, finding data to enable that task, and building a model using that data. \\n\\nFiddler captures this workflow with **project**, **dataset**, and **model** entities.\\n\\n## Project\\n\\nIn Fiddler, a project is essentially a parent folder that hosts one or more **model** (s) for the ML task (e.g. A Project HousePredict for predicting house prices will LinearRegression-HousePredict, RandomForest-HousePredict).\\n\\n## Models\\n\\nA model in Fiddler represents a **placeholder** for a machine-learning model. It\\'s a placeholder because we may not need the **[model artifacts](doc:artifacts-and-surrogates#Model-Artifacts)**. Instead, we may just need adequate [information about the model](ref:fdlmodelinfo) in order to monitor model-specific data. \\n\\n> 📘 Info\\n> \\n> You can [upload your model artifacts](https://dash.readme.com/project/fiddler/v1.6/docs/uploading-model-artifacts) to Fiddler to unlock high-fidelity explainability for your model. However, it is not required. If you do not wish to upload your artifact but want to explore explainability with Fiddler, we can build a [**surrogate model**](doc:artifacts-and-surrogates#surrogate-model) on the backend to be used in place of your artifact.\\n\\n## Datasets\\n\\nA dataset in Fiddler is a data table containing [information about data](ref:fdldatasetinfo) such as **features**, **model outputs**, and a **target** for machine learning models. Optionally, you can also upload **metadata** and “**decision**” columns, which can be used to segment the dataset for analyses, track business decisions, and work as protected attributes in bias-related workflows. \\n\\nIn order to monitor **production data**, a [dataset must be uploaded](ref:clientupload_dataset) to be used as a **baseline** for making comparisons. This baseline dataset should be sampled from your model\\'s **training data**. The sample should be unbiased and should faithfully capture moments of the parent distribution. Further, values appearing in the baseline dataset\\'s columns should be representative of their entire ranges within the complete training dataset.\\n\\n**Datasets are used by Fiddler in the following ways:**\\n\\n1. As a reference for [drift calculations](doc:data-drift-platform) and [data integrity violations ](doc:data-integrity-platform)on the **[Monitor](doc:monitoring-ui)** page\\n2. To train a model to be used as a [surrogate](doc:artifacts-and-surrogates#surrogate-model) when using [`add_model_surrogate`](/reference/clientadd_model_surrogate)\\n3. For computing model performance metrics globally on the **[Evaluate](doc:evaluation-ui)** page, or on slices on the **[Analyze](doc:analytics-ui)** page\\n4. As a reference for explainability algorithms (e.g. partial dependence plots, permutation feature impact, approximate Shapley values, and ICE plots).\\n\\nBased on the above uses, _datasets with sizes much in excess of 10K rows are often unnecessary_ and can lead to excessive upload, precomputation, and query times. That being said, here are some situations where larger datasets may be desirable:\\n\\n- **Auto-modeling for tasks with significant class imbalance; or strong and complex feature interactions, possibly with deeply encoded semantics**\\n  - However, in use cases like these, most users opt to upload carefully-engineered model artifacts tailored to the specific application.\\n- **Deep segmentation analysis**\\n  - If it’s desirable to perform model analyses on very specific subpopulations (e.g. “55-year-old Canadian home-owners who have been customers between 18 and 24 months”), large datasets may be necessary to have sufficient reference representation to drive model analytics.\\n\\n> 📘 Info\\n> \\n> Datasets can be uploaded to Fiddler using the[ Python API client](doc:installation-and-setup).\\n\\n [Check the UI Guide to Visualize Project Architecture on our User Interface](doc:project-structure)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_ ---\\ntitle: \"Administration\"\\nslug: \"administration-platform\"\\nhidden: false\\ncreatedAt: \"2022-11-15T18:09:04.719Z\"\\nupdatedAt: \"2023-02-03T20:49:07.153Z\"\\n---\\n## Organization Roles\\n\\nFiddler access control comes with some preset roles. There are two global roles at the organizational level \\n\\n- **_ADMINISTRATOR_** — Has complete access to every aspect of the organization.\\n  - As an administrator, you can [invite users](doc:inviting-users) to the platform.\\n- **_MEMBER_** — Access is assigned at the project and model level.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/0fbfca7-roles.png\",\\n        \"roles.png\",\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"550px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n## Project Roles\\n\\nEach project supports its own set of permissions for its users.\\n\\nThere are three roles that can be assigned:\\n\\n- **_OWNER_** — Assigns super-user permissions to the user.\\n- **_WRITE_** — Allows a user to perform write operations (e.g. uploading datasets and/or models, using slice and explain, sending events to Fiddler for monitoring, etc).\\n- **_READ_** — Allows a user to perform read operations (e.g. getting project/dataset/model metadata, accessing pre-existing charts, etc.).\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/3b07b46-project_roles.png\",\\n        \"project_roles.png\",\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"550px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n**Some notes about these roles:**\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\'re ready, the dataset can be uploaded using [client.upload_dataset()](ref:clientupload_dataset).\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"PROJECT_ID = \\'example_project\\'\\\\nDATASET_ID = \\'example_dataset\\'\\\\n\\\\nclient.upload_dataset(\\\\n    project_id=PROJECT_ID,\\\\n    dataset_id=DATASET_ID,\\\\n    dataset={\\\\n        \\'baseline\\': df\\\\n    },\\\\n    info=dataset_info\\\\n)\",\\n      \"language\": \"python\"\\n    }\\n  ]\\n}\\n[/block] ---\\ntitle: \"Surrogate Models - Client Guide\"\\nslug: \"surrogate-models-client-guide\"\\nhidden: false\\ncreatedAt: \"2022-12-13T22:22:39.933Z\"\\nupdatedAt: \"2023-02-14T01:20:44.499Z\"\\n---\\nFiddler’s explainability features require a model on the backend that can generate explanations for you.\\n\\n> 📘 If you don\\'t want to or cannot upload your actual model file, Surrogate Models serve as a way for Fiddler to generate approximate explanations.\\n\\nA surrogate model **will be built automatically** for you when you call  [`add_model_surrogate`](/reference/clientadd_model_surrogate).  \\nYou just need to provide a few pieces of information about how your model operates.\\n\\n## What you need to specify\\n\\n- Your model’s task (regression, binary classification, etc.)\\n- Your model’s target column (ground truth labels)\\n- Your model’s output column (model predictions)\\n- Your model’s feature columns\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block] ---\\ntitle: \"ML Framework Examples\"\\nslug: \"ml-framework-examples\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:13:20.434Z\"\\nupdatedAt: \"2022-04-19T20:13:20.434Z\"\\n---\\n ---\\ntitle: \"Installation and Setup\"\\nslug: \"installation-and-setup\"\\nhidden: false\\ncreatedAt: \"2022-05-10T17:14:02.670Z\"\\nupdatedAt: \"2023-02-14T01:20:21.990Z\"\\n---\\nFiddler offers a **Python SDK client** that allows you to connect to Fiddler directly from a Jupyter notebook or automated pipeline.\\n\\n***\\n\\n\\n\\nThe client is available for download from PyPI via `pip`:\\n\\n```\\npip install fiddler-client\\n```\\n\\n\\n\\n<br>\\n\\nOnce you\\'ve installed the client, you can import the `fiddler` package into any Python script:\\n\\n```python\\nimport fiddler as fdl\\n```\\n\\n\\n\\n***\\n\\n\\n\\n> 📘 Info\\n> \\n> For detailed documentation on the client’s many features, check out the [API reference](https://api.fiddler.ai) section.\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block] ---\\ntitle: \"Uploading model artifacts\"\\nslug: \"uploading-model-artifacts\"\\nexcerpt: \"Upload a model artifact in Fiddler\"\\nhidden: false\\ncreatedAt: \"2023-02-01T16:04:40.181Z\"\\nupdatedAt: \"2023-03-08T21:10:00.091Z\"\\n---\\nBefore uploading your model artifact into Fiddler, you need to add the model with [client.add_model](ref:clientadd_model).\\n\\nOnce you have prepared the [model artifacts directory](doc:artifacts-and-surrogates), you can upload your model using [client.add_model_artifact](ref:clientadd_model_artifact)\\n\\n```python\\nPROJECT_ID = \\'example_project\\'\\nMODEL_ID = \\'example_model\\'\\nMODEL_ARTIFACTS_DIR = Path(\\'model/\\')\\n\\nclient.add_model_artifact(\\n    model_dir=MODEL_ARTIFACTS_DIR,\\n    project_id=PROJECT_ID,\\n    model_id=MODEL_ID\\n)\\n``` ---\\ntitle: \"Explanation of Client Design, Process, and Flow\"\\nslug: \"explanation-of-client-logic\"\\nhidden: true\\ncreatedAt: \"2022-11-18T22:13:28.416Z\"\\nupdatedAt: \"2022-11-18T23:30:15.963Z\"\\n---\\n ---\\ntitle: \"Specifying Custom Missing Value Encodings\"\\nslug: \"specifying-custom-missing-value-encodings\"\\nhidden: false\\ncreatedAt: \"2022-08-30T18:19:30.145Z\"\\nupdatedAt: \"2022-12-13T22:47:08.171Z\"\\n---\\nThere may be cases in which you have missing values in your data, but you encode these values in a special way (other than the standard `NaN`).\\n\\nIn such cases, Fiddler offers a way to specify **your own missing value encodings for each column**.\\n\\n***\\n\\n\\n\\nYou can create a \"fall back\" dictionary, which holds the values you would like to have treated as missing for each column. Then just pass that dictionary into your [`fdl.ModelInfo`](/reference/fdlmodelinfo)  object before onboarding your model.\\n\\n```python\\nfall_back = {\\n  \\'column_1\\': [-999, \\'missing\\'],\\n  \\'column_2\\': [-1, \\'?\\', \\'na\\']\\n}\\n\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n  ...\\n  fall_back=fall_back\\n)\\n``` ---\\ntitle: \"Ranking\"\\nslug: \"ranking\"\\nhidden: false\\ncreatedAt: \"2022-05-02T15:39:22.424Z\"\\nupdatedAt: \"2023-04-07T01:32:43.476Z\"\\n---\\n## Onboarding a Ranking Model\\n\\nSuppose you would like to onboard a ranking model for the following dataset.\\n\\n![](https://files.readme.io/1d6eb09-expedia_df.png \"expedia_df.png\")\\n\\nFollowing is an example of how you would construct a [`fdl.ModelInfo`](https://api.fiddler.ai/#fdl-modelinfo) object for a ranking model.\\n\\n```python\\nPROJECT_ID = \\'example_project\\'\\nDATASET_ID = \\'expedia_data\\'\\nMODEL_ID = \\'ranking_model\\'\\n\\nmodel_task = fdl.ModelTask.RANKING\\nmodel_group_by = \\'srch_id\\'\\nmodel_target = \\'click_bool\\'\\nmodel_outputs = [\\'score\\']\\nraning_top_k = 20\\nmodel_features = [\\n    \\'price_usd\\',\\n    \\'promotion_flag\\',\\n    \\'weekday\\',\\n    \\'week_of_year\\',\\n    \\'hour_time\\',\\n    \\'minute_time\\'\\n\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n    dataset_info=dataset_info,\\n    dataset_id=DATASET_ID,\\n    features=model_features,\\n    group_by=model_group_by,\\n    ranking_top_k=ranking_top_k,\\n    target=model_target,\\n    outputs=model_outputs,\\n    model_task=model_task,\\n)\\n\\nclient.add_model(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID,\\n    model'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "':clientpublish_events_batch) for more information). In this case, you will need to send Fiddler the original event timestamps as to accurately populate the time series charts.\\n\\n## Updating events\\n\\nFiddler supports [partial updates of events](doc:updating-events) for your **target** column. This can be useful when you don’t have access to the ground truth for your model at the time the model\\'s prediction is made. Other columns can only be sent at insertion time (with `update_event=False`).\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block] ---\\ntitle: \"Handling Class Imbalance\"\\nslug: \"handling-class-imbalance\"\\nhidden: true\\ncreatedAt: \"2022-11-15T18:08:49.079Z\"\\nupdatedAt: \"2022-12-02T17:29:38.678Z\"\\n---\\nHere there will be a general description of class imbalance and the other pages will become housed within this - for now class-imbalanced data page is standalone ---\\ntitle: \"Model: Artifacts, Package, Surrogate\"\\nslug: \"artifacts-and-surrogates\"\\nexcerpt: \"Important terminologies for the ease of use of Fiddler Explainability\"\\nhidden: false\\ncreatedAt: \"2022-11-15T18:06:36.567Z\"\\nupdatedAt: \"2023-03-17T18:30:38.451Z\"\\n---\\n## Model Artifacts and Model Package\\n\\nA model in Fiddler is a placeholder that may not need the **model artifacts** for monitoring purposes. However, for explainability, model artifacts are needed. \\n\\n_Required_ model artifacts include: \\n\\n- The **[model file](doc:artifacts-and-surrogates#model-file) **(e.g. `*.pkl`)\\n- [`package.py`](doc:artifacts-and-surrogates#packagepy-wrapper-script): A wrapper script containing all of the code needed to standardize the execution of the model.\\n\\nA collection of model artifacts in a directory is referred to as a **model package**. To start, **place your model artifacts in a new directory**. This directory will be the model package you will upload to Fiddler to add or update model artifacts. \\n\\nWhile the model file and package.py are required artifacts in a model package, you can also _optionally_ add other artifacts such as:\\n\\n- [`model.yaml`](doc:artifacts-and-surrogates#modelyaml-configuration-file): A YAML file containing all the information about the model as specified in [ModelInfo](ref:fdlmodelinfo). This model metadata is used in Fiddler’s explanations, analytics, and UI.\\n- Any serialized [preprocessing objects](#preprocessing-objects) needed to transform data before running predictions or after.\\n\\nIn the following, we discuss the various model artifacts.\\n\\n### Model File\\n\\nA model file is a **serialized representation of your model** as a Python object.\\n\\nModel files can be stored in a variety of formats. Some include\\n\\n- Pickle (`.pkl`)\\n- Protocol buffer (`.pb`)\\n- Hierarchical Data Format/HDF5 (`.h5`)\\n\\n### package.py wrapper script\\n\\nFiddler’s artifact upload process is **framework-agnostic**. Because of this, a **wrapper script** is needed to let Fiddler know how to interact with your particular model and framework.\\n\\nThe wrapper script should be named `package.py`, and it should be **placed in the same directory as your model artifact**. Below is an example of what `package.py` should look like.\\n\\n```python\\nfrom pathlib import Path\\nimport pandas as pd\\n\\nPACKAGE_PATH = Path(__file__).parent\\n\\nclass MyModel:\\n\\n    def __init__(self):\\n        \"\"\"\\n        Here we can load in the model and any other necessary\\n            serialized objects from the PACKAGE_PATH.\\n        \"\"\"\\n\\n    def predict(self, input_df):\\n        \"\"\"\\n        The predict() function should return a DataFrame of predictions\\n            whose columns correspond to the outputs of your model.\\n        \"\"\"\\n\\ndef get_model():\\n    return MyModel()\\n```\\n\\n\\n\\nThe only hard requirements for `package.py` are\\n\\n- The script must be named `package.py`\\n- The script must implement a function called `get_model`, which returns a model object\\n- This model object must implement a function called `predict`, which takes in a pandas DataFrame of model inputs and returns a pandas DataFrame of model predictions\\n\\n### model.yaml configuration file\\n\\nIn case you want to update the custom explanations (`custom_explanation_names`) or the preferred explanation method (`preferred_explanation_method`) in the model info, you will need to construct a YAML file with **specifications for how your model operates**. This can be easily obtained from [fdl.ModelInfo()](ref:fdlmodelinfo) object.\\n\\n> 📘 Info\\n> \\n> For information on constructing a [fdl.ModelInfo()](ref:fdlmodelinfo) object, see [Creating ModelInfo Object](doc:registering-a-model#creating-a-modelinfo-object).\\n\\n> 🚧 Warning\\n> \\n> Currently, only the following fields in model info can be updated:\\n> \\n> - `custom_explanation_names`\\n> - `preferred_explanation_method`\\n> - `display_name`\\n> - `description`\\n\\nOnce you have your [fdl.ModelInfo()](ref:fdlmodelinfo), you can call its [fdl.ModelInfo.to_dict()](ref:fdlmodelinfoto_dict) function to **generate a dictionary** that can be used for the YAML configuration file.\\n\\n```python\\nimport yaml\\n\\nwith open(\\'model.yaml\\', \\'w\\') as yaml_file:\\n    yaml.dump({\\'model\\': model_info.to_dict()}, yaml_file)\\n```\\n\\n\\n\\nNote that we are adding `model` key whose value is the dictionary produced by the [`fdl.ModelInfo`](https://api.fiddler.ai/#fdl-modelinfo) object.\\n\\nOnce it’s been created, you can place it in the directory with your model artifact and `package.py` script.\\n\\n### Preprocessing objects\\n\\nAnother component of your model package could be any **serialized preprocessing objects** that are used to transform the data before or after making predictions.\\n\\nYou can place these in the model package directory as well.\\n\\n> 📘 Info\\n> \\n> For example, in the case that we have a **categorical feature**, we may need to **encode** it as one or more numeric columns before calling the model’s prediction function. In that case, we may have a serialized transform object called `encoder.pkl`. This should also be included in the model package directory.\\n\\n### requirements.txt file\\n\\n> 📘 Info\\n> \\n> This is only used starting at 23.1 version with Model Deployment enabled.\\n\\nEach base image (see [image_uri](https://dash.readme.com/project/fiddler/v1.6/docs/model-deployment) for more information on base images) comes with a few pre-installed libraries and these can be overridden by specifying `requirements.txt` file inside your model artifact directory where `package.py`'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "strings, relatednesses = strings_ranked_by_relatedness(\"What API to use to upload a baseline dataset on Fiddler platform?\", df, top_n=10)\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    print(f\"{relatedness=:.3f}\")\n",
    "    display(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82880ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
