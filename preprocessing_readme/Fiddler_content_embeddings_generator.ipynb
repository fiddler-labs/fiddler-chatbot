{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31579f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import openai\n",
    "import tiktoken\n",
    "from scipy import spatial \n",
    "import ast\n",
    "\n",
    "openai.api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL_16k = \"gpt-3.5-turbo-16k\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e591dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def chunked_string(\n",
    "    string: str,\n",
    "    model: str = EMBEDDING_MODEL,\n",
    "    max_tokens: int = 2000,\n",
    ") -> str:\n",
    "    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    chunked_string = [encoding.decode(encoded_string[i:i+max_tokens]) for i in range(0, len(encoded_string), max_tokens)]\n",
    "    return chunked_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25f91a8",
   "metadata": {},
   "source": [
    "## Readme docs processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "370b655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "href_pattern = r'href=\"([^\"]+\\.ipynb)\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "592cb8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipynb_slug = r'/([^/.]+)\\.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ff4b194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #appending content of quickstart notebooks to the docs quickstart page\n",
    "\n",
    "# for root, dirs, files in os.walk(\"./fiddler-2023-8-15/v1.8/QuickStart Notebooks\"):\n",
    "#     for name in files:\n",
    "#         path = os.path.join(root, name)\n",
    "#         if path[-3:] == '.md':\n",
    "#             with open(path,'r') as f:\n",
    "#                 file_str = f.read()\n",
    "#             ipynb_links = re.search(ipynb_slug, file_str)\n",
    "#             if ipynb_links:\n",
    "#                 with open(\"./fiddler-2023-8-15/quickstart/\"+ipynb_links.group(1)+\".md\") as l:\n",
    "#                     QS = l.read()\n",
    "                \n",
    "#                 with open(path, 'a') as f:\n",
    "#                     f.write(QS)\n",
    "                \n",
    "                    \n",
    "# #                 print(ipynb_links.group(1))\n",
    "            \n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01f806d-171a-4a1e-9f9c-63f195220305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunked_doc = []\n",
    "# for root, dirs, files in os.walk(\"./fiddler-2023-8-15/Changelog Posts\"):\n",
    "#     for name in files:\n",
    "#         path = os.path.join(root, name)\n",
    "#         if path[-3:] == '.md':\n",
    "#             with open(path,'r') as f:\n",
    "#                 file_str = f.read()\n",
    "#                 chunked_doc.append(file_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f3d91c7-4ddb-4d2c-9f7b-491e983c150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(chunked_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b7ad4c-d4cd-4941-82c5-1864b78d4e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #find and remove hidden pages\n",
    "# pattern = r'hidden:\\s*(\\w+)'\n",
    "\n",
    "# for doc in chunked_doc:\n",
    "#     match = re.search(pattern, doc)\n",
    "#     if match and match.group(1) == \"true\":\n",
    "#         chunked_doc.remove(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "743cb2d8-897d-4309-9393-1ce16ee2414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(chunked_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e32e3521-f7db-42a8-9837-04124d17e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings=[]\n",
    "# for i in range(len(chunked_doc)):\n",
    "#     response = openai.Embedding.create(model=EMBEDDING_MODEL, input=chunked_doc[i])\n",
    "#     embeddings.append(response[\"data\"][0][\"embedding\"])\n",
    "\n",
    "# df = pd.DataFrame({\"text\": chunked_doc, \"embedding\": embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3dbb841-2cfe-4e16-aa28-5d3a61daad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"release_notes.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88937bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_doc = []\n",
    "for root, dirs, files in os.walk(\"./fiddler-2023-10-9/v23.4\"):\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        if path[-3:] == '.md':\n",
    "            with open(path,'r') as f:\n",
    "                file_str = f.read()\n",
    "                chunked_doc.append(file_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f96abc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c2e03aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find and remove hidden pages\n",
    "pattern = r'hidden:\\s*(\\w+)'\n",
    "\n",
    "for doc in chunked_doc:\n",
    "    match = re.search(pattern, doc)\n",
    "    if match and match.group(1) == \"true\":\n",
    "        chunked_doc.remove(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff9858d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b41983dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "slug_pattern = r'slug:\\s*\"(.*?)\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90950fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.search(slug_pattern, \"\"\" slug: \"abc\" bla bla slug: \"pqr\" \"\"\").group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "536ab591",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lim_doc = []\n",
    "for doc in chunked_doc:\n",
    "    if num_tokens(doc) > 750:\n",
    "        chunked_list = chunked_string(doc, max_tokens=750)\n",
    "        chunked_doc_slug = re.search(slug_pattern, chunked_list[0]).group(0)\n",
    "        for i in range(1, len(chunked_list)):\n",
    "            chunked_list[i] = chunked_doc_slug + ' ' + chunked_list[i]\n",
    "        \n",
    "        token_lim_doc += chunked_list\n",
    "    else:\n",
    "        token_lim_doc.append(doc)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40bee0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_lim_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d953445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_lim_doc\n",
    "token_sizes = []\n",
    "for doc in token_lim_doc:\n",
    "    token_sizes.append(num_tokens(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4911e0fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# token_sizes\n",
    "# token_lim_doc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "23432bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498.3344370860927"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mean\n",
    "mean(token_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "699681ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=[]\n",
    "for i in range(len(token_lim_doc)):\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=token_lim_doc[i])\n",
    "    embeddings.append(response[\"data\"][0][\"embedding\"])\n",
    "\n",
    "df = pd.DataFrame({\"text\": token_lim_doc, \"embedding\": embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2eb48e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...</td>\n",
       "      <td>[-0.014995344914495945, -0.0026680435985326767...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slug: \"client-setup\" _TOKEN\\n)\\n```\\n```python...</td>\n",
       "      <td>[-0.021182812750339508, -0.004293339792639017,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>---\\ntitle: \"Customer Churn Prediction\"\\nslug:...</td>\n",
       "      <td>[-0.00745741231366992, -0.010434732772409916, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>slug: \"customer-churn-prediction\" /bb02793-chu...</td>\n",
       "      <td>[0.002079722471535206, -0.0047837127931416035,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slug: \"customer-churn-prediction\" Churn-image5...</td>\n",
       "      <td>[0.0011718893656507134, 0.000443618802819401, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>---\\ntitle: \"Uploading a TensorFlow HDF5 Model...</td>\n",
       "      <td>[-0.0022432487457990646, 0.016934363171458244,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>---\\ntitle: \"Uploading an XGBoost Model Artifa...</td>\n",
       "      <td>[0.0012045818148180842, 0.013162259012460709, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>---\\ntitle: \"Uploading a TensorFlow SavedModel...</td>\n",
       "      <td>[0.0018563421908766031, 0.0009264442487619817,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>---\\ntitle: \"Uploading a scikit-learn Model Ar...</td>\n",
       "      <td>[0.0031662925612181425, 0.018320860341191292, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...</td>\n",
       "      <td>[-0.007673030719161034, -0.033769577741622925,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>274 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    ---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...   \n",
       "1    slug: \"client-setup\" _TOKEN\\n)\\n```\\n```python...   \n",
       "2    ---\\ntitle: \"Customer Churn Prediction\"\\nslug:...   \n",
       "3    slug: \"customer-churn-prediction\" /bb02793-chu...   \n",
       "4    slug: \"customer-churn-prediction\" Churn-image5...   \n",
       "..                                                 ...   \n",
       "269  ---\\ntitle: \"Uploading a TensorFlow HDF5 Model...   \n",
       "270  ---\\ntitle: \"Uploading an XGBoost Model Artifa...   \n",
       "271  ---\\ntitle: \"Uploading a TensorFlow SavedModel...   \n",
       "272  ---\\ntitle: \"Uploading a scikit-learn Model Ar...   \n",
       "273  ---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...   \n",
       "\n",
       "                                             embedding  \n",
       "0    [-0.014995344914495945, -0.0026680435985326767...  \n",
       "1    [-0.021182812750339508, -0.004293339792639017,...  \n",
       "2    [-0.00745741231366992, -0.010434732772409916, ...  \n",
       "3    [0.002079722471535206, -0.0047837127931416035,...  \n",
       "4    [0.0011718893656507134, 0.000443618802819401, ...  \n",
       "..                                                 ...  \n",
       "269  [-0.0022432487457990646, 0.016934363171458244,...  \n",
       "270  [0.0012045818148180842, 0.013162259012460709, ...  \n",
       "271  [0.0018563421908766031, 0.0009264442487619817,...  \n",
       "272  [0.0031662925612181425, 0.018320860341191292, ...  \n",
       "273  [-0.007673030719161034, -0.033769577741622925,...  \n",
       "\n",
       "[274 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "431685ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"latest_v_23-4_tk750.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34f36332-1577-4665-b3df-b2fce8704bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "embeddings_path = \"../documentation_data/chatbot_08_15_23_tk750.csv\"\n",
    "df_old = pd.read_csv(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cffd0e76-117b-4f08-b73c-675c59044dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_arr = np.array(df_old.iloc[0]['embedding'].split(','))\n",
    "len(emb_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "395b1b0e-18a6-47f3-9117-e6c904c0dd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Once you have added a model on the Fiddler pla...</td>\n",
       "      <td>[-0.022226542234420776, 0.011369146406650543, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Custom metrics is an upcoming feature and it i...</td>\n",
       "      <td>[-0.017716489732265472, -0.0035160724073648453...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Re-uploading in Fiddler essentially means havi...</td>\n",
       "      <td>[-0.01760503277182579, 0.011651406064629555, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "307  Once you have added a model on the Fiddler pla...   \n",
       "308  Custom metrics is an upcoming feature and it i...   \n",
       "309  Re-uploading in Fiddler essentially means havi...   \n",
       "\n",
       "                                             embedding  \n",
       "307  [-0.022226542234420776, 0.011369146406650543, ...  \n",
       "308  [-0.017716489732265472, -0.0035160724073648453...  \n",
       "309  [-0.01760503277182579, 0.011651406064629555, 0...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_old[307:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5214322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_doc = []\n",
    "path = \"/Users/murtuzashergadwala/fiddler-chatbot/fiddler-2023-10-9/Changelog Posts/release-233.md\"\n",
    "with open(path,'r') as f:\n",
    "    file_str = f.read()\n",
    "    chunked_doc.append(file_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af90a432-9177-4a2c-a690-f13d5f39144c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---\\ntitle: \"Release 23.3 Notes\"\\nslug: \"release-233\"\\ncreatedAt: \"2023-08-15T18:03:45.797Z\"\\nhidden: false\\n---\\nThis page enumerates the new features and updates in Release 23.3 of the Fiddler platform.\\n\\n> 📘 Platform Release Version 23.3 & Doc v1.8 compatability note\\n> \\n> Note that the documentation version remains v1.8 with this release. The new and improved functionalities are added to their respective pages with the note regarding platform version 23.3 as a requirement.\\n\\n## Release of Fiddler platform version 23.3:\\n\\n- Support for added charting up to 6 metrics for one or multiple models \\n\\n- Ability to assign metrics to the left or right y-axis in monitoring charts\\n\\n- Addition of automatically created model monitoring dashboards\\n\\n- New Root Cause Analysis tab with data drift and data integrity information in monitoring charts \\n\\n## What\\'s New and Improved:\\n\\n- **Multiple metric queries in monitoring charts**\\n  - Flexibility to add up to 6 metrics queries to visualize multiple metrics or models in one chart.\\n  - Enables model-to-model comparison in a single chart.\\n  - Learn more on the [Monitoring Charts Platform Guide](doc:monitoring-charts-platform).\\n\\n- **Y-axis assignment in monitoring charts**\\n  - Further, customize charts by assigning metric queries to a left or right y-axis in the customize tab.\\n  - Learn more on the [Monitoring Charts UI Guide](doc:monitoring-charts-ui).\\n\\n- **Automatically generated model dashboards**\\n  - Fiddler will automatically create a model dashboard for all models added to the platform, consisting of charts that display data drift, performance, data integrity, and traffic information.\\n  - Learn more on the[Dashboards Platform Guide](doc:dashboards-platform).\\n\\n- **Root cause analysis in monitoring charts**\\n  - Examine specific timestamps within a monitoring time series chart to reveal the underlying reasons for model underperformance, using visualizations of data drift and data integrity insights.\\n  - Learn more on the page  [Monitoring Charts UI Guide](doc:monitoring-charts-ui).\\n\\n### Client Version\\n\\nClient version 1.8 is required for the updates and features mentioned in this release.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "abec24dd-038d-44c6-9e48-5ae6d9af6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=[]\n",
    "response = openai.Embedding.create(model=EMBEDDING_MODEL, input=chunked_doc[0])\n",
    "embeddings.append(response[\"data\"][0][\"embedding\"])\n",
    "\n",
    "df_release233 = pd.DataFrame({\"text\": chunked_doc, \"embedding\": embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d6a96bc-daac-4a4c-8b5c-ff10749a287b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---\\ntitle: \"Release 23.3 Notes\"\\nslug: \"relea...</td>\n",
       "      <td>[-0.005071498453617096, 0.0015428883489221334,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ---\\ntitle: \"Release 23.3 Notes\"\\nslug: \"relea...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.005071498453617096, 0.0015428883489221334,...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_release233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ab315-4de7-4179-8b31-a1cd69f621f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41a400e7-af7d-47cf-b7dd-030c8006d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df, df_old[:5], df_release234,df_old[307:]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d978e05c-1596-4362-a33e-41caa1941c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>package.py for R based models```python\\nimport...</td>\n",
       "      <td>[-0.009950872510671616, -0.011770655401051044,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>---\\ntitle: \"Release 22.11 Notes\"\\nslug: \"rele...</td>\n",
       "      <td>[-0.005742393434047699, 0.001501509454101324, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>---\\ntitle: \"Release 23.2 Notes\"\\nslug: \"relea...</td>\n",
       "      <td>[0.010167686268687248, -0.001216516480781138, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>---\\ntitle: \"Release 22.12 Notes\"\\nslug: \"rele...</td>\n",
       "      <td>[-0.010491670109331608, -0.0011956370435655117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---\\ntitle: \"Release 23.1 Notes\"\\nslug: \"2023-...</td>\n",
       "      <td>[-0.00474295811727643, 0.0016190075548365712, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...</td>\n",
       "      <td>[-0.01489181537181139, -0.0028792002703994513,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  package.py for R based models```python\\nimport...   \n",
       "1  ---\\ntitle: \"Release 22.11 Notes\"\\nslug: \"rele...   \n",
       "2  ---\\ntitle: \"Release 23.2 Notes\"\\nslug: \"relea...   \n",
       "3  ---\\ntitle: \"Release 22.12 Notes\"\\nslug: \"rele...   \n",
       "4  ---\\ntitle: \"Release 23.1 Notes\"\\nslug: \"2023-...   \n",
       "5  ---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.009950872510671616, -0.011770655401051044,...  \n",
       "1  [-0.005742393434047699, 0.001501509454101324, ...  \n",
       "2  [0.010167686268687248, -0.001216516480781138, ...  \n",
       "3  [-0.010491670109331608, -0.0011956370435655117...  \n",
       "4  [-0.00474295811727643, 0.0016190075548365712, ...  \n",
       "5  [-0.01489181537181139, -0.0028792002703994513,...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_old[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5589d170-b895-4150-8a32-acc45601f21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...</td>\n",
       "      <td>[-0.014995344914495945, -0.0026680435985326767...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slug: \"client-setup\" _TOKEN\\n)\\n```\\n```python...</td>\n",
       "      <td>[-0.021182812750339508, -0.004293339792639017,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>---\\ntitle: \"Customer Churn Prediction\"\\nslug:...</td>\n",
       "      <td>[-0.00745741231366992, -0.010434732772409916, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>slug: \"customer-churn-prediction\" /bb02793-chu...</td>\n",
       "      <td>[0.002079722471535206, -0.0047837127931416035,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slug: \"customer-churn-prediction\" Churn-image5...</td>\n",
       "      <td>[0.0011718893656507134, 0.000443618802819401, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>---\\ntitle: \"Release 23.4 Notes\"\\nslug: \"relea...</td>\n",
       "      <td>[-0.004607734736055136, -0.008832967840135098,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Once you have added a model on the Fiddler pla...</td>\n",
       "      <td>[-0.022226542234420776, 0.011369146406650543, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>Custom metrics is an upcoming feature and it i...</td>\n",
       "      <td>[-0.017716489732265472, -0.0035160724073648453...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>Re-uploading in Fiddler essentially means havi...</td>\n",
       "      <td>[-0.01760503277182579, 0.011651406064629555, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>---\\ntitle: \"Release 23.3 Notes\"\\nslug: \"relea...</td>\n",
       "      <td>[-0.005071498453617096, 0.0015428883489221334,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    ---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...   \n",
       "1    slug: \"client-setup\" _TOKEN\\n)\\n```\\n```python...   \n",
       "2    ---\\ntitle: \"Customer Churn Prediction\"\\nslug:...   \n",
       "3    slug: \"customer-churn-prediction\" /bb02793-chu...   \n",
       "4    slug: \"customer-churn-prediction\" Churn-image5...   \n",
       "..                                                 ...   \n",
       "279  ---\\ntitle: \"Release 23.4 Notes\"\\nslug: \"relea...   \n",
       "280  Once you have added a model on the Fiddler pla...   \n",
       "281  Custom metrics is an upcoming feature and it i...   \n",
       "282  Re-uploading in Fiddler essentially means havi...   \n",
       "283  ---\\ntitle: \"Release 23.3 Notes\"\\nslug: \"relea...   \n",
       "\n",
       "                                             embedding  \n",
       "0    [-0.014995344914495945, -0.0026680435985326767...  \n",
       "1    [-0.021182812750339508, -0.004293339792639017,...  \n",
       "2    [-0.00745741231366992, -0.010434732772409916, ...  \n",
       "3    [0.002079722471535206, -0.0047837127931416035,...  \n",
       "4    [0.0011718893656507134, 0.000443618802819401, ...  \n",
       "..                                                 ...  \n",
       "279  [-0.004607734736055136, -0.008832967840135098,...  \n",
       "280  [-0.022226542234420776, 0.011369146406650543, ...  \n",
       "281  [-0.017716489732265472, -0.0035160724073648453...  \n",
       "282  [-0.01760503277182579, 0.011651406064629555, 0...  \n",
       "283  [-0.005071498453617096, 0.0015428883489221334,...  \n",
       "\n",
       "[284 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.concat([df2,df_release233], ignore_index=True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0814a676-dbbc-423b-8847-d92c6a043803",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_old['text'].str.contains('Quickstart', case=False)\n",
    "\n",
    "# Filter the DataFrame based on the mask\n",
    "result = df_old[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "73891939-e43d-412e-ba6f-d3b462641f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>slug: \"fraud-detection\"  to Data Integrity Iss...</td>\n",
       "      <td>[-0.015683425590395927, -0.005404040217399597,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>---\\ntitle: \"CV Monitoring\"\\nslug: \"cv-monitor...</td>\n",
       "      <td>[-0.030289800837635994, -0.002532508224248886,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>slug: \"cv-monitoring\"     auth_token=AUTH_TOKE...</td>\n",
       "      <td>[-0.025598619133234024, -0.004655908793210983,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>slug: \"cv-monitoring\" _ID, 'monitor']))\\n```\\n...</td>\n",
       "      <td>[-0.02605891041457653, -0.009838425554335117, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>---\\ntitle: \"Explainability with Model Artifac...</td>\n",
       "      <td>[0.0029013229068368673, 0.017439918592572212, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>slug: \"explainability-with-model-artifact-quic...</td>\n",
       "      <td>[0.005093783605843782, 0.004136959556490183, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>slug: \"explainability-with-model-artifact-quic...</td>\n",
       "      <td>[-0.01352930348366499, 0.01176003273576498, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>slug: \"explainability-with-model-artifact-quic...</td>\n",
       "      <td>[-0.005808187648653984, -0.011234029196202755,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>slug: \"explainability-with-model-artifact-quic...</td>\n",
       "      <td>[-0.021635664626955986, 0.008698590099811554, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>---\\ntitle: \"Simple Monitoring\"\\nslug: \"quick-...</td>\n",
       "      <td>[-0.0051681166514754295, 0.012547116726636887,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>slug: \"quick-start\" ** page.\\n\\n&lt;table&gt;\\n    &lt;...</td>\n",
       "      <td>[-0.00370140396989882, -0.0030459468252956867,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>slug: \"quick-start\"  your project, you should ...</td>\n",
       "      <td>[-0.01539567019790411, -0.0014442047104239464,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>slug: \"quick-start\" absolute** or **relative**...</td>\n",
       "      <td>[-0.01444741990417242, -0.0058169118128716946,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>slug: \"quick-start\"  following code block to g...</td>\n",
       "      <td>[-0.019419711083173752, 0.005196480546146631, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>---\\ntitle: \"Explainability with a Surrogate M...</td>\n",
       "      <td>[0.0014923522248864174, -0.0023144488222897053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>slug: \"monitoring-xai-quick-start\" .png\" /&gt;&lt;/t...</td>\n",
       "      <td>[0.005264581646770239, -0.0007557418430224061,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>slug: \"monitoring-xai-quick-start\" raw.githubu...</td>\n",
       "      <td>[-0.000461430725408718, 0.010870600119233131, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>slug: \"monitoring-xai-quick-start\"  informatio...</td>\n",
       "      <td>[-0.015217158943414688, -0.015805063769221306,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>---\\ntitle: \"NLP Monitoring\"\\nslug: \"simple-nl...</td>\n",
       "      <td>[-0.022013897076249123, 0.01987425610423088, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>slug: \"simple-nlp-monitoring-quick-start\"  the...</td>\n",
       "      <td>[-0.01585082896053791, 0.01199599914252758, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>---\\ntitle: \"Ranking Monitoring Example\"\\nslug...</td>\n",
       "      <td>[-0.0026677444111555815, 0.01041774358600378, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>slug: \"ranking-model\" )\\nelse:\\n    print(f'Pr...</td>\n",
       "      <td>[0.003431991208344698, 0.004281357396394014, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>slug: \"ranking-model\" if not MODEL_ID in clien...</td>\n",
       "      <td>[-0.01290346123278141, 0.0027702641673386097, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>---\\ntitle: \"Class Imbalance Monitoring Exampl...</td>\n",
       "      <td>[-0.01375657506287098, -0.00021721386292483658...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>slug: \"class-imbalance-monitoring-example\" ler...</td>\n",
       "      <td>[0.0003084054624196142, -0.009778305888175964,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>slug: \"class-imbalance-monitoring-example\" # S...</td>\n",
       "      <td>[-0.019176218658685684, -0.015687033534049988,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>---\\ntitle: \"Class-Imbalanced Data\"\\nslug: \"cl...</td>\n",
       "      <td>[-0.01668614149093628, 0.00771642941981554, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>---\\ntitle: \"Ranking Model Package.py\"\\nslug: ...</td>\n",
       "      <td>[0.007682955823838711, 0.011558671481907368, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "14   slug: \"fraud-detection\"  to Data Integrity Iss...   \n",
       "36   ---\\ntitle: \"CV Monitoring\"\\nslug: \"cv-monitor...   \n",
       "37   slug: \"cv-monitoring\"     auth_token=AUTH_TOKE...   \n",
       "41   slug: \"cv-monitoring\" _ID, 'monitor']))\\n```\\n...   \n",
       "42   ---\\ntitle: \"Explainability with Model Artifac...   \n",
       "43   slug: \"explainability-with-model-artifact-quic...   \n",
       "44   slug: \"explainability-with-model-artifact-quic...   \n",
       "45   slug: \"explainability-with-model-artifact-quic...   \n",
       "46   slug: \"explainability-with-model-artifact-quic...   \n",
       "47   ---\\ntitle: \"Simple Monitoring\"\\nslug: \"quick-...   \n",
       "48   slug: \"quick-start\" ** page.\\n\\n<table>\\n    <...   \n",
       "49   slug: \"quick-start\"  your project, you should ...   \n",
       "50   slug: \"quick-start\" absolute** or **relative**...   \n",
       "51   slug: \"quick-start\"  following code block to g...   \n",
       "52   ---\\ntitle: \"Explainability with a Surrogate M...   \n",
       "53   slug: \"monitoring-xai-quick-start\" .png\" /></t...   \n",
       "54   slug: \"monitoring-xai-quick-start\" raw.githubu...   \n",
       "55   slug: \"monitoring-xai-quick-start\"  informatio...   \n",
       "56   ---\\ntitle: \"NLP Monitoring\"\\nslug: \"simple-nl...   \n",
       "57   slug: \"simple-nlp-monitoring-quick-start\"  the...   \n",
       "62   ---\\ntitle: \"Ranking Monitoring Example\"\\nslug...   \n",
       "63   slug: \"ranking-model\" )\\nelse:\\n    print(f'Pr...   \n",
       "64   slug: \"ranking-model\" if not MODEL_ID in clien...   \n",
       "66   ---\\ntitle: \"Class Imbalance Monitoring Exampl...   \n",
       "67   slug: \"class-imbalance-monitoring-example\" ler...   \n",
       "68   slug: \"class-imbalance-monitoring-example\" # S...   \n",
       "238  ---\\ntitle: \"Class-Imbalanced Data\"\\nslug: \"cl...   \n",
       "283  ---\\ntitle: \"Ranking Model Package.py\"\\nslug: ...   \n",
       "\n",
       "                                             embedding  \n",
       "14   [-0.015683425590395927, -0.005404040217399597,...  \n",
       "36   [-0.030289800837635994, -0.002532508224248886,...  \n",
       "37   [-0.025598619133234024, -0.004655908793210983,...  \n",
       "41   [-0.02605891041457653, -0.009838425554335117, ...  \n",
       "42   [0.0029013229068368673, 0.017439918592572212, ...  \n",
       "43   [0.005093783605843782, 0.004136959556490183, -...  \n",
       "44   [-0.01352930348366499, 0.01176003273576498, -0...  \n",
       "45   [-0.005808187648653984, -0.011234029196202755,...  \n",
       "46   [-0.021635664626955986, 0.008698590099811554, ...  \n",
       "47   [-0.0051681166514754295, 0.012547116726636887,...  \n",
       "48   [-0.00370140396989882, -0.0030459468252956867,...  \n",
       "49   [-0.01539567019790411, -0.0014442047104239464,...  \n",
       "50   [-0.01444741990417242, -0.0058169118128716946,...  \n",
       "51   [-0.019419711083173752, 0.005196480546146631, ...  \n",
       "52   [0.0014923522248864174, -0.0023144488222897053...  \n",
       "53   [0.005264581646770239, -0.0007557418430224061,...  \n",
       "54   [-0.000461430725408718, 0.010870600119233131, ...  \n",
       "55   [-0.015217158943414688, -0.015805063769221306,...  \n",
       "56   [-0.022013897076249123, 0.01987425610423088, 0...  \n",
       "57   [-0.01585082896053791, 0.01199599914252758, -0...  \n",
       "62   [-0.0026677444111555815, 0.01041774358600378, ...  \n",
       "63   [0.003431991208344698, 0.004281357396394014, -...  \n",
       "64   [-0.01290346123278141, 0.0027702641673386097, ...  \n",
       "66   [-0.01375657506287098, -0.00021721386292483658...  \n",
       "67   [0.0003084054624196142, -0.009778305888175964,...  \n",
       "68   [-0.019176218658685684, -0.015687033534049988,...  \n",
       "238  [-0.01668614149093628, 0.00771642941981554, -0...  \n",
       "283  [0.007682955823838711, 0.011558671481907368, 0...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "773650de-962d-4370-bcae-2547cef1e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.concat([df3,result], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "49c9230a-f2c5-403d-8965-8b45ce204b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...</td>\n",
       "      <td>[-0.014995344914495945, -0.0026680435985326767...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slug: \"client-setup\" _TOKEN\\n)\\n```\\n```python...</td>\n",
       "      <td>[-0.021182812750339508, -0.004293339792639017,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>---\\ntitle: \"Customer Churn Prediction\"\\nslug:...</td>\n",
       "      <td>[-0.00745741231366992, -0.010434732772409916, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>slug: \"customer-churn-prediction\" /bb02793-chu...</td>\n",
       "      <td>[0.002079722471535206, -0.0047837127931416035,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slug: \"customer-churn-prediction\" Churn-image5...</td>\n",
       "      <td>[0.0011718893656507134, 0.000443618802819401, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>---\\ntitle: \"Class Imbalance Monitoring Exampl...</td>\n",
       "      <td>[-0.01375657506287098, -0.00021721386292483658...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>slug: \"class-imbalance-monitoring-example\" ler...</td>\n",
       "      <td>[0.0003084054624196142, -0.009778305888175964,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>slug: \"class-imbalance-monitoring-example\" # S...</td>\n",
       "      <td>[-0.019176218658685684, -0.015687033534049988,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>---\\ntitle: \"Class-Imbalanced Data\"\\nslug: \"cl...</td>\n",
       "      <td>[-0.01668614149093628, 0.00771642941981554, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>---\\ntitle: \"Ranking Model Package.py\"\\nslug: ...</td>\n",
       "      <td>[0.007682955823838711, 0.011558671481907368, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    ---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...   \n",
       "1    slug: \"client-setup\" _TOKEN\\n)\\n```\\n```python...   \n",
       "2    ---\\ntitle: \"Customer Churn Prediction\"\\nslug:...   \n",
       "3    slug: \"customer-churn-prediction\" /bb02793-chu...   \n",
       "4    slug: \"customer-churn-prediction\" Churn-image5...   \n",
       "..                                                 ...   \n",
       "307  ---\\ntitle: \"Class Imbalance Monitoring Exampl...   \n",
       "308  slug: \"class-imbalance-monitoring-example\" ler...   \n",
       "309  slug: \"class-imbalance-monitoring-example\" # S...   \n",
       "310  ---\\ntitle: \"Class-Imbalanced Data\"\\nslug: \"cl...   \n",
       "311  ---\\ntitle: \"Ranking Model Package.py\"\\nslug: ...   \n",
       "\n",
       "                                             embedding  \n",
       "0    [-0.014995344914495945, -0.0026680435985326767...  \n",
       "1    [-0.021182812750339508, -0.004293339792639017,...  \n",
       "2    [-0.00745741231366992, -0.010434732772409916, ...  \n",
       "3    [0.002079722471535206, -0.0047837127931416035,...  \n",
       "4    [0.0011718893656507134, 0.000443618802819401, ...  \n",
       "..                                                 ...  \n",
       "307  [-0.01375657506287098, -0.00021721386292483658...  \n",
       "308  [0.0003084054624196142, -0.009778305888175964,...  \n",
       "309  [-0.019176218658685684, -0.015687033534049988,...  \n",
       "310  [-0.01668614149093628, 0.00771642941981554, -0...  \n",
       "311  [0.007682955823838711, 0.011558671481907368, 0...  \n",
       "\n",
       "[312 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fb239b34-87a2-49d7-9628-8b1da07a4dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv(\"latest_v_23-4_tk750.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "306f52fb-c520-40d6-ba65-32248a951f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...</td>\n",
       "      <td>[-0.014995344914495945, -0.0026680435985326767...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slug: \"client-setup\" _TOKEN\\n)\\n```\\n```python...</td>\n",
       "      <td>[-0.021182812750339508, -0.004293339792639017,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>---\\ntitle: \"Customer Churn Prediction\"\\nslug:...</td>\n",
       "      <td>[-0.00745741231366992, -0.010434732772409916, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>slug: \"customer-churn-prediction\" /bb02793-chu...</td>\n",
       "      <td>[0.002079722471535206, -0.0047837127931416035,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slug: \"customer-churn-prediction\" Churn-image5...</td>\n",
       "      <td>[0.0011718893656507134, 0.000443618802819401, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>---\\ntitle: \"Class Imbalance Monitoring Exampl...</td>\n",
       "      <td>[-0.01375657506287098, -0.00021721386292483658...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>slug: \"class-imbalance-monitoring-example\" ler...</td>\n",
       "      <td>[0.0003084054624196142, -0.009778305888175964,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>slug: \"class-imbalance-monitoring-example\" # S...</td>\n",
       "      <td>[-0.019176218658685684, -0.015687033534049988,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>---\\ntitle: \"Class-Imbalanced Data\"\\nslug: \"cl...</td>\n",
       "      <td>[-0.01668614149093628, 0.00771642941981554, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>---\\ntitle: \"Ranking Model Package.py\"\\nslug: ...</td>\n",
       "      <td>[0.007682955823838711, 0.011558671481907368, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    ---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...   \n",
       "1    slug: \"client-setup\" _TOKEN\\n)\\n```\\n```python...   \n",
       "2    ---\\ntitle: \"Customer Churn Prediction\"\\nslug:...   \n",
       "3    slug: \"customer-churn-prediction\" /bb02793-chu...   \n",
       "4    slug: \"customer-churn-prediction\" Churn-image5...   \n",
       "..                                                 ...   \n",
       "307  ---\\ntitle: \"Class Imbalance Monitoring Exampl...   \n",
       "308  slug: \"class-imbalance-monitoring-example\" ler...   \n",
       "309  slug: \"class-imbalance-monitoring-example\" # S...   \n",
       "310  ---\\ntitle: \"Class-Imbalanced Data\"\\nslug: \"cl...   \n",
       "311  ---\\ntitle: \"Ranking Model Package.py\"\\nslug: ...   \n",
       "\n",
       "                                             embedding  \n",
       "0    [-0.014995344914495945, -0.0026680435985326767...  \n",
       "1    [-0.021182812750339508, -0.004293339792639017,...  \n",
       "2    [-0.00745741231366992, -0.010434732772409916, ...  \n",
       "3    [0.002079722471535206, -0.0047837127931416035,...  \n",
       "4    [0.0011718893656507134, 0.000443618802819401, ...  \n",
       "..                                                 ...  \n",
       "307  [-0.01375657506287098, -0.00021721386292483658...  \n",
       "308  [0.0003084054624196142, -0.009778305888175964,...  \n",
       "309  [-0.019176218658685684, -0.015687033534049988,...  \n",
       "310  [-0.01668614149093628, 0.00771642941981554, -0...  \n",
       "311  [0.007682955823838711, 0.011558671481907368, 0...  \n",
       "\n",
       "[312 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = pd.read_csv(\"latest_v_23-4_tk750.csv\")\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e49a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stray_doc[0] = \"package.py for R based models\" + stray_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d08613d0-647b-41ea-a04f-34ab4e3d93db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'package.py for R based models```python\\nimport fiddler as fdl\\n```\\n\\n\\n```python\\nprint(fdl.__version__)\\n```\\n\\n    1.6.2\\n\\n\\n\\n```python\\nurl = \\'\\'\\ntoken = \\'\\'\\norg_id = \\'\\'\\n\\nclient = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=token, version=2)\\n```\\n\\n\\n```python\\nproject_id = \\'test_r3\\'\\nmodel_id = \\'iris\\'\\ndataset_id = \\'iris\\'\\n```\\n\\n\\n```python\\n# client.create_project(project_id=project_id)\\n```\\n\\n\\n```python\\nimport pandas as pd\\nfrom pathlib import Path\\nimport yaml\\n```\\n\\n\\n```python\\ndf = pd.read_csv(\\'test_R/data_r.csv\\')\\ndf.head()\\n```\\n\\n\\n\\n\\n<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>Sepal.Length</th>\\n      <th>Sepal.Width</th>\\n      <th>Petal.Length</th>\\n      <th>Petal.Width</th>\\n      <th>Species</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>5.1</td>\\n      <td>3.5</td>\\n      <td>1.4</td>\\n      <td>0.2</td>\\n      <td>setosa</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>4.9</td>\\n      <td>3.0</td>\\n      <td>1.4</td>\\n      <td>0.2</td>\\n      <td>setosa</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>4.7</td>\\n      <td>3.2</td>\\n      <td>1.3</td>\\n      <td>0.2</td>\\n      <td>setosa</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>4.6</td>\\n      <td>3.1</td>\\n      <td>1.5</td>\\n      <td>0.2</td>\\n      <td>setosa</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>5.0</td>\\n      <td>3.6</td>\\n      <td>1.4</td>\\n      <td>0.2</td>\\n      <td>setosa</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>\\n\\n\\n\\n\\n```python\\ndataset_info = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=100)\\ndataset_info\\n```\\n\\n\\n\\n\\n<div style=\"border: thin solid rgb(41, 57, 141); padding: 10px;\"><h3 style=\"text-align: center; margin: auto;\">DatasetInfo\\n</h3><pre>display_name: \\nfiles: []\\n</pre><hr>Columns:<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>column</th>\\n      <th>dtype</th>\\n      <th>count(possible_values)</th>\\n      <th>is_nullable</th>\\n      <th>value_range</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>Sepal.Length</td>\\n      <td>FLOAT</td>\\n      <td></td>\\n      <td>False</td>\\n      <td>4.3 - 7.9</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>Sepal.Width</td>\\n      <td>FLOAT</td>\\n      <td></td>\\n      <td>False</td>\\n      <td>2.0 - 4.4</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>Petal.Length</td>\\n      <td>FLOAT</td>\\n      <td></td>\\n      <td>False</td>\\n      <td>1.0 - 6.9</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>Petal.Width</td>\\n      <td>FLOAT</td>\\n      <td></td>\\n      <td>False</td>\\n      <td>0.1 - 2.5</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>Species</td>\\n      <td>CATEGORY</td>\\n      <td>3</td>\\n      <td>False</td>\\n      <td></td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div></div>\\n\\n\\n\\n\\n```python\\nclient.upload_dataset(project_id=project_id, dataset={\\'baseline\\': df},\\n                      dataset_id=dataset_id, info=dataset_info)\\n```\\n\\n\\n```python\\ntarget = \\'Species\\'\\noutputs = [\\'proba_setosa\\', \\'proba_versicolor\\', \\'proba_virginica\\']\\nfeatures = list(df.drop(columns=[target]).columns)\\n    \\n# Generate ModelInfo\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n    dataset_info=dataset_info,\\n    dataset_id=dataset_id,\\n    target=target,\\n    outputs=outputs,\\n    features=features,\\n    categorical_target_class_details=[\\'setosa\\', \\'versicolor\\', \\'virginica\\'],\\n    model_task=fdl.ModelTask.MULTICLASS_CLASSIFICATION,\\n)\\nmodel_info\\n```\\n\\n\\n```python\\nmodel_dir = Path(\\'test_R/iris_r\\')\\n```\\n\\n\\n```python\\n# save model schema\\nwith open(model_dir / \\'model.yaml\\', \\'w\\') as yaml_file:\\n    yaml.dump({\\'model\\': model_info.to_dict()}, yaml_file)\\n```\\n\\n\\n```python\\n%%writefile test_R/iris_r/package.py\\n\\nfrom pathlib import Path\\n\\nimport numpy as np\\nimport pandas as pd\\nimport rpy2.robjects as robjects\\nfrom rpy2.robjects import numpy2ri, pandas2ri\\nfrom rpy2.robjects.packages import importr\\n\\npandas2ri.activate()\\nnumpy2ri.activate()\\nr = robjects.r\\n\\n\\nclass Model:\\n    \"\"\"\\n    R Model Loader\\n\\n    Attributes\\n    ----------\\n    model : R object\\n    \"\"\"\\n\\n    def __init__(self):\\n        self.model = None\\n\\n    def load(self, path):\\n        \"\"\"\\n        load the model at `path`\\n        \"\"\"\\n        model_rds_path = f\\'{path}.rds\\'\\n\\n        self.model = r.readRDS(model_rds_path)\\n        \\n        _ = [importr(dep.strip()) for dep in [\\'randomForest\\'] if dep.strip() != \\'\\']\\n\\n\\n        return self\\n\\n    def predict(self, input_df):\\n        \"\"\"\\n        Perform classification on samples in X.\\n\\n        Parameters\\n        ----------\\n        input_df : pandas dataframe, shape (n_samples, n_features)\\n        Returns\\n        -------\\n        pred : array, shape (n_samples)\\n        \"\"\"\\n\\n        if self.model is None:\\n            raise Exception(\\'There is no Model\\')\\n\\n\\n        pred = r.predict(self.model, [input_df], type=\\'prob\\')\\n        df = pd.DataFrame(np.array(pred), columns=[\\'proba_setosa\\', \\'proba_versicolor\\', \\'proba_virginica\\'])\\n\\n        return df\\n\\n\\nMODEL_PATH = \\'iris\\'\\nPACKAGE_PATH = Path(__file__).parent\\n\\n\\ndef get_model():\\n    return Model().load(str(PACKAGE_PATH / MODEL_PATH))\\n```\\n\\n\\n```python\\nclient.upload_model_package(artifact_path=model_dir, project_id=project_id, model_id=model_id)\\n```\\n\\n\\n```python\\nclient.run_model(project_id=project_id, model_id=model_id, df=df.head())\\n```\\n\\n\\n```python\\nclient.run_feature_importance()\\n```\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stray_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22a995b1-4e8d-4a70-9f7d-0027fd33110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=[]\n",
    "for i in range(len(stray_doc)):\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=stray_doc[i])\n",
    "    embeddings.append(response[\"data\"][0][\"embedding\"])\n",
    "\n",
    "df = pd.DataFrame({\"text\": stray_doc, \"embedding\": embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3581ca59-49a9-4c18-a75b-6a700f8a79a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"rmodel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8000044-d06d-4627-8b46-35d7306778bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fdd45e9",
   "metadata": {},
   "source": [
    "### Assembling docs and caveats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a30c38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_path4 = \"/Users/murtuzashergadwala/fiddler-chatbot/caveats.csv\"\n",
    "# df4 = pd.read_csv(embeddings_path4)\n",
    "\n",
    "# embeddings_path5 = \"/Users/murtuzashergadwala/fiddler-chatbot/caveats2.csv\"\n",
    "# df5 = pd.read_csv(embeddings_path5)\n",
    "\n",
    "# embeddings_path6 = \"/Users/murtuzashergadwala/fiddler-chatbot/caveats3.csv\"\n",
    "# df6 = pd.read_csv(embeddings_path6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "193c39a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4['embedding'] = df4['embedding'].apply(ast.literal_eval)\n",
    "# df5['embedding'] = df5['embedding'].apply(ast.literal_eval)\n",
    "# df6['embedding'] = df6['embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d1725e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = pd.concat([df, df4, df5, df6], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0566d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "46bb509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.to_csv(\"chatbot_08_15_23_tk750.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e26b37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>package.py for R based models```python\\nimport...</td>\n",
       "      <td>[-0.009950872510671616, -0.011770655401051044,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  package.py for R based models```python\\nimport...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.009950872510671616, -0.011770655401051044,...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48c3c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path2 = \"/Users/murtuzashergadwala/fiddler-chatbot/chatbot_08_15_23_tk750.csv\"\n",
    "df2 = pd.read_csv(embeddings_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b019dc5-86dc-43a1-9229-7de9845613ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>---\\ntitle: \"Release 22.11 Notes\"\\nslug: \"rele...</td>\n",
       "      <td>[-0.005742393434047699, 0.001501509454101324, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>---\\ntitle: \"Release 23.2 Notes\"\\nslug: \"relea...</td>\n",
       "      <td>[0.010167686268687248, -0.001216516480781138, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>---\\ntitle: \"Release 22.12 Notes\"\\nslug: \"rele...</td>\n",
       "      <td>[-0.010491670109331608, -0.0011956370435655117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---\\ntitle: \"Release 23.1 Notes\"\\nslug: \"2023-...</td>\n",
       "      <td>[-0.00474295811727643, 0.0016190075548365712, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...</td>\n",
       "      <td>[-0.01489181537181139, -0.0028792002703994513,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>---\\ntitle: \"Uploading a scikit-learn Model Ar...</td>\n",
       "      <td>[0.0031662925612181425, 0.018320860341191292, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...</td>\n",
       "      <td>[-0.0003570110129658133, -0.021801473572850227...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Once you have added a model on the Fiddler pla...</td>\n",
       "      <td>[-0.022226542234420776, 0.011369146406650543, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Custom metrics is an upcoming feature and it i...</td>\n",
       "      <td>[-0.017716489732265472, -0.0035160724073648453...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Re-uploading in Fiddler essentially means havi...</td>\n",
       "      <td>[-0.01760503277182579, 0.011651406064629555, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "1    ---\\ntitle: \"Release 22.11 Notes\"\\nslug: \"rele...   \n",
       "2    ---\\ntitle: \"Release 23.2 Notes\"\\nslug: \"relea...   \n",
       "3    ---\\ntitle: \"Release 22.12 Notes\"\\nslug: \"rele...   \n",
       "4    ---\\ntitle: \"Release 23.1 Notes\"\\nslug: \"2023-...   \n",
       "5    ---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...   \n",
       "..                                                 ...   \n",
       "305  ---\\ntitle: \"Uploading a scikit-learn Model Ar...   \n",
       "306  ---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...   \n",
       "307  Once you have added a model on the Fiddler pla...   \n",
       "308  Custom metrics is an upcoming feature and it i...   \n",
       "309  Re-uploading in Fiddler essentially means havi...   \n",
       "\n",
       "                                             embedding  \n",
       "1    [-0.005742393434047699, 0.001501509454101324, ...  \n",
       "2    [0.010167686268687248, -0.001216516480781138, ...  \n",
       "3    [-0.010491670109331608, -0.0011956370435655117...  \n",
       "4    [-0.00474295811727643, 0.0016190075548365712, ...  \n",
       "5    [-0.01489181537181139, -0.0028792002703994513,...  \n",
       "..                                                 ...  \n",
       "305  [0.0031662925612181425, 0.018320860341191292, ...  \n",
       "306  [-0.0003570110129658133, -0.021801473572850227...  \n",
       "307  [-0.022226542234420776, 0.011369146406650543, ...  \n",
       "308  [-0.017716489732265472, -0.0035160724073648453...  \n",
       "309  [-0.01760503277182579, 0.011651406064629555, 0...  \n",
       "\n",
       "[309 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.drop([0], inplace=True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b44f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([df, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ad0aade-e39e-4845-b34c-a5742a9f6e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>package.py for R based models```python\\nimport...</td>\n",
       "      <td>[-0.009950872510671616, -0.011770655401051044,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>---\\ntitle: \"Release 22.11 Notes\"\\nslug: \"rele...</td>\n",
       "      <td>[-0.005742393434047699, 0.001501509454101324, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>---\\ntitle: \"Release 23.2 Notes\"\\nslug: \"relea...</td>\n",
       "      <td>[0.010167686268687248, -0.001216516480781138, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>---\\ntitle: \"Release 22.12 Notes\"\\nslug: \"rele...</td>\n",
       "      <td>[-0.010491670109331608, -0.0011956370435655117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---\\ntitle: \"Release 23.1 Notes\"\\nslug: \"2023-...</td>\n",
       "      <td>[-0.00474295811727643, 0.0016190075548365712, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>---\\ntitle: \"Uploading a scikit-learn Model Ar...</td>\n",
       "      <td>[0.0031662925612181425, 0.018320860341191292, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...</td>\n",
       "      <td>[-0.0003570110129658133, -0.021801473572850227...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Once you have added a model on the Fiddler pla...</td>\n",
       "      <td>[-0.022226542234420776, 0.011369146406650543, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Custom metrics is an upcoming feature and it i...</td>\n",
       "      <td>[-0.017716489732265472, -0.0035160724073648453...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Re-uploading in Fiddler essentially means havi...</td>\n",
       "      <td>[-0.01760503277182579, 0.011651406064629555, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    package.py for R based models```python\\nimport...   \n",
       "1    ---\\ntitle: \"Release 22.11 Notes\"\\nslug: \"rele...   \n",
       "2    ---\\ntitle: \"Release 23.2 Notes\"\\nslug: \"relea...   \n",
       "3    ---\\ntitle: \"Release 22.12 Notes\"\\nslug: \"rele...   \n",
       "4    ---\\ntitle: \"Release 23.1 Notes\"\\nslug: \"2023-...   \n",
       "..                                                 ...   \n",
       "305  ---\\ntitle: \"Uploading a scikit-learn Model Ar...   \n",
       "306  ---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...   \n",
       "307  Once you have added a model on the Fiddler pla...   \n",
       "308  Custom metrics is an upcoming feature and it i...   \n",
       "309  Re-uploading in Fiddler essentially means havi...   \n",
       "\n",
       "                                             embedding  \n",
       "0    [-0.009950872510671616, -0.011770655401051044,...  \n",
       "1    [-0.005742393434047699, 0.001501509454101324, ...  \n",
       "2    [0.010167686268687248, -0.001216516480781138, ...  \n",
       "3    [-0.010491670109331608, -0.0011956370435655117...  \n",
       "4    [-0.00474295811727643, 0.0016190075548365712, ...  \n",
       "..                                                 ...  \n",
       "305  [0.0031662925612181425, 0.018320860341191292, ...  \n",
       "306  [-0.0003570110129658133, -0.021801473572850227...  \n",
       "307  [-0.022226542234420776, 0.011369146406650543, ...  \n",
       "308  [-0.017716489732265472, -0.0035160724073648453...  \n",
       "309  [-0.01760503277182579, 0.011651406064629555, 0...  \n",
       "\n",
       "[310 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce7b729b-a588-43d8-82eb-c2c9c2c6b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv(\"chatbot_08_15_23_tk750.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7011404-9e19-4e08-aa73-7dddf567c9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>package.py for R based models```python\\nimport...</td>\n",
       "      <td>[-0.00931614637374878, -0.011871236376464367, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>---\\ntitle: \"Release 22.11 Notes\"\\nslug: \"rele...</td>\n",
       "      <td>[-0.005742393434047699, 0.001501509454101324, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>---\\ntitle: \"Release 23.2 Notes\"\\nslug: \"relea...</td>\n",
       "      <td>[0.010167686268687248, -0.001216516480781138, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>---\\ntitle: \"Release 22.12 Notes\"\\nslug: \"rele...</td>\n",
       "      <td>[-0.010491670109331608, -0.0011956370435655117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---\\ntitle: \"Release 23.1 Notes\"\\nslug: \"2023-...</td>\n",
       "      <td>[-0.00474295811727643, 0.0016190075548365712, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>---\\ntitle: \"Uploading a scikit-learn Model Ar...</td>\n",
       "      <td>[0.0031662925612181425, 0.018320860341191292, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...</td>\n",
       "      <td>[-0.0003570110129658133, -0.021801473572850227...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Once you have added a model on the Fiddler pla...</td>\n",
       "      <td>[-0.022226542234420776, 0.011369146406650543, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Custom metrics is an upcoming feature and it i...</td>\n",
       "      <td>[-0.017716489732265472, -0.0035160724073648453...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Re-uploading in Fiddler essentially means havi...</td>\n",
       "      <td>[-0.01760503277182579, 0.011651406064629555, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    package.py for R based models```python\\nimport...   \n",
       "1    ---\\ntitle: \"Release 22.11 Notes\"\\nslug: \"rele...   \n",
       "2    ---\\ntitle: \"Release 23.2 Notes\"\\nslug: \"relea...   \n",
       "3    ---\\ntitle: \"Release 22.12 Notes\"\\nslug: \"rele...   \n",
       "4    ---\\ntitle: \"Release 23.1 Notes\"\\nslug: \"2023-...   \n",
       "..                                                 ...   \n",
       "305  ---\\ntitle: \"Uploading a scikit-learn Model Ar...   \n",
       "306  ---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...   \n",
       "307  Once you have added a model on the Fiddler pla...   \n",
       "308  Custom metrics is an upcoming feature and it i...   \n",
       "309  Re-uploading in Fiddler essentially means havi...   \n",
       "\n",
       "                                             embedding  \n",
       "0    [-0.00931614637374878, -0.011871236376464367, ...  \n",
       "1    [-0.005742393434047699, 0.001501509454101324, ...  \n",
       "2    [0.010167686268687248, -0.001216516480781138, ...  \n",
       "3    [-0.010491670109331608, -0.0011956370435655117...  \n",
       "4    [-0.00474295811727643, 0.0016190075548365712, ...  \n",
       "..                                                 ...  \n",
       "305  [0.0031662925612181425, 0.018320860341191292, ...  \n",
       "306  [-0.0003570110129658133, -0.021801473572850227...  \n",
       "307  [-0.022226542234420776, 0.011369146406650543, ...  \n",
       "308  [-0.017716489732265472, -0.0035160724073648453...  \n",
       "309  [-0.01760503277182579, 0.011651406064629555, 0...  \n",
       "\n",
       "[310 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"/Users/murtuzashergadwala/fiddler-chatbot/chatbot_08_15_23_tk750.csv\"\n",
    "df_test = pd.read_csv(test)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7309998",
   "metadata": {},
   "source": [
    "## Testing Chatbot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cc21720",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = \"./chatbot_08_15_23_tk750.csv\"\n",
    "df = pd.read_csv(embeddings_path)\n",
    "# convert embeddings from CSV str type back to list type\n",
    "df['embedding'] = df['embedding'].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100\n",
    "):\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = openai.Embedding.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n], query_embedding\n",
    "\n",
    "\n",
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int,\n",
    "    introduction='You are a tool called Fiddler Chatbot and your purpose is to use the below documentation from the company Fiddler to answer the subsequent documentation questions. Also, if possible, give me the reference URLs according to the following instructions. The way to create the URLs is: if you are discussing a client method or an API reference add \"https://docs.fiddler.ai/reference/\" before the \"slug\" value of the document. If it is Guide documentation add \"https://docs.fiddler.ai/docs/\" before before the \"slug\" value of the document. Only use the value following \"slug:\" to create the URLs and do not use page titles for slugs. If you are using quickstart notebooks, do not generate references. Note that if a user asks about uploading events, it means the same as publishing events. If the answer cannot be found in the documentation, write \"I could not find an answer.\"'\n",
    "\n",
    "):\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    strings, relatednesses, query_embed = strings_ranked_by_relatedness(query, df)\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    for string in strings:\n",
    "        next_article = string\n",
    "        if (\n",
    "            num_tokens(message + next_article + question, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_article\n",
    "    return message + question, query_embed\n",
    "\n",
    "\n",
    "def ask(\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096 - 500,\n",
    "    print_message: bool = False,\n",
    "    temperature: int = 0,\n",
    "    # chat_history=None,\n",
    "    introduction='You are a tool called Fiddler Chatbot and your purpose is to use the below documentation from the company Fiddler to answer the subsequent documentation questions. Also, if possible, give me the reference URLs according to the following instructions. The way to create the URLs is: add \"https://docs.fiddler.ai/docs/\" before the \"slug\" value of the document. For any URL references that start with \"doc:\" or \"ref:\" use its value to create a URL by adding \"https://docs.fiddler.ai/docs/\" before that value. For reference URLs about release notes add \"https://docs.fiddler.ai/changelog/\" before the \"slug\" value of the document. Do not use page titles to create urls. Note that if a user asks about uploading events, it means the same as publishing events.  If the answer cannot be found in the documentation, write \"I could not find an answer. Join our [Slack community](https://www.fiddler.ai/slackinvite) for further clarifications.\"'\n",
    "\n",
    "):\n",
    "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
    "    \n",
    "    message, query_embed = query_message(query, df=df, model=model, token_budget=token_budget, introduction = introduction)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions about Fiddler documentation.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "#     response_embedding_response = openai.Embedding.create(\n",
    "#         model=EMBEDDING_MODEL,\n",
    "#         input=response_message,\n",
    "#     )\n",
    "#     response_embed = response_embedding_response[\"data\"][0][\"embedding\"]\n",
    "   \n",
    "    return response_message, message, query_embed\n",
    "\n",
    "\n",
    "def ask2(\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096 - 500,\n",
    "    print_message: bool = False,\n",
    "    temperature: int = 0,\n",
    "    # chat_history=None,\n",
    "    introduction='You are a tool called Fiddler Chatbot and your purpose is to use the below documentation from the company Fiddler to answer the subsequent documentation questions. Also, if possible, give me the reference URLs according to the following instructions. The way to create the URLs is: add \"https://docs.fiddler.ai/docs/\" before the \"slug\" value of the document. For any URL references that start with \"doc:\" or \"ref:\" use its value to create a URL by adding \"https://docs.fiddler.ai/docs/\" before that value. For reference URLs about release notes add \"https://docs.fiddler.ai/changelog/\" before the \"slug\" value of the document. Do not use page titles to create urls. Note that if a user asks about uploading events, it means the same as publishing events.  If the answer cannot be found in the provided context, write \"I could not find an answer. Join our [Slack community](https://www.fiddler.ai/slackinvite) for further clarifications.\" Do not try to make up an answer if it is not present in the context.'\n",
    "\n",
    "):\n",
    "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
    "    \n",
    "    message, query_embed = query_message(query, df=df, model=model, token_budget=token_budget, introduction = introduction)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions about Fiddler documentation.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "#     response_embedding_response = openai.Embedding.create(\n",
    "#         model=EMBEDDING_MODEL,\n",
    "#         input=response_message,\n",
    "#     )\n",
    "#     response_embed = response_embedding_response[\"data\"][0][\"embedding\"]\n",
    "   \n",
    "    return response_message, message, query_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb53e46-195c-4453-8ef3-19fbc524dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "s,r,emb = strings_ranked_by_relatedness(\"What all metrics do you support in LLMs\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "493eec0d-459c-45f8-87a5-3259ec30080a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Custom metrics is an upcoming feature and it is currently not supported.',\n",
       " '---\\ntitle: \"Monitoring Charts\"\\nslug: \"monitoring-charts-platform\"\\nhidden: false\\ncreatedAt: \"2023-02-23T22:56:27.756Z\"\\nupdatedAt: \"2023-05-24T17:29:04.123Z\"\\n---\\nFiddler AI’s monitoring charts allow you to easily track your models and ensure that they are performing optimally. For any of your models, monitoring charts for data drift, performance, data integrity, or traffic metrics can be displayed using Fiddler Dashboards.\\n\\n## Supported Metric Types\\n\\nMonitoring charts enable you to plot one of the following metric types for a given model:\\n\\n- [**Data Drift**](doc:data-drift-platform#what-is-being-tracked)\\n  - Plot drift for up to 20 columns at once and track it using your choice of Jensen–Shannon distance (JSD) or Population Stability Index (PSI).\\n- [**Performance**](doc:performance-tracking-platform#what-is-being-tracked)\\n  - Available metrics are model dependent.\\n- [**Data Integrity Violations**](doc:data-integrity-platform#what-is-being-tracked)\\n  - Plot data integrity violations for up to 20 columns and track one of the three violations at once.\\n- [**Traffic **](doc:traffic-platform#what-is-being-tracked)\\n\\n## Key Features:\\n\\n### Multiple Charting Options\\n\\nYou can [plot up to 20 columns](doc:monitoring-charts-ui#chart-metric-queries--filters) for a model when charting data drift or data integrity metrics, allowing you to compare them side by side.\\n\\n### Downloadable CSV Data\\n\\nYou can [easily download a CSV of the raw chart data](doc:monitoring-charts-ui#breakdown-summary). This feature allows you to analyze your data further.\\n\\n### Advanced Chart Functionality\\n\\nThe monitoring charts feature offers [advanced chart functionalities ](doc:monitoring-charts-ui#chart-metric-queries--filters)  to provide you with the flexibility to customize your charts and view your data in a way that is most useful to you. Features include:\\n\\n- Zoom\\n- Dragging of time ranges\\n- Toggling between bar and line chart types\\n- Adjusting the scale between linear and log options\\n- Adjusting the range of the y-axis\\n\\n![](https://files.readme.io/9ad4867-image.png)\\n\\n\\n\\nCheck out more on the [Monitoring Charts UI Guide](doc:monitoring-charts-ui).',\n",
       " '---\\ntitle: \"fdl.Metric\"\\nslug: \"fdlmetric\"\\nexcerpt: \"Supported Metric for different Alert Types in Alert Rules\"\\nhidden: false\\ncreatedAt: \"2023-01-31T07:32:12.906Z\"\\nupdatedAt: \"2023-02-03T03:25:53.558Z\"\\n---\\n**Following is the list of metrics, with corresponding alert type and model task, for which an alert rule can be created.**\\n\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"Enum Values\",\\n    \"h-1\": \"Supported for [Alert Types](https://docs.fiddler.ai/v1.5/reference/fdlalerttype)  \\\\n([ModelTask ](https://docs.fiddler.ai/v1.5/reference/fdlmodeltask)restriction if any)\",\\n    \"h-2\": \"Description\",\\n    \"0-0\": \"fdl.Metric.PSI\",\\n    \"0-1\": \"fdl.AlertType.DATA_DRIFT\",\\n    \"0-2\": \"Population Stability Index\",\\n    \"1-0\": \"fdl.Metric.JSD\",\\n    \"1-1\": \"fdl.AlertType.DATA_DRIFT\",\\n    \"1-2\": \"Jensen–Shannon divergence\",\\n    \"2-0\": \"fdl.Metric.MISSING_VALUE\",\\n    \"2-1\": \"fdl.AlertType.DATA_INTEGRITY\",\\n    \"2-2\": \"Missing Value\",\\n    \"3-0\": \"fdl.Metric.TYPE_VIOLATION\",\\n    \"3-1\": \"fdl.AlertType.DATA_INTEGRITY\",\\n    \"3-2\": \"Type Violation\",\\n    \"4-0\": \"fdl.Metric.RANGE_VIOLATION\",\\n    \"4-1\": \"fdl.AlertType.DATA_INTEGRITY\",\\n    \"4-2\": \"Range violation\",\\n    \"5-0\": \"fdl.Metric.TRAFFIC\",\\n    \"5-1\": \"fdl.AlertType.SERVICE_METRICS\",\\n    \"5-2\": \"Traffic Count\",\\n    \"6-0\": \"fdl.Metric.ACCURACY\",\\n    \"6-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION,  \\\\nfdl.ModelTask.MULTICLASS_CLASSIFICATION)\",\\n    \"6-2\": \"Accuracy\",\\n    \"7-0\": \"fdl.Metric.RECALL\",\\n    \"7-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION)\",\\n    \"7-2\": \"Recall\",\\n    \"8-0\": \"fdl.Metric.FPR\",\\n    \"8-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION)\",\\n    \"8-2\": \"False Positive Rate\",\\n    \"9-0\": \" fdl.Metric.PRECISION\",\\n    \"9-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION)\",\\n    \"9-2\": \"Precision\",\\n    \"10-0\": \"fdl.Metric.TPR\",\\n    \"10-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION)\",\\n    \"10-2\": \"True Positive Rate\",\\n    \"11-0\": \"fdl.Metric.AUC\",\\n    \"11-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION)\",\\n    \"11-2\": \"Area under',\n",
       " 'slug: \"data-integrity\"  Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Performance Tracking\"\\nslug: \"performance-tracking-platform\"\\nexcerpt: \"Platform Guide\"\\nhidden: false\\ncreatedAt: \"2022-12-19T19:27:22.159Z\"\\nupdatedAt: \"2023-08-04T23:21:39.375Z\"\\n---\\n## What is being tracked?\\n\\n![](https://files.readme.io/4a646d4-qs_monitoring.png \"qs_monitoring.png\")\\n\\n- **_Decisions_** - The post-prediction business decisions made as a result of the model output. Decisions are calculated before [client.publish_event()](ref:clientpublish_event) (they\\'re not inferred by Fiddler). For binary classification models, a decision is usually determined using a threshold. For multi-class classification models, it\\'s usually determined using the argmax value of the model outputs.\\n\\n- **_Performance metrics_**\\n\\n| Model Task Type       | Metric                                                         | Description                                                                                                                                        |\\n| :-------------------- | :------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| Binary Classification | Accuracy                                                       | (TP + TN) / (TP + TN + FP + FN)                                                                                                                    |\\n| Binary Classification | True Positive Rate/Recall                                      | TP / (TP + FN)                                                                                                                                     |\\n| Binary Classification | False Positive Rate                                            | FP / (FP + TN)                                                                                                                                     |\\n| Binary Classification | Precision                                                      | TP / (TP + FP)                                                                                                                                     |\\n| Binary Classification | F1 Score                                                       | 2  \\\\* ( Precision \\\\*  Recall ) / ( Precision + Recall )                                                                                            |\\n| Binary Classification | AUROC                                                          | Area Under the Receiver Operating Characteristic (ROC) curve, which plots the true positive rate against the false positive rate                   |\\n| Binary Classification | Binary Cross Entropy                                           | Measures the difference between the predicted probability distribution and the true distribution                                                   |\\n| Binary Classification | Geometric Mean                                                 | Square Root of ( Precision \\\\* Recall )                                                                                                             |\\n| Binary Classification | Calibrated Threshold                                           | A threshold that balances precision and recall at a particular operating point                                                                     |\\n| Binary Classification | Data Count                                                     | The number of events where target and output are both not NULL. **_This will be used as the denominator when calculating accuracy_**.              |\\n| Binary Classification | Expected Calibration Error                                     | Measures the difference between predicted probabilities and empirical probabilities                                                                |\\n| Multi Classification  | Accuracy                                                       | (Number of correctly classified samples) / ( Data Count ). Data Count refers to the number of events where the target and output are both not NULL |\\n| Multi Classification  | Log Loss                                                       | Measures the difference between the predicted probability distribution and the true distribution, in a logarithmic scale                           |\\n| Regression            | Coefficient of determination (R-squared)                       | Measures the proportion of variance in the dependent variable that is explained by the independent variables                                       |\\n| Regression            | Mean Squared Error (MSE)                                       | Average of the squared differences between the predicted and true values                                                                           |\\n| Regression            | Mean Absolute Error (MAE)                                      | Average of the absolute differences between the predicted and true values                                                                          |\\n| Regression            | Mean Absolute Percentage Error (MAPE)                          | Average of the absolute percentage differences between the predicted and true values                                                               |\\n| Regression            | Weighted Mean Absolute Percentage Error (WMAPE)                | The weighted average of the absolute percentage differences between the predicted and true values                                                  |\\n| Ranking               | Mean Average Precision (MAP)—for binary relevance ranking only | Measures the average precision of the relevant items in the top-k results                                                                          |\\n| Ranking               | Normalized Discounted Cumulative Gain (NDCG)                   | Measures the quality of',\n",
       " '---\\ntitle: \"Dashboards\"\\nslug: \"dashboards-platform\"\\nhidden: false\\ncreatedAt: \"2023-02-21T22:34:44.508Z\"\\nupdatedAt: \"2023-02-27T20:05:52.789Z\"\\n---\\n## Overview\\n\\nWith Fiddler, you can create comprehensive dashboards that bring together all of your monitoring data in one place. This includes monitoring charts for data drift, traffic, data integrity, and performance metrics. Adding monitoring charts to your dashboards lets you create a detailed view of your model\\'s performance. These dashboards can inform your team, management, or stakeholders, and make data-driven decisions that help improve your AI performance. \\n\\nView a list of the **[available metrics for monitoring charts here](doc:monitoring-charts-platform#supported-metric-types)**.\\n\\n## Dashboards Functionality\\n\\nDashboards offer a powerful way to analyze the overall health and performance of your models, as well as to compare multiple models. \\n\\n### Dashboard Filters\\n\\n- [Flexible filters](doc:dashboards-ui#dashboard-filters) including date range, time zone, and bin size to customize your view\\n\\n### Chart Utilities\\n\\n- [Leverage the chart toolbar ](doc:dashboard-interactions#zoom)to zoom into data and toggle between line and bar chart types\\n\\n### [Dashboard Basics](doc:dashboard-utilities)\\n\\n- Easily save, delete, or share your dashboard\\n- Click on a chart name to edit the base chart\\n- Remove and add monitoring charts to your dashboard\\n- Perform model-to-model comparison\\n- Plot drift or data integrity for multiple columns in one view\\n\\n![](https://files.readme.io/9bf5fc2-image.png)\\n\\nCheckout more on the [Dashboards UI Guide](doc:dashboards-ui).',\n",
       " 'slug: \"performance-tracking-platform\"  the ranking of the retrieved items, by discounting the relevance scores of items at lower ranks                            |\\n\\n## Why is it being tracked?\\n\\n- Model performance tells us how well a model is doing on its task. A poorly performing model can have significant business implications.\\n- The volume of decisions made on the basis of the predictions give visibility into the business impact of the model.\\n\\n## What steps should I take based on this information?\\n\\n- For decisions, if there is an increase or decrease in approvals, we can cross-check with the average prediction and prediction drift trendlines on the [Data Drift Tab](doc:data-drift). In general, the average prediction value should increase with an increase in the number of approvals, and vice-versa.\\n- For changes in model performance—again, the best way to cross-verify the results is by checking the [Data Drift Tab](doc:data-drift) ). Once you confirm that the performance issue is not due to the data, you need to assess if the change in performance is due to temporary factors, or due to longer-lasting issues.\\n- You can check if there are any lightweight changes you can make to help recover performance—for example, you could try modifying the decision threshold.\\n- Retraining the model with the latest data and redeploying it is usually the solution that yields the best results, although it may be time-consuming and expensive.\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Traffic\"\\nslug: \"traffic-platform\"\\nexcerpt: \"Platform Guide\"\\nhidden: false\\ncreatedAt: \"2022-12-19T19:28:11.378Z\"\\nupdatedAt: \"2023-08-04T23:21:11.689Z\"\\n---\\nTraffic as a service metric gives you basic insights into the operational health of your ML service in production.\\n\\n![](https://files.readme.io/d2c1eaa-Screenshot_2023-02-01_at_5.13.34_PM.png)\\n\\n## What is being tracked?\\n\\n- **_Traffic_** — The volume of traffic received by the model over time.\\n\\n## Why is it being tracked?\\n\\n- Traffic is a basic high-level metric that informs us of the overall system\\'s health.\\n\\n## What steps should I take when I see an outlier?\\n\\n- A dip or spike in traffic needs to be investigated. For example, a dip could be due to a production model server going down; a spike could be an adversarial attack.\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Alerts\"\\nslug: \"alerts-platform\"\\nhidden: false\\ncreatedAt: \"2023-01-27T19:53:56.493Z\"\\nupdatedAt: \"2023-08-04T23:20:46.288Z\"\\n---\\nFiddler enables users to set up alert rules to track a model\\'s health and performance over time. Fiddler alerts also enable users to dig into triggered alerts and perform root cause analysis to discover what is causing a model to degrade. Users can set up alerts using both the [Fiddler UI](https://docs.fiddler.ai/v1.6/docs/alerts-ui) and the [Fiddler API Client](https://docs.fiddler.ai/v1.6/docs/alerts-client).\\n\\n## Supported Metric Types\\n\\nYou can get alerts for the following metrics:\\n\\n- [**Data Drift**](doc:data-drift)  — Predictions and all features\\n  - Model performance can be poor if models trained on a specific dataset encounter different data in production.\\n- [**Performance**](doc:performance) \\n  - Model performance tells us how well a model is doing on its task. A poorly performing model can have significant business implications.\\n- [**Data Integrity**](doc:data-integrity)  — All features\\n  - There are three types of violations that can occur at model inference: missing feature values, type mismatches (e.g. sending a float input for a categorical feature type) or range mismatches (e.g. sending an unknown US State for a State categorical feature).\\n- [**Service Metrics**](doc:traffic-platform) \\n  - The volume of traffic received by the model over time that informs us of the overall system health.\\n\\n## Supported Comparison Types\\n\\nYou have two options for deciding when to be alerted:\\n\\n1. **Absolute** — Compare the metric to an absolute value\\n   1. e.g. if traffic for a given hour is less than 1000, then alert.\\n2. **Relative** — Compare the metric to a previous time period\\n   1. e.g. if traffic is down 10% or more than it was at the same time one week ago, then alert.\\n\\nYou can set the alert threshold in either case.\\n\\n## Alert Rule Priority\\n\\nWhether you\\'re setting up an alert rule to keep tabs on a model in a test environment, or data for production scenarios, Fiddler has you covered. Easily set the Alert Rule Priority to indicate the importance of any given Alert Rule. Users can select from Low, Medium, and High priorities. \\n\\n## Alert Rule Severity\\n\\nFor additional flexibility, users can now specify up to two threshold values, **Critical** and **Warning** severities. Critical severity is always required when setting up an Alert Rule, but Warning can be optionally set as well.\\n\\n## Why do we need alerts?\\n\\n- It’s not possible to manually track all metrics 24/7.\\n- Sensible alerts are your first line of defense, and they are meant to warn about issues in production.\\n\\n## What should I do when I receive an alert?\\n\\n- Click on the link in the email to go to the tab where the alert originated (e.g. Data Drift). \\n- Under the Monitoring tab, more information can be obtained from the drill down below the main chart.\\n- You can also examine the data in the Analyze tab. You can use SQL to slice and dice the data, and use custom visualization tools and operators to make sense of the model’s behavior within the time range under consideration.\\n\\n## Sample Alert Email\\n\\nHere\\'s a sample of an email that\\'s sent if an alert is triggered:\\n\\n!',\n",
       " '---\\ntitle: \"Evaluation\"\\nslug: \"evaluation-ui\"\\nexcerpt: \"UI Guide\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:24:53.469Z\"\\nupdatedAt: \"2023-06-12T19:09:54.672Z\"\\n---\\nModel performance evaluation is one of the key tasks in the ML model lifecycle. A model\\'s performance indicates how successful the model is at making useful predictions on data.\\n\\nOnce your trained model is loaded into Fiddler, click on **Evaluate** to see its performance.\\n\\n![](https://files.readme.io/2eac9b7-Model_Eval.png \"Model_Eval.png\")\\n\\n## Regression Models\\n\\nTo measure model performance for regression tasks, we provide some useful performance metrics and tools.\\n\\n![](https://files.readme.io/e7e7a01-Model_Regression.png \"Model_Regression.png\")\\n\\n- **_Root Mean Square Error (RMSE)_**\\n  - Measures the variation between the predicted and the actual value.\\n  - RMSE = SQRT[Sum of all observation (predicted value - actual value)^2/number of observations]\\n- **_Mean Absolute Error (MAE)_**\\n  - Measures the average magnitude of the error in a set of predictions, without considering their direction.\\n  - MAE = Sum of all observation[Abs(predicted value - actual value)]/number of observations\\n- **_Coefficient of Determination (R<sup>2</sup>)_**\\n  - Measures how much better the model\\'s predictions are than just predicting a single value for all examples.\\n  - R<sup>2</sup> = variance explained by the model / total variance\\n- **_Prediction Scatterplot_**\\n  - Plots the predicted values against the actual values. The more closely the plot hugs the y=x line, the better the fit of the model.\\n- **_Error Distribution_**\\n  - A histogram showing the distribution of errors (differences between model predictions and actuals). The closer to 0 the errors are, the better the fit of the model.\\n\\n## Classification Models\\n\\nTo measure model performance for classification tasks, we provide some useful performance metrics and tools.\\n\\n![](https://files.readme.io/b60acfb-Model_Classification.png \"Model_Classification.png\")\\n\\n- **_Precision_**\\n  - Measures the proportion of positive predictions which were correctly classified.\\n- **_Recall_**\\n  - Measures the proportion of positive examples which were correctly classified.\\n- **_Accuracy_**\\n  - Measures the proportion of all examples which were correctly classified.\\n- **_F1-Score_**\\n  - Measures the harmonic mean of precision and recall. In the multi-class classification case, Fiddler computes micro F1-Score.\\n- **_AUC_**\\n  - Measures the area under the Receiver Operating Characteristic (ROC) curve.\\n- **_Log Loss_**\\n  - Measures the performance of a classification model where the prediction input is a probability value between 0 and 1. The goal of the ML model is to minimize this value.\\n- **_Confusion Matrix_**\\n  - A table that shows how many predicted and actual values exist for different classes. Also referred as an error matrix.\\n- **_Receiver Operating Characteristic (ROC) Curve_**\\n  - A graph showing the performance of a classification model at different classification thresholds. Plots the true positive rate (TPR), also known as recall, against the false positive rate (FPR).\\n- **_Precision-Recall Curve_**\\n  - A graph that plots the precision against the recall for different classification thresholds.\\n- **_',\n",
       " 'slug: \"data-drift\" e-Monitor_DriftAnaly.png \"Monitor_DriftAnaly.png\")\\n\\n## Why is it being tracked?\\n\\n- Data drift is a great proxy metric for **performance decline**, especially if there is delay in getting labels for production events. (e.g. In a credit lending use case, an actual default may happen after months or years.)\\n- Monitoring data drift also helps you stay informed about **distributional shifts in the data for features of interest**, which could have business implications even if there is no decline in model performance.\\n\\n## What do I do next with this information?\\n\\n- High drift can occur as a result of _data integrity issues_ (bugs in the data pipeline), or as a result of _an actual change in the distribution of data_ due to external factors (e.g. a dip in income due to COVID). The former is more in our control to solve directly. The latter may not be solvable directly, but can serve as an indicator that further investigation (and possible retraining) may be needed.\\n- You can drill down deeper into the data by examining it in the Analyze tab. \\n\\nThe image below shows how to open the Analyze view for a specific feature and time range identified in the Data Drift page.\\n\\n![](https://files.readme.io/8a699e1-Monitor_DDrift_Analyze.png \"Monitor_DDrift_Analyze.png\")\\n\\nThis will bring you to the Analyze tab, where you can then use SQL to slice and dice the data.  You can then apply visualizations upon these slices to analyze the model’s behavior.\\n\\n![](https://files.readme.io/25eca03-Monitor_Analyze.png \"Monitor_Analyze.png\")\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions_\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"cv-monitoring\" _ID, \\'monitor\\']))\\n```\\n\\n*Please allow 3-5 minutes for monitoring data to populate the charts.*\\n  \\nThe following screen (without the annotation bubbles) will be available to you upon completion.\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://github.com/fiddler-labs/fiddler-examples/raw/main/quickstart/images/image_monitoring_drift.png\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n\\n\\n---\\n\\n\\n**Questions?**  \\n  \\nCheck out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\\n\\nJoin our [community Slack](http://fiddler-community.slack.com/) to ask any questions!\\n\\nIf you\\'re still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we\\'ll get back to you shortly.\\n',\n",
       " '---\\ntitle: \"Data Drift\"\\nslug: \"data-drift\"\\nexcerpt: \"UI Guide\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:25:14.478Z\"\\nupdatedAt: \"2023-02-14T01:18:50.009Z\"\\n---\\nModel performance can be poor if models trained on a specific dataset encounter different data in production. This is called data drift. In the **Monitor** tab for your model, Fiddler gives you a visual way to explore data drift and identify what data is drifting, when it’s drifting, and how it’s drifting. This is the first step in identifying possible model performance issues.\\n\\n![](https://files.readme.io/0d04342-Monitoring-DataDrift.png \"Monitoring-DataDrift.png\")\\n\\nYou can change the time range using the controls in the upper-right:\\n\\n![](https://files.readme.io/d5809f8-Monitoring-TimeRange.png \"Monitoring-TimeRange.png\")\\n\\n## What is being tracked?\\n\\n- **_Drift Metrics_**\\n  - **Jensen–Shannon distance (JSD)**\\n    - A distance metric calculated between the distribution of a field in the baseline dataset and that same distribution for the time period of interest.\\n    - For more information on JSD, click [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html).\\n  - **Population Stability Index (PSI)**\\n    - A drift metric based on the multinomial classification of a variable into bins or categories. The differences in each bin between the baseline and the time period of interest are then utilized to calculate it as follows:\\n\\n![](https://files.readme.io/0baeb90-psi_calculation.png \"psi_calculation.png\")\\n\\nHere, `B` is the total number of bins, `ActualProp(b)` is the proportion of counts within bin `b` from the target distribution, and `ExpectedProp(b)` is the proportion of counts within bin `b` from the reference distribution. Thus, PSI is a number that ranges from zero to infinity and has a value of zero when the two distributions exactly match.\\n\\n> 🚧 Note\\n> \\n> Since there is a possibility that a particular bin may be empty, PSI can shoot to infinity. To avoid this, PSI calculation in Fiddler is done such that each bin count is incremented with a base_count=1. Thus, there might be a slight difference in the PSI values obtained from manual calculations.\\n\\n- **_Average Values_** – The mean of a field (feature or prediction) over time. This can be thought of as an intuitive drift score.\\n- **_Drift Analytics_** – You can drill down into the features responsible for the prediction drift using the table at the bottom.\\n  - **_Feature Impact_**: The contribution of a feature to the model’s predictions, averaged over the baseline dataset. The contribution is calculated using random ablation feature impact.\\n  - **_Feature Drift_**: Drift of the feature, calculated using the drift metric of choice.\\n  - **_Prediction Drift Impact_**: A heuristic calculated using the product of the feature impact and the feature drift. The higher the score, the more this feature is likely to have contributed to the prediction drift.\\n\\nIn the Drift Analytics table, you can select a feature to see the feature distribution for both the time period under consideration and the baseline dataset. If it’s a numerical feature, you will also see a time series of the average feature value over time.\\n\\n![](https://files.readme.io/63a452',\n",
       " '---\\ntitle: \"Fraud Detection\"\\nslug: \"fraud-detection\"\\nexcerpt: \"How to monitor and improve your Fraud Detection ML Models using Fiddler\\'s AI Observability platform\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:06:54.951Z\"\\nupdatedAt: \"2023-08-04T23:19:48.015Z\"\\n---\\nMachine learning-based fraud detection models have been proven to be more effective than humans when it comes to detecting fraud. However, if left unattended, the performance of fraud detection models can degrade over time leading to big losses for the company and dissatisfied customers.  \\nThe **Fiddler AI Observability** platform provides a variety of tools that can be used to monitor, explain, analyze, and improve the performance of your fraud detection model.\\n\\n## Monitoring\\n\\n### Drift Detection\\n\\n- **Class-imbalanced Data** - Fraud use cases suffer from highly imbalanced data. Users can specify model weights on a global or event level to improve drift detection. Please see more information in  [Class-Imbalanced Data](https://docs.fiddler.ai/v1.3/docs/class-imbalanced-data). \\n\\n- **Feature Impact** - Tells us the contribution of features to the model\\'s prediction, averaged over the baseline dataset. The contribution is calculated using [random ablation feature impact](https://arxiv.org/pdf/1910.00174.pdf).\\n\\n- **Feature Drift** - Tells us how much a feature is drifting away from the baseline dataset for the time period of interest. For more information on how drift metrics are calculated, see [Data Drift](doc:data-drift-platform).\\n\\n- **Prediction Drift Impact** - A heuristic calculated by taking the product of Feature Impact and Feature Drift. The higher the score the more this feature contributed to the prediction value drift.\\n\\n### Performance Metrics\\n\\nAccuracy might not be a good measure of model performance in the case of fraud detection as most of the cases are non-fraud. Therefore, we use monitor metrics like: \\n\\n1. **Recall** - How many of the non-fraudulent cases were actually detected as fraud? A low recall value might lead to an increased number of cases for review even though all the fraud cases were predicted correctly.\\n2. **False Positive Rate** - Non-Fraud cases labeled as fraud, high FPR rate leads to dissatisfied customers.\\n\\n### Data Integrity\\n\\n- **Range Violations** - This metric shows the percentage of data in the selected production data that has violated the range specified in the baseline data through [`DatasetInfo`](https://api.fiddler.ai/#fdl-datasetinfo) API.\\n- **Missing Value Violations** - This metric shows the percentage of missing data for a feature in the selected production data.\\n- **Type Violations** - This metric shows the percentage of data in the selected production data that has violated the type specified in the baseline data through the DatasetInfo API.\\n\\n## Explanability\\n\\n### Point Overview\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/c7249cf-XAI21.gif\",\\n        \"XAI21.gif\",\\n        1083\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Point Overview\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nThis tab in the Fiddler AI Observability platform gives an overview for the data point selected. The prediction value for the point along with the strongest positive and negative feature attributions. We can choose from the explanation types. In the case of fraud detection, we can choose from SHAP, Fiddler SHAP, Mean',\n",
       " '---\\ntitle: \"Performance\"\\nslug: \"performance\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:25:22.895Z\"\\nupdatedAt: \"2023-02-14T01:18:55.377Z\"\\n---\\n## What is being tracked?\\n\\n![](https://files.readme.io/4a646d4-qs_monitoring.png \"qs_monitoring.png\")\\n\\n- **_Decisions_** - The post-prediction business decisions made as a result of the model output. Decisions are calculated before [client.publish_event()](ref:clientpublish_event) (they\\'re not inferred by Fiddler). For binary classification models, a decision is usually determined using a threshold. For multi-class classification models, it\\'s usually determined using the argmax value of the model outputs.\\n\\n- **_Performance metrics_**\\n  1. For binary classification models:\\n     - Accuracy\\n     - True Positive Rate/Recall\\n     - False Positive Rate\\n     - Precision\\n     - F1 Score\\n     - AUC\\n     - AUROC\\n     - Binary Cross Entropy\\n     - Geometric Mean\\n     - Calibrated Threshold\\n     - Data Count\\n     - Expected Calibration Error\\n  2. For multi-class classification models:\\n     - Accuracy\\n     - Log loss\\n  3. For regression models:\\n     - Coefficient of determination (R-squared)\\n     - Mean Squared Error (MSE)\\n     - Mean Absolute Error (MAE)\\n     - Mean Absolute Percentage Error (MAPE)\\n     - Weighted Mean Absolute Percentage Error (WMAPE)\\n  4. For ranking models:\\n     - Mean Average Precision (MAP)—for binary relevance ranking only\\n     - Normalized Discounted Cumulative Gain (NDCG)\\n\\n## Why is it being tracked?\\n\\n- Model performance tells us how well a model is doing on its task. A poorly performing model can have significant business implications.\\n- The volume of decisions made on the basis of the predictions give visibility into the business impact of the model.\\n\\n## What steps should I take based on this information?\\n\\n- For decisions, if there is an increase or decrease in approvals, we can cross-check with the average prediction and prediction drift trendlines on the [Data Drift Tab](doc:data-drift). In general, the average prediction value should increase with an increase in the number of approvals, and vice-versa.\\n- For changes in model performance—again, the best way to cross-verify the results is by checking the [Data Drift Tab](doc:data-drift) ). Once you confirm that the performance issue is not due to the data, you need to assess if the change in performance is due to temporary factors, or due to longer-lasting issues.\\n- You can check if there are any lightweight changes you can make to help recover performance—for example, you could try modifying the decision threshold.\\n- Retraining the model with the latest data and redeploying it is usually the solution that yields the best results, although it may be time-consuming and expensive.\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"',\n",
       " 'slug: \"fraud-detection\"  model\\n3. Monitoring data integrity issues that could harm the model performance\\n4. Investigating the features which have drifted/ compromised and analyzing them to mitigate the issue\\n5. Performing a root cause analysis to identify the exact cause and fix it\\n6. Diving into point explanations to identify how much the issue has an impact on a particular data point\\n7. Setting up alerts to make sure the issue does not happen again\\n\\nWe discovered there was an issue with the ‘Category’ column, wherein a new value was discovered in the production data. This led to the performance drop in the data likely due to the range violation. We suggest two steps to mitigate this issue:\\n\\n1. Setting up ‘alerts’ to identify similar issues in data integrity\\n2. Retraining the ML model after including the new data (with the ground truth labels) to teach the model of the new values\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\\\\n\"\\n}\\n[/block]',\n",
       " 'slug: \"fdlmetric\" =\\'some_org_name\\',\\n           project_id=\\'project-a\\',\\n           model_id=\\'binary_classification_model-a\\',\\n           name=\\'perf-gt-5prec-1hr-1d-ago\\',\\n           alert_type=AlertType.PERFORMANCE,\\n           metric=Metric.PRECISION, <---\\n           priority=Priority.HIGH,\\n           compare_to=\\'CompareTo.TIME_PERIOD,\\n           compare_period=ComparePeriod.ONE_DAY,\\n           compare_threshold=None,\\n           raw_threshold=None,\\n           warning_threshold=0.05,\\n           critical_threshold=0.1,\\n           condition=AlertCondition.GREATER,\\n           bin_size=BinSize.ONE_HOUR)]\\n```',\n",
       " '---\\ntitle: \"Data Drift\"\\nslug: \"data-drift-platform\"\\nexcerpt: \"Platform Guide\"\\nhidden: false\\ncreatedAt: \"2022-12-19T19:26:33.091Z\"\\nupdatedAt: \"2023-08-04T23:20:52.677Z\"\\n---\\nModel performance can be poor if models trained on a specific dataset encounter different data in production. This is called data drift. \\n\\n## What is being tracked?\\n\\nFiddler supports the following:\\n\\n- **_Drift Metrics_**\\n  - **Jensen–Shannon distance (JSD)**\\n    - A distance metric calculated between the distribution of a field in the baseline dataset and that same distribution for the time period of interest.\\n    - For more information on JSD, click [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html).\\n  - **Population Stability Index (PSI)**\\n    - A drift metric based on the multinomial classification of a variable into bins or categories. The differences in each bin between the baseline and the time period of interest are then utilized to calculate it as follows:\\n\\n> 🚧 Note\\n> \\n> There is a possibility that PSI can shoot to infinity. To avoid this, PSI calculation in Fiddler is done such that each bin count is incremented with a base_count=1. Thus, there might be a slight difference in the PSI values obtained from manual calculations.\\n\\n- **_Average Values_** – The mean of a field (feature or prediction) over time. This can be thought of as an intuitive drift score.\\n- **_Drift Analytics_** – You can drill down into the features responsible for the prediction drift using the table at the bottom.\\n  - **_Feature Impact_**: The contribution of a feature to the model’s predictions, averaged over the baseline dataset. The contribution is calculated using random ablation feature impact.\\n  - **_Feature Drift_**: Drift of the feature, calculated using the drift metric of choice.\\n  - **_Prediction Drift Impact_**: A heuristic calculated using the product of the feature impact and the feature drift. The higher the score, the more this feature is likely to have contributed to the prediction drift.\\n\\n## Why is it being tracked?\\n\\n- Data drift is a great proxy metric for **performance decline**, especially if there is delay in getting labels for production events. (e.g. In a credit lending use case, an actual default may happen after months or years.)\\n- Monitoring data drift also helps you stay informed about **distributional shifts in the data for features of interest**, which could have business implications even if there is no decline in model performance. \\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Traffic\"\\nslug: \"traffic-ui\"\\nexcerpt: \"UI Guide\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:25:31.308Z\"\\nupdatedAt: \"2023-02-14T01:19:05.392Z\"\\n---\\nTraffic as a service metric gives you basic insights into the operational health of your model\\'s service in production.\\n\\n![](https://files.readme.io/d2c1eaa-Screenshot_2023-02-01_at_5.13.34_PM.png)\\n\\n## What is being tracked?\\n\\n- **_Traffic_** — The volume of traffic received by the model over time.\\n\\n## Why is it being tracked?\\n\\n- Traffic is a basic high-level metric that informs us of the overall model\\'s usage.\\n\\n## What steps should I take when I see an outlier?\\n\\n- A dip or spike in traffic needs to be investigated. For example, a dip could be due to a production model server going down; a spike could be an adversarial attack.\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Fairness\"\\nslug: \"fairness-ui\"\\nexcerpt: \"UI Guide\"\\nhidden: false\\ncreatedAt: \"2022-12-20T17:16:23.668Z\"\\nupdatedAt: \"2022-12-20T17:27:52.786Z\"\\n---\\nIn the context of [intersectional fairness](doc:fairness#intersectional-fairness), we compute the [fairness metrics](doc:fairness#fairness-metrics) for each subgroup. The values should be similar among subgroups. If there exists some bias in the model, we display the min-max ratio, which takes the minimum value divided by the maximum value for a given metric. If this ratio is close to 1, then the metric is very similar among subgroups. The figure below gives an example of two protected attributes, Gender and Education, and the Equal Opportunity metric.\\n\\n![](https://files.readme.io/906df04-intersectional_metrics.svg \"intersectional_metrics.svg\")\\n\\nFor the[ Disparate Impact metric](doc:fairness#disparate-impact), we don’t display a min-max ratio but an absolute min. The intersectional version of this metric is a little different. For a given subgroup, take all possible permutations of 2 subgroups and then display the minimum. If the absolute minimum is greater than 80%, then all combinations are greater than 80%.\\n\\n## Model Behavior\\n\\nIn addition to the fairness metrics, we provide information about model outcomes and model performance for each subgroup. In the platform, you can see a visualization like the one below by default. You have the option to display the same numbers in a table for a deeper analysis.\\n\\n![](https://files.readme.io/e03e620-model_behavior_1.svg \"model_behavior_1.svg\")\\n\\n![](https://files.readme.io/ca0c5be-model_behavior_2.svg \"model_behavior_2.svg\")\\n\\n## Dataset Fairness\\n\\nFinally, we provide a section for dataset fairness, with a mutual information matrix and a label distribution. Note that this is a pre-modeling step.\\n\\n![](https://files.readme.io/c96f7fd-data_fairness.svg \"data_fairness.svg\")\\n\\nMutual information gives information about existing dependence in your dataset between the protected attributes and the remaining features. We are displaying Normalized Mutual Information (NMI). This metric is symmetric, and has values between 0 and 1, where 1 means perfect dependency.\\n\\n![](https://files.readme.io/a946365-mutual_info.svg \"mutual_info.svg\")\\n\\nFor more details about the implementation of the intersectional framework, please refer to this [research paper](https://arxiv.org/pdf/2101.01673.pdf).\\n\\n## Reference\\n\\n[^1]\\\\: USEEOC article on [_Discrimination By Type_](https://www.eeoc.gov/discrimination-type)  \\n[^2]\\\\:  USEEOC article on [_Intersectional Discrimination/Harassment_](https://www.eeoc.gov/initiatives/e-race/significant-eeoc-racecolor-casescovering-private-and-federal-sectors#intersectional)',\n",
       " '---\\ntitle: \"Monitoring Charts UI\"\\nslug: \"monitoring-charts-ui\"\\nhidden: false\\ncreatedAt: \"2023-02-23T23:06:54.406Z\"\\nupdatedAt: \"2023-05-24T17:50:37.837Z\"\\n---\\n## Getting Started:\\n\\nTo use Fiddler AI’s monitoring charts, navigate to the Charts tab in the top-level navigation bar on the Fiddler AI platform. Choose between opening a previously saved chart or creating a new chart.\\n\\n## Create a New Monitoring Chart\\n\\nTo create a new monitoring chart, click on the Add Chart button on the top right of the screen.\\n\\n![](https://files.readme.io/2c98736-image.png)\\n\\nSearch for and select the [project](doc:project-structure) to create the chart, and press Add Chart.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/5e5cc8e-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n## Chart Functions\\n\\n![](https://files.readme.io/c5b2029-image.png)\\n\\n### Chart Properties\\n\\nBefore the first save, you can change the project space to ensure you’re focusing on the right models and data. After confirming the project selection, you can choose to name and add a description to your chart.\\n\\n### Save & Share\\n\\nManually save your chart using the Save button on the top right corner of the chart studio. Copy a link to your chart and share it with other [fiddler accounts who have access](doc:inviting-users) to the project where the chart resides.\\n\\n### Global Undo & Redo\\n\\nEasily control the following actions with the undo and redo buttons:\\n\\n- Metric query selection\\n- Time range selections\\n- Time range selections\\n- Bin size selections\\n\\nTo learn how to undo actions taken using the chart toolbar, see the Toolbar information in the next section.\\n\\n## Chart Metric Queries & Filters\\n\\n### Metric Query\\n\\nA metric query enables you to define what model to focus on, and which metrics and columns to plot on your monitoring chart. To get started with the metric query, choose a model of choice. Note: only models within the same project as your chart are accessible.\\n\\nOnce a model is selected, choose a metric type from Performance, Data Drift, Data Integrity, or Traffic metrics and relevant metrics. For example, we may choose to chart accuracy for our binary classification model. \\n\\n![](https://files.readme.io/a46e656-image.png)\\n\\n\\n\\n\\n\\n### Charting Multiple Columns\\n\\nIf you choose to chart data drift or data integrity, you can choose to plot up to 20 different columns including outputs, inputs, and metadata columns. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/e7b2019-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"350px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n### Chart Filters & Capabilities\\n\\nThere are three major chart filter capabilities, chart filters, chart toolbar, and zoom slider.  \\nThey work together to enable you to best analyze the slices of data that may be worth investigating. \\n\\n![](https://files.readme.io/f58936d-image.png)\\n\\n### Filters\\n\\nYou can customize your chart view using time range, time zone, and bin size chart filters. The data range can be one of the pre-defined time ranges or a custom range. The bin size',\n",
       " 'slug: \"data-integrity-platform\"  the data, and try to find the root cause of the issues.\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"customer-churn-prediction\"  churn model drops due to range violation in one of the features. We can improve the performance by retraining the model with new data but before that we must perform mitigation actions which would help us in preemptively detecting the model performance degradation and inform our retraining frequency.\\n\\n#### Step 8 - Mitigation Actions\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/6190a63-churn-image13-mitigate.png\",\\n        \"churn-image13-mitigate.png\",\\n        1618\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Add to dashboard\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n1. **Add to dashboard**  \\n   We can add the chart generated to the dashboard by clicking on **Pin this chart** on the RHS of the Analyze tab. This would help us in monitoring importance aspects of the model.\\n\\n2. **Add alerts**  \\n   We can alert users to make sure we are notified the next time there is a performance degradation. For instance, in this example, there was a performance degradation due to range data integrity violation. To mitigate this, we can set up an alert which would notify us in case the percentage range violation exceeds a certain threshold (10% would be a good number in our case). We can also set up alerts on drift values for prediction etc. Check out this [link](doc:alerts-ui) to learn how to set up alerts on Fiddler platform.\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Dashboards\"\\nslug: \"dashboards-ui\"\\nhidden: false\\ncreatedAt: \"2023-02-21T22:35:31.234Z\"\\nupdatedAt: \"2023-02-27T20:04:02.598Z\"\\n---\\n## Creating Dashboards\\n\\nTo begin using our dashboard feature, navigate to the dashboard page by clicking on \"Dashboards\" from the top-level navigation bar. On the Dashboards page, you can choose to either select from previously created dashboards or create a new one. This simple process allows you to quickly access your dashboards and begin monitoring your models\\' performance, data drift, data integrity, and traffic.\\n\\n![](https://files.readme.io/570614f-image.png)\\n\\nWhen creating a new dashboard, it\\'s important to note that each dashboard is tied to a specific project space. This means that only models and charts associated with that project can be added to the dashboard. To ensure you\\'re working within the correct project space, select the desired project space before entering the dashboard editor page, then click \"Continue.\" This will ensure that you can add relevant charts and models to your dashboard.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/ef961be-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n## Add Monitoring Chart\\n\\nOnce you’ve created a dashboard, you can add previously saved monitoring charts that display these metrics over time, making it easy to track changes and identify patterns.\\n\\n![](https://files.readme.io/b862277-image.png)\\n\\nTo create a new monitoring chart for your dashboard, simply select \"New Monitoring Chart\" from the \"Add\" dropdown menu. For more information on creating and customizing monitoring charts, check out our Monitoring Charts UI Guide.\\n\\nIf you\\'d like to add an existing chart to your dashboard, select \"Saved Charts\" to display a full list of monitoring charts that are available in your project space. This makes it easy to quickly access and add the charts you need to your dashboard for monitoring and reporting purposes.\\n\\n![](https://files.readme.io/2c3857c-image.png)\\n\\nTo further customize your dashboard, you can select the saved monitoring charts of interest by clicking on their respective cards. For instance, you might choose to add charts for Accuracy, Drift, Traffic, and Range Violation to your dashboard for a more comprehensive model overview. By adding these charts to your dashboard, you can quickly access important metrics and visualize your model\\'s performance over time, enabling you to identify trends and patterns that might require further investigation.\\n\\n## Dashboard Filters\\n\\nThere are three main filters that can be applied to all the charts within dashboards, these include date range, time zone, and bin size. \\n\\n![](https://files.readme.io/0795752-image.png)\\n\\n### Date Range\\n\\nWhen the `Default` time range is selected, the data range, time zone, and bin size that each monitoring chart was originally saved with will be applied. This enables you to create a dashboard where each chart shows a unique filter set to highlight what matters to each team. Updating the date range will unlock the time zone and bin size filters. You can select from a number of predefined ranges or choose `Custom` to select a start and end date-time.\\n\\n![](https://files.readme.io/960262c-image.png)\\n\\n### Bin Size\\n\\nBin size controls the frequency at which data is displayed on your monitoring charts. You can select from the following bin sizes: `Hour`, `Day`, `',\n",
       " 'slug: \"explainability-with-model-artifact-quickstart-notebook\" models\\', MODEL_ID, \\'monitor\\']))\\n```\\n\\n*Please allow 3-5 minutes for monitoring data to populate the charts.*\\n\\n\\n\\n---\\n\\n\\n**Questions?**  \\n  \\nCheck out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\\n\\nIf you\\'re still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we\\'ll get back to you shortly.\\n',\n",
       " 'slug: \"fairness\"  compares the pass rate of two groups.\\n\\nThe pass rate is the rate of positive outcome for a given group. It is defined as follow:\\n\\n`pass rate = passed / (num of ppl in the group)`\\n\\nIf the decisions are fair, the pass rates should be the same.\\n\\n## Group Benefit\\n\\nGroup benefit aims to measure the rate at which a particular event is predicted to occur within a subgroup compared to the rate at which it actually occurs.\\n\\nMathematically, group benefit for a given group is defined as follows:\\n\\n`Group Benefit = (TP+FP) / (TP + FN).`\\n\\nGroup benefit equality compares the group benefit between two groups. If the two groups are treated equally, the group benefit should be the same.\\n\\n## Equal Opportunity\\n\\nEqual opportunity means that all people will be treated equally or similarly and not disadvantaged by prejudices or bias.\\n\\nMathematically, equal opportunity compares the true positive rate (TPR) between two groups. TPR is the probability that an actual positive will test positive. The true positive rate is defined as follows:\\n\\n`TPR = TP/(TP+FN)`\\n\\nIf the two groups are treated equally, the TPR should be the same.\\n\\n## Intersectional Fairness\\n\\nWe believe fairness should be ensured to all subgroups of the population. We extended the classical metrics (which are defined for two classes) to multiple classes. In addition, we allow multiple protected features (e.g. race _and_ gender). By measuring fairness along overlapping dimensions, we introduce the concept of intersectional fairness.\\n\\nTo understand why we decided to go with intersectional fairness, we can take a simple example. In the figure below, we observe that equal numbers of black and white people pass. Similarly, there is an equal number of men and women passing. However, this classification is unfair because we don’t have any black women and white men that passed, and all black men and white women passed. Here, we observe bias within subgroups when we take race and gender as protected attributes.\\n\\n![](https://files.readme.io/21f6b94-intersectional_fairness.svg \"intersectional_fairness.svg\")\\n\\nThe EEOC provides examples of past intersectional discrimination/harassment cases.[<sup>\\\\[2\\\\]</sup>](#reference)\\n\\n## Model Behavior\\n\\nIn addition to the fairness metrics, we provide information about model outcomes and model performance for each subgroup. \\n\\n## Dataset Fairness\\n\\nWe also provide a section for dataset fairness, with a mutual information matrix and a label distribution. Note that this is a pre-modeling step.\\n\\n**Mutual information **gives information about existing dependence in your dataset between the protected attributes and the remaining features. We are displaying Normalized Mutual Information (NMI). This metric is symmetric, and has values between 0 and 1, where 1 means perfect dependency.\\n\\nFor more details about the implementation of the intersectional framework, please refer to this [research paper](https://arxiv.org/pdf/2101.01673.pdf).\\n\\n## Reference\\n\\n[^1]\\\\: USEEOC article on [_Discrimination By Type_](https://www.eeoc.gov/discrimination-type)  \\n[^2]\\\\:  USEEOC article on [_Intersectional Discrimination/Harassment_](https://www.eeoc.gov/initiatives/e-race/significant-eeoc-racecolor-casescovering-private-and-federal-sectors#intersectional)',\n",
       " '---\\ntitle: \"Monitoring\"\\nslug: \"monitoring-platform\"\\nhidden: false\\ncreatedAt: \"2022-11-15T18:06:49.755Z\"\\nupdatedAt: \"2023-08-04T23:20:38.662Z\"\\n---\\nFiddler Monitoring helps you identify issues with the performance of your ML models after deployment. Fiddler Monitoring has five main features:\\n\\n1. **Data Drift**\\n2. **Performance**\\n3. **Data Integrity**\\n4. **Service Metrics**\\n5. **Alerts**\\n\\n## Integrate with Fiddler Monitoring\\n\\nIntegrating Fiddler monitoring is a four-step process:\\n\\n1. **Upload dataset**\\n\\n   Fiddler needs a dataset to be used as a baseline for monitoring. A dataset can be uploaded to Fiddler using our UI and Python package. For more information, see:\\n\\n   - [client.upload_dataset()](ref:clientupload_dataset) \\n\\n2. **Onboard model**\\n\\n   Fiddler needs some specifications about your model in order to help you troubleshoot production issues. Fiddler supports a wide variety of model formats. For more information, see:\\n\\n   - [client.add_model()](ref:clientadd_model) \\n\\n3. **Configure monitoring for this model**\\n\\n   You will need to configure bins and alerts for your model. These will be discussed in detail below.\\n\\n4. **Send traffic from your live deployed model to Fiddler**\\n\\n   Use the Fiddler SDK to send us traffic from your live deployed model.\\n\\n## Publish events to Fiddler\\n\\nIn order to send traffic to Fiddler, use the [`publish_event`](ref:clientpublish_event) API from the Fiddler SDK.\\n\\nThe `publish_event` API can be called in real-time right after your model inference. \\n\\nAn event can contain the following:\\n\\n- Inputs\\n- Outputs\\n- Target\\n- Decisions (categorical only)\\n- Metadata\\n\\nThese aspects of an event can be monitored on the platform.\\n\\n> 📘 Info\\n> \\n> You can also publish events as part of a batch call after the fact using the `publish_events_batch` API (click [here](ref:clientpublish_events_batch) for more information). In this case, you will need to send Fiddler the original event timestamps as to accurately populate the time series charts.\\n\\n## Updating events\\n\\nFiddler supports [partial updates of events](doc:updating-events) for your **target** column. This can be useful when you don’t have access to the ground truth for your model at the time the model\\'s prediction is made. Other columns can only be sent at insertion time (with `update_event=False`).\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"analytics-ui\"  details of your dataset to help you understand the data’s distribution and correlations.\\n\\nSelect a target variable to see the dependence between that variable and the others, measured by [mutual information (MI)](https://en.wikipedia.org/wiki/Mutual_information). A low MI is an indicator of low correlation between two variables, and can be used to decide if particular variables should be dropped from the model.\\n\\n![](https://files.readme.io/69f1a0a-Dataset_details_1.png \"Dataset_details_1.png\")\\n\\n![](https://files.readme.io/d3a97a8-Dataset_details_2.png \"Dataset_details_2.png\")\\n\\n![](https://files.readme.io/cd0b499-Dataset_details_3.png \"Dataset_details_3.png\")\\n\\n## Point Overview\\n\\n> 📘 Info\\n> \\n> To view this visualization, click on **Explain** (light bulb icon) for any row in the **Data** table.\\n\\nThis visualization provides a human-readable overview of a point explanation.\\n\\n![](https://files.readme.io/335714a-Explain_Overview.png \"Explain_Overview.png\")\\n\\n## Feature Attribution\\n\\n> 📘 Info\\n> \\n> To view this visualization, click on **Explain** (light bulb icon) for any row in the **Data** table.\\n\\nFeature attributions can help you understand which model inputs were responsible for arriving at the model output for a particulat prediction.\\n\\nWhen you want to check how the model is behaving for one prediction instance, use this visualization first.\\n\\nMore information is available on the [Point Explainability](doc:point-explainability) page.\\n\\n![](https://files.readme.io/08d409f-Explain_Chart.png \"Explain_Chart.png\")\\n\\n## Feature Sensitivity\\n\\n> 📘 Info\\n> \\n> To view this visualization, click on **Explain** (light bulb icon) for any row in the **Data** table.\\n\\nThis visualization helps you understand how changes in the model’s input values could impact the model’s prediction for this instance.\\n\\n**_ICE plots_**\\n\\nOn initial load, the visualization shows an Individual Conditional Expectation (ICE) plot for each model input.\\n\\n![](https://files.readme.io/ac5f0b2-WhatIF_Chart.png \"WhatIF_Chart.png\")\\n\\nICE plots shows how the model prediction is affected by changes in an input for a particular instance. They’re computed by changing the value of an input—while keeping all other inputs constant—and plotting the resulting predictions.\\n\\nRecall the [partial dependence plots](#partial-dependence-plot-pdp) discussed earlier, which showed the average effect of the feature across the entire slice. In essence, the PDP is the average of all the ICE plots. The PDP can mask interactions at the instance level, which an ICE plot will capture.\\n\\nYou can update any input value to see its impact on the model output, and then view the updated ICE plots for the changed input values.\\n\\nThis is a powerful technique for performing counterfactual analysis of a model prediction. When you plot the updated ICE plots, you’ll see two lines (or sets of bars in the case of categorical inputs).\\n\\nIn the image below, the solid line is the original ICE plot, and the dotted line is the ICE plot using the updated input values. Comparing these two sets of plots can help you understand if the model’s behavior changes as expected with a hypothetical model input.\\n\\n![](https://files.readme.io/9311aea-WhatIF_After.png \"WhatIF_After.png\")\\n\\n## Dashboard\\n\\nOnce visualizations are created, you can pin them to the',\n",
       " 'slug: \"fraud-detection\"  to Data Integrity Issues\\n\\n#### Step 1 - Setting up baseline and publishing production events\\n\\nPlease refer to our [`Quick Start Guide`](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/quickstart/Fiddler_Quick_Start_Guide.ipynb) for a step-by-step walkthrough of how to upload baseline and production data to the Fiddler platform.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/fa8ded4-DatasetReady2.gif\",\\n        \"DatasetReady2.gif\",\\n        1064\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Setting up baseline\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 2 - Monitor Drift\\n\\nOnce the production events are published, we can monitor drift for the model output in the ‘drift’ tab i.e. - pred_is_fraud, which is the probability value of a case being a fraud. Here we can see that the prediction value of pred_is_fraud increased from February 15 to February 16. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/2f4bd83-MonitorDrift2.jpg\",\\n        \"MonitorDrift2.jpg\",\\n        1221\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Monitor drift\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 3 - Monitor Performance Metrics\\n\\nNext, To check if the performance has degraded, we can check the performance metrics in the ‘Performance’ tab. Here we will monitor the ‘Recall’ and ‘FPR’ of the model. We can see that the recall has gone down and FPR has gone up in the same period.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/048968e-ModelPerformance1.png\",\\n        \"ModelPerformance1.png\",\\n        2624\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Performance Chart\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 4 - Data Integrity\\n\\nThe performance drop could be due to a change in the quality of the data. To check that we can go to the ‘Data Integrity’ tab to look for Missing Value Violations, Type Violations, Range Violations, etc. We can see the columns ‘Category’ suffers range violations. Since this is a ‘categorical’ column, there is likely a new value that the model did not encounter during training.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/eef991e-DataIntegrity1.png\",\\n        \"DataIntegrity1.png\",\\n        3260\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Data Integrity\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 5 - Check the impact of drift\\n\\nWe can go back to the ‘Data Drift’ tab to measure how much the data integrity issue has impacted the prediction. We can select the bin in which the drift increased. The table below shows the Feature Impact, Feature Drift, and Prediction Drift Impact values for the selected bin. We can see that even though the Feature Impact for ‘Category’ value is less than the ‘Amt’ (Amount) value, because of the drift, its Prediction Drift Impact is more. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io',\n",
       " 'slug: \"project-structure\" \\n\\nYou can collate specific visualizations under the Project Dashboard. After visualizations are created using the Model Analytics tool, you can pin them to the dashboard, which can then be shared with others.\\n\\n![](https://files.readme.io/b7cb9ce-Chart_Dashboard.png \"Chart_Dashboard.png\")\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_',\n",
       " 'slug: \"customer-churn-prediction\" /bb02793-churn-image2-monitor-performance-metrics.png\",\\n        \"churn-image2-monitor-performance-metrics.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Monitor Performance Metrics\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 4 - Data Integrity\\n\\nOur next step would be to check if this could be due to any data integrity issues. On navigating to the **Data Integrity** tab under the **Monitor** tab, we see that there has been a range violation. On selecting the bins which have the range violations, we notice it is due to the field `numofproducts`. \\n\\nIt is advised to check all the fields which cause data integrity violations. Since we see a range violation, we can check how much the data has drifted.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/5819966-churn-image3-data-integrity.png\",\\n        \"churn-image3-data-integrity.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"smart\",\\n      \"caption\": \"Data Integrity\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 5 - Check the impact of drift on ‘numofproducts’ features\\n\\nOur next step would be to go back to the **Data Drift** tab to measure the amount of drift in the field `numofproducts`. The drift is calculated using **Jensen Shannon Divergence**, which compares the distributions of the two data sets being compared. \\n\\nWe can select the bin where we see an increase in average value as well as drift. We see a significant increase in the `numofproducts` average value and drift. We can also see there is a difference in the distribution of the baseline and production data which leads to a drift. \\n\\nNext step could be to find out if the change in distribution was only for a subsection of data or was it due to other factors like time (seasonality etc.), fault in data reporting (sensor data), change in the unit in which the metric is reported etc.  \\nSeasonality could be observed by plotting the data across time (provided we have enough data), a fault in data reporting would mean missing values, and change in unit of data would mean change in values for all subsections of data.\\n\\nIn order to investigate if the change was only for a subsection of data, we will go to the **Analyze** tab. We can do this by clicking **Export bin and feature to Analyze**. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/1d1a5b3-churn-image4-impact-of-drift.png\",\\n        \"churn-image4-impact-of-drift.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Impact of Drift\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 6 - Root Cause Analysis in the ‘Analyze’ tab\\n\\nIn the analyze tab, we will have an auto-generated SQL query based on our selection in the **Monitor** tab, we can also write custom SQL queries to investigate the data. \\n\\nWe check the distribution of the field `numofproducts` for our selection. We can do this by selecting **Chart Type - Feature Distribution** on the RHS of the tab. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/9b1f7d1-Churn-image5-analyze-rca-1.png\",\\n        \"',\n",
       " 'slug: \"evaluation-ui\" Calibration Plot_**\\n  - A graph that tell us how well the model is calibrated. The plot is obtained by dividing the predictions into 10 quantile buckets (0-10th percentile, 10-20th percentile, etc.). The average predicted probability is plotted against the true observed probability for that set of points.',\n",
       " 'slug: \"product-tour\" )\\n\\n**Projects** represent your organization\\'s distinct AI applications or use cases. Within Fiddler, Projects house all the **Models** specific to a given application, and thus serve as a jumping-off point for the majority of Fiddler’s model monitoring and explainability features.\\n\\nGo ahead and click on the _Lending project_ to navigate to the Project Overview page.\\n\\n![](https://files.readme.io/b008f03-image.png)\\n\\nHere you can see a list of the models contained within the Lending project, as well as a project dashboard to which analyze charts can be pinned. Go ahead and click the “logreg-all” model.\\n\\n![](https://files.readme.io/f3e024d-image.png)\\n\\nFrom the Model Overview page, you can view details about the model: its metadata (schema), the files in its model directory, and its features, which are sorted by impact (the degree to which each feature influences the model’s prediction score).\\n\\nYou can then navigate to the platform\\'s core monitoring and explainability capabilities. These include:\\n\\n- **_Monitor_** — Track and configure alerts on your model’s performance, data drift, data integrity, and overall service metrics. Read the [Monitoring](doc:monitoring-platform) documentation for more details.\\n- **_Analyze_** — Analyze the behavior of your model in aggregate or with respect to specific segments of your population. Read the [Analytics](doc:analytics-ui) documentation for more details.\\n- **_Explain_** — Generate “point” or prediction-level explanations on your training or production data for insight into how each model decision was made. Read the [Explainability](doc:explainability-platform) documentation for more details.\\n- **_Evaluate_** — View your model’s performance on its training and test sets for quick validation prior to deployment. Read the [Evaluation](doc:evaluation-ui) documentation for more details.\\n\\n## Fiddler Samples\\n\\nFiddler Samples is a set of datasets and models that are preloaded into Fiddler. They represent different data types, model frameworks, and machine learning techniques. See the table below for more details.\\n\\n| **Project**   | **Model**                       | **Dataset** | **Model Framework** | **Algorithm**       | **Model Task**             | **Explanation Algos** |\\n| ------------- | ------------------------------- | ----------- | ------------------- | ------------------- | -------------------------- | --------------------- |\\n| Bank Churn    | Bank Churn                      | Tabular     | scikit-learn        | Random Forest       | Binary Classification      | Fiddler Shapley       |\\n| Heart Disease | Heart Disease                   | Tabular     | Tensorflow          |                     | Binary Classification      | Fiddler Shapley, IG   |\\n| IMDB          | Imdb Rnn                        | Text        | Tensorflow          | BiLSTM              | Binary Classfication       | Fiddler Shapley, IG   |\\n| Iris          | Iris                            | Tabular     | scikit-learn        | Logistic Regression | Multi-class Classification | Fiddler Shapley       |\\n| Lending       | Logreg-all                      | Tabular     | scikit-learn        | Logistic Regression | Binary Classification      | Fiddler Shapley       |\\n|               | Logreg-simple                   | Tabular     | scikit-learn        | Logistic Regression | Binary Classification      | Fiddler Shapley       |\\n|               | Xgboost-simple-sagemaker        | Tabular     | scikit-learn        | XGboost             | Binary Classification      | Fiddler Shapley       |\\n| Newsgroup     | Christianity',\n",
       " '---\\ntitle: \"Analytics and Evaluation\"\\nslug: \"analytics-eval-platform\"\\nexcerpt: \"Platform Guide\"\\nhidden: false\\ncreatedAt: \"2023-02-01T21:50:06.052Z\"\\nupdatedAt: \"2023-02-14T01:18:11.015Z\"\\n---\\n## Analytics\\n\\nFiddler’s industry-first model analytics tool, called Slice and Explain, allows you to perform an exploratory or targeted analysis of model behavior.\\n\\n1. **_Slice_** — Identify a selection, or slice, of data. Or, you can start with the entire dataset for global analysis.\\n2. **_Explain_** — Analyze model behavior on that slice using Fiddler’s visual explanations and other data insights.\\n\\nSlice and Explain is designed to help data scientists, model validators, and analysts drill down into a model and dataset to see global, local, or instance-level explanations for the model’s predictions.\\n\\nSlice and Explain can help you answer questions like:\\n\\n- What are the key drivers of my model output in a subsection of the data?\\n- How are the model inputs correlated to other inputs and to the output?\\n- Where is my model underperforming?\\n- How is my model performing across the classes in a protected group?\\n\\nAccess Slice and Explain from the Analyze tab for your model. Slice and Explain currently support all tabular models.\\n\\n**For details on how to use Fiddler Analytics through our interface check the [Analytics Page](doc:analytics-ui) on our UI Guide**\\n\\n## Evaluation\\n\\nModel performance evaluation is one of the key tasks in the ML model lifecycle. A model\\'s performance indicates how successful the model is at making useful predictions on data.\\n\\n**For details on how to use Fiddler Evaluation through our interface check the [Evaluation Page](doc:evaluation-ui) on our UI Guide**\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Fairness\"\\nslug: \"fairness\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:24:34.945Z\"\\nupdatedAt: \"2023-02-03T20:59:41.894Z\"\\n---\\n> 🚧 Note\\n> \\n> Model Fairness is in preview mode. Contact us for early access.\\n\\nFiddler provides powerful visualizations and metrics to detect model bias. Currently, _we support structured (tabular) models for classification tasks_ in both the Fiddler UI and the [API client](ref:about-the-fiddler-client). These visualizations are available for both production and dataset queries.\\n\\n## Definitions of Fairness\\n\\nModels are trained on real-world examples to mimic past outcomes on unseen data. The training data could be biased, which means the model will perpetuate the biases in the decisions it makes.\\n\\nWhile there is not a universally agreed upon definition of fairness, we define a ‘fair’ ML model as a model that does not favor any group of people based on their characteristics.\\n\\nEnsuring fairness is key before deploying a model in production. For example, in the US, the government prohibited discrimination in credit and real-estate transactions with fair lending laws like the Equal Credit Opportunity Act (ECOA) and the Fair Housing Act (FHAct).\\n\\nThe Equal Employment Opportunity Commission (EEOC) acknowledges 12 factors of discrimination:[<sup>\\\\[1\\\\]</sup>](#reference) age, disability, equal pay/compensation, genetic information, harassment, national origin, pregnancy, race/color, religion, retaliation, sex, sexual harassment. These are what we call protected attributes.\\n\\n## Fairness Metrics\\n\\nFiddler provides the following fairness metrics:\\n\\n- Disparate Impact\\n- Group Benefit\\n- Equal Opportunity\\n- Demographic Parity\\n\\nThe choice of the metric is use case-dependent. An important point to make is that it\\'s impossible to optimize all the metrics at the same time. This is something to keep in mind when analyzing fairness metrics.\\n\\n## Disparate Impact\\n\\nDisparate impact is a form of **indirect and unintentional discrimination**, in which certain decisions disproportionately affect members of a protected group.\\n\\nMathematically, disparate impact compares the pass rate of one group to that of another.\\n\\nThe pass rate is the rate of positive outcomes for a given group. It\\'s defined as follows:\\n\\npass rate = passed / (num of ppl in the group)\\n\\nDisparate impact is calculated by:\\n\\n`DI = (pass rate of group 1) / (pass rate of group 2)`\\n\\nGroups 1 and 2 are interchangeable. Therefore, the following formula can be used to calculate disparate impact:\\n\\n`DI = min{pr_1, pr_2} / max{pr_1, pr_2}.`\\n\\nThe disparate impact value is between 0 and 1. The Four-Fifths rule states that the disparate impact has to be greater than 80%.\\n\\nFor example:\\n\\n`pass-rate_1 = 0.3, pass-rate_2 = 0.4, DI = 0.3/0.4 = 0.75`\\n\\n`pass-rate_1 = 0.4, pass-rate_2 = 0.3, DI = 0.3/0.4 = 0.75`\\n\\n> 📘 Info\\n> \\n> Disparate impact is the only legal metric available. The other metrics are not yet codified in US law.\\n\\n## Demographic Parity\\n\\nDemographic parity states that the proportion of each segment of a protected class should receive the positive outcome at equal rates.\\n\\nMathematically, demographic parity',\n",
       " 'slug: \"fdldeploymentparams\" 2000               | 1200             |\\n| \\\\<400                    | 2800               | 1300             |\\n| \\\\<500                    | 2900               | 1500             |\\n\\n2. **User Uploaded guide**\\n\\nFor uploading your artifact model, refer to the table above and increase the memory number, depending on your model framework and complexity. Surrogate models use lightgbm framework. \\n\\nFor example, an NLP model for a TEXT input might need memory set at 1024 or higher and CPU at 1000.\\n\\n> 📘 Usage Reference\\n> \\n> See the usage with:\\n> \\n> - [add_model_artifact](ref:clientadd_model_artifact)\\n> - [add_model_surrogate](ref:clientadd_model_surrogate)\\n> - [update_model_artifact](ref:clientupdate_model_artifact)\\n> - [update_model_surrogate](ref:clientupdate_model_surrogate)\\n> \\n> Check more about the [Model Deployment](doc:model-deployment) feature set.',\n",
       " 'slug: \"useful-queries-for-root-cause-analysis\"   count(*)\\nFROM\\n  \"production.churn_classifier_test\"\\nGROUP BY\\n  geography\\n\\n```\\n\\n\\n\\n![](https://files.readme.io/cbc5c25-5.png)\\n\\n## 5. Frequency by Metadata\\n\\nWe can even query w.r.t to a metadata field. For example, if we consider gender to be a metadata column (specified in ModelInfo object), then we can obtain a frequency of events by the metadata field using the following query \\n\\n```sql\\nSELECT\\n  gender,\\n  count(*)\\nFROM\\n  \"production.churn_classifier_test\"\\nGROUP BY\\n  gender\\n\\n```\\n\\n\\n\\n![](https://files.readme.io/4e5a79d-6.png)\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Baselines\"\\nslug: \"fiddler-baselines\"\\nexcerpt: \"A baseline is a set of reference data that is used to compare the performance of our model for monitoring purposes.\"\\nhidden: false\\ncreatedAt: \"2023-01-19T22:47:23.862Z\"\\nupdatedAt: \"2023-05-09T18:32:22.276Z\"\\n---\\nA model needs a baseline dataset for comparing its performance and identifying any degradation. A baseline is a set of reference data that is used to compare with our current data. \\n\\nThe dataset that was used to train the model is often a good starting point for a baseline. For more in-depth analysis, we may want to use a specific time period or a rolling window of production events. \\n\\nIn Fiddler, **the default baseline for all monitoring metrics is the training dataset **that was associated with the model during registration. Use this default baseline if you do not anticipate any differences between training and production. [New baselines can be added to existing models using the Python client APIs](ref:add_baseline).',\n",
       " 'slug: \"fraud-detection\"  11, we can see that the output probability value was 0 (predicted as fraud according to the threshold of 0.5) but the actual value was ‘not fraud’. \\n\\nThe bulb icon will take us to the ‘Explain’ tab. Here we can see that the ‘category’ value contributed to the model predicting the case as ‘fraud’.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/16d1150-RCA7.png\",\\n        \"RCA7.png\",\\n        3330\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Point Explanation\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 7 - Actions\\n\\nWe discovered that the prediction drift and performance drop were due to the introduction of a new value in the ‘Category’ column. We can take steps so that we could identify this kind of issue in the future before it can result in business impact.\\n\\n##### Setting up Alerts\\n\\nIn the ‘Analyze’ tab, we can set up alerts to notify us of as soon as a certain data issue happens. For example, for the case we discussed, we can set up alerts as shown below to alert us when the range violation increases beyond a certain threshold (e.g.-5%).\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/f7ece9a-Alert2.png\",\\n        \"Alert2.png\",\\n        1386\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Setting up Alerts\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nThese alerts can further influence the retraining of the ML model, we can retrain the model including the new data so the newly trained model contains the ‘insurance’ category value. This should result in improved performance.\\n\\n#### Data Insights\\n\\nBelow we can see the confusion matrix for February 16, 2019 (before drift starts). We can observe a good performance with Recall at 100% and 0.1% FP\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/09c82f2-FraudInsights2.png\",\\n        \"FraudInsights2.png\",\\n        1574\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Slice Evaluation - Feb 17\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nBelow we can see the confusion matrix for February 17, 2019 (after drift starts). We can observe a performance drop with Recall at 50% and 9% FP\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/c1c6e39-FraudInsights1.png\",\\n        \"FraudInsights1.png\",\\n        1574\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Slice Evaluation - Feb 16\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n### Conclusion\\n\\nUndetected fraud cases can lead to losses for the company and customers, not to mention damage reputation and relationship with customers. The Fiddler AI Observability platform can be used to identify the pitfalls in your ML model and mitigate them before they have an impact on your business.\\n\\nIn this walkthrough, we investigated one such issue with a fraud detection model where a data integrity issue caused the performance of the ML model to drop. \\n\\nFiddler can be used to keep the health of your fraud detection model up by:  \\n\\n1. Monitoring the drift of the performance metric\\n2. Monitoring various performance metrics associated with the',\n",
       " 'slug: \"simple-nlp-monitoring-quick-start\"  adding timestamps\\n    )\\n```\\n\\n# 7. Get insights\\n\\n\\n**You\\'re all done!**\\n  \\nYou can now head to Fiddler URL and start getting enhanced observability into your model\\'s performance. Run the following code block to get your URL:\\n\\n\\n\\n\\n```python\\nprint(\\'/\\'.join([URL, \\'projects\\', PROJECT_ID, \\'models\\', MODEL_ID, \\'monitor\\']))\\n```\\n\\nIn particular, you can go to the monitoring tab in your Fiddler URL and check the resulting drift chart for the TF-IDF embedding vectors. Bellow is a sceernshot of the data drift chart after running this notebook on the [Fiddler demo](https://demo.fiddler.ai/) deployment. (Annotation bubbles are not generated by the Fiddler UI.)\\n\\n\\n--------\\n**Questions?**  \\n  \\nCheck out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\\n\\nJoin our [community Slack](http://fiddler-community.slack.com/) to ask any questions!\\n\\nIf you\\'re still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we\\'ll get back to you shortly.\\n\\n\\n```python\\n\\n```\\n',\n",
       " 'slug: \"clientrun_fairness\"     dataset_id=DATASET_ID,\\\\n    protected_features=protected_features,\\\\n    positive_outcome=positive_outcome,\\\\n    slice_query=slice_query\\\\n)\",\\n      \"language\": \"python\",\\n      \"name\": \"Usage - With a SQL Query\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"Return Type\",\\n    \"h-1\": \"Description\",\\n    \"0-0\": \"dict\",\\n    \"0-1\": \"A dictionary containing fairness metric results.\"\\n  },\\n  \"cols\": 2,\\n  \"rows\": 1\\n}\\n[/block]',\n",
       " 'slug: \"fraud-detection\" /328c6b6-DriftImpact1.png\",\\n        \"DriftImpact1.png\",\\n        3300\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Drift Impact\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe will now move on to check the difference between the production and baseline data for this bin. For this, we can click on ‘Export bin and feature to Analyze’. Which will land us on the Analyze tab.\\n\\n#### Step 6 - Root Cause Analysis in the ‘Analyze’ tab\\n\\nThe analyze tab pre-populated the left side of the tab with the query based on our selection. We can also write custom queries to slice the data for analysis.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/31a6110-RCA2.jpg\",\\n        \"RCA2.jpg\",\\n        1226\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Analyze Tab\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/a3e4b27-RCA3.png\",\\n        \"RCA3.png\",\\n        1660\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Analyze Query\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nOn the right-hand side of the tab we can build charts on the tabular data based on the results of our custom query. For this RCA we will build a ‘Feature Distribution’ chart on the ‘Category’ column to check the distinct values and also measure the percentage of each value. We can see there are 15 distinct values along with their percentages.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/4996cad-RCA4.png\",\\n        \"RCA4.png\",\\n        1634\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Feature Distribution - Production\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nNext, we will compare the Feature Distribution chart in production data vs the baseline data to find out about the data integrity violation. We can modify the query to obtain data for baseline data and produce a ‘Feature Distribution’ chart for the same.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/303c243-RCA5.png\",\\n        \"RCA5.png\",\\n        1600\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Feature Distribution - Baseline\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe can see that the baseline data has just 14 unique values and ‘insurance’ is not present in baseline data. This ‘Category’ value wasn’t present in the training data and crept in production data likely causing performance degradation.  \\nNext, we can perform a ‘point explanation’ for one such case where the ‘Category’ value was ‘Insurance’ and the prediction was incorrect to measure how much the ‘Category’ column contributed to the prediction by looking at its SHAP value.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/c1c1c81-RCA6.png\",\\n        \"RCA6.png\",\\n        1650\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Mislabelled Data Point\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe can click on the bulb sign beside the row to produce a point explanation. If we look at example',\n",
       " '---\\ntitle: \"Class-Imbalanced Data\"\\nslug: \"class-imbalanced-data\"\\nhidden: false\\ncreatedAt: \"2022-07-05T17:20:48.830Z\"\\nupdatedAt: \"2023-08-04T23:21:57.003Z\"\\n---\\n## Monitoring class-imbalanced models\\n\\nDrift is a measure of how different the production distribution is from the baseline distribution on which the model was trained. In practice, the distributions are approximated using histograms and then compared using divergence metrics like Jensen–Shannon divergence or Population Stability Index. Generally, when constructing the histograms, every event contributes equally to the bin counts.\\n\\nHowever, for scenarios with large class imbalance the minority class’ contribution to the histograms would be minimal. Hence, any change in production distribution with respect to the minority class would not lead to a significant change in the production histograms. Consequently, even if there is a significant change in distribution with respect to the minority class, the drift value would not change significantly.\\n\\nTo solve this issue, Fiddler monitoring provides a way for events to be weighted based on the class distribution. For such models, when computing the histograms, events belonging to the minority class would be up-weighted whereas those belonging to the majority class would be down-weighted.\\n\\nFiddler has implemented two solutions for class imbalance use cases.\\n\\n**Workflow 1: User provided global class weights**  \\nThe user computes the class distribution on baseline data and then provides the class weights via the Model-Info object.  \\nClass weights can either be manually entered by the user or computed from their dataset\\n\\n- Please refer to the API docs on how to [specify global class-weights](/reference/fdlweightingparams)\\n\\n- To tease out drift in a class-imbalanced fraud usecase checkout out the [class-imbalanced-fraud-notebook](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/Fiddler_Quickstart_Imbalanced_Data.ipynb)\\n\\n**Workflow 2: User provided event level weights**  \\nUser provides event level weights as a metadata column in baseline data and provides them while publishing events  \\nDetails:\\n\\n- Users will add an \"\\\\_\\\\_weight\" column in their model_info (must be a metadata type column, and must be nullable=True).\\n\\n- The reference data needs to have an \"\\\\_\\\\_weight\" column, which may never be all null/missing/NaN  weights; all rows must contain valid float values. We expect the user to enforce this assumption.\\n\\n- Note that the use of weighting parameters requires the presence of model outputs for both workflows in the baseline dataset.\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"point-explainability-platform\" - _The first number_ is the **attribution**. The sum of these values over all features will always equal the difference between the model prediction and a baseline prediction value.\\n\\n- _The second number_, the percentage in parentheses, is the **feature attribution divided by the sum of the absolute values of all the feature attributions**. This provides an easy to compare, relative measure of feature strength and directionality (notice that negative attributions have negative percentages) and is bounded by ±100%.\\n\\n> 📘 Info\\n> \\n> An input box labeled **“Top N”** controls how many attributions are visible at once.  If the values don’t add up as described above, it’s likely that weaker attributions are being filtered-out by this control.\\n\\nFinally, it’s important to note that **feature attributions combine model behavior with characteristics of the data distribution**.\\n\\n## Language (NLP) Models\\n\\nFor language models, Fiddler’s Point Explanation provides the word-level impact on the prediction score when using perturbative methods (SHAP and Fiddler); for the Integrated Gradients method, tokenization can be customized in your model’s `package.py` wrapper script. The explanations are interactive—edit the text, and the explanation updates immediately.\\n\\nHere is an example of an explanation of a prediction from a sentiment analysis model:\\n\\n![](https://files.readme.io/970a86b-NLP_Explain.png \"NLP_Explain.png\")\\n\\n## Point Explanation Methods: How to Quantify Prediction Impact of a Feature?\\n\\n**Introduction**\\n\\nOne strategy for explaining the prediction of a machine learning model is to measure the influence that each of its inputs have on the prediction made. This is called Feature Impact.\\n\\nTo measure Feature Impact, **additive attribution methods** can be quite powerful. Fiddler includes:\\n\\n- **SHAP** and **Fiddler SHAP**, which require only the ability to ask a model for predictions, and are thus suitable across all types of models; no knowledge of the model implementation is necessary.\\n- **Integrated Gradients**, a method that takes advantage of the gradient vector of the prediction, which is typically available in deep learning models, to efficiently explain complex models with large input dimensionality.\\n\\n**Additive Attributions**\\n\\nTo explain a prediction with an additive attribution method, we look at how individual features contribute to the _prediction difference_. The prediction difference is a comparison between the prediction as a point in feature space (we refer to this as the _explain-point_), and a counterfactual baseline position (or a distribution of positions), representing an uninteresting or typical model inference.\\n\\nEach feature is assigned a fraction of the prediction difference for which it is responsible. This fraction is called the feature attribution, and it’s what we show in our explanations.\\n\\nAdditive attribution methods have the following characteristics:\\n\\n- The sum of feature attributions always equals the prediction difference.\\n- Features that have no effect on a model’s prediction receive a feature attribution of zero.\\n- Features that have the identical effect receive the same attribution.\\n- Features with mutual information share the attribution for any effect that information has on the prediction.\\n\\nAdditionally, each of these methods takes into account interactions between the features (e.g. two features that have no effect individually but in combination change the model output). This is explicitly built into the Shapley value formalism, and is captured in the path integral over gradients in Integrated Gradients.\\n\\n**Shapley Values and their Approximation**\\n\\nThe Shapley value[<sup>\\\\[1\\\\]</sup>](#references) (proposed by Lloyd Shapley in 1953) is one way to derive feature attributions. Shapley values',\n",
       " 'slug: \"dashboards-ui\" Week`, or `Month`. \\n\\n> 📘 Note: Hour bin sizes are not supported for time ranges above 90 days.\\n> \\n> For example, if we select the `6M` data range, we see that the `Hourly` bin selection is disabled. This is disabled to avoid long computation and dashboard loading times.\\n\\n![](https://files.readme.io/93f7576-image.png)\\n\\n### Saved Model Updates\\n\\nIf you recently created or updated a saved chart and are not seeing the changes either on the dashboard itself or the Saved Charts list, click the refresh button on the main dashboard studio or within the saved charts list to reflect updates.\\n\\n![](https://files.readme.io/706c198-image.png)\\n\\n## Model Comparison\\n\\nWith our dashboard feature, you can also compare multiple models side-by-side, making it easy to see which models are performing the best and which may require additional attention. To create model-to-model comparison dashboards, ensure the models you wish to compare belong to the same project. Create the desired charts for each model and then add them to a single dashboard. By creating a single dashboard that tracks the health of all of your models, you can save time and simplify your AI monitoring efforts. With these dashboards, you can easily share insights with your team, management, or stakeholders, and ensure that everyone is up-to-date on your AI performance.\\n\\n![](https://files.readme.io/33b97ae-image.png)\\n\\n### Check [Dashboard Utilities ](doc:dashboard-utilities)and [Dashboard Interactions](doc:dashboard-interactions) pages for more info on dashboard usage.',\n",
       " '---\\ntitle: \"ML Platform Integrations\"\\nslug: \"ml-platform-integrations\"\\nhidden: false\\ncreatedAt: \"2022-06-22T14:27:52.893Z\"\\nupdatedAt: \"2022-06-22T14:27:52.893Z\"\\n---\\n',\n",
       " 'slug: \"fdlmetric\"  the ROC Curve\",\\n    \"12-0\": \"fdl.Metric.F1_SCORE\",\\n    \"12-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION)\",\\n    \"12-2\": \"F1 score\",\\n    \"13-0\": \"fdl.Metric.ECE\",\\n    \"13-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION)\",\\n    \"13-2\": \"Expected Calibration Error\",\\n    \"14-0\": \"fdl.Metric.R2\",\\n    \"14-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.REGRESSION)\",\\n    \"14-2\": \"R Squared\",\\n    \"15-0\": \"fdl.Metric.MSE\",\\n    \"15-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.REGRESSION)\",\\n    \"15-2\": \"Mean squared error\",\\n    \"16-0\": \"fdl.Metric.MAPE\",\\n    \"16-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.REGRESSION)\",\\n    \"16-2\": \"Mean Absolute Percentage Error\",\\n    \"17-0\": \"fdl.Metric.WMAPE\",\\n    \"17-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.REGRESSION)\",\\n    \"17-2\": \"Weighted Mean Absolute Percentage Error\",\\n    \"18-0\": \"fdl.Metric.MAE\",\\n    \"18-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.REGRESSION)\",\\n    \"18-2\": \"Mean Absolute Error\",\\n    \"19-0\": \"fdl.Metric.LOG_LOSS\",\\n    \"19-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.MULTICLASS_CLASSIFICATION)\",\\n    \"19-2\": \"Log Loss\",\\n    \"20-0\": \"fdl.Metric.MAP\",\\n    \"20-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.RANKING)\",\\n    \"20-2\": \"Mean Average Precision\",\\n    \"21-0\": \"fdl.Metric.MEAN_NDCG\",\\n    \"21-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.RANKING)\",\\n    \"21-2\": \"Normalized Discounted Cumulative Gain\"\\n  },\\n  \"cols\": 3,\\n  \"rows\": 22,\\n  \"align\": [\\n    \"left\",\\n    \"left\",\\n    \"left\"\\n  ]\\n}\\n[/block]\\n\\n```coffeescript Usage\\nimport fiddler as fdl\\n\\nclient.add_alert_rule(\\n    name = \"perf-gt-5prec-1hr-1d-ago\",\\n    project_name = \\'project-a\\',\\n    model_name = \\'binary_classification_model-a\\',\\n    alert_type = fdl.AlertType.PERFORMANCE,\\n    metric = fdl.Metric.PRECISION, <----\\n    bin_size = fdl.BinSize.ONE_HOUR, \\n    compare_to = fdl.CompareTo.TIME_PERIOD,\\n    compare_period = fdl.ComparePeriod.ONE_DAY,\\n    warning_threshold = 0.05,\\n    critical_threshold = 0.1,\\n    condition = fdl.AlertCondition.GREATER,\\n    priority = fdl.Priority.HIGH,\\n    notifications_config = notifications_config\\n)\\n```\\n```coffeescript Outputs\\n[AlertRule(alert_rule_uuid=\\'9b8711fa-735e-4a72-977c-c4c8b16543ae\\',\\n           organization_name',\n",
       " '---\\ntitle: \"Data Integrity\"\\nslug: \"data-integrity-platform\"\\nexcerpt: \"platform guide\"\\nhidden: false\\ncreatedAt: \"2022-12-19T18:33:03.797Z\"\\nupdatedAt: \"2023-08-04T23:21:25.682Z\"\\n---\\nML models are increasingly driven by complex feature pipelines and automated workflows that involve dynamic data. Data is transformed from source to model input, which can result in data inconsistencies and errors.\\n\\nThere are three types of violations that can occur at model inference: **missing feature values**, **type mismatches** (e.g. sending a float input for a categorical feature type) or **range mismatches** (e.g. sending an unknown US State for a State categorical feature).\\n\\nYou can track all these violations in the Data Integrity tab. The time series shown above tracks the violations of data integrity constraints set up for this model.\\n\\n## What is being tracked?\\n\\n![](https://files.readme.io/8a59eb0-Monitoring_DataIntegrity.png \"Monitoring_DataIntegrity.png\")\\n\\nThe time series above tracks the violations of data integrity constraints set up for this model.\\n\\n- **_Missing value violations_** — The percentage of missing value violations over all features for a given period of time.\\n- **_Type violations_** — The percentage of data type mismatch violations over all features for a given period of time.\\n- **_Range violations_** — The percentage of range mismatch violations over all features for a given period of time.\\n- **_All violating events_** — An aggregation of all the data integrity violations above for a given period of time.\\n\\n## Why is it being tracked?\\n\\n- Data integrity issues can cause incorrect data to flow into the model, which can lead to poor model performance and have a negative impact on the business or end-user experience. \\n\\n## How does it work?\\n\\nIt can be tedious to set up constraints for individual features when they number in the tens or hundreds. To avoid this, you can provide Fiddler with a baseline dataset that\\'s representative of the data you expect your model to infer on in production. This should be sampled from your model\\'s training set, and can be [uploaded to Fiddler using the Python API client](ref:clientupload_dataset).\\n\\nFiddler will automatically generate constraints based on the distribution of data in this dataset.\\n\\n- **Missing values**: If a feature has no missing values, then the data integrity violation will be set up to trigger when any missing values are seen. Similarly, if the feature has 50% of its values missing, then the data integrity violation will be set up to trigger when more than 50% of the values encountered are missing in a specified time range.\\n- **Type mismatch**: A data integrity violation will be triggered when the type of a feature value differs from what was specified for that feature in the baseline dataset.\\n- **Range mismatch**: For categorical features, a data integrity violation will be triggered when it sees any value other than the ones specified in the baseline. Similarly, for continuous variables, the violation will be triggered if the values are outside the range specified in the baseline.\\n\\n## What steps should I take with this information?\\n\\n- The visualization above informs us of the feature-wise breakdown of the violations. The raw counts of the violations are shown in parentheses.\\n- If there is a spike in violations, or an unexpected violation occurs (such as missing values for a feature that doesn’t accept a missing value), then a deeper examination of the feature pipeline may be required.\\n- You can also drill down deeper into the data by examining it in the **Analyze** tab. We can use SQL to slice and dice',\n",
       " 'slug: \"monitoring-charts-ui\"  selected controls the frequency for which the data is displayed. So selecting Day will show daily data over the date range selected.\\n\\n### Toolbar\\n\\nThe charts toolbar is made up of 5 functions:\\n\\n- Drag to zoom\\n- Reset zoom\\n- Toggle to a line chart\\n- Toggle to a bar chart\\n- Undo all toolbar actions\\n\\n> 📘 Note: If the zoom reset or toolbar undo is selected, this will also undo any actions taken with the zoom slider.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/0a9224c-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Zoom\\n\\nTo utilize the drag-to-zoom functionality, click on the associated icon, it should turn blue on selection. Once selected, move your mouse over the chart area and drag it to zoom into the data points of interest. If you want to return to the original view, you can leverage the Reset Zoom button, which is the icon directly to the right of the drag-to-zoom functionality.\\n\\n![](https://files.readme.io/ed8ef2b-image.png)\\n\\n#### Line & Bar Chart Toggle\\n\\nYou can switch between visualizing your chart as a line or bar chart using the toolbar icons. Click on the line chart icon on the top right of the chart to switch to the line chart view. Likewise, select the bar chart icon in the toolbar to switch to the bar chart view. However, note that these views are only temporary and any settings you specify using the toolbar will not be saved to the chart.\\n\\n![](https://files.readme.io/c8c0e79-image.png)\\n\\n#### Zoom Slider\\n\\nYou can also use the horizontal zoom bar to zoom, located at the base of the chart. Once you\\'ve identified the time range you want to focus on, you can use the zoom bar to drag the range across time. For instance, if you want to analyze your data week-by-week over the past six months, you can use the toolbar or horizontal zoom bar to zoom in on the desired time range and then click and drag the selected range using the base horizontal zoom bar.\\n\\n![](https://files.readme.io/c73c24c-image.png)\\n\\n### Breakdown Summary\\n\\nYou can easily visualize your charts\\' raw data as a table within the fiddler chart studio, or download the content as a CSV for further analysis. If you choose to chart multiple columns, as shown below, you can search for and sort by Model name, Metric name, Column name, or values for a specific date.\\n\\n![](https://files.readme.io/0ddc155-image.png)\\n\\n## Customize Tab\\n\\n The Customize tab enables users to adjust the scale and range of the y-axis on their monitoring charts. In the example below, we have adjusted the minimum value of the y-axis for the plotted traffic to make more use of the chart space. For values with large variance, logarithmic scale can be applied to more clearly analyze the chart.\\n\\n![](https://files.readme.io/1683a1f-image.png)',\n",
       " 'slug: \"customer-churn-prediction\"  the subset of Hawaii, we see a huge performance drop.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/974b118-churn-image8--analyze-rca-4.png\",\\n        \"churn-image8--analyze-rca-4.png\",\\n        1624\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 7\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nOn the contrary, we see a good performance for the subset of data without the ‘Hawaii’. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/ee29b35-churn-image6-analyze-rca-4-1-1.png\",\\n        \"churn-image6-analyze-rca-4-1-1.png\",\\n        924\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 8\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/fc54636-churn-image9--analyze-rca-5.png\",\\n        \"churn-image9--analyze-rca-5.png\",\\n        1606\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 9\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 7 - Measuring the impact of the ‘numofproducts’ feature\\n\\nIn order to measure the impact of features - `numofproducts`, we can navigate back to the **Monitor** tab. We can see that the prediction drift impact is highest for `numofproducts` due to its high drift value, which means it is contributing the most to the prediction drift.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/e78d838-churn-image10-impact1.png\",\\n        \"churn-image10-impact1.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Feature Impact - 1\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe can further measure the attribution of the feature - `numofproducts` for a single data point. We can select a data point which was incorrectly predicted to not churn (false negative). We can check point explanations for a point from the **Analyze** by running a query or from the **Explain** tab. Below we check point explanations for a data point form analyze tab by clicking the **bulb** symbol from the query results. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/026d5cb-churn-image11-impact2.png\",\\n        \"churn-image11-impact2.png\",\\n        1654\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Feature Impact - 2\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe see that the feature - `numofproducts` attributes significantly towards the data point being predicted not to churn.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/65fd05d-churn-image12-impact3.png\",\\n        \"churn-image12-impact3.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Feature Impact - 3\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe have seen that the performance of the',\n",
       " 'slug: \"fraud-detection\" -reset feature impact, Permutation Feature Impact.\\n\\nFor the data point chosen, ‘category’ has the highest positive attribution (35.1%), pushing the prediction value towards fraud, and ‘amt’ has the highest negative attribution(-45.8%), pushing the prediction value towards non-fraud.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/b4704f6-XAI11.png\",\\n        \"XAI11.png\",\\n        1807\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Explanation Type\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n### Feature Attribution\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/9b91f72-XAI22.gif\",\\n        \"XAI22.gif\",\\n        1078\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Feature Attribution\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nThe Feature Attribution tab gives us information about how much each feature can be attributed to the prediction value based on the Explanation Type chosen. We can also change the value of a particular feature to measure how much the prediction value changes.  \\nIn the example below we can see that on changing the value of feature ‘amt’ from 110 to 10k the prediction value changes from 0.001 to 0.577 (not fraud to fraud).\\n\\n### Feature Sensitivity\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/3fba7b9-XAI23.gif\",\\n        \"XAI23.gif\",\\n        1073\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Feature Sensitivity\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nThis tab plots the prediction value against the range of values for different features (top n user selected). We can change the value for any feature and measure the resulting prediction sensitivity plot of all other features against the initial sensitivity plot. \\n\\nOn reducing the value of the ‘amt’ feature below from 331 to 10, we can see that the final prediction sensitivity plot shows a prediction value \\\\< 0.5 for any value of ‘age’ and ‘unique_merchant_card’. This shows that a lower value for ‘amt’ will result in a prediction value close to 0 (non-fraud)\\n\\n## Make your Fraud Detections Model better with Fiddler!\\n\\nPlease refer to our [Colab Notebook](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/business-use-cases/fraud-detection/Fraud_Detection_Usecase_Fiddler.ipynb) for a walkthrough on how to get started with using Fiddler for your fraud detection use case and an interactive demo on usability.\\n\\n### Overview\\n\\nIt is often the case that a model’s performance will degrade over time. We can use the Fiddler AI Observability platform to monitor the model’s performance in production, look at various metrics and also provide explanations to predictions on various data points. In this walkthrough, we will look at a few scenarios common to a fraud model when monitoring for performance. We will show how you would:\\n\\n1. Get baseline and production data onto the Fiddler Platform\\n2. Monitor drift for various features\\n3. Monitor performance metrics associated with fraud detection like recall, false-positive rate\\n4. Monitor data integrity Issues like range violations\\n5. Provide point explanations to the mislabelled points\\n6. Get to the root cause of the issues\\n\\n### Example - Model Performance Degradation due',\n",
       " 'slug: \"alerts-ui\"  rules across any projects and model at a glance.\\n\\n![](https://files.readme.io/ec2fde7-image.png)\\n\\nA few high level details from the alert rule definition are displayed in the table, but users can select to view the full alert definition by selecting the overflow button (⋮) on the right-hand side of any Alert Rule record and clicking `View All Details`. \\n\\n![](https://files.readme.io/0e1dbdc-image.png)\\n\\nDelete an existing alert by clicking on the overflow button (⋮) on the right-hand side of any Alert Rule record and clicking `Delete`. To make any other changes to an Alert Rule, you will need to delete the alert and create a new one with the desired specifications. \\n\\n![](https://files.readme.io/eddf05e-image.png)\\n\\n## Visualizations\\n\\nThroughout the Alert Rules, Triggered Alerts, and Home pages users will see references to the monitors they set up. These visualizations include Alert Rule priority, threshold severities, and more.\\n\\n### Alert Rule Priority\\n\\nAlert rule priority allows users to specify how important an alert rule is to their workflows, learn more on the [Alerts Platform Guide](https://docs.fiddler.ai/v1.6/docs/alerts-platform).\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/4f87100-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"300px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n### Threshold Severity\\n\\nUsers can specify Warning and Critical thresholds as additional customization on their monitors, learn more on the [Alerts Platform Guide](https://docs.fiddler.ai/v1.6/docs/alerts-platform).\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/664e72e-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"300px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n### Alert Summary\\n\\nOn the Fiddler home page, users can get a summary glance of their triggered alerts, categorized by Alert Type. This view allows users to easily navigate to their degraded models.\\n\\n![](https://files.readme.io/3f76938-image.png)\\n\\n## View Triggered Alerts on Fiddler\\n\\nThe Triggered Alerts view gives a single pane of glass experience where you can view all triggered alerts across any Project and Model. Easily apply time filters to see alerts that fired in a desired range, or customize the table to only show columns that matter the most to you. This view aggregates all triggered alerts by alert rule, where the number of times a given alert rule has been triggered is called out by the `Count` column. Explore the triggered alerts further by clicking on the `Monitor` button to further diagnose your model and data.\\n\\n![](https://files.readme.io/30a5ab5-Screen_Shot_2022-10-03_at_3.39.32_PM.png)\\n\\n## Sample Alert Email\\n\\nHere\\'s a sample of an email that\\'s sent if an alert is triggered:\\n\\n![](https://files.readme.io/9dfc566-Monitor_Alert_Email_0710.png \"Monitor_Alert_Email_0710.png\")\\n\\n## Integrations\\n\\nThe Integrations tab is a read-only view of all the integrations your Admin has enabled for use. As of today, users can configure their Alert Rules to notify them via email or Pager Duty services.\\n\\n![](https://files.readme.io/7462149-image',\n",
       " 'slug: \"alerts-ui\" .png)\\n\\nAdmins can add new integrations by clicking on the setting cog icon in the main navigation bar and selecting the integration tab of interest.\\n\\n![](https://files.readme.io/6ee3027-Screen_Shot_2022-10-03_at_4.16.00_PM.png)\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n- Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Useful Queries for Root Cause Analysis\"\\nslug: \"useful-queries-for-root-cause-analysis\"\\nexcerpt: \"This page has an examples of queries which one can use in the **Analyze** tab to perform Root Cause Analysis of an issue or look at various aspect of the data.\"\\nhidden: false\\ncreatedAt: \"2022-09-07T14:46:59.340Z\"\\nupdatedAt: \"2023-02-14T01:20:01.435Z\"\\n---\\n## 1. Count of events from the previous day\\n\\nIn order to look at how many events were published from the previous/last publishing date, we can do it in two ways - \\n\\n### i. Jump from **Monitor** tab\\n\\nThis can be done in the following steps - \\n\\n1. In the monitor tab, click on the \\'jump to last event\\' button to get to the most recent event \\n2. Select the appropriate time bin, in this case, we can select **1D bin** to get day-wise aggregated data\\n3. Once we have the data in the chart, we can select the most recent bin\\n4. Select \\'Export bin and feature to analyze\\' to jump to analyze tab\\n\\n![](https://files.readme.io/54545c9-1a.png)\\n\\n5. In the analyze tab, query will be auto-populated based on the **Monitor** tab selection\\n6. Modify the query to count the number of events from the selection \\n\\n   ```sql\\n   SELECT\\n     count(*)\\n   FROM\\n     production.\"bank_churn\"\\n   WHERE\\n     fiddler_timestamp BETWEEN \\'2022-07-20 00:00:00\\'\\n     AND \\'2022-07-20 23:59:59\\'\\n   ```\\n\\n### ii. Using `date` function in Analyze tab\\n\\nTo know how many events were published on the last publishing day, we can use `date` function of SQL  \\nUse the following query to query number of events\\n\\n```sql\\nselect\\n  *\\nfrom\\n  \"production.churn_classifier_test\"\\nwhere\\n  date(fiddler_timestamp) = (\\n    select\\n      date(max(fiddler_timestamp))\\n    from\\n      \"production.churn_classifier_test\"\\n  )\\n```\\n\\n\\n\\n![](https://files.readme.io/2676acb-2.png)\\n\\n## 2. Number of events on last day by output label\\n\\nIf we want to check how many events were published on the last day by the output class, we can use the following query \\n\\n```sql SQL\\nselect\\n  churn,\\n  count(*)\\nfrom\\n  \"production.churn_classifier_test\"\\nwhere\\n  date(fiddler_timestamp) = (\\n    select\\n      date(max(fiddler_timestamp))\\n    from\\n      \"production.churn_classifier_test\"\\n  )\\ngroup by \\n  churn\\n```\\n\\n\\n\\n![](https://files.readme.io/29e443f-3.png)\\n\\n## 3. Check events with missing values\\n\\nIf you want to check events where one of the columns is has null values, you can use the `isnull` function. \\n\\n```sql\\nSELECT\\n  *\\nFROM\\n  production.\"churn_classifier_test\"\\nWHERE\\n  isnull(\"estimatedsalary\")\\nLIMIT\\n  1000\\n```\\n\\n\\n\\n![](https://files.readme.io/43c2eac-4.png)\\n\\n## 4. Frequency by Categorical column\\n\\nWe query w.r.t to a categorical field. For example, we can count the number of events by geography which is a categorical column using the following query \\n\\n```sql\\nSELECT\\n  geography,\\n',\n",
       " 'slug: \"point-explainability-platform\" .g. images, text), where differentiable deep-learning models typically excel, this method can be very performant (O(N) vs. the O(2^n) of the Shapley methods)\\n2. Attributions can be computed for intermediate layers within the model, providing fine-grained model diagnostics. This is naturally extensible to models with hybrid and multimodal inputs.\\n3. In comparison to local gradients and saliency methods, the IG path integral samples the large-scale behavior of the model and is resistant to amplifying noise in the possibly saturated region around the explain-point.\\n\\n## References\\n\\n1. <https://en.wikipedia.org/wiki/Shapley_value>\\n2. S. Lundberg, S Lee. “A Unified Approach to Interpreting Model Predictions.” NeurIPS, 2017 <http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf>\\n3. L. Merrick  and A. Taly “The Explanation Game: Explaining Machine Learning Models Using Shapley Values” <https://arxiv.org/abs/1909.08128>\\n4. M. Sundararajan, A. Taly, Q. Yan “Axiomatic Attribution for Deep Networks”  <http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf>\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"fdlmulticlassattributionexplanation\" ={\\'background_dataset_size\\': 120, \\n            \\'baseline_prediction\\': 0.32470778860007266, \\n            \\'explanation_ci\\': {\\n              \\'PetalLength\\': 0.03880995606758349,\\n              \\'PetalWidth\\': 0.011875959565576195,\\n              \\'SepalLength\\': 0.007051209191437458,\\n              \\'SepalWidth\\': 0.0031140002491094245\\n            },\\n            \\'explanation_ci_level\\': 0.95,\\n            \\'model_prediction\\': 0.696459162361538\\n           }\\n    )\\n  }\\n)\\n```',\n",
       " '---\\ntitle: \"ML Algorithms In Fiddler\"\\nslug: \"ds\"\\nhidden: true\\ncreatedAt: \"2022-11-18T22:11:48.747Z\"\\nupdatedAt: \"2022-11-18T22:12:58.704Z\"\\n---\\n',\n",
       " '---\\ntitle: \"About Models\"\\nslug: \"about-models\"\\nhidden: false\\ncreatedAt: \"2022-05-23T19:03:52.998Z\"\\nupdatedAt: \"2022-12-13T22:54:17.166Z\"\\n---\\nA model is a **representation of your machine learning model**. Each model must have an associated dataset to be used as a baseline for monitoring, explainability, and fairness capabilities.\\n\\nYou **do not need to upload your model artifact in order to onboard your model**, but doing so will significantly improve the quality of explanations generated by Fiddler.',\n",
       " '---\\ntitle: \"Datadog Integration\"\\nslug: \"datadog-integration\"\\nhidden: false\\ncreatedAt: \"2023-06-21T15:21:52.559Z\"\\nupdatedAt: \"2023-06-21T15:51:15.017Z\"\\n---\\nFiddler offers an integration with Datadog which allows Fiddler and Datadog customers to bring their AI Observability metrics from Fiddler into their centralized Datadog dashboards.  Additionally, a Fiddler license can now be procured through the [Datadog Marketplace](https://www.datadoghq.com/blog/tag/datadog-marketplace/). This [integration](https://www.datadoghq.com/blog/monitor-machine-learning-models-fiddler/) enables you to centralize your monitoring of ML models and the applications that utilize them within one unified platform.\\n\\n## Integrating Fiddler with Datadog\\n\\nInstructions for integrating Fiddler with Datadog can be found on the \"Integrations\" section of your Datadog console.  Simply search for \"Fiddler\" and follow the installation instructions provided on the \"Configure\" tab.  Please reach out to [support@fiddler.ai](mailto:support@fiddler.ai) with any issues or questions.\\n\\n![](https://files.readme.io/3f9dcd9-Screenshot_2023-06-21_at_10.28.17_AM.png)\\n\\n![](https://files.readme.io/9fa9503-Screenshot_2023-06-21_at_10.31.54_AM.png)\\n\\n![](https://files.readme.io/218dfc2-Screenshot_2023-06-21_at_10.45.14_AM.png)',\n",
       " 'slug: \"point-explainability\"  _The first number_ is the **attribution**. The sum of these values over all features will always equal the difference between the model prediction and a baseline prediction value.\\n\\n- _The second number_, the percentage in parentheses, is the **feature attribution divided by the sum of the absolute values of all the feature attributions**. This provides an easy to compare, relative measure of feature strength and directionality (notice that negative attributions have negative percentages) and is bounded by ±100%.\\n\\n> 📘 Info\\n> \\n> An input box labeled **“Top N”** controls how many attributions are visible at once.  If the values don’t add up as described above, it’s likely that weaker attributions are being filtered-out by this control.\\n\\nFinally, it’s important to note that **feature attributions combine model behavior with characteristics of the data distribution**.\\n\\n## Language (NLP) Models\\n\\nFor language models, Fiddler’s Point Explanation provides the word-level impact on the prediction score when using perturbative methods (SHAP and Fiddler); for the Integrated Gradients method, tokenization can be customized in your model’s `package.py` wrapper script. The explanations are interactive—edit the text, and the explanation updates immediately.\\n\\nHere is an example of an explanation of a prediction from a sentiment analysis model:\\n\\n![](https://files.readme.io/970a86b-NLP_Explain.png \"NLP_Explain.png\")\\n\\n## Point Explanation Methods: How to Quantify Prediction Impact of a Feature?\\n\\n**Introduction**\\n\\nOne strategy for explaining the prediction of a machine learning model is to measure the influence that each of its inputs have on the prediction made. This is called Feature Impact.\\n\\nTo measure Feature Impact, **additive attribution methods** can be quite powerful. Fiddler includes:\\n\\n- **SHAP** and **Fiddler SHAP**, which require only the ability to ask a model for predictions, and are thus suitable across all types of models; no knowledge of the model implementation is necessary.\\n- **Integrated Gradients**, a method that takes advantage of the gradient vector of the prediction, which is typically available in deep learning models, to efficiently explain complex models with large input dimensionality.\\n\\n**Additive Attributions**\\n\\nTo explain a prediction with an additive attribution method, we look at how individual features contribute to the _prediction difference_. The prediction difference is a comparison between the prediction as a point in feature space (we refer to this as the _explain-point_), and a counterfactual baseline position (or a distribution of positions), representing an uninteresting or typical model inference.\\n\\nEach feature is assigned a fraction of the prediction difference for which it is responsible. This fraction is called the feature attribution, and it’s what we show in our explanations.\\n\\nAdditive attribution methods have the following characteristics:\\n\\n- The sum of feature attributions always equals the prediction difference.\\n- Features that have no effect on a model’s prediction receive a feature attribution of zero.\\n- Features that have the identical effect receive the same attribution.\\n- Features with mutual information share the attribution for any effect that information has on the prediction.\\n\\nAdditionally, each of these methods takes into account interactions between the features (e.g. two features that have no effect individually but in combination change the model output). This is explicitly built into the Shapley value formalism, and is captured in the path integral over gradients in Integrated Gradients.\\n\\n**Shapley Values and their Approximation**\\n\\nThe Shapley value[<sup>\\\\[1\\\\]</sup>](#references) (proposed by Lloyd Shapley in 1953) is one way to derive feature attributions. Shapley values distribute',\n",
       " '---\\ntitle: \"ML Framework Examples\"\\nslug: \"ml-framework-examples\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:13:20.434Z\"\\nupdatedAt: \"2022-04-19T20:13:20.434Z\"\\n---\\n',\n",
       " 'slug: \"alerts-platform\" [](https://files.readme.io/9dfc566-Monitor_Alert_Email_0710.png \"Monitor_Alert_Email_0710.png\")\\n\\n## Integrations\\n\\nFiddler supports the following alert notification integrations:\\n\\n- Email\\n- Slack\\n- PagerDuty\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"point-explainability\" . images, text), where differentiable deep-learning models typically excel, this method can be very performant (O(N) vs. the O(2^n) of the Shapley methods)\\n2. Attributions can be computed for intermediate layers within the model, providing fine-grained model diagnostics. This is naturally extensible to models with hybrid and multimodal inputs.\\n3. In comparison to local gradients and saliency methods, the IG path integral samples the large-scale behavior of the model and is resistant to amplifying noise in the possibly saturated region around the explain-point.\\n\\n## References\\n\\n1. <https://en.wikipedia.org/wiki/Shapley_value>\\n2. S. Lundberg, S Lee. “A Unified Approach to Interpreting Model Predictions.” NeurIPS, 2017 <http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf>\\n3. L. Merrick  and A. Taly “The Explanation Game: Explaining Machine Learning Models Using Shapley Values” <https://arxiv.org/abs/1909.08128>\\n4. M. Sundararajan, A. Taly, Q. Yan “Axiomatic Attribution for Deep Networks”  <http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf>\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"point-explanations\"  space where distinct, individually relevant explanations might be important (e.g. “your loan application was rejected for a set of reasons when compared to applications in your region, and for another set of reasons when compared to applications with the same profession”).\\n\\nApproximate Shapley value methods can be used to explain nearly any model, since you only need to be able to ask the model for predictions at a variety of positions in the feature space.\\n\\n**Integrated Gradients**\\n\\nAnother additive attribution method: the Integrated Gradients method.\\n\\nFor models whose prediction is continuous and piecewise differentiable in the feature space, it can be useful to provide additional information through the gradient (slope vector) of a prediction.\\n\\nFiddler supports Integrated Gradients (IG)[<sup>\\\\[4\\\\]</sup>](#references). In this method, an approximate integral tabulates components of the slope along a linear path from baseline to explain-point, and attributes them to respective input features. This has several advantages:\\n\\n1. For models with very high dimensional feature volumes (e.g. images, text), where differentiable deep-learning models typically excel, this method can be very performant (O(N) vs. the O(2^n) of the Shapley methods)\\n2. Attributions can be computed for intermediate layers within the model, providing fine-grained model diagnostics. This is naturally extensible to models with hybrid and multimodal inputs.\\n3. In comparison to local gradients and saliency methods, the IG path integral samples the large-scale behavior of the model and is resistant to amplifying noise in the possibly saturated region around the explain-point.\\n\\n## References\\n\\n1. <https://en.wikipedia.org/wiki/Shapley_value>\\n2. S. Lundberg, S Lee. “A Unified Approach to Interpreting Model Predictions.” NeurIPS, 2017 <http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf>\\n3. L. Merrick  and A. Taly “The Explanation Game: Explaining Machine Learning Models Using Shapley Values” <https://arxiv.org/abs/1909.08128>\\n4. M. Sundararajan, A. Taly, Q. Yan “Axiomatic Attribution for Deep Networks”  <http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf>\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Global Explainability\"\\nslug: \"global-explainability\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:25:47.634Z\"\\nupdatedAt: \"2023-07-06T17:02:09.458Z\"\\n---\\nFiddler provides powerful visualizations to describe the impact of features in your model. Feature impact and importance can be found in either the Explain or Analyze tab.\\n\\nGlobal explanations are available in the UI for **structured (tabular)** and **natural language (NLP)** models, for both classification and regression. They are also supported via API using the Fiddler Python package. Global explanations are available for both production and baseline queries.\\n\\n## Tabular Models\\n\\nFor tabular models, Fiddler’s Global Explanation tool shows the impact/importance of the features in the model.\\n\\nTwo global explanation methods are available:\\n\\n- **_Feature impact_** — Gives the average absolute change in the model prediction when a feature is randomly ablated (removed).\\n- **_Feature importance_** — Gives the average change in loss when a feature is randomly ablated.\\n\\nFeature impact and importance are displayed as percentages of all attributions.\\n\\nThe following is an example of feature impact for a model predicting the likelihood of successful loan repayment:\\n\\n![](https://files.readme.io/2548d18-Global-Expln-Tabular.png \"Global-Expln-Tabular.png\")\\n\\n## Language (NLP) Models\\n\\nFor language models, Fiddler’s Global Explanation performs ablation feature impact on a collection of text samples, determining which words have the most impact on the prediction.\\n\\n> 📘 Info\\n> \\n> For speed performance, Fiddler uses a random corpus of 200 documents from the dataset. When using the [`run_feature_importance`](https://api.fiddler.ai/#client-run_feature_importance) function from the Fiddler API client, the argument `n_inputs` can be changed to use a bigger corpus of texts.\\n\\nTwo types of visualization are available:\\n\\n- **_Word cloud_** — Displays a word cloud of top 150 words from a collection of text for this model. Fiddler provides three options:\\n  - **Average change**: The average impact of a word in the corpus of documents. This takes into account the impact\\'s directionality.\\n  - **Average absolute feature impact**:  The average absolute impact of a word in the corpus of documents. This only takes the absolute impact of the word into account, and not its directionality.\\n  - **Occurrences**: The number of times a word is present in the corpus of text.\\n\\n- **_Bar chart_** — Displays the impact for the **Top N** words. By default, only words with at least 15 occurrences are displayed. This number can be modified in the UI and will be reflected in real time in the bar chart. Fiddler provides two options:\\n  - **Average change**: The average impact of a word in the corpus of documents. This takes into account the impact\\'s directionality. Since positive and negative directionalities can cancel out, Fiddler provides a histogram of the individual impact, which can be found by clicking on the word.\\n  - **Average absolute feature impact**: The average absolute impact of a word in the corpus of documents. This only takes the absolute impact of the word into account, and not its directionality.\\n\\nThe following image shows an example of word impact for a sentiment analysis model:\\n\\n![](https://files.readme.io/f02245d-Screen_Shot_2023-01-20_at_2.39.08_PM.png)\\n\\n[^1]\\\\:',\n",
       " '---\\ntitle: \"Analytics\"\\nslug: \"analytics-ui\"\\nexcerpt: \"UI Guide\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:24:49.443Z\"\\nupdatedAt: \"2023-04-06T22:23:54.929Z\"\\n---\\n## Interface\\n\\nThe **Analyze** tab has three parts:\\n\\n1. **_Slice Query box_** _(top-left)_ — Accepts a SQL query as input, allowing quick access to the slice.\\n2. **_Data table_** _(bottom-left)_ — Lets you browse instances of data returned by the query.\\n3. **_Explanations column_** _(right)_ — Allows you to view explanations for the slice and choose from a range of rich visualizations for different data insights.\\n\\n![](https://files.readme.io/f2daf80-S_E_Landing.png \"S_E_Landing.png\")\\n\\n**Workflow**\\n\\n1. Write a SQL query in the **Slice Query** box and click **Run**.\\n\\n![](https://files.readme.io/a76a852-S_E_Step_2.png \"S_E_Step_2.png\")\\n\\n2. View the data returned by the query in the **Data** table.\\n\\n![](https://files.readme.io/8771686-S_E_Step_3.png \"S_E_Step_3.png\")\\n\\n3. Explore a variety of visualizations using the **Explanations** column on the right.\\n\\n![](https://files.readme.io/3d16c4e-S_E_Step_4.png \"S_E_Step_4.png\")\\n\\n## SQL Queries\\n\\n![](https://files.readme.io/80c4bfe-S_E_First_Time.png \"S_E_First_Time.png\")\\n\\nThe **Slice Query** box lets you:\\n\\n1. Write a SQL query\\n2. Search and auto-complete field names (i.e. your dataset, the names of your inputs or outputs)\\n3. Run the SQL query\\n\\nIn the UI, you will see examples for different types of queries:\\n\\n- Example query to analyze your dataset:\\n\\n```\\nselect * from \"your_dataset_id\" limit 100\\n```\\n\\n\\n\\n- Example query to analyze your model:\\n\\n```\\nselect * from \"your_dataset_id.your_model_id\" limit 100\\n```\\n\\n\\n\\n- Example query to analyze production traffic:\\n\\n```\\nselect * FROM production.\"your_model_id\"\\nwhere fiddler_timestamp between \\'2020-10-20 00:00:00\\' AND \\'2020-10-20 12:00:00\\'limit 100\\n```\\n\\n\\n\\n> 🚧 Note\\n> \\n> Only read-only SQL operations are supported. Slices are auto-detected based on your model, dataset, and query. Certain SQL operations like aggregations and joins might not result in a valid slice.\\n\\n## Data\\n\\nIf the query successfully returns a slice, the results display in the **Data** table below the **Slice Query** box.\\n\\n![](https://files.readme.io/1d7dd42-S_E_Data.png \"S_E_Data.png\")\\n\\nYou can view all data rows and their values or download the data as a CSV file to plug it into another system. By clicking on **Explain** (light bulb icon) in any row in the table, you can access explanations for that individual input (more on this in the next section).\\n\\n## Explanations\\n\\nThe Analyze tab offers a variety of powerful visualizations to quickly let you analyze and explain slices of your dataset.\\n\\n1. [**Feature Correlation**](#feature-correlation) — View the correlation between model inputs and/or outputs.\\n2.',\n",
       " 'slug: \"point-explainability-platform\"  distribute the total payoff of a collaborative game across a coalition of cooperating players. They are computed by tabulating the average gain in payoff when a particular player is added to the coalition, over all coalition sizes and permutations of players.\\n\\nIn our case, we consider the “total gains” to be the prediction value, and a “player” is a single model feature. The collaborative “game” is all of the model features cooperating to form a prediction value.\\n\\nWhy do we create “coalitions” with only a subset of the features? In some scenarios, it may be appropriate to replace a feature with a zero value when removed from the coalition (e.g. text models where no mask token is available). In others (e.g. models with dense tabular inputs), values are swapped in from a reference distribution or baseline example as a zero value may have a specific meaning (like zero income on a credit application).\\n\\nShapley values have desirable properties including:\\n\\n- **_Linearity_**: If two games are combined, then the total gains correspond to the gains derived from a linear combination of the gains of each game.\\n- **_Efficiency_**: The sum of the values of all players equals the value of the grand coalition, so that all the gain is distributed among the players. In our case, the efficiency property says _the feature attributions should sum to the prediction value_. The attributions can be positive or negative, since a feature can raise or lower a predicted value.\\n\\n**Approximating Shapley Values**\\n\\nComputation of exact Shapley values can be extremely computationally expensive—in fact, exponentially so, in the number of input features. Fiddler makes two approximation methods available:\\n\\n- **SHAP**[<sup>\\\\[2\\\\]</sup>](#references) (SHapely Additive exPlanations) approximates Shapley values by sampling coalitions according to a combinatorially weighted kernel (compensating for the number of permutations of features in coalitions of different cardinality). It samples the feature space uniformly between baseline-like feature vectors and explain-point-like feature vectors. This has the effect of downsampling behavior in the immediate vicinity of the explain-point, a region where the model may be saturated or uniform in its prediction, and attributions may not be helpful.\\n- **Fiddler SHAP**[<sup>\\\\[3\\\\]</sup>](#references) builds on the SHAP approach and is optimized for computing distributions of Shapley values for each feature by comparing the explain-point against a distribution of baselines. This makes it possible to compute confidence intervals around the mean attribution for each feature and identify clusters in attribution space where distinct, individually relevant explanations might be important (e.g. “your loan application was rejected for a set of reasons when compared to applications in your region, and for another set of reasons when compared to applications with the same profession”).\\n\\nApproximate Shapley value methods can be used to explain nearly any model, since you only need to be able to ask the model for predictions at a variety of positions in the feature space.\\n\\n**Integrated Gradients**\\n\\nAnother additive attribution method: the Integrated Gradients method.\\n\\nFor models whose prediction is continuous and piecewise differentiable in the feature space, it can be useful to provide additional information through the gradient (slope vector) of a prediction.\\n\\nFiddler supports Integrated Gradients (IG)[<sup>\\\\[4\\\\]</sup>](#references). In this method, an approximate integral tabulates components of the slope along a linear path from baseline to explain-point, and attributes them to respective input features. This has several advantages:\\n\\n1. For models with very high dimensional feature volumes (e',\n",
       " '---\\ntitle: \"Release 22.12 Notes\"\\nslug: \"release-notes-2022-2-10\"\\ncreatedAt: \"2023-02-10T17:06:45.449Z\"\\nhidden: false\\n---\\nThis page enumerates the new features and updates in this release of the Fiddler platform.\\n\\n## Release of Fiddler platform version 22.12:\\n\\n- Scale & performance improvements\\n\\n- Alert on Metadata Columns\\n\\n- New API for updating existing model artifacts or surrogate models\\n\\n## What\\'s New and Improved:\\n\\n- **Scale and performance improvements for monitoring metrics**\\n  - Significant service refactoring for faster computing of monitoring metrics\\n\\n- **Support for setting monitoring alerts on metadata columns**\\n  - Ability to configure Data Drift and Data Integrity alerts on metadata columns\\n\\n- **Support for updating existing model artifacts or surrogate models to user-uploaded models**\\n  - The [`update_model_artifact`](ref:clientupdate_model_artifact) method allows you to modify existing surrogate or user-uploaded models with new user uploaded-models. This will be replacing the previously used `update_model` method\\n  - Read the [API Reference Documentation](https://docs.fiddler.ai/reference/clientupdate_model_artifact) to learn more\\n\\n### Client Version\\n\\nClient version 1.6 is required for the updates and features mentioned in this release.',\n",
       " 'slug: \"cv-monitoring\" model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION\\n# name of the column that contains ground truth\\ntarget = \\'target\\'\\n\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n    dataset_info=dataset_info,\\n    dataset_id=DATASET_ID,\\n    features=embedding_cols,\\n    target=target,\\n    outputs=CIFAR_CLASSES,\\n    custom_features=[CF1],\\n    model_task=model_task,\\n    description=\\'An example to showcase monitoring Image data using model embeddings.\\',\\n    categorical_target_class_details=list(CIFAR_CLASSES),\\n)\\nmodel_info\\n```\\n\\nNow we specify a unique model ID and use the client\\'s [add_model](https://docs.fiddler.ai/reference/clientadd_model) function to send this information to Fiddler.\\n\\n\\n```python\\nMODEL_ID = \\'resnet18\\'\\n\\nclient.add_model(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID,\\n    model_id=MODEL_ID,\\n    model_info=model_info,\\n)\\n```\\n\\n# 5. Inject data drift and publish production events\\n\\nNetx, we\\'ll inject data drift in form of blurring and brightness-reduction. The following cell illustrates these transforms.\\n\\n\\n```python\\ndrift_xform_lut = {\\n    \\'original\\': None,\\n    \\'blurred\\': get_blur_transforms(),\\n    \\'brightness_reduced\\': get_brightness_transforms(),\\n}\\nfor drift_type, xform in drift_xform_lut.items():\\n    cifar_testloader = get_cifar_dataloader(train_data=False, batch_size=32, image_transforms=xform)\\n    # get some test images\\n    dataiter = iter(cifar_testloader)\\n    images, labels = dataiter.next()\\n\\n    # show images\\n    print(f\\'Image type: {drift_type}\\')\\n    imshow(torchvision.utils.make_grid(images))\\n```\\n\\n### Publish events to Fiddler\\n\\nWe\\'ll publish events over past 3 weeks. \\n\\n- Week 1: We publish CIFAR-10 test set, which would signify no distributional shift\\n- Week 2: We publish **blurred** CIFAR-10 test set \\n- Week 3: We publish **brightness reduce** CIFAR-10 test set \\n\\n\\n```python\\nimport time\\n\\nfor i, drift_type in enumerate([\\'original\\', \\'blurred\\', \\'brightness_reduced\\']):\\n    week_days = 6\\n    xform = drift_xform_lut[drift_type]\\n    cifar_testloader = get_cifar_dataloader(train_data=False, batch_size=32, image_transforms=xform)\\n    prod_df = generate_embeddings(resnet_model, device, cifar_testloader)\\n    week_offset = (2-i)*7*24*60*60*1e3 \\n    day_offset = 24*60*60*1e3\\n    print(f\\'Publishing events with {drift_type} transformation for week {i}.\\')\\n    for day in range(week_days): \\n        now = time.time() * 1000\\n        timestamp = int(now - week_offset - day*day_offset)\\n        events_df = prod_df.sample(1000)\\n        events_df[\\'timestamp\\'] = timestamp\\n        client.publish_events_batch(\\n            project_id=PROJECT_ID,\\n            model_id=MODEL_ID,\\n            batch_source=events_df,\\n            timestamp_field=\\'timestamp\\',\\n        )\\n```\\n\\n## 6. Get insights\\n\\n**You\\'re all done!**\\n  \\nYou can now head to Fiddler URL and start getting enhanced observability into your model\\'s performance.\\n\\nRun the following code block to get your URL.\\n\\n\\n```python\\nprint(\\'/\\'.join([URL, \\'projects\\', PROJECT_ID, \\'models\\', MODEL',\n",
       " 'slug: \"ranking-model\" time_epoch\\'] = df_logs[\\'time_epoch\\'] + (float(time.time()) - df_logs[\\'time_epoch\\'].max())\\n```\\n\\nFor ranking, we need to ingest all events from a given Query ID together. To do that, we need to transform the data to a grouped format.  \\nYou can use the `convert_flat_csv_data_to_grouped` utility function to do the transformation.\\n\\n\\n\\n```python\\ndf_logs_grouped = fdl.utils.pandas.convert_flat_csv_data_to_grouped(input_data=df_logs, group_by_col=\\'srch_id\\')\\n```\\n\\n\\n```python\\ndf_logs_grouped.head(2)\\n```\\n\\n### 5.b Publish events\\n\\n\\n```python\\nclient.publish_events_batch(project_id=PROJECT_ID,\\n                            model_id=MODEL_ID,\\n                            batch_source=df_logs_grouped,\\n                            id_field=\\'event_id\\',\\n                            timestamp_field=\\'time_epoch\\')\\n```\\n\\n# 7. Get insights\\n\\n\\n**You\\'re all done!**\\n  \\nYou can now head to Fiddler URL and start getting enhanced observability into your model\\'s performance. Run the following code block to get your URL:\\n\\n\\n\\n\\n```python\\nprint(\\'/\\'.join([URL, \\'projects\\', PROJECT_ID, \\'models\\', MODEL_ID, \\'monitor\\']))\\n```\\n\\n*Please allow 3-5 minutes for monitoring data to populate the charts.*\\n\\n--------\\n**Questions?**  \\n  \\nCheck out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\\n\\nJoin our [community Slack](http://fiddler-community.slack.com/) to ask any questions!\\n\\nIf you\\'re still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we\\'ll get back to you shortly.\\n',\n",
       " '---\\ntitle: \"Point Explanations\"\\nslug: \"point-explanations\"\\nhidden: true\\ncreatedAt: \"2022-12-16T23:19:37.269Z\"\\nupdatedAt: \"2023-02-14T01:17:52.463Z\"\\n---\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"\",\\n    \"h-1\": \"Model Input Type\",\\n    \"h-2\": \"Default Reference Sice\",\\n    \"h-3\": \"Permutations\",\\n    \"0-0\": \"Point Explanations  \\\\nSHAP and Fiddler SHAP\",\\n    \"0-1\": \"Tabular\",\\n    \"0-2\": \"200\",\\n    \"0-3\": \"\",\\n    \"1-0\": \"\",\\n    \"1-1\": \"Text\",\\n    \"1-2\": \"200\",\\n    \"1-3\": \"\",\\n    \"2-0\": \"Global Explanations  \\\\nRandom Ablation Feature Impact (and Importance for models with tabular inputs)\",\\n    \"2-1\": \"Tabular\",\\n    \"2-2\": \"10K\",\\n    \"2-3\": \"\",\\n    \"3-0\": \"\",\\n    \"3-1\": \"Text\",\\n    \"3-2\": \"200\",\\n    \"3-3\": \"\"\\n  },\\n  \"cols\": 4,\\n  \"rows\": 4,\\n  \"align\": [\\n    \"left\",\\n    \"left\",\\n    \"left\",\\n    \"left\"\\n  ]\\n}\\n[/block]\\n\\n# How to Quantify Prediction Impact of a Feature?\\n\\nOne approach for explaining the prediction of a machine learning model is to measure the influence that each of its inputs has on the prediction made. This is called Feature Impact.\\n\\n- SHAP and Fiddler SHAP, using Kernel SHAP implementations, are game-theory based methods. They work for all models, because they only require the ability to ask a model for predictions.\\n- Integrated Gradients, which is particularly performant for deep learning models with a large number of inputs. It requires the model’s prediction to be mathematically differentiable, and a prediction gradient must be made available to Fiddler.\\n- Tree SHAP, is not enabled by default but can be used for Tree-based model. This is a faster and model-specific method to approximate Shapley values.\\n\\nThese methods are discussed in more detail below.\\n\\nIn addition to the previous out of the box explanation methods, Fiddler allows to bring your own explanation method. This can be customized in your model’s `package.py` wrapper script.\\n\\n## Tabular Models\\n\\n## Language (NLP) Models\\n\\n## Point Explanation Methods:\\n\\n**Introduction**\\n\\nTo measure Feature Impact, **additive attribution methods** can be quite powerful. Fiddler includes:\\n\\n- **SHAP** and **Fiddler SHAP**, which require only the ability to ask a model for predictions, and are thus suitable across all types of models; no knowledge of the model implementation is necessary.\\n- **Integrated Gradients**, a method that takes advantage of the gradient vector of the prediction, which is typically available in deep learning models, to efficiently explain complex models with large input dimensionality.\\n\\n**Additive Attributions**\\n\\nTo explain a prediction with an additive attribution method, we look at how individual features contribute to the _prediction difference_. The prediction difference is a comparison between the prediction as a point in feature space (we refer to this as the _explain-point_), and a counterfactual baseline position (or a distribution of positions), representing an uninteresting or typical model inference.\\n\\nEach feature is assigned a fraction of the',\n",
       " '---\\ntitle: \"Data Integrity\"\\nslug: \"data-integrity\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:25:27.271Z\"\\nupdatedAt: \"2023-07-28T18:52:17.679Z\"\\n---\\nML models are increasingly driven by complex feature pipelines and automated workflows that involve dynamic data. Data is transformed from source to model input, which can result in data inconsistencies and errors.\\n\\nThere are three types of violations that can occur at model inference: **missing feature values**, **type mismatches** (e.g. sending a float input for a categorical feature type) or **range mismatches** (e.g. sending an unknown US State for a State categorical feature).\\n\\nYou can track all these violations in the Data Integrity tab. \\n\\n## What is being tracked?\\n\\n![](https://files.readme.io/8a59eb0-Monitoring_DataIntegrity.png \"Monitoring_DataIntegrity.png\")\\n\\nThe time series above tracks the violations of data integrity constraints set up for this model.\\n\\n- **_Missing value violations_** — The percentage of missing value violations over all features for a given period of time.\\n- **_Type violations_** — The percentage of data type mismatch violations over all features for a given period of time.\\n- **_Range violations_** — The percentage of range mismatch violations over all features for a given period of time.\\n- **_All violating events_** — An aggregation of all the data integrity violations above for a given period of time.\\n\\n## Why is it being tracked?\\n\\n- Data integrity issues can cause incorrect data to flow into the model, which can lead to poor model performance and have a negative impact on the business or end-user experience. \\n\\n## How does it work?\\n\\nIt can be tedious to set up constraints for individual features when they number in the tens or hundreds. To avoid this, you can provide Fiddler with a baseline dataset that\\'s representative of the data you expect your model to infer on in production. This should be sampled from your model\\'s training set, and can be [uploaded to Fiddler using the Python API client](ref:clientupload_dataset).\\n\\nFiddler will automatically generate constraints based on the distribution of data in this dataset.\\n\\n- **Missing values**: If a feature has no missing values, then the data integrity violation will be set up to trigger when any missing values are seen. Similarly, if the feature has 50% of its values missing, then the data integrity violation will be set up to trigger when more than 50% of the values encountered are missing in a specified time range.\\n- **Type mismatch**: A data integrity violation will be triggered when the type of a feature value differs from what was specified for that feature in the baseline dataset.\\n- **Range mismatch**: For categorical features, a data integrity violation will be triggered when it sees any value other than the ones specified in the baseline. Similarly, for continuous variables, the violation will be triggered if the values are outside the range specified in the baseline.\\n\\n## What steps should I take with this information?\\n\\n- The visualization above informs us of the feature-wise breakdown of the violations. The raw counts of the violations are shown in parentheses.\\n- If there is a spike in violations, or an unexpected violation occurs (such as missing values for a feature that doesn’t accept a missing value), then a deeper examination of the feature pipeline may be required.\\n- You can also drill down deeper into the data by examining it in the **Analyze** tab. We can use SQL to slice and dice the data, and try to find the root cause of the issues.\\n\\n**Reference**\\n\\n- See our article on [_The',\n",
       " '---\\ntitle: \"Customer Churn Prediction\"\\nslug: \"customer-churn-prediction\"\\nhidden: false\\ncreatedAt: \"2022-05-17T19:12:12.382Z\"\\nupdatedAt: \"2023-08-04T23:20:11.330Z\"\\n---\\nChurn prediction is a common use case in the machine learning domain. Churn means “leaving the company”. It is very critical for a business to have an idea about why and when customers are likely to churn. Having a robust and accurate churn prediction model helps businesses to take action to prevent customers from leaving the company. Machine learning models have proved to be effective in detecting churn. However, if left unattended, the performance of churn models can degrade over time leading to losing customers. \\n\\nThe Fiddler AI Observability platform provides a variety of tools that can be used to monitor, explain, analyze, and improve the performance of your machine learning-based churn model.\\n\\nIn this article we will go over a churn example and how we can mitigate performance degradation in a churn machine learning model.\\n\\nRefer to the [colab notebook](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/business-use-cases/churn-usecase/Fiddler_Churn_Use_Case.ipynb) to learn how to -\\n\\n1. Onboard model on the Fiddler platform\\n2. Publish events on the Fiddler platform\\n3. Use the Fiddler API to run explanations\\n\\n### Example - Model Performance Degradation due to Data Integrity Issues\\n\\n#### Step 1 - Setting up baseline and publishing production events\\n\\nPlease refer to our [Getting Started guide](https://docs.fiddler.ai/pages/getting-started/product-tour/) for a step-by-step walkthrough of how to upload baseline and production data to the Fiddler platform.\\n\\n#### Step 2 - Monitor Drift\\n\\nWhen we check the monitoring dashboard, we notice a drop in the predicted churn value and a rise in the predicted churn drift value. Our next step is to check if this has resulted in a drop in performance.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/2f20fd2-Churn-image1-monitor-drift.png\",\\n        \"Churn-image1-monitor-drift.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Monitor Drift\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 3 - Monitor Performance Metrics\\n\\nWe use **precision, recall, and F1-score** as accuracy metrics for this example. We’re choosing these metrics as they are suited for classification problems and help us in identifying the number of false positives and false negatives. We notice that although the precision has remained constant, there is a drop in the F1-score and recall, which means that there are a few customers who are likely to churn but the model is not able to predict their outcome correctly. \\n\\nThere could be a number of reasons for drop in performance, some of them are-\\n\\n1. Cases of extreme events (Outliers)\\n2. Data distribution changes\\n3. Model/Concept drift\\n4. Pipeline health issues\\n\\nWhile **Pipeline health issues** could be due to a component in the Data pipeline failing, the first 3 could be due to changes in data. In order to check that we can go to the **Data Integrity** tab to first check if the incoming data is consistent with the baseline data.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io',\n",
       " 'slug: \"customer-churn-prediction\" Churn-image5-analyze-rca-1.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 1\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe further check the performance of the model for our selection by selecting the **Chart Type - Slice Evaluation**.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/350aef8-Churn-image6-analyze-rca-2.png\",\\n        \"Churn-image6-analyze-rca-2.png\",\\n        1578\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 2\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nIn order to check if the change in the range violation has occurred for a subsection of data, we can plot it against the categorical variable. In our case, we can check distribution of `numofproducts` against `age` and `geography`. For this we can plot a feature correlation plot for two features by querying data and selecting **Chart type - Feature Correlation**.\\n\\nOn plotting the feature correlation plot of `gender` vs `numofprodcuts`, we observe the distribution to be similar.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/4f73274-churn-image6-analyze-rca-2-1.png\",\\n        \"churn-image6-analyze-rca-2-1.png\",\\n        512\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 3\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/aff3dbf-churn-image6-analyze-rca-2-2.png\",\\n        \"churn-image6-analyze-rca-2-2.png\",\\n        464\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 4\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nFor the sake of this example, let’s say that state of Hawaii (which is a value in the `geography` field in the data) announced that it has eased restrictions on number of loans, since loans is one of products, our hypothesis is the `numofproducts` would be higher for the state. To test this we will check the feature correlation between `geography` and `numofproducts`.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/e1c31a1-churn-image6-analyze-rca-2-3.png\",\\n        \"churn-image6-analyze-rca-2-3.png\",\\n        463\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 5\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe do see higher values for the state of Hawaii as compared to other states. We can further check distribution for the field `numofproducts` just for the state of Hawaii. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/6664850-churn-image7--analyze-rca-3.png\",\\n        \"churn-image7--analyze-rca-3.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 6\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nOn checking performance for',\n",
       " '---\\ntitle: \"Vector Monitoring\"\\nslug: \"vector-monitoring-platform\"\\nexcerpt: \"\\\\\"Patent Pending\\\\\"\"\\nhidden: false\\ncreatedAt: \"2022-12-19T19:22:52.779Z\"\\nupdatedAt: \"2023-08-04T22:36:41.836Z\"\\n---\\n# Vector Monitoring for Unstructured Data\\n\\nWhile Fiddler calculates data drift at deployment time for numerical features that are stored in columns of the baseline dataset, many modern machine learning systems use input features that cannot be represented as a single number (e.g., text or image data). Such complex features are usually rather represented by high-dimensional vectors which are obtained by applying a vectorization method (e.g., text embeddings generated by NLP models). Furthermore, Fiddler users might be interested in monitoring a group of univariate features together and detecting data drift in multi-dimensional feature spaces.\\n\\nIn order to address the above needs, Fiddler provides vector monitoring capability which involves enabling users to define custom features, and a novel method for monitoring data drift in multi-dimensional spaces.\\n\\n### Defining Custom Features\\n\\nUsers can use the Fiddler client to define one or more custom features. Each custom feature is specified by a group of dataset columns that need to be monitored together as a vector. Once a list of custom features is defined and passed to Fiddler (the details of how to use the Fiddler client to define custom features are provided in the following.), Fiddler runs a clustering-based data drift detection algorithm for each custom feature and calculates a corresponding drift value between the baseline and the published events at the selected time period.\\n\\n```python pyth\\nCF1 = fdl.CustomFeature.from_columns([\\'f1\\',\\'f2\\',\\'f3\\'], custom_name = \\'vector1\\')\\nCF2 = fdl.CustomFeature.from_columns([\\'f1\\',\\'f2\\',\\'f3\\'], n_clusters=5, custom_name = \\'vector2\\')\\n```\\n\\n\\n\\n### Passing Custom Features List to Model Info\\n\\n```python\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n    dataset_info=dataset_info,\\n    dataset_id = DATASET_ID,\\n    features = data_cols,\\n    target=\\'target\\',\\n    outputs=\\'predicted_score\\',\\n    custom_features = [CF1,CF2]\\n)\\n```\\n\\n\\n\\n> 📘 Quick Start for NLP Monitoring\\n> \\n> Check out our [Quick Start guide for NLP monitoring](https://docs.fiddler.ai/docs/simple-nlp-monitoring-quick-start) for a fully functional notebook example.',\n",
       " '---\\ntitle: \"Welcome to Fiddler\\'s Documentation!\"\\nslug: \"welcome\"\\nexcerpt: \"This is Fiddler’s AI Observability Platform Documentation. Fiddler is a pioneer in AI Observability for responsible AI. Data Science, MLOps, and LOB teams use Fiddler to monitor, explain, analyze, and improve ML models, generative AI models, and AI applications.\"\\nhidden: false\\nmetadata: \\n  title: \"Fiddler Documentation\"\\n  description: \"This is Fiddler\\'s Model Performance Management Platform Documentation. Fiddler is a pioneer in enterprise Model Performance Management. Data Science, MLOps, and business teams use Fiddler to monitor, explain, analyze, and improve their models and build trust into AI.\"\\ncreatedAt: \"2023-02-27T18:08:02.575Z\"\\nupdatedAt: \"2023-08-04T23:18:40.327Z\"\\n---\\nHere you can find a number of helpful guides to aid with onboarding. These include:\\n\\n[block:html]\\n{\\n  \"html\": \"<style>\\\\n  .index-container {\\\\n      display: grid;\\\\n      grid: auto / 50% 50%;\\\\n      grid-gap: 20px;\\\\n      max-width: 97.5%;\\\\n  }\\\\n  .index-container .index-item {\\\\n    padding: 20px;\\\\n    border: 1px solid #CCCCCC;\\\\n    border-radius: 5px;\\\\n    grid-gap: 15px;\\\\n    \\\\n}\\\\n.index-item{\\\\n  text-decoration: none !important;\\\\n  color: #000000;\\\\n }\\\\n.index-item:hover{\\\\n  color: #000000;\\\\n  border-color: #1A5EF3;\\\\n  -webkit-box-shadow: 0 2px 4px rgb(0 0 0 / 10%);\\\\n  -moz-box-shadown: 0 2px 4px rgb(0 0 0 / 10%);\\\\n  box-shadow: 0 2px 4px rgb(0 0 0 / 10%);\\\\n } \\\\n  \\\\n.index-title {\\\\n    font-size: 20px !important;\\\\n    color: #111111;\\\\n    margin-top: 0px !important;\\\\n    margin-bottom: 20px;\\\\n}\\\\n@media only screen and (max-width: 420px){\\\\n  .index-container {\\\\n    grid: auto / 100%;\\\\n  }\\\\n}\\\\n  </style>\\\\n<div class=\\\\\"index-container\\\\\">\\\\n  <a class=\\\\\"index-item\\\\\" href=\\\\\"https://docs.fiddler.ai/v1.7/docs/administration-platform\\\\\">\\\\n    <div>\\\\n\\\\t\\\\t\\\\t<h2 class=\\\\\"index-title\\\\\">Platform Guide</h2>\\\\n    \\\\t<p>How Fiddler does AI Observability and Fiddler-specific terminologies.</p>\\\\n \\\\t\\\\t</div>\\\\n  </a>\\\\n\\\\n  <a class=\\\\\"index-item\\\\\" href=\\\\\"https://docs.fiddler.ai/v1.7/docs/administration-ui\\\\\">\\\\n    <div>\\\\n      <h2 class=\\\\\"index-title\\\\\">User Interface (UI) Guide</h2>\\\\n      <p>An introduction to the product UI with screenshots that illustrate how to interface with the product.</p>\\\\n    </div>\\\\n  </a>\\\\n\\\\n  <a class=\\\\\"index-item\\\\\" href=\\\\\"https://docs.fiddler.ai/v1.7/docs/installation-and',\n",
       " 'slug: \"clientget_mutual_information\" )\",\\n      \"language\": \"python\",\\n      \"name\": \"Usage with SQL Query\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"Return Type\",\\n    \"h-1\": \"Description\",\\n    \"0-0\": \"dict\",\\n    \"0-1\": \"A dictionary containing mutual information results.\"\\n  },\\n  \"cols\": 2,\\n  \"rows\": 1\\n}\\n[/block]',\n",
       " 'slug: \"quick-start\"  following code block to get your URL.\\n\\n\\n```python\\nprint(\\'/\\'.join([URL, \\'projects\\', PROJECT_ID, \\'models\\', MODEL_ID, \\'monitor\\']))\\n```\\n\\n*Please allow 5-10 minutes for monitoring data to populate the charts.*\\n  \\nThe following screen will be available to you upon completion.\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/8.png\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n**What\\'s Next?**\\n\\nTry the [NLP Monitoring - Quickstart Notebook](https://docs.fiddler.ai/docs/simple-nlp-monitoring-quick-start)\\n\\n---\\n\\n\\n**Questions?**  \\n  \\nCheck out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\\n\\nJoin our [community Slack](http://fiddler-community.slack.com/) to ask any questions!\\n\\nIf you\\'re still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we\\'ll get back to you shortly.\\n',\n",
       " 'slug: \"fdlmodelinfo\"  \"None\",\\n    \"12-3\": \"A **ModelDeploymentParams** object containing information about model deployment.\",\\n    \"13-0\": \"artifact_status\",\\n    \"13-1\": \"Optional [fdl.ArtifactStatus]\",\\n    \"13-2\": \"None\",\\n    \"13-3\": \"An **ArtifactStatus** object containing information about the model artifact.\",\\n    \"14-0\": \"preferred_explanation_method\",\\n    \"14-1\": \"Optional [fdl.ExplanationMethod]\",\\n    \"14-2\": \"None\",\\n    \"14-3\": \"An **ExplanationMethod** object that specifies the default explanation algorithm to use for the model.\",\\n    \"15-0\": \"custom_explanation_names\",\\n    \"15-1\": \"Optional [list]\",\\n    \"15-2\": \"[ ]\",\\n    \"15-3\": \"A list of names that can be passed to the _explanation_name \\\\\\\\_argument of the optional user-defined \\\\\\\\_explain_custom_ method of the model object defined in _package.py._\",\\n    \"16-0\": \"binary_classification_threshold\",\\n    \"16-1\": \"Optional [float]\",\\n    \"16-2\": \".5\",\\n    \"16-3\": \"The threshold used for classifying inferences for binary classifiers.\",\\n    \"17-0\": \"ranking_top_k\",\\n    \"17-1\": \"Optional [int]\",\\n    \"17-2\": \"50\",\\n    \"17-3\": \"Used only for ranking models. Sets the top _k_ results to take into consideration when computing performance metrics like MAP and NDCG.\",\\n    \"18-0\": \"group_by\",\\n    \"18-1\": \"Optional [str]\",\\n    \"18-2\": \"None\",\\n    \"18-3\": \"Used only for ranking models.  The column by which to group events for certain performance metrics like MAP and NDCG.\",\\n    \"19-0\": \"fall_back\",\\n    \"19-1\": \"Optional [dict]\",\\n    \"19-2\": \"None\",\\n    \"19-3\": \"A dictionary mapping a column name to custom missing value encodings for that column.\",\\n    \"20-0\": \"target_class_order\",\\n    \"20-1\": \"Optional [list]\",\\n    \"20-2\": \"None\",\\n    \"20-3\": \"A list denoting the order of classes in the target. This parameter is **required** in the following cases:  \\\\n  \\\\n_- Binary classification tasks_: If the **target** is of type _string_, you must tell Fiddler which class is considered the positive class for your **output** column. You need to provide a list with two elements. The 0th element by convention is considered the negative class, and the 1st element is considered the positive class.  When your **target** is _boolean_, you don\\'t need to specify this argument. By default Fiddler considers `True` as the positive class. In case your target is _numerical_, you don\\'t need to  specify this argument, by default Fiddler considers the higher of the two possible values as the positive class.  \\\\n  \\\\n- _Multi-class classification tasks_: You must tell Fiddler which class corresponds to which output by giving an ordered list of classes. This order should be the same as the order of the outputs.  \\\\n  \\\\n- _Ranking tasks_: If the target is of type _string_, you must provide a list of all the possible target values in the order of relevance. The first element will be considered as the least relevant grade',\n",
       " '---\\ntitle: \"Alerts with Fiddler UI\"\\nslug: \"alerts-ui\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:25:34.901Z\"\\nupdatedAt: \"2023-04-06T22:22:10.370Z\"\\n---\\nFiddler allows you to set up [alerts](https://docs.fiddler.ai/v1.6/docs/alerts-platform) for your model. View your alerts by clicking on the Alerts tab in the navigation bar. The Alerts tab presents three views: Triggered Alerts, Alert Rules, and Integrations. Users can set up alerts using both the Fiddler UI and the Fiddler API Client. This page introduces the available alert types, and how to set up and view alerts in the Fiddler UI. For instructions about how to use the Fiddler API client for alert configuration see [Alert Configuration with Fiddler Client](doc:alerts-client).\\n\\n![](https://files.readme.io/1730387-image.png)\\n\\n## Setting up Alert Rules\\n\\nTo create a new alert using the Fiddler UI, click the **Add Alert** button on the top-right corner of any screen on the Alerts tab. \\n\\n![](https://files.readme.io/78537d3-image.png)\\n\\nIn the Alert Rule form, provide the basic information such as the desired alert name, and the project and model of interest. \\n\\n![](https://files.readme.io/8418e4f-image.png)\\n\\nNext, select the Alert Type you would like to monitor. Users can select from Performance, Data Drift, Data Integrity, or Traffic monitors. For this example, we\\'ll set up a Data Drift alert to measure distribution drift.\\n\\n![](https://files.readme.io/d51ca30-image.png)\\n\\nOnce an Alert Type is selected, users can choose a metric corresponding to the Alert Type for which to set the alert on. For our Data Drift alert, we will use JSD (Jensen–Shannon distance) as our metric. The next consideration are the bin size, which is the duration for which fiddler monitoring calculates the metric values, and the column to apply this monitor on. Let\\'s choose a 1 hour bin and the CreditScore column for this example. \\n\\n![](https://files.readme.io/9ba686e-image.png)\\n\\nNext, users can focus on the alerts comparison method. Learn more about Alert comparisons on the [Alerts Platform Guide](https://docs.fiddler.ai/v1.6/docs/alerts-platform). For our example we will select the Relative comparison option, and compare to the same time 7 days back. Users can select the alert condition as well as a Warning and Critical threshold. We will ask for an alert when the production data is greater than 10%.\\n\\n![](https://files.readme.io/cb3f4b0-image.png)\\n\\nFinally user can set the alert rules priority- how important this alert is to a customers work streams, along with how to get notified of triggered alerts. \\n\\n![](https://files.readme.io/0e75a9e-image.png)\\n\\n Last, click **Add Alert Rule** when you\\'re done. In order to create and configure alerts using the Fiddler API client see [Alert Configuration with Fiddler Client](https://docs.fiddler.ai/v1.5/docs/fiddler-ui).\\n\\n![](https://files.readme.io/72a1e8b-image.png)\\n\\n### Alert Rules Tab\\n\\nOnce an alert rule is created it can be viewed in the **Alert Rules** tab. This view enables you to view all alert',\n",
       " 'slug: \"analytics-ui\"  project dashboard, which can be shared with others.\\n\\nTo pin a chart, click on the thumbtack icon and click **Send**. If the **Update with Query** option is enabled, the pinned chart will update automatically whenever the underlying query is changed on the **Analyze** tab.\\n\\n![](https://files.readme.io/c4247d1-Pinning_Chart.png \"Pinning_Chart.png\")\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"About Datasets\"\\nslug: \"about-datasets\"\\nhidden: false\\ncreatedAt: \"2022-05-23T16:27:08.892Z\"\\nupdatedAt: \"2022-05-23T16:41:26.298Z\"\\n---\\nDatasets (or baseline datasets) are used for making comparisons with production data.\\n\\nA baseline dataset should be sampled from your model\\'s training set, so it can serve as a representation of what the model expects to see in production.\\n\\nFor more information, see [Uploading a Baseline Dataset](doc:uploading-a-baseline-dataset).\\n\\nFor guidance on how to design a baseline dataset, see [Designing a Baseline Dataset](doc:designing-a-baseline-dataset).',\n",
       " 'slug: \"point-explanations\"  prediction difference for which it is responsible. This fraction is called the feature attribution, and it’s what we show in our explanations.\\n\\nAdditive attribution methods have the following characteristics:\\n\\n- The sum of feature attributions always equals the prediction difference.\\n- Features that have no effect on a model’s prediction receive a feature attribution of zero.\\n- Features that have the identical effect receive the same attribution.\\n- Features with mutual information share the attribution for any effect that information has on the prediction.\\n\\nAdditionally, each of these methods takes into account interactions between the features (e.g. two features that have no effect individually but in combination change the model output). This is explicitly built into the Shapley value formalism, and is captured in the path integral over gradients in Integrated Gradients.\\n\\n**Shapley Values and their Approximation**\\n\\nThe Shapley value[<sup>\\\\[1\\\\]</sup>](#references) (proposed by Lloyd Shapley in 1953) is one way to derive feature attributions. Shapley values distribute the total payoff of a collaborative game across a coalition of cooperating players. They are computed by tabulating the average gain in payoff when a particular player is added to the coalition, over all coalition sizes and permutations of players.\\n\\nIn our case, we consider the “total gains” to be the prediction value, and a “player” is a single model feature. The collaborative “game” is all of the model features cooperating to form a prediction value.\\n\\nWhy do we create “coalitions” with only a subset of the features? In some scenarios, it may be appropriate to replace a feature with a zero value when removed from the coalition (e.g. text models where no mask token is available). In others (e.g. models with dense tabular inputs), values are swapped in from a reference distribution or baseline example as a zero value may have a specific meaning (like zero income on a credit application).\\n\\nShapley values have desirable properties including:\\n\\n- **_Linearity_**: If two games are combined, then the total gains correspond to the gains derived from a linear combination of the gains of each game.\\n- **_Efficiency_**: The sum of the values of all players equals the value of the grand coalition, so that all the gain is distributed among the players. In our case, the efficiency property says _the feature attributions should sum to the prediction value_. The attributions can be positive or negative, since a feature can raise or lower a predicted value.\\n\\n**Approximating Shapley Values**\\n\\nComputation of exact Shapley values can be extremely computationally expensive—in fact, exponentially so, in the number of input features. Fiddler makes two approximation methods available:\\n\\n- **SHAP**[<sup>\\\\[2\\\\]</sup>](#references) (SHapely Additive exPlanations) approximates Shapley values by sampling coalitions according to a combinatorially weighted kernel (compensating for the number of permutations of features in coalitions of different cardinality). It samples the feature space uniformly between baseline-like feature vectors and explain-point-like feature vectors. This has the effect of downsampling behavior in the immediate vicinity of the explain-point, a region where the model may be saturated or uniform in its prediction, and attributions may not be helpful.\\n- **Fiddler SHAP**[<sup>\\\\[3\\\\]</sup>](#references) builds on the SHAP approach and is optimized for computing distributions of Shapley values for each feature by comparing the explain-point against a distribution of baselines. This makes it possible to compute confidence intervals around the mean attribution for each feature and identify clusters in attribution',\n",
       " '---\\ntitle: \"On-prem Technical Requirements\"\\nslug: \"technical-requirements\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:20:05.290Z\"\\nupdatedAt: \"2023-05-18T15:43:28.706Z\"\\n---\\n## Minimum System Requirements\\n\\nFiddler is horizontally scalable to support the throughput requirements for enormous production use-cases. The minimum system requirements below correspond to approximately 20 million inference events monitored per day (~230 EPS) for models with around 100 features, with 90 day retention.\\n\\n- **Deployment**: Kubernetes namespace in AWS, Azure or GCP\\n- **Compute**: A minimum of 96 vCPU cores\\n- **Memory**: 384Gi\\n- **Persistent volumes**: 500 Gi storage across 10 volumes \\n  - POSIX-compliant block storage\\n  - 125 MB/s recommended\\n  - 3,000 IOPS recommended\\n- **Container Registry**: Quay.io or similar\\n- **Ingress Controller**: Ingress-nginx or AWS/GCP/Azure Load Balancer Controller\\n- **DNS**: FQDN that resolves to an L4 or L7 load balancer/proxy that provides TLS termination\\n\\n## Kubernetes Cluster Requirements\\n\\nAs stated above, Fiddler requires a Kubernetes cluster to install into.  The following outlines the requirements for this K8 cluster:\\n\\n- **Node Groups**:  2 node groups -  1 for core Fiddler services, 1 for Clickhouse (Fiddler\\'s event database)\\n- **Resources**:\\n  - Fiddler :  48 vCPUs, 192 Gi\\n  - Clickhouse :  64 vCPUs, 256 Gi [tagged & tainted]\\n- **Persistent Volumes**: 500 GB (minimum) /  1 TB (recommended)\\n- **Instance Sizes**\\n\\n  | Instance Size | AWS    | Azure      | GCP        |\\n  | :------------ | :----- | :--------- | :--------- |\\n  | Minimum       | m5.4xl | Std_D16_v3 | c2d_std_16 |\\n  | Recommended   | m5.8xl | Std_D32_v3 | c2d_std_32 |',\n",
       " '---\\ntitle: \"Monitoring\"\\nslug: \"monitoring-ui\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:24:28.175Z\"\\nupdatedAt: \"2023-02-14T01:18:31.256Z\"\\n---\\nFiddler Monitoring helps you identify issues with the performance of your ML models after deployment. Fiddler Monitoring has five main features:\\n\\n1. **Data Drift**\\n2. **Performance**\\n3. **Data Integrity**\\n4. **Service Metrics**\\n5. **Alerts**\\n\\n## Integrate with Fiddler Monitoring\\n\\nIntegrating Fiddler monitoring is a four-step process:\\n\\n1. **Upload dataset**\\n\\n   Fiddler needs a dataset to be used as a baseline for monitoring. A dataset can be uploaded to Fiddler using our UI and Python package. For more information, see:\\n\\n   - [client.upload_dataset()](ref:clientupload_dataset) \\n\\n2. **Onboard model**\\n\\n   Fiddler needs some specifications about your model in order to help you troubleshoot production issues. Fiddler supports a wide variety of model formats. For more information, see:\\n\\n   - [client.add_model()](ref:clientadd_model)\\n\\n3. **Configure monitoring for this model**\\n\\n   You will need to configure bins and alerts for your model. These will be discussed in details below.\\n\\n4. **Send traffic from your live deployed model to Fiddler**\\n\\n   Use the Fiddler SDK to send us traffic from your live deployed model.\\n\\n## Publish events to Fiddler\\n\\nIn order to send traffic to Fiddler, use the [`publish_event`](https://api.fiddler.ai/#client-publish_event) API from the Fiddler SDK. Here is a sample of the API call:\\n\\n```python Publish Event\\nimport fiddler as fdl\\n\\tfiddler_api = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=token)\\n\\t# Publish an event\\n\\tfiddler_api.publish_event(\\n\\t\\tproject_id=\\'bank_churn\\',\\n\\t\\tmodel_id=\\'bank_churn\\',\\n\\t\\tevent={\\n\\t\\t\\t\"CreditScore\": 650,      # data type: int\\n\\t\\t\\t\"Geography\": \"France\",   # data type: category\\n\\t\\t\\t\"Gender\": \"Female\",\\n\\t\\t\\t\"Age\": 45,\\n\\t\\t\\t\"Tenure\": 2,\\n\\t\\t\\t\"Balance\": 10000.0,      # data type: float\\n\\t\\t\\t\"NumOfProducts\": 1,\\n\\t\\t\\t\"HasCrCard\": \"Yes\",\\n\\t\\t\\t\"isActiveMember\": \"Yes\",\\n\\t\\t\\t\"EstimatedSalary\": 120000,\\n\\t\\t\\t\"probability_churned\": 0.105,\\n      \"churn\": 1\\n\\t\\t},\\n\\t\\tevent_id=’some_unique_id’, #optional\\n\\t\\tupdate_event=False, #optional\\n\\t\\tevent_timestamp=1511253040519 #optional\\n\\t)\\n```\\n\\n\\n\\nThe `publish_event` API can be called in real-time right after your model inference. \\n\\n> 📘 Info\\n> \\n> You can also publish events as part of a batch call after the fact using the `publish_events_batch` API (click [here](https://api.fiddler.ai/#client-publish_events_batch) for more information). In this case, you will need to send Fiddler the original event timestamps as to accurately populate the time series charts.\\n\\nFollowing is a description of all the parameters for `publish_event`:\\n\\n- `project_id`: Project ID for the project this event belongs to.\\n\\n- `model_id`: Model ID for the model this event belongs to',\n",
       " 'slug: \"point-explainability\"  the total payoff of a collaborative game across a coalition of cooperating players. They are computed by tabulating the average gain in payoff when a particular player is added to the coalition, over all coalition sizes and permutations of players.\\n\\nIn our case, we consider the “total gains” to be the prediction value, and a “player” is a single model feature. The collaborative “game” is all of the model features cooperating to form a prediction value.\\n\\nWhy do we create “coalitions” with only a subset of the features? In some scenarios, it may be appropriate to replace a feature with a zero value when removed from the coalition (e.g. text models where no mask token is available). In others (e.g. models with dense tabular inputs), values are swapped in from a reference distribution or baseline example as a zero value may have a specific meaning (like zero income on a credit application).\\n\\nShapley values have desirable properties including:\\n\\n- **_Linearity_**: If two games are combined, then the total gains correspond to the gains derived from a linear combination of the gains of each game.\\n- **_Efficiency_**: The sum of the values of all players equals the value of the grand coalition, so that all the gain is distributed among the players. In our case, the efficiency property says _the feature attributions should sum to the prediction value_. The attributions can be positive or negative, since a feature can raise or lower a predicted value.\\n\\n**Approximating Shapley Values**\\n\\nComputation of exact Shapley values can be extremely computationally expensive—in fact, exponentially so, in the number of input features. Fiddler makes two approximation methods available:\\n\\n- **SHAP**[<sup>\\\\[2\\\\]</sup>](#references) (SHapely Additive exPlanations) approximates Shapley values by sampling coalitions according to a combinatorially weighted kernel (compensating for the number of permutations of features in coalitions of different cardinality). It samples the feature space uniformly between baseline-like feature vectors and explain-point-like feature vectors. This has the effect of downsampling behavior in the immediate vicinity of the explain-point, a region where the model may be saturated or uniform in its prediction, and attributions may not be helpful.\\n- **Fiddler SHAP**[<sup>\\\\[3\\\\]</sup>](#references) builds on the SHAP approach and is optimized for computing distributions of Shapley values for each feature by comparing the explain-point against a distribution of baselines. This makes it possible to compute confidence intervals around the mean attribution for each feature and identify clusters in attribution space where distinct, individually relevant explanations might be important (e.g. “your loan application was rejected for a set of reasons when compared to applications in your region, and for another set of reasons when compared to applications with the same profession”).\\n\\nApproximate Shapley value methods can be used to explain nearly any model, since you only need to be able to ask the model for predictions at a variety of positions in the feature space.\\n\\n**Integrated Gradients**\\n\\nAnother additive attribution method: the Integrated Gradients method.\\n\\nFor models whose prediction is continuous and piecewise differentiable in the feature space, it can be useful to provide additional information through the gradient (slope vector) of a prediction.\\n\\nFiddler supports Integrated Gradients (IG)[<sup>\\\\[4\\\\]</sup>](#references). In this method, an approximate integral tabulates components of the slope along a linear path from baseline to explain-point, and attributes them to respective input features. This has several advantages:\\n\\n1. For models with very high dimensional feature volumes (e.g',\n",
       " '---\\ntitle: \"fdl.BinSize\"\\nslug: \"fdlbinsize\"\\nexcerpt: \"Supported Bin Size values for Alert Rules\"\\nhidden: false\\ncreatedAt: \"2023-01-31T07:28:35.834Z\"\\nupdatedAt: \"2023-01-31T07:28:35.834Z\"\\n---\\n**This field signifies the durations for which fiddler monitoring calculates the metric values **\\n\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"Enums\",\\n    \"h-1\": \"Values\",\\n    \"0-0\": \"fdl.BinSize.ONE_HOUR\",\\n    \"0-1\": \"3600 \\\\\\\\* 1000 millisecond  \\\\ni.e one hour\",\\n    \"1-0\": \"fdl.BinSize.ONE_DAY\",\\n    \"1-1\": \"86400 \\\\\\\\* 1000 millisecond  \\\\ni.e one day\",\\n    \"2-0\": \"fdl.BinSize.SEVEN_DAYS\",\\n    \"2-1\": \"604800 \\\\\\\\* 1000 millisecond  \\\\ni.e seven days\"\\n  },\\n  \"cols\": 2,\\n  \"rows\": 3,\\n  \"align\": [\\n    \"left\",\\n    \"left\"\\n  ]\\n}\\n[/block]\\n\\n```coffeescript Usage\\nimport fiddler as fdl\\n\\nclient.add_alert_rule(\\n    name = \"perf-gt-5prec-1hr-1d-ago\",\\n    project_name = \\'project-a\\',\\n    model_name = \\'model-a\\',\\n    alert_type = fdl.AlertType.PERFORMANCE, \\n    metric = fdl.Metric.PRECISION,\\n    bin_size = fdl.BinSize.ONE_HOUR, <----\\n    compare_to = fdl.CompareTo.TIME_PERIOD,\\n    compare_period = fdl.ComparePeriod.ONE_DAY,\\n    warning_threshold = 0.05,\\n    critical_threshold = 0.1,\\n    condition = fdl.AlertCondition.GREATER,\\n    priority = fdl.Priority.HIGH,\\n    notifications_config = notifications_config\\n)\\n```\\n```coffeescript Outputs\\n[AlertRule(alert_rule_uuid=\\'9b8711fa-735e-4a72-977c-c4c8b16543ae\\',\\n           organization_name=\\'some_org_name\\',\\n           project_id=\\'project-a\\',\\n           model_id=\\'model-a\\',\\n           name=\\'perf-gt-5prec-1hr-1d-ago\\',\\n           alert_type=AlertType.PERFORMANCE, \\n           metric=Metric.PRECISION,\\n           priority=Priority.HIGH,\\n           compare_to=\\'CompareTo.TIME_PERIOD,\\n           compare_period=ComparePeriod.ONE_DAY,\\n           compare_threshold=None,\\n           raw_threshold=None,\\n           warning_threshold=0.05,\\n           critical_threshold=0.1,\\n           condition=AlertCondition.GREATER,\\n           bin_size=BinSize.ONE_HOUR)] <-----\\n```',\n",
       " 'slug: \"clientadd_alert_rule\" > The Fiddler client can be used to create a variety of alert rules. Rules can be of **Data Drift**, **Performance**, **Data Integrity**, and **Service Metrics ** types and they can be compared to absolute (compare_to = RAW_VALUE) or to relative values (compare_to = TIME_PERIOD).\\n\\n```python Usage - time_period\\n# To add a Performance type alert rule which triggers an email notification \\n# when precision metric is 5% higher than that from 1 hr bin one day ago.\\n\\nimport fiddler as fdl\\n\\nnotifications_config = client.build_notifications_config(\\n    emails = \"user_1@abc.com, user_2@abc.com\",\\n)\\nclient.add_alert_rule(\\n    name = \"perf-gt-5prec-1hr-1d-ago\",\\n    project_id = \\'project-a\\',\\n    model_id = \\'model-a\\',\\n    alert_type = fdl.AlertType.PERFORMANCE,\\n    metric = fdl.Metric.PRECISION,\\n    bin_size = fdl.BinSize.ONE_HOUR, \\n    compare_to = fdl.CompareTo.TIME_PERIOD,\\n    compare_period = fdl.ComparePeriod.ONE_DAY,\\n    warning_threshold = 0.05,\\n    critical_threshold = 0.1,\\n    condition = fdl.AlertCondition.GREATER,\\n    priority = fdl.Priority.HIGH,\\n    notifications_config = notifications_config\\n)\\n```\\n```python Usage - raw_value\\n\\n# To add Data Integrity alert rule which triggers an email notification when \\n# published events have more than 5 null values in any 1 hour bin for the _age_ column. \\n# Notice compare_to = fdl.CompareTo.RAW_VALUE.\\n\\nimport fiddler as fdl\\n\\nclient.add_alert_rule(\\n    name = \"age-null-1hr\",\\n    project_id = \\'project-a\\',\\n    model_id = \\'model-a\\',\\n    alert_type = fdl.AlertType.DATA_INTEGRITY,\\n    metric = fdl.Metric.MISSING_VALUE,\\n    bin_size = fdl.BinSize.ONE_HOUR, \\n    compare_to = fdl.CompareTo.RAW_VALUE,\\n    priority = fdl.Priority.HIGH,\\n    warning_threshold = 5,\\n    critical_threshold = 10,\\n    condition = fdl.AlertCondition.GREATER,\\n    column = \"age\",\\n    notifications_config = notifications_config\\n)\\n```\\n```python Usage - baseline\\n# To add a Data Drift type alert rule which triggers an email notification \\n# when PSI metric for \\'age\\' column from an hr is 5% higher than that from \\'baseline_name\\' dataset.\\n\\nimport fiddler as fdl\\n\\nclient.add_baseline(project_id=\\'project-a\\', \\n                    model_id=\\'model-a\\', \\n                    baseline_name=\\'baseline_name\\', \\n                    type=fdl.BaselineType.PRE_PRODUCTION, \\n                    dataset_id=\\'dataset-a\\')\\n\\nnotifications_config = client.build_notifications_config(\\n    emails = \"user_1@abc.com, user_2@abc.com\",\\n)\\n\\nclient.add_alert_rule(\\n    name = \"psi-gt-5prec-age-baseline_name\",\\n    project_id = \\'project-a\\',\\n    model_id = \\'model-a\\',\\n    alert_type = fdl.AlertType.DATA_DRIFT,\\n    metric = fdl.Metric.PSI,\\n    bin_size = fdl.BinSize.ONE_HOUR, \\n    compare_to = fdl.CompareTo.RAW_VALUE,\\n    warning_threshold = 0.05,\\n    critical_threshold = 0.1,\\n    condition = fdl.AlertCondition.GREATER,\\n    priority = fdl.Priority.HIGH,\\n    notifications_config = notifications_config,\\n    column = \"age\",\\n    baseline_id = \\'baseline_name\\'\\n',\n",
       " '---\\ntitle: \"System Architecture\"\\nslug: \"system-architecture\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:19:53.311Z\"\\nupdatedAt: \"2023-05-18T21:09:05.870Z\"\\n---\\nFiddler deploys into your private cloud\\'s existing Kubernetes clusters, for which the minimum system requirements can be found [here](doc:technical-requirements).  Fiddler supports deployment into Kubernetes in AWS, Azure, or GCP.  All services of the Fiddler platform are containerized in order to eliminate reliance on other cloud services and to reduce the deployment and maintenance friction of the platform.  This includes storage services like object storage and databases as well as system monitoring services like Grafana.  \\n\\nUpdates to the Fiddler containers is accomplished through a shared container registry (that Fiddler is provided access to).  Updates to the containers are orchestrated using Helm charts.\\n\\nA full-stack deployment of Fiddler is shown in the diagram below. \\n\\n![](https://files.readme.io/7cbfe31-reference_architecture.png)\\n\\nThe Fiddler system components are deployed within a single namespace on a Kubernetes cluster, using the official Fiddler Helm chart.\\n\\n- Fiddler core infrastructure relies on persistent volumes provided within the Kubernetes cluster. We recommend using encrypted storage volumes wherever possible.\\n- Fiddler may be configured to utilize external infrastructure in a self-hosted environment, such as existing PostgresQL servers, but this is not required as all services are containerized by default.\\n- Full-stack \"any-prem\" Fiddler includes observability infrastructure to monitor Fiddler system health and performance. These mainstream observability components may be integrated with external observability systems to support administration in a self-hosted environment.\\n- HTTP traffic to the Fiddler system is handled by an L4 or L7 load balancer or other proxy. TLS termination should usually occur outside the Fiddler system.\\n\\nOnce the platform is running, end users can interface with the Fiddler platform using their browser, the [Fiddler Python client](ref:about-the-fiddler-client), or Fiddler\\'s RESTful APIs.',\n",
       " '---\\ntitle: \"Product Tour\"\\nslug: \"product-tour\"\\nexcerpt: \"Here\\'s a tour of our product UI!\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:09:29.819Z\"\\nupdatedAt: \"2023-08-04T23:19:01.026Z\"\\n---\\n# Video Demo\\n\\nWatch the video to learn how Fiddler AI Observability provides data science and MLOps teams with a unified platform to monitor, analyze, explain, and improve machine learning models at scale, and build trust in AI.\\n\\n\\n[block:embed]\\n{\\n  \"html\": \"<iframe class=\\\\\"embedly-embed\\\\\" src=\\\\\"//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FPENnn3YUAcg&display_name=YouTube&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DPENnn3YUAcg&image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FPENnn3YUAcg%2Fhqdefault.jpg&key=7788cb384c9f4d5dbbdbeffd9fe4b92f&type=text%2Fhtml&schema=youtube\\\\\" width=\\\\\"854\\\\\" height=\\\\\"480\\\\\" scrolling=\\\\\"no\\\\\" title=\\\\\"YouTube embed\\\\\" frameborder=\\\\\"0\\\\\" allow=\\\\\"autoplay; fullscreen\\\\\" allowfullscreen=\\\\\"true\\\\\"></iframe>\",\\n  \"url\": \"https://www.youtube.com/watch?v=PENnn3YUAcg\",\\n  \"favicon\": \"https://www.google.com/favicon.ico\",\\n  \"image\": \"http://i.ytimg.com/vi/PENnn3YUAcg/hqdefault.jpg\",\\n  \"provider\": \"youtube.com\",\\n  \"href\": \"https://www.youtube.com/watch?v=PENnn3YUAcg\",\\n  \"typeOfEmbed\": \"youtube\"\\n}\\n[/block]\\n\\n\\n# Documented UI Tour\\n\\nWhen you log in to Fiddler, you are on the Home page and you can visualize monitoring information for your models across all your projects. \\n\\n- At the top of the page, you will see donut charts for the number of triggered alerts for [Performance](doc:performance-tracking-platform), [Data Drift](doc:data-drift-platform), and [Data Integrity](doc:data-integrity-platform). \\n- To the right of the donut charts, you will find the Bookmarks as well as a Recent Job Status card that lets you keep track of long-running async jobs and whether they have failed, are in progress, or successfully completed. \\n- The [Monitoring](doc:monitoring-ui) summary table displays your models across different [projects](doc:project-architecture) along with information on their traffic, drift, and the number of triggered alerts.  \\n  ![](https://files.readme.io/e959fe5-image.png)\\n\\nOn the navigation bar at the top, next to the Home Tab, is the [Projects](doc:project-structure) Tab. You can click on the Projects tab and it lands on a page that lists all your projects contained within Fiddler. See the [Fiddler Samples](doc:product-tour#fiddler-samples)  section below for more information on these projects. You can create new projects within the UI (by clicking the “Add Project” button) or via the [Fiddler Client](ref:about-the-fiddler-client).\\n\\n![](https://files.readme.io/8ffbd1b-image.png',\n",
       " '---\\ntitle: \"fdl.Baseline\"\\nslug: \"fdlbaseline\"\\nexcerpt: \"Schema object with Fields describing a baseline returned from the backend\"\\nhidden: false\\ncreatedAt: \"2023-01-31T23:52:21.971Z\"\\nupdatedAt: \"2023-05-11T19:23:06.971Z\"\\n---\\n| Field        | Type | Description                                                                           |\\n| :----------- | :--- | :------------------------------------------------------------------------------------ |\\n| id           | int  | Autogenerated unique identifier of the baseline object                                |\\n| name         | str  | The user provided identifier for the baseline                                         |\\n| project_name | str  | The unique identifier for the project                                                 |\\n| model_name   | str  | The unique identifier for the model the baseline is associated with                   |\\n| type         | str  | [The baseline type](ref:fdlbaselinetype)                                              |\\n| dataset_name | str  | The unique identifier for the dataset object when PRE_PRODUCTION baseline is returned |\\n| start_time   | int  | Start time in seconds for a STATIC_PRODUCTION baseline                                |\\n| end_time     | int  | End time in seconds for a STATIC_PRODUCTION baseline                                  |\\n| offset       | int  | Offset from current time in seconds for ROLLING_PRODUCTION baseline                   |\\n| window_size  | int  | Window width in seconds for ROLLING_PRODUCTION baseline                               |',\n",
       " 'slug: \"global-explainability\"  _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"analytics-ui\"  [**Feature Distribution**](#feature-distribution) — Visualize the distribution of an input or output.\\n3. [**Feature Impact**](#feature-impact) — Understand the aggregate impact of model inputs to the output.\\n4. [**Partial Dependence Plot**](#partial-dependence-plot-pdp) — Understand the aggregate impact of a single model input in its output.\\n5. [**Slice Evaluation**](#slice-evaluation) — View the model metrics for a given slice.\\n6. [**Dataset Details**](#dataset-details) — Analyze statistical qualities of the dataset.\\n\\nYou can also access the following _point explanation methods_ by clicking on **Explain** (light bulb icon) for a given data point:\\n\\n1. [**Point Overview**](#point-overview) — Get an overview of the model inputs responsible for a prediction.\\n2. [**Feature Attribution**](#feature-attribution) — Understand how responsible each model input is for the model output.\\n3. [**Feature Sensitivity**](#feature-sensitivity) – Understand how changes in the model’s input values will impact the model’s output.\\n\\n> 📘 Info\\n> \\n> For more information on point explanations, click [here](doc:point-explainability).\\n\\n## Feature Correlation\\n\\nThe feature correlation visualization plots a single variable against another variable. This plot helps identify any visual clusters that might be useful for further analysis. This visualization supports integer, float, and categorical variables.\\n\\n![](https://files.readme.io/e36a237-S_E_Correlation.png \"S_E_Correlation.png\")\\n\\n## Feature Distribution\\n\\nThe feature distribution visualization is one of the most basic plots, used for viewing how the data is distributed for a particular variable. This plot helps surface any data abnormalities or data insights to help root-cause issues or drive further analysis.\\n\\n![](https://files.readme.io/26e2658-S_E_Distribution.png \"S_E_Distribution.png\")\\n\\n## Feature Impact\\n\\nThis visualization provides the feature impact of the dataset (global explanation) or the selected slice (local explanation), showcasing the overall sensitivity of the model output to each feature (more on this in the [Global Explainability](doc:global-explainability) section). We calculate Feature Impact by randomly intervening on every input using ablations and noting the average absolute change in the prediction.\\n\\nA high impact suggests that the model’s behavior on a particular slice is sensitive to changes in feature values. Feature impact only provides the absolute impact of the input—not its directionality. Since positive and negative directionality can cancel out, we recommend using a Partial Dependence Plot (PDP) to understand how an input impacts the output in aggregate.\\n\\n![](https://files.readme.io/09cb939-S_E_FeatureImpact.png \"S_E_FeatureImpact.png\")\\n\\n## Partial Dependence Plot (PDP)\\n\\nPartial dependence plots show the marginal effect of a selected model input on the model output. This plot helps understand whether the relationship between the input and the output is linear, monotonic, or more complex.\\n\\n![](https://files.readme.io/e1c0f84-PDP.png \"PDP.png\")\\n\\n## Slice Evaluation\\n\\nThe slice evaluation visualization gives you key model performance metrics and plots, which can be helpful to identify performance issues or model bias on protected classes. In addition to key metrics, you get a confusion matrix along with precision recall, ROC, and calibration plots. This visualization supports classification, regression, and multi-class models.\\n\\n![](https://files.readme.io/96aa3d0-Slice_Evaluation.png \"Slice_Evaluation.png\")\\n\\n## Dataset Details\\n\\nThis visualization provides statistical',\n",
       " '---\\ntitle: \"Designing a Baseline Dataset\"\\nslug: \"designing-a-baseline-dataset\"\\nhidden: false\\ncreatedAt: \"2022-05-23T16:30:17.415Z\"\\nupdatedAt: \"2023-01-12T18:02:44.450Z\"\\n---\\nIn order for Fiddler to monitor drift or data integrity issues in incoming production data, it needs something to compare this data to.\\n\\nA baseline dataset is a **representative sample** of the kind of data you expect to see in production. It represents the ideal form of data that your model works best on.\\n\\n*For this reason,* ***it should be sampled from your model’s training set.***\\n\\n***\\n\\n**A few things to keep in mind when designing a baseline dataset:**\\n\\n* It’s important to include **enough data** to ensure you have a representative sample of the training set.\\n* You may want to consider **including extreme values (min/max)** of each column in your training set so you can properly monitor range violations in production data. However, if you choose not to, you can manually specify these ranges before upload (see [Customizing Your Dataset Schema])(doc:customizing-your-dataset-schema).',\n",
       " '---\\ntitle: \"Project Structure on UI\"\\nslug: \"project-structure\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:26:33.568Z\"\\nupdatedAt: \"2023-02-03T19:45:51.093Z\"\\n---\\nSupervised machine learning involves identifying a predictive task, finding data to enable that task, and building a model using that data. Fiddler captures this workflow with project, dataset, and model entities.\\n\\n## Projects\\n\\nA project represents a machine learning task (e.g. predicting house prices, assessing creditworthiness, or detecting fraud).\\n\\nA project can contain one or more models for the ML task (e.g. LinearRegression-HousePredict, RandomForest-HousePredict).\\n\\nCreate a project by clicking on **Projects** and then clicking on **Add Project**.\\n\\n![](https://files.readme.io/8e4b429-Add_project_0710.png \"Add_project_0710.png\")\\n\\n- **_Create New Project_** — A window will pop up where you can enter the project name and click **Create**. Once the project is created, it will be displayed on the projects page.\\n\\nYou can access your projects from the Projects Page.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/82404e6-Screenshot_2022-12-27_at_1.00.15_PM.png\",\\n        null,\\n        \"Projects Page on Fiddler UI\"\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Projects Page on Fiddler UI\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n## Datasets\\n\\nA dataset in Fiddler is a data table containing features, model outputs, and a target for machine learning models. Optionally, you can also upload metadata and “decision” columns, which can be used to segment the dataset for analyses, track business decisions, and work as protected attributes in bias-related workflows. For more details refer to [Datasets](doc:project-architecture#datasets) in the Platform Guide.\\n\\nOnce you click on a particular project, you will be able to see if there are any datasets associated with the project. For example, the bank_churn project, in the following screenshot, has the bank_churn dataset. [Datasets are uploaded via the Fiddler client](ref:clientupload_dataset). \\n\\n![](https://files.readme.io/3fa7700-Screenshot_2022-12-27_at_1.05.05_PM.png)\\n\\n## Models\\n\\nA model in Fiddler represents a machine learning model. A project will have one or more models for the ML task (e.g. a project to predict house prices might contain LinearRegression-HousePredict and RandomForest-HousePredict). For further details refer to the [Models](doc:project-architecture#models) section in the Platform Guide.\\n\\n![](https://files.readme.io/e151df5-Model_Dashboard.png \"Model_Dashboard.png\")\\n\\n### Model Artifacts\\n\\nAt its most basic level, a model in Fiddler is simply a directory that contains [model artifacts](doc:artifacts-and-surrogates) such as:\\n\\n- The model file (e.g. `*.pkl`)\\n- `package.py`: A wrapper script containing all of the code needed to standardize the execution of the model.\\n\\n![](https://files.readme.io/7170489-Model_Details.png \"Model_Details.png\")\\n\\n![](https://files.readme.io/2b3d52e-Model_Details_1.png \"Model_Details_1.png\")\\n\\n## Project Dashboard',\n",
       " '---\\ntitle: \"Release 23.2 Notes\"\\nslug: \"release-232\"\\ncreatedAt: \"2023-06-21T18:53:58.434Z\"\\nhidden: false\\n---\\nThis page enumerates the new features and updates in Release 23.2 of the Fiddler platform.\\n\\n## Release of Fiddler platform version 23.2:\\n\\n- Support for uploading multiple baselines to a model\\n\\n- Alert context overlay on the chart editor\\n\\n- Ability to customize scale and range of y-axis on the chart editor\\n\\n## What\\'s New and Improved:\\n\\n- **Support for uploading multiple baselines**\\n  - Flexibility to add baseline datasets or use production data as the baseline.\\n  - Perform comparisons amongst multiple baselines to understand how different baselines — data shifts due seasonality or geography for example — may influence model drift and model behavior.\\n  - Learn more on the [Baselines Platform Guide](doc:fiddler-baselines).\\n\\n- **Alert context overlay on the chart editor**\\n  - For absolute alerts, alert context is an overlay on the chart area to easily identify critical and warning thresholds.\\n  - For relative alerts, Fiddler will automatically plot historic comparison data for additional context on why the alert fired.\\n\\n- **Customization in the chart editor**\\n  - Further customize charts by toggling between logarithmic and linear scale, and manually setting the min and max values of the y-axis.\\n  - Learn more on the [Monitoring Charts](doc:monitoring-charts-ui) page.\\n\\n### Client Version\\n\\nClient version 1.8 is required for the updates and features mentioned in this release.',\n",
       " '---\\ntitle: \"Explainability\"\\nslug: \"explainability\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:24:31.709Z\"\\nupdatedAt: \"2023-02-14T01:19:19.993Z\"\\n---\\nThere are three topics related to Explainability to cover:\\n\\n- [Point Explainability](doc:point-explainability) \\n- [Global Explainability](doc:global-explainability) \\n- [Surrogate Models](doc:surrogate-models)\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"About Event Publication\"\\nslug: \"publish_event\"\\nhidden: false\\ncreatedAt: \"2022-05-13T14:36:29.265Z\"\\nupdatedAt: \"2022-06-14T15:30:43.091Z\"\\n---\\nEvent publication is the process of sending your model\\'s prediction logs, or events, to the Fiddler platform.  Using the [Fiddler Client](ref:about-the-fiddler-client), events can be published in batch or streaming mode.  Using these events, Fiddler will calculate metrics around feature drift, prediction drift, and model performance.  These events are also stored in Fiddler to allow for ad hoc segment analysis.  Please read the sections that follow to learn more about how to use the Fiddler Client for event publication.',\n",
       " '---\\ntitle: \"Release 23.1 Notes\"\\nslug: \"2023-3-31\"\\ncreatedAt: \"2023-03-30T15:41:01.198Z\"\\nhidden: false\\n---\\nThis page enumerates the new features and updates in Release 23.1 of the Fiddler platform.\\n\\n## Release of Fiddler platform version 23.1:\\n\\n- New monitoring chart editor\\n\\n- New dashboard reporting tool\\n\\n- Flexible model deployment options\\n\\n- Scale & performance improvements\\n\\n- GitHub samples migration\\n\\n## What\\'s New and Improved:\\n\\n- **New flexible monitoring chart editor**\\n  - Create customized charts for model monitoring metrics like Performance,Data Drift, Data Integrity, and more.\\n  - Learn more on the [Monitoring Charts Platform Guide](https://docs.fiddler.ai/v1.7/docs/monitoring-charts-platform).\\n\\n- **New dashboard reporting tool for monitoring charts**\\n  - Combine the monitoring charts that help track model performance and health in a cohesive dashboard for your reporting needs.\\n  - Learn more on the [Dashboards Platform Guide](https://docs.fiddler.ai/v1.7/docs/dashboards-platform).\\n\\n- **Flexible model deployment options**\\n  - Fiddler now supports flexible model deployment by allowing users to spin up separate k8s pods for each model and varying dependencies for their models.\\n  - Learn more on the [Flexible Model Deployment](doc:model-deployment) page.\\n\\n- **Scale and performance improvements**\\n  - Efficiently register models with 2,000 features within just 30 minutes\\n  - Performance improvements across Vector Monitoring, Multiclass Classification, and Ranking scenarios.\\n\\n- **Migrating all Fiddler samples to new GitHub repository**\\n  - The `fiddler-samples` GitHub repository will be deprecated and replaced by the new [`fiddler-examples`](https://github.com/fiddler-labs/fiddler-examples) repository.\\n\\n### Client Version\\n\\nClient version 1.7 is required for the updates and features mentioned in this release.',\n",
       " '---\\ntitle: \"Model Task Types\"\\nslug: \"task-types\"\\nhidden: false\\ncreatedAt: \"2022-11-15T18:06:58.284Z\"\\nupdatedAt: \"2023-02-10T16:29:14.851Z\"\\n---\\nFiddler currently supports four model tasks. These include:\\n\\n- Binary Classification\\n- Multi-class Classification\\n- Regression\\n- Ranking\\n\\n**Binary classification** is the task of classifying the elements of an outcome set into two groups (each called class) on the basis of a classification rule. Typical binary classification problems include:\\n\\n- Determining whether a customer will churn or not. Here the outcome set has two outcomes: The customer will churn or the customer will not. Further, the outcome can only belong to either of the two classes.\\n- Determining whether a patient has a disease or not. Here the outcome set has two outcomes: the patient has the disease or does not.\\n\\n**Multiclass classification** is the task of classifying the elements of an outcome set into three or more groups (each called class) on the basis of a classification rule. Typical multiclass classification problems include:\\n\\n- Determining whether an image is a cat, a dog, or a bird. Here the outcome set has more than two outcomes. Further, the image can only be determined to be one of the three outcomes and it\\'s thus a multiclass classification problem.\\n\\n**Regression** is the task of predicting a continuous numeric quantity. Typical regression problems include:\\n\\n- Determining the average home price based on a given set of housing related features such as it\\'s square footage, number of beds and bath, it\\'s location etc.\\n- Determining the income of an individual based on features such as their age, work location, their job sector etc.\\n\\n**Ranking** is the task of constructing a rank ordered list of items given a particular query that seeks some information. Typical ranking problems include:\\n\\n- Ranking documents in information retrieval systems.\\n- Ranking relevancy of advertisements based on user search queries.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcc15db3-0861-4d3e-a235-9fc6cf9afd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message, message, query_embed = ask(\"What all metrics do you support in LLMs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61e33312-37a4-4868-bf65-8a8addb46baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a tool called Fiddler Chatbot and your purpose is to use the below documentation from the company Fiddler to answer the subsequent documentation questions. Also, if possible, give me the reference URLs according to the following instructions. The way to create the URLs is: if you are discussing a client method or an API reference add \"https://docs.fiddler.ai/reference/\" before the \"slug\" value of the document. If it is Guide documentation add \"https://docs.fiddler.ai/docs/\" before before the \"slug\" value of the document. Only use the value following \"slug:\" to create the URLs and do not use page titles for slugs. If you are using quickstart notebooks, do not generate references. Note that if a user asks about uploading events, it means the same as publishing events. If the answer cannot be found in the documentation, write \"I could not find an answer.\"Custom metrics is an upcoming feature and it is currently not supported.---\\ntitle: \"Monitoring Charts\"\\nslug: \"monitoring-charts-platform\"\\nhidden: false\\ncreatedAt: \"2023-02-23T22:56:27.756Z\"\\nupdatedAt: \"2023-05-24T17:29:04.123Z\"\\n---\\nFiddler AI’s monitoring charts allow you to easily track your models and ensure that they are performing optimally. For any of your models, monitoring charts for data drift, performance, data integrity, or traffic metrics can be displayed using Fiddler Dashboards.\\n\\n## Supported Metric Types\\n\\nMonitoring charts enable you to plot one of the following metric types for a given model:\\n\\n- [**Data Drift**](doc:data-drift-platform#what-is-being-tracked)\\n  - Plot drift for up to 20 columns at once and track it using your choice of Jensen–Shannon distance (JSD) or Population Stability Index (PSI).\\n- [**Performance**](doc:performance-tracking-platform#what-is-being-tracked)\\n  - Available metrics are model dependent.\\n- [**Data Integrity Violations**](doc:data-integrity-platform#what-is-being-tracked)\\n  - Plot data integrity violations for up to 20 columns and track one of the three violations at once.\\n- [**Traffic **](doc:traffic-platform#what-is-being-tracked)\\n\\n## Key Features:\\n\\n### Multiple Charting Options\\n\\nYou can [plot up to 20 columns](doc:monitoring-charts-ui#chart-metric-queries--filters) for a model when charting data drift or data integrity metrics, allowing you to compare them side by side.\\n\\n### Downloadable CSV Data\\n\\nYou can [easily download a CSV of the raw chart data](doc:monitoring-charts-ui#breakdown-summary). This feature allows you to analyze your data further.\\n\\n### Advanced Chart Functionality\\n\\nThe monitoring charts feature offers [advanced chart functionalities ](doc:monitoring-charts-ui#chart-metric-queries--filters)  to provide you with the flexibility to customize your charts and view your data in a way that is most useful to you. Features include:\\n\\n- Zoom\\n- Dragging of time ranges\\n- Toggling between bar and line chart types\\n- Adjusting the scale between linear and log options\\n- Adjusting the range of the y-axis\\n\\n![](https://files.readme.io/9ad4867-image.png)\\n\\n\\n\\nCheck out more on the [Monitoring Charts UI Guide](doc:monitoring-charts-ui).---\\ntitle: \"fdl.Metric\"\\nslug: \"fdlmetric\"\\nexcerpt: \"Supported Metric for different Alert Types in Alert Rules\"\\nhidden: false\\ncreatedAt: \"2023-01-31T07:32:12.906Z\"\\nupdatedAt: \"2023-02-03T03:25:53.558Z\"\\n---\\n**Following is the list of metrics, with corresponding alert type and model task, for which an alert rule can be created.**\\n\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"Enum Values\",\\n    \"h-1\": \"Supported for [Alert Types](https://docs.fiddler.ai/v1.5/reference/fdlalerttype)  \\\\n([ModelTask ](https://docs.fiddler.ai/v1.5/reference/fdlmodeltask)restriction if any)\",\\n    \"h-2\": \"Description\",\\n    \"0-0\": \"fdl.Metric.PSI\",\\n    \"0-1\": \"fdl.AlertType.DATA_DRIFT\",\\n    \"0-2\": \"Population Stability Index\",\\n    \"1-0\": \"fdl.Metric.JSD\",\\n    \"1-1\": \"fdl.AlertType.DATA_DRIFT\",\\n    \"1-2\": \"Jensen–Shannon divergence\",\\n    \"2-0\": \"fdl.Metric.MISSING_VALUE\",\\n    \"2-1\": \"fdl.AlertType.DATA_INTEGRITY\",\\n    \"2-2\": \"Missing Value\",\\n    \"3-0\": \"fdl.Metric.TYPE_VIOLATION\",\\n    \"3-1\": \"fdl.AlertType.DATA_INTEGRITY\",\\n    \"3-2\": \"Type Violation\",\\n    \"4-0\": \"fdl.Metric.RANGE_VIOLATION\",\\n    \"4-1\": \"fdl.AlertType.DATA_INTEGRITY\",\\n    \"4-2\": \"Range violation\",\\n    \"5-0\": \"fdl.Metric.TRAFFIC\",\\n    \"5-1\": \"fdl.AlertType.SERVICE_METRICS\",\\n    \"5-2\": \"Traffic Count\",\\n    \"6-0\": \"fdl.Metric.ACCURACY\",\\n    \"6-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION,  \\\\nfdl.ModelTask.MULTICLASS_CLASSIFICATION)\",\\n    \"6-2\": \"Accuracy\",\\n    \"7-0\": \"fdl.Metric.RECALL\",\\n    \"7-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION)\",\\n    \"7-2\": \"Recall\",\\n    \"8-0\": \"fdl.Metric.FPR\",\\n    \"8-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION)\",\\n    \"8-2\": \"False Positive Rate\",\\n    \"9-0\": \" fdl.Metric.PRECISION\",\\n    \"9-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION)\",\\n    \"9-2\": \"Precision\",\\n    \"10-0\": \"fdl.Metric.TPR\",\\n    \"10-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION)\",\\n    \"10-2\": \"True Positive Rate\",\\n    \"11-0\": \"fdl.Metric.AUC\",\\n    \"11-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION)\",\\n    \"11-2\": \"Area underslug: \"data-integrity\"  Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]---\\ntitle: \"Performance Tracking\"\\nslug: \"performance-tracking-platform\"\\nexcerpt: \"Platform Guide\"\\nhidden: false\\ncreatedAt: \"2022-12-19T19:27:22.159Z\"\\nupdatedAt: \"2023-08-04T23:21:39.375Z\"\\n---\\n## What is being tracked?\\n\\n![](https://files.readme.io/4a646d4-qs_monitoring.png \"qs_monitoring.png\")\\n\\n- **_Decisions_** - The post-prediction business decisions made as a result of the model output. Decisions are calculated before [client.publish_event()](ref:clientpublish_event) (they\\'re not inferred by Fiddler). For binary classification models, a decision is usually determined using a threshold. For multi-class classification models, it\\'s usually determined using the argmax value of the model outputs.\\n\\n- **_Performance metrics_**\\n\\n| Model Task Type       | Metric                                                         | Description                                                                                                                                        |\\n| :-------------------- | :------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| Binary Classification | Accuracy                                                       | (TP + TN) / (TP + TN + FP + FN)                                                                                                                    |\\n| Binary Classification | True Positive Rate/Recall                                      | TP / (TP + FN)                                                                                                                                     |\\n| Binary Classification | False Positive Rate                                            | FP / (FP + TN)                                                                                                                                     |\\n| Binary Classification | Precision                                                      | TP / (TP + FP)                                                                                                                                     |\\n| Binary Classification | F1 Score                                                       | 2  \\\\* ( Precision \\\\*  Recall ) / ( Precision + Recall )                                                                                            |\\n| Binary Classification | AUROC                                                          | Area Under the Receiver Operating Characteristic (ROC) curve, which plots the true positive rate against the false positive rate                   |\\n| Binary Classification | Binary Cross Entropy                                           | Measures the difference between the predicted probability distribution and the true distribution                                                   |\\n| Binary Classification | Geometric Mean                                                 | Square Root of ( Precision \\\\* Recall )                                                                                                             |\\n| Binary Classification | Calibrated Threshold                                           | A threshold that balances precision and recall at a particular operating point                                                                     |\\n| Binary Classification | Data Count                                                     | The number of events where target and output are both not NULL. **_This will be used as the denominator when calculating accuracy_**.              |\\n| Binary Classification | Expected Calibration Error                                     | Measures the difference between predicted probabilities and empirical probabilities                                                                |\\n| Multi Classification  | Accuracy                                                       | (Number of correctly classified samples) / ( Data Count ). Data Count refers to the number of events where the target and output are both not NULL |\\n| Multi Classification  | Log Loss                                                       | Measures the difference between the predicted probability distribution and the true distribution, in a logarithmic scale                           |\\n| Regression            | Coefficient of determination (R-squared)                       | Measures the proportion of variance in the dependent variable that is explained by the independent variables                                       |\\n| Regression            | Mean Squared Error (MSE)                                       | Average of the squared differences between the predicted and true values                                                                           |\\n| Regression            | Mean Absolute Error (MAE)                                      | Average of the absolute differences between the predicted and true values                                                                          |\\n| Regression            | Mean Absolute Percentage Error (MAPE)                          | Average of the absolute percentage differences between the predicted and true values                                                               |\\n| Regression            | Weighted Mean Absolute Percentage Error (WMAPE)                | The weighted average of the absolute percentage differences between the predicted and true values                                                  |\\n| Ranking               | Mean Average Precision (MAP)—for binary relevance ranking only | Measures the average precision of the relevant items in the top-k results                                                                          |\\n| Ranking               | Normalized Discounted Cumulative Gain (NDCG)                   | Measures the quality of---\\ntitle: \"Dashboards\"\\nslug: \"dashboards-platform\"\\nhidden: false\\ncreatedAt: \"2023-02-21T22:34:44.508Z\"\\nupdatedAt: \"2023-02-27T20:05:52.789Z\"\\n---\\n## Overview\\n\\nWith Fiddler, you can create comprehensive dashboards that bring together all of your monitoring data in one place. This includes monitoring charts for data drift, traffic, data integrity, and performance metrics. Adding monitoring charts to your dashboards lets you create a detailed view of your model\\'s performance. These dashboards can inform your team, management, or stakeholders, and make data-driven decisions that help improve your AI performance. \\n\\nView a list of the **[available metrics for monitoring charts here](doc:monitoring-charts-platform#supported-metric-types)**.\\n\\n## Dashboards Functionality\\n\\nDashboards offer a powerful way to analyze the overall health and performance of your models, as well as to compare multiple models. \\n\\n### Dashboard Filters\\n\\n- [Flexible filters](doc:dashboards-ui#dashboard-filters) including date range, time zone, and bin size to customize your view\\n\\n### Chart Utilities\\n\\n- [Leverage the chart toolbar ](doc:dashboard-interactions#zoom)to zoom into data and toggle between line and bar chart types\\n\\n### [Dashboard Basics](doc:dashboard-utilities)\\n\\n- Easily save, delete, or share your dashboard\\n- Click on a chart name to edit the base chart\\n- Remove and add monitoring charts to your dashboard\\n- Perform model-to-model comparison\\n- Plot drift or data integrity for multiple columns in one view\\n\\n![](https://files.readme.io/9bf5fc2-image.png)\\n\\nCheckout more on the [Dashboards UI Guide](doc:dashboards-ui).slug: \"performance-tracking-platform\"  the ranking of the retrieved items, by discounting the relevance scores of items at lower ranks                            |\\n\\n## Why is it being tracked?\\n\\n- Model performance tells us how well a model is doing on its task. A poorly performing model can have significant business implications.\\n- The volume of decisions made on the basis of the predictions give visibility into the business impact of the model.\\n\\n## What steps should I take based on this information?\\n\\n- For decisions, if there is an increase or decrease in approvals, we can cross-check with the average prediction and prediction drift trendlines on the [Data Drift Tab](doc:data-drift). In general, the average prediction value should increase with an increase in the number of approvals, and vice-versa.\\n- For changes in model performance—again, the best way to cross-verify the results is by checking the [Data Drift Tab](doc:data-drift) ). Once you confirm that the performance issue is not due to the data, you need to assess if the change in performance is due to temporary factors, or due to longer-lasting issues.\\n- You can check if there are any lightweight changes you can make to help recover performance—for example, you could try modifying the decision threshold.\\n- Retraining the model with the latest data and redeploying it is usually the solution that yields the best results, although it may be time-consuming and expensive.\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]---\\ntitle: \"Traffic\"\\nslug: \"traffic-platform\"\\nexcerpt: \"Platform Guide\"\\nhidden: false\\ncreatedAt: \"2022-12-19T19:28:11.378Z\"\\nupdatedAt: \"2023-08-04T23:21:11.689Z\"\\n---\\nTraffic as a service metric gives you basic insights into the operational health of your ML service in production.\\n\\n![](https://files.readme.io/d2c1eaa-Screenshot_2023-02-01_at_5.13.34_PM.png)\\n\\n## What is being tracked?\\n\\n- **_Traffic_** — The volume of traffic received by the model over time.\\n\\n## Why is it being tracked?\\n\\n- Traffic is a basic high-level metric that informs us of the overall system\\'s health.\\n\\n## What steps should I take when I see an outlier?\\n\\n- A dip or spike in traffic needs to be investigated. For example, a dip could be due to a production model server going down; a spike could be an adversarial attack.\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]\\n\\nQuestion: What all metrics do you support in LLMs'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac067ce6-5c41-4781-9e47-1e237af98c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fiddler supports the following metrics for monitoring charts in LLMs:\\n\\n1. Data Drift: You can plot drift for up to 20 columns at once and track it using either Jensen–Shannon distance (JSD) or Population Stability Index (PSI). Reference: [Supported Metric Types - Data Drift](https://docs.fiddler.ai/reference/data-drift-platform#what-is-being-tracked)\\n\\n2. Performance: The available metrics depend on the model task. For binary classification models, supported metrics include Accuracy, True Positive Rate/Recall, False Positive Rate, Precision, F1 Score, AUROC, Binary Cross Entropy, Geometric Mean, Calibrated Threshold, Data Count, and Expected Calibration Error. Reference: [Supported Metric Types - Performance](https://docs.fiddler.ai/reference/performance-tracking-platform#what-is-being-tracked)\\n\\n3. Data Integrity Violations: You can plot data integrity violations for up to 20 columns and track one of the three violations at once. Reference: [Supported Metric Types - Data Integrity Violations](https://docs.fiddler.ai/reference/data-integrity-platform#what-is-being-tracked)\\n\\n4. Traffic: This metric tracks the volume of traffic received by the model over time. Reference: [Supported Metric Types - Traffic](https://docs.fiddler.ai/reference/traffic-platform#what-is-being-tracked)\\n\\nPlease note that custom metrics are not currently supported.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba9d115c-5884-4f30-aba7-1063f13c48a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('slug: \"data-integrity\"  Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"fraud-detection\"  model\\n3. Monitoring data integrity issues that could harm the model performance\\n4. Investigating the features which have drifted/ compromised and analyzing them to mitigate the issue\\n5. Performing a root cause analysis to identify the exact cause and fix it\\n6. Diving into point explanations to identify how much the issue has an impact on a particular data point\\n7. Setting up alerts to make sure the issue does not happen again\\n\\nWe discovered there was an issue with the ‘Category’ column, wherein a new value was discovered in the production data. This led to the performance drop in the data likely due to the range violation. We suggest two steps to mitigate this issue:\\n\\n1. Setting up ‘alerts’ to identify similar issues in data integrity\\n2. Retraining the ML model after including the new data (with the ground truth labels) to teach the model of the new values\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\\\\n\"\\n}\\n[/block]',\n",
       " 'slug: \"data-integrity-platform\"  the data, and try to find the root cause of the issues.\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " \"Re-uploading in Fiddler essentially means having to delete what was uploaded or ingested earlier, making the updates you want to make, and then following the same steps as before for the specific thing you are looking to upload. So for example, if you want make changes to the baseline dataset you uploaded, you will have to delete the dataset and then make modifications for the datasetinfo object or the dataset itself and then upload the dataset again. As for events, currently there isn't a way for the user to directly delete events. Please contact Fiddler personnell for the same. \",\n",
       " 'slug: \"fraud-detection\"  to Data Integrity Issues\\n\\n#### Step 1 - Setting up baseline and publishing production events\\n\\nPlease refer to our [`Quick Start Guide`](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/quickstart/Fiddler_Quick_Start_Guide.ipynb) for a step-by-step walkthrough of how to upload baseline and production data to the Fiddler platform.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/fa8ded4-DatasetReady2.gif\",\\n        \"DatasetReady2.gif\",\\n        1064\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Setting up baseline\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 2 - Monitor Drift\\n\\nOnce the production events are published, we can monitor drift for the model output in the ‘drift’ tab i.e. - pred_is_fraud, which is the probability value of a case being a fraud. Here we can see that the prediction value of pred_is_fraud increased from February 15 to February 16. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/2f4bd83-MonitorDrift2.jpg\",\\n        \"MonitorDrift2.jpg\",\\n        1221\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Monitor drift\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 3 - Monitor Performance Metrics\\n\\nNext, To check if the performance has degraded, we can check the performance metrics in the ‘Performance’ tab. Here we will monitor the ‘Recall’ and ‘FPR’ of the model. We can see that the recall has gone down and FPR has gone up in the same period.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/048968e-ModelPerformance1.png\",\\n        \"ModelPerformance1.png\",\\n        2624\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Performance Chart\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 4 - Data Integrity\\n\\nThe performance drop could be due to a change in the quality of the data. To check that we can go to the ‘Data Integrity’ tab to look for Missing Value Violations, Type Violations, Range Violations, etc. We can see the columns ‘Category’ suffers range violations. Since this is a ‘categorical’ column, there is likely a new value that the model did not encounter during training.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/eef991e-DataIntegrity1.png\",\\n        \"DataIntegrity1.png\",\\n        3260\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Data Integrity\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 5 - Check the impact of drift\\n\\nWe can go back to the ‘Data Drift’ tab to measure how much the data integrity issue has impacted the prediction. We can select the bin in which the drift increased. The table below shows the Feature Impact, Feature Drift, and Prediction Drift Impact values for the selected bin. We can see that even though the Feature Impact for ‘Category’ value is less than the ‘Amt’ (Amount) value, because of the drift, its Prediction Drift Impact is more. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io',\n",
       " '---\\ntitle: \"Publishing Production Data\"\\nslug: \"publishing-production-data\"\\nhidden: false\\ncreatedAt: \"2022-11-18T23:28:25.348Z\"\\nupdatedAt: \"2022-12-19T19:14:28.171Z\"\\n---\\nThis Section guides you on the various ways you can provide event data to Fiddler and update and retrieve them.',\n",
       " 'slug: \"data-drift\" e-Monitor_DriftAnaly.png \"Monitor_DriftAnaly.png\")\\n\\n## Why is it being tracked?\\n\\n- Data drift is a great proxy metric for **performance decline**, especially if there is delay in getting labels for production events. (e.g. In a credit lending use case, an actual default may happen after months or years.)\\n- Monitoring data drift also helps you stay informed about **distributional shifts in the data for features of interest**, which could have business implications even if there is no decline in model performance.\\n\\n## What do I do next with this information?\\n\\n- High drift can occur as a result of _data integrity issues_ (bugs in the data pipeline), or as a result of _an actual change in the distribution of data_ due to external factors (e.g. a dip in income due to COVID). The former is more in our control to solve directly. The latter may not be solvable directly, but can serve as an indicator that further investigation (and possible retraining) may be needed.\\n- You can drill down deeper into the data by examining it in the Analyze tab. \\n\\nThe image below shows how to open the Analyze view for a specific feature and time range identified in the Data Drift page.\\n\\n![](https://files.readme.io/8a699e1-Monitor_DDrift_Analyze.png \"Monitor_DDrift_Analyze.png\")\\n\\nThis will bring you to the Analyze tab, where you can then use SQL to slice and dice the data.  You can then apply visualizations upon these slices to analyze the model’s behavior.\\n\\n![](https://files.readme.io/25eca03-Monitor_Analyze.png \"Monitor_Analyze.png\")\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions_\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"cv-monitoring\" _ID, \\'monitor\\']))\\n```\\n\\n*Please allow 3-5 minutes for monitoring data to populate the charts.*\\n  \\nThe following screen (without the annotation bubbles) will be available to you upon completion.\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://github.com/fiddler-labs/fiddler-examples/raw/main/quickstart/images/image_monitoring_drift.png\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n\\n\\n---\\n\\n\\n**Questions?**  \\n  \\nCheck out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\\n\\nJoin our [community Slack](http://fiddler-community.slack.com/) to ask any questions!\\n\\nIf you\\'re still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we\\'ll get back to you shortly.\\n',\n",
       " '---\\ntitle: \"About Datasets\"\\nslug: \"about-datasets\"\\nhidden: false\\ncreatedAt: \"2022-05-23T16:27:08.892Z\"\\nupdatedAt: \"2022-05-23T16:41:26.298Z\"\\n---\\nDatasets (or baseline datasets) are used for making comparisons with production data.\\n\\nA baseline dataset should be sampled from your model\\'s training set, so it can serve as a representation of what the model expects to see in production.\\n\\nFor more information, see [Uploading a Baseline Dataset](doc:uploading-a-baseline-dataset).\\n\\nFor guidance on how to design a baseline dataset, see [Designing a Baseline Dataset](doc:designing-a-baseline-dataset).',\n",
       " '---\\ntitle: \"About Models\"\\nslug: \"about-models\"\\nhidden: false\\ncreatedAt: \"2022-05-23T19:03:52.998Z\"\\nupdatedAt: \"2022-12-13T22:54:17.166Z\"\\n---\\nA model is a **representation of your machine learning model**. Each model must have an associated dataset to be used as a baseline for monitoring, explainability, and fairness capabilities.\\n\\nYou **do not need to upload your model artifact in order to onboard your model**, but doing so will significantly improve the quality of explanations generated by Fiddler.',\n",
       " 'slug: \"customer-churn-prediction\"  churn model drops due to range violation in one of the features. We can improve the performance by retraining the model with new data but before that we must perform mitigation actions which would help us in preemptively detecting the model performance degradation and inform our retraining frequency.\\n\\n#### Step 8 - Mitigation Actions\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/6190a63-churn-image13-mitigate.png\",\\n        \"churn-image13-mitigate.png\",\\n        1618\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Add to dashboard\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n1. **Add to dashboard**  \\n   We can add the chart generated to the dashboard by clicking on **Pin this chart** on the RHS of the Analyze tab. This would help us in monitoring importance aspects of the model.\\n\\n2. **Add alerts**  \\n   We can alert users to make sure we are notified the next time there is a performance degradation. For instance, in this example, there was a performance degradation due to range data integrity violation. To mitigate this, we can set up an alert which would notify us in case the percentage range violation exceeds a certain threshold (10% would be a good number in our case). We can also set up alerts on drift values for prediction etc. Check out this [link](doc:alerts-ui) to learn how to set up alerts on Fiddler platform.\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Data Pipeline Integrations\"\\nslug: \"data-pipeline-integrations\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:18:12.053Z\"\\nupdatedAt: \"2022-04-19T20:18:12.053Z\"\\n---\\n',\n",
       " '---\\ntitle: \"Designing a Baseline Dataset\"\\nslug: \"designing-a-baseline-dataset\"\\nhidden: false\\ncreatedAt: \"2022-05-23T16:30:17.415Z\"\\nupdatedAt: \"2023-01-12T18:02:44.450Z\"\\n---\\nIn order for Fiddler to monitor drift or data integrity issues in incoming production data, it needs something to compare this data to.\\n\\nA baseline dataset is a **representative sample** of the kind of data you expect to see in production. It represents the ideal form of data that your model works best on.\\n\\n*For this reason,* ***it should be sampled from your model’s training set.***\\n\\n***\\n\\n**A few things to keep in mind when designing a baseline dataset:**\\n\\n* It’s important to include **enough data** to ensure you have a representative sample of the training set.\\n* You may want to consider **including extreme values (min/max)** of each column in your training set so you can properly monitor range violations in production data. However, if you choose not to, you can manually specify these ranges before upload (see [Customizing Your Dataset Schema])(doc:customizing-your-dataset-schema).',\n",
       " 'slug: \"explainability-with-model-artifact-quickstart-notebook\" models\\', MODEL_ID, \\'monitor\\']))\\n```\\n\\n*Please allow 3-5 minutes for monitoring data to populate the charts.*\\n\\n\\n\\n---\\n\\n\\n**Questions?**  \\n  \\nCheck out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\\n\\nIf you\\'re still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we\\'ll get back to you shortly.\\n',\n",
       " 'slug: \"fraud-detection\"  11, we can see that the output probability value was 0 (predicted as fraud according to the threshold of 0.5) but the actual value was ‘not fraud’. \\n\\nThe bulb icon will take us to the ‘Explain’ tab. Here we can see that the ‘category’ value contributed to the model predicting the case as ‘fraud’.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/16d1150-RCA7.png\",\\n        \"RCA7.png\",\\n        3330\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Point Explanation\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 7 - Actions\\n\\nWe discovered that the prediction drift and performance drop were due to the introduction of a new value in the ‘Category’ column. We can take steps so that we could identify this kind of issue in the future before it can result in business impact.\\n\\n##### Setting up Alerts\\n\\nIn the ‘Analyze’ tab, we can set up alerts to notify us of as soon as a certain data issue happens. For example, for the case we discussed, we can set up alerts as shown below to alert us when the range violation increases beyond a certain threshold (e.g.-5%).\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/f7ece9a-Alert2.png\",\\n        \"Alert2.png\",\\n        1386\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Setting up Alerts\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nThese alerts can further influence the retraining of the ML model, we can retrain the model including the new data so the newly trained model contains the ‘insurance’ category value. This should result in improved performance.\\n\\n#### Data Insights\\n\\nBelow we can see the confusion matrix for February 16, 2019 (before drift starts). We can observe a good performance with Recall at 100% and 0.1% FP\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/09c82f2-FraudInsights2.png\",\\n        \"FraudInsights2.png\",\\n        1574\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Slice Evaluation - Feb 17\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nBelow we can see the confusion matrix for February 17, 2019 (after drift starts). We can observe a performance drop with Recall at 50% and 9% FP\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/c1c6e39-FraudInsights1.png\",\\n        \"FraudInsights1.png\",\\n        1574\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Slice Evaluation - Feb 16\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n### Conclusion\\n\\nUndetected fraud cases can lead to losses for the company and customers, not to mention damage reputation and relationship with customers. The Fiddler AI Observability platform can be used to identify the pitfalls in your ML model and mitigate them before they have an impact on your business.\\n\\nIn this walkthrough, we investigated one such issue with a fraud detection model where a data integrity issue caused the performance of the ML model to drop. \\n\\nFiddler can be used to keep the health of your fraud detection model up by:  \\n\\n1. Monitoring the drift of the performance metric\\n2. Monitoring various performance metrics associated with the',\n",
       " 'slug: \"customer-churn-prediction\" /bb02793-churn-image2-monitor-performance-metrics.png\",\\n        \"churn-image2-monitor-performance-metrics.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Monitor Performance Metrics\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 4 - Data Integrity\\n\\nOur next step would be to check if this could be due to any data integrity issues. On navigating to the **Data Integrity** tab under the **Monitor** tab, we see that there has been a range violation. On selecting the bins which have the range violations, we notice it is due to the field `numofproducts`. \\n\\nIt is advised to check all the fields which cause data integrity violations. Since we see a range violation, we can check how much the data has drifted.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/5819966-churn-image3-data-integrity.png\",\\n        \"churn-image3-data-integrity.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"smart\",\\n      \"caption\": \"Data Integrity\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 5 - Check the impact of drift on ‘numofproducts’ features\\n\\nOur next step would be to go back to the **Data Drift** tab to measure the amount of drift in the field `numofproducts`. The drift is calculated using **Jensen Shannon Divergence**, which compares the distributions of the two data sets being compared. \\n\\nWe can select the bin where we see an increase in average value as well as drift. We see a significant increase in the `numofproducts` average value and drift. We can also see there is a difference in the distribution of the baseline and production data which leads to a drift. \\n\\nNext step could be to find out if the change in distribution was only for a subsection of data or was it due to other factors like time (seasonality etc.), fault in data reporting (sensor data), change in the unit in which the metric is reported etc.  \\nSeasonality could be observed by plotting the data across time (provided we have enough data), a fault in data reporting would mean missing values, and change in unit of data would mean change in values for all subsections of data.\\n\\nIn order to investigate if the change was only for a subsection of data, we will go to the **Analyze** tab. We can do this by clicking **Export bin and feature to Analyze**. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/1d1a5b3-churn-image4-impact-of-drift.png\",\\n        \"churn-image4-impact-of-drift.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Impact of Drift\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 6 - Root Cause Analysis in the ‘Analyze’ tab\\n\\nIn the analyze tab, we will have an auto-generated SQL query based on our selection in the **Monitor** tab, we can also write custom SQL queries to investigate the data. \\n\\nWe check the distribution of the field `numofproducts` for our selection. We can do this by selecting **Chart Type - Feature Distribution** on the RHS of the tab. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/9b1f7d1-Churn-image5-analyze-rca-1.png\",\\n        \"',\n",
       " '---\\ntitle: \"Dashboard Utilities\"\\nslug: \"dashboard-utilities\"\\nhidden: false\\ncreatedAt: \"2023-02-22T18:05:38.134Z\"\\nupdatedAt: \"2023-02-22T18:08:05.939Z\"\\n---\\n## Dashboards Utilities\\n\\n### Dashboard Name\\n\\nTo rename your dashboard, simply click on the \"Untitled Dashboard\" title on the top-left corner of the dashboard studio. This will allow you to give your dashboard a more descriptive name that reflects its purpose and contents, making it easier to find and manage among your other dashboards.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/5006167-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\nOnce you\\'ve clicked on the \"Untitled Dashboard\" title to rename your dashboard, simply type in the desired name and hit \"Enter\" on your keyboard to save the new name.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/7ec6e36-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/453ed10-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\nIf you change your mind and want to discard the changes, simply click anywhere on the page outside of the name box. This will cancel the renaming process and leave the dashboard name as it was before.\\n\\n### Save, Copy Link, and Delete\\n\\nYou can easily manage your dashboard by using the control panel located on the top left of the dashboard studio. This panel allows you to save your dashboard, copy a link to it, or delete it entirely. By using these controls, you can easily share your dashboard with others or remove it from your collection if it is no longer needed.\\n\\n![](https://files.readme.io/17c9043-image.png)\\n\\n#### Save\\n\\nIt\\'s important to note that dashboards are not automatically saved, so you\\'ll need to manually save your dashboard in order to lock in the current charts and filters. Once you\\'ve made the desired changes to your dashboard, simply click the \"Save\" button to save your progress. This will also enable you to share or delete your dashboard as needed. By saving your dashboard frequently, you can ensure that you never lose important information or data visualizations.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/c624c13-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n#### Copy Link\\n\\nIf you want to share your dashboard with other users on Fiddler, the first step is to ensure that they have access to the project that the dashboard belongs to. Once you\\'ve confirmed that they have access, you can easily share the dashboard by copying the dashboard link and sending it to them. This makes it simple to collaborate and share insights with others who are working on the same project or who have an interest in your findings. Note that you can\\'t share a dashboard link until you\\'ve saved the dashboard.\\n\\n[block:image]\\n{\\n  \"images\": [\\n   ',\n",
       " '---\\ntitle: \"Customer Churn Prediction\"\\nslug: \"customer-churn-prediction\"\\nhidden: false\\ncreatedAt: \"2022-05-17T19:12:12.382Z\"\\nupdatedAt: \"2023-08-04T23:20:11.330Z\"\\n---\\nChurn prediction is a common use case in the machine learning domain. Churn means “leaving the company”. It is very critical for a business to have an idea about why and when customers are likely to churn. Having a robust and accurate churn prediction model helps businesses to take action to prevent customers from leaving the company. Machine learning models have proved to be effective in detecting churn. However, if left unattended, the performance of churn models can degrade over time leading to losing customers. \\n\\nThe Fiddler AI Observability platform provides a variety of tools that can be used to monitor, explain, analyze, and improve the performance of your machine learning-based churn model.\\n\\nIn this article we will go over a churn example and how we can mitigate performance degradation in a churn machine learning model.\\n\\nRefer to the [colab notebook](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/business-use-cases/churn-usecase/Fiddler_Churn_Use_Case.ipynb) to learn how to -\\n\\n1. Onboard model on the Fiddler platform\\n2. Publish events on the Fiddler platform\\n3. Use the Fiddler API to run explanations\\n\\n### Example - Model Performance Degradation due to Data Integrity Issues\\n\\n#### Step 1 - Setting up baseline and publishing production events\\n\\nPlease refer to our [Getting Started guide](https://docs.fiddler.ai/pages/getting-started/product-tour/) for a step-by-step walkthrough of how to upload baseline and production data to the Fiddler platform.\\n\\n#### Step 2 - Monitor Drift\\n\\nWhen we check the monitoring dashboard, we notice a drop in the predicted churn value and a rise in the predicted churn drift value. Our next step is to check if this has resulted in a drop in performance.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/2f20fd2-Churn-image1-monitor-drift.png\",\\n        \"Churn-image1-monitor-drift.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Monitor Drift\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 3 - Monitor Performance Metrics\\n\\nWe use **precision, recall, and F1-score** as accuracy metrics for this example. We’re choosing these metrics as they are suited for classification problems and help us in identifying the number of false positives and false negatives. We notice that although the precision has remained constant, there is a drop in the F1-score and recall, which means that there are a few customers who are likely to churn but the model is not able to predict their outcome correctly. \\n\\nThere could be a number of reasons for drop in performance, some of them are-\\n\\n1. Cases of extreme events (Outliers)\\n2. Data distribution changes\\n3. Model/Concept drift\\n4. Pipeline health issues\\n\\nWhile **Pipeline health issues** could be due to a component in the Data pipeline failing, the first 3 could be due to changes in data. In order to check that we can go to the **Data Integrity** tab to first check if the incoming data is consistent with the baseline data.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io',\n",
       " 'slug: \"dashboards-ui\" Week`, or `Month`. \\n\\n> 📘 Note: Hour bin sizes are not supported for time ranges above 90 days.\\n> \\n> For example, if we select the `6M` data range, we see that the `Hourly` bin selection is disabled. This is disabled to avoid long computation and dashboard loading times.\\n\\n![](https://files.readme.io/93f7576-image.png)\\n\\n### Saved Model Updates\\n\\nIf you recently created or updated a saved chart and are not seeing the changes either on the dashboard itself or the Saved Charts list, click the refresh button on the main dashboard studio or within the saved charts list to reflect updates.\\n\\n![](https://files.readme.io/706c198-image.png)\\n\\n## Model Comparison\\n\\nWith our dashboard feature, you can also compare multiple models side-by-side, making it easy to see which models are performing the best and which may require additional attention. To create model-to-model comparison dashboards, ensure the models you wish to compare belong to the same project. Create the desired charts for each model and then add them to a single dashboard. By creating a single dashboard that tracks the health of all of your models, you can save time and simplify your AI monitoring efforts. With these dashboards, you can easily share insights with your team, management, or stakeholders, and ensure that everyone is up-to-date on your AI performance.\\n\\n![](https://files.readme.io/33b97ae-image.png)\\n\\n### Check [Dashboard Utilities ](doc:dashboard-utilities)and [Dashboard Interactions](doc:dashboard-interactions) pages for more info on dashboard usage.',\n",
       " 'slug: \"analytics-ui\"  details of your dataset to help you understand the data’s distribution and correlations.\\n\\nSelect a target variable to see the dependence between that variable and the others, measured by [mutual information (MI)](https://en.wikipedia.org/wiki/Mutual_information). A low MI is an indicator of low correlation between two variables, and can be used to decide if particular variables should be dropped from the model.\\n\\n![](https://files.readme.io/69f1a0a-Dataset_details_1.png \"Dataset_details_1.png\")\\n\\n![](https://files.readme.io/d3a97a8-Dataset_details_2.png \"Dataset_details_2.png\")\\n\\n![](https://files.readme.io/cd0b499-Dataset_details_3.png \"Dataset_details_3.png\")\\n\\n## Point Overview\\n\\n> 📘 Info\\n> \\n> To view this visualization, click on **Explain** (light bulb icon) for any row in the **Data** table.\\n\\nThis visualization provides a human-readable overview of a point explanation.\\n\\n![](https://files.readme.io/335714a-Explain_Overview.png \"Explain_Overview.png\")\\n\\n## Feature Attribution\\n\\n> 📘 Info\\n> \\n> To view this visualization, click on **Explain** (light bulb icon) for any row in the **Data** table.\\n\\nFeature attributions can help you understand which model inputs were responsible for arriving at the model output for a particulat prediction.\\n\\nWhen you want to check how the model is behaving for one prediction instance, use this visualization first.\\n\\nMore information is available on the [Point Explainability](doc:point-explainability) page.\\n\\n![](https://files.readme.io/08d409f-Explain_Chart.png \"Explain_Chart.png\")\\n\\n## Feature Sensitivity\\n\\n> 📘 Info\\n> \\n> To view this visualization, click on **Explain** (light bulb icon) for any row in the **Data** table.\\n\\nThis visualization helps you understand how changes in the model’s input values could impact the model’s prediction for this instance.\\n\\n**_ICE plots_**\\n\\nOn initial load, the visualization shows an Individual Conditional Expectation (ICE) plot for each model input.\\n\\n![](https://files.readme.io/ac5f0b2-WhatIF_Chart.png \"WhatIF_Chart.png\")\\n\\nICE plots shows how the model prediction is affected by changes in an input for a particular instance. They’re computed by changing the value of an input—while keeping all other inputs constant—and plotting the resulting predictions.\\n\\nRecall the [partial dependence plots](#partial-dependence-plot-pdp) discussed earlier, which showed the average effect of the feature across the entire slice. In essence, the PDP is the average of all the ICE plots. The PDP can mask interactions at the instance level, which an ICE plot will capture.\\n\\nYou can update any input value to see its impact on the model output, and then view the updated ICE plots for the changed input values.\\n\\nThis is a powerful technique for performing counterfactual analysis of a model prediction. When you plot the updated ICE plots, you’ll see two lines (or sets of bars in the case of categorical inputs).\\n\\nIn the image below, the solid line is the original ICE plot, and the dotted line is the ICE plot using the updated input values. Comparing these two sets of plots can help you understand if the model’s behavior changes as expected with a hypothetical model input.\\n\\n![](https://files.readme.io/9311aea-WhatIF_After.png \"WhatIF_After.png\")\\n\\n## Dashboard\\n\\nOnce visualizations are created, you can pin them to the',\n",
       " 'slug: \"fraud-detection\" /328c6b6-DriftImpact1.png\",\\n        \"DriftImpact1.png\",\\n        3300\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Drift Impact\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe will now move on to check the difference between the production and baseline data for this bin. For this, we can click on ‘Export bin and feature to Analyze’. Which will land us on the Analyze tab.\\n\\n#### Step 6 - Root Cause Analysis in the ‘Analyze’ tab\\n\\nThe analyze tab pre-populated the left side of the tab with the query based on our selection. We can also write custom queries to slice the data for analysis.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/31a6110-RCA2.jpg\",\\n        \"RCA2.jpg\",\\n        1226\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Analyze Tab\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/a3e4b27-RCA3.png\",\\n        \"RCA3.png\",\\n        1660\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Analyze Query\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nOn the right-hand side of the tab we can build charts on the tabular data based on the results of our custom query. For this RCA we will build a ‘Feature Distribution’ chart on the ‘Category’ column to check the distinct values and also measure the percentage of each value. We can see there are 15 distinct values along with their percentages.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/4996cad-RCA4.png\",\\n        \"RCA4.png\",\\n        1634\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Feature Distribution - Production\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nNext, we will compare the Feature Distribution chart in production data vs the baseline data to find out about the data integrity violation. We can modify the query to obtain data for baseline data and produce a ‘Feature Distribution’ chart for the same.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/303c243-RCA5.png\",\\n        \"RCA5.png\",\\n        1600\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Feature Distribution - Baseline\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe can see that the baseline data has just 14 unique values and ‘insurance’ is not present in baseline data. This ‘Category’ value wasn’t present in the training data and crept in production data likely causing performance degradation.  \\nNext, we can perform a ‘point explanation’ for one such case where the ‘Category’ value was ‘Insurance’ and the prediction was incorrect to measure how much the ‘Category’ column contributed to the prediction by looking at its SHAP value.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/c1c1c81-RCA6.png\",\\n        \"RCA6.png\",\\n        1650\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Mislabelled Data Point\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe can click on the bulb sign beside the row to produce a point explanation. If we look at example',\n",
       " '---\\ntitle: \"Analytics\"\\nslug: \"analytics-ui\"\\nexcerpt: \"UI Guide\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:24:49.443Z\"\\nupdatedAt: \"2023-04-06T22:23:54.929Z\"\\n---\\n## Interface\\n\\nThe **Analyze** tab has three parts:\\n\\n1. **_Slice Query box_** _(top-left)_ — Accepts a SQL query as input, allowing quick access to the slice.\\n2. **_Data table_** _(bottom-left)_ — Lets you browse instances of data returned by the query.\\n3. **_Explanations column_** _(right)_ — Allows you to view explanations for the slice and choose from a range of rich visualizations for different data insights.\\n\\n![](https://files.readme.io/f2daf80-S_E_Landing.png \"S_E_Landing.png\")\\n\\n**Workflow**\\n\\n1. Write a SQL query in the **Slice Query** box and click **Run**.\\n\\n![](https://files.readme.io/a76a852-S_E_Step_2.png \"S_E_Step_2.png\")\\n\\n2. View the data returned by the query in the **Data** table.\\n\\n![](https://files.readme.io/8771686-S_E_Step_3.png \"S_E_Step_3.png\")\\n\\n3. Explore a variety of visualizations using the **Explanations** column on the right.\\n\\n![](https://files.readme.io/3d16c4e-S_E_Step_4.png \"S_E_Step_4.png\")\\n\\n## SQL Queries\\n\\n![](https://files.readme.io/80c4bfe-S_E_First_Time.png \"S_E_First_Time.png\")\\n\\nThe **Slice Query** box lets you:\\n\\n1. Write a SQL query\\n2. Search and auto-complete field names (i.e. your dataset, the names of your inputs or outputs)\\n3. Run the SQL query\\n\\nIn the UI, you will see examples for different types of queries:\\n\\n- Example query to analyze your dataset:\\n\\n```\\nselect * from \"your_dataset_id\" limit 100\\n```\\n\\n\\n\\n- Example query to analyze your model:\\n\\n```\\nselect * from \"your_dataset_id.your_model_id\" limit 100\\n```\\n\\n\\n\\n- Example query to analyze production traffic:\\n\\n```\\nselect * FROM production.\"your_model_id\"\\nwhere fiddler_timestamp between \\'2020-10-20 00:00:00\\' AND \\'2020-10-20 12:00:00\\'limit 100\\n```\\n\\n\\n\\n> 🚧 Note\\n> \\n> Only read-only SQL operations are supported. Slices are auto-detected based on your model, dataset, and query. Certain SQL operations like aggregations and joins might not result in a valid slice.\\n\\n## Data\\n\\nIf the query successfully returns a slice, the results display in the **Data** table below the **Slice Query** box.\\n\\n![](https://files.readme.io/1d7dd42-S_E_Data.png \"S_E_Data.png\")\\n\\nYou can view all data rows and their values or download the data as a CSV file to plug it into another system. By clicking on **Explain** (light bulb icon) in any row in the table, you can access explanations for that individual input (more on this in the next section).\\n\\n## Explanations\\n\\nThe Analyze tab offers a variety of powerful visualizations to quickly let you analyze and explain slices of your dataset.\\n\\n1. [**Feature Correlation**](#feature-correlation) — View the correlation between model inputs and/or outputs.\\n2.',\n",
       " 'slug: \"dashboard-utilities\"  {\\n      \"image\": [\\n        \"https://files.readme.io/520189a-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n#### Delete\\n\\nTo delete a dashboard, click the overflow button next to the copy link icon. NOTE: Once a dashboard has been deleted it cannot be recovered.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/e768501-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]',\n",
       " 'slug: \"clientrun_fairness\"     dataset_id=DATASET_ID,\\\\n    protected_features=protected_features,\\\\n    positive_outcome=positive_outcome,\\\\n    slice_query=slice_query\\\\n)\",\\n      \"language\": \"python\",\\n      \"name\": \"Usage - With a SQL Query\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"Return Type\",\\n    \"h-1\": \"Description\",\\n    \"0-0\": \"dict\",\\n    \"0-1\": \"A dictionary containing fairness metric results.\"\\n  },\\n  \"cols\": 2,\\n  \"rows\": 1\\n}\\n[/block]',\n",
       " 'slug: \"analytics-ui\"  project dashboard, which can be shared with others.\\n\\nTo pin a chart, click on the thumbtack icon and click **Send**. If the **Update with Query** option is enabled, the pinned chart will update automatically whenever the underlying query is changed on the **Analyze** tab.\\n\\n![](https://files.readme.io/c4247d1-Pinning_Chart.png \"Pinning_Chart.png\")\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Alerts with Fiddler UI\"\\nslug: \"alerts-ui\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:25:34.901Z\"\\nupdatedAt: \"2023-04-06T22:22:10.370Z\"\\n---\\nFiddler allows you to set up [alerts](https://docs.fiddler.ai/v1.6/docs/alerts-platform) for your model. View your alerts by clicking on the Alerts tab in the navigation bar. The Alerts tab presents three views: Triggered Alerts, Alert Rules, and Integrations. Users can set up alerts using both the Fiddler UI and the Fiddler API Client. This page introduces the available alert types, and how to set up and view alerts in the Fiddler UI. For instructions about how to use the Fiddler API client for alert configuration see [Alert Configuration with Fiddler Client](doc:alerts-client).\\n\\n![](https://files.readme.io/1730387-image.png)\\n\\n## Setting up Alert Rules\\n\\nTo create a new alert using the Fiddler UI, click the **Add Alert** button on the top-right corner of any screen on the Alerts tab. \\n\\n![](https://files.readme.io/78537d3-image.png)\\n\\nIn the Alert Rule form, provide the basic information such as the desired alert name, and the project and model of interest. \\n\\n![](https://files.readme.io/8418e4f-image.png)\\n\\nNext, select the Alert Type you would like to monitor. Users can select from Performance, Data Drift, Data Integrity, or Traffic monitors. For this example, we\\'ll set up a Data Drift alert to measure distribution drift.\\n\\n![](https://files.readme.io/d51ca30-image.png)\\n\\nOnce an Alert Type is selected, users can choose a metric corresponding to the Alert Type for which to set the alert on. For our Data Drift alert, we will use JSD (Jensen–Shannon distance) as our metric. The next consideration are the bin size, which is the duration for which fiddler monitoring calculates the metric values, and the column to apply this monitor on. Let\\'s choose a 1 hour bin and the CreditScore column for this example. \\n\\n![](https://files.readme.io/9ba686e-image.png)\\n\\nNext, users can focus on the alerts comparison method. Learn more about Alert comparisons on the [Alerts Platform Guide](https://docs.fiddler.ai/v1.6/docs/alerts-platform). For our example we will select the Relative comparison option, and compare to the same time 7 days back. Users can select the alert condition as well as a Warning and Critical threshold. We will ask for an alert when the production data is greater than 10%.\\n\\n![](https://files.readme.io/cb3f4b0-image.png)\\n\\nFinally user can set the alert rules priority- how important this alert is to a customers work streams, along with how to get notified of triggered alerts. \\n\\n![](https://files.readme.io/0e75a9e-image.png)\\n\\n Last, click **Add Alert Rule** when you\\'re done. In order to create and configure alerts using the Fiddler API client see [Alert Configuration with Fiddler Client](https://docs.fiddler.ai/v1.5/docs/fiddler-ui).\\n\\n![](https://files.readme.io/72a1e8b-image.png)\\n\\n### Alert Rules Tab\\n\\nOnce an alert rule is created it can be viewed in the **Alert Rules** tab. This view enables you to view all alert',\n",
       " '---\\ntitle: \"About Projects\"\\nslug: \"about-projects\"\\nhidden: false\\ncreatedAt: \"2022-05-23T16:10:39.711Z\"\\nupdatedAt: \"2022-06-13T20:14:54.951Z\"\\n---\\nProjects are **used to organize your models and datasets**. Each project can represent a machine learning task (e.g. predicting house prices, assessing creditworthiness, or detecting fraud).\\n\\nA project **can contain one or more models** (e.g. lin_reg_house_predict, random_forest_house_predict).\\n\\nFor more information on projects, click [here](doc:project-structure).',\n",
       " '---\\ntitle: \"Datadog Integration\"\\nslug: \"datadog-integration\"\\nhidden: false\\ncreatedAt: \"2023-06-21T15:21:52.559Z\"\\nupdatedAt: \"2023-06-21T15:51:15.017Z\"\\n---\\nFiddler offers an integration with Datadog which allows Fiddler and Datadog customers to bring their AI Observability metrics from Fiddler into their centralized Datadog dashboards.  Additionally, a Fiddler license can now be procured through the [Datadog Marketplace](https://www.datadoghq.com/blog/tag/datadog-marketplace/). This [integration](https://www.datadoghq.com/blog/monitor-machine-learning-models-fiddler/) enables you to centralize your monitoring of ML models and the applications that utilize them within one unified platform.\\n\\n## Integrating Fiddler with Datadog\\n\\nInstructions for integrating Fiddler with Datadog can be found on the \"Integrations\" section of your Datadog console.  Simply search for \"Fiddler\" and follow the installation instructions provided on the \"Configure\" tab.  Please reach out to [support@fiddler.ai](mailto:support@fiddler.ai) with any issues or questions.\\n\\n![](https://files.readme.io/3f9dcd9-Screenshot_2023-06-21_at_10.28.17_AM.png)\\n\\n![](https://files.readme.io/9fa9503-Screenshot_2023-06-21_at_10.31.54_AM.png)\\n\\n![](https://files.readme.io/218dfc2-Screenshot_2023-06-21_at_10.45.14_AM.png)',\n",
       " 'slug: \"cv-monitoring\" model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION\\n# name of the column that contains ground truth\\ntarget = \\'target\\'\\n\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n    dataset_info=dataset_info,\\n    dataset_id=DATASET_ID,\\n    features=embedding_cols,\\n    target=target,\\n    outputs=CIFAR_CLASSES,\\n    custom_features=[CF1],\\n    model_task=model_task,\\n    description=\\'An example to showcase monitoring Image data using model embeddings.\\',\\n    categorical_target_class_details=list(CIFAR_CLASSES),\\n)\\nmodel_info\\n```\\n\\nNow we specify a unique model ID and use the client\\'s [add_model](https://docs.fiddler.ai/reference/clientadd_model) function to send this information to Fiddler.\\n\\n\\n```python\\nMODEL_ID = \\'resnet18\\'\\n\\nclient.add_model(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID,\\n    model_id=MODEL_ID,\\n    model_info=model_info,\\n)\\n```\\n\\n# 5. Inject data drift and publish production events\\n\\nNetx, we\\'ll inject data drift in form of blurring and brightness-reduction. The following cell illustrates these transforms.\\n\\n\\n```python\\ndrift_xform_lut = {\\n    \\'original\\': None,\\n    \\'blurred\\': get_blur_transforms(),\\n    \\'brightness_reduced\\': get_brightness_transforms(),\\n}\\nfor drift_type, xform in drift_xform_lut.items():\\n    cifar_testloader = get_cifar_dataloader(train_data=False, batch_size=32, image_transforms=xform)\\n    # get some test images\\n    dataiter = iter(cifar_testloader)\\n    images, labels = dataiter.next()\\n\\n    # show images\\n    print(f\\'Image type: {drift_type}\\')\\n    imshow(torchvision.utils.make_grid(images))\\n```\\n\\n### Publish events to Fiddler\\n\\nWe\\'ll publish events over past 3 weeks. \\n\\n- Week 1: We publish CIFAR-10 test set, which would signify no distributional shift\\n- Week 2: We publish **blurred** CIFAR-10 test set \\n- Week 3: We publish **brightness reduce** CIFAR-10 test set \\n\\n\\n```python\\nimport time\\n\\nfor i, drift_type in enumerate([\\'original\\', \\'blurred\\', \\'brightness_reduced\\']):\\n    week_days = 6\\n    xform = drift_xform_lut[drift_type]\\n    cifar_testloader = get_cifar_dataloader(train_data=False, batch_size=32, image_transforms=xform)\\n    prod_df = generate_embeddings(resnet_model, device, cifar_testloader)\\n    week_offset = (2-i)*7*24*60*60*1e3 \\n    day_offset = 24*60*60*1e3\\n    print(f\\'Publishing events with {drift_type} transformation for week {i}.\\')\\n    for day in range(week_days): \\n        now = time.time() * 1000\\n        timestamp = int(now - week_offset - day*day_offset)\\n        events_df = prod_df.sample(1000)\\n        events_df[\\'timestamp\\'] = timestamp\\n        client.publish_events_batch(\\n            project_id=PROJECT_ID,\\n            model_id=MODEL_ID,\\n            batch_source=events_df,\\n            timestamp_field=\\'timestamp\\',\\n        )\\n```\\n\\n## 6. Get insights\\n\\n**You\\'re all done!**\\n  \\nYou can now head to Fiddler URL and start getting enhanced observability into your model\\'s performance.\\n\\nRun the following code block to get your URL.\\n\\n\\n```python\\nprint(\\'/\\'.join([URL, \\'projects\\', PROJECT_ID, \\'models\\', MODEL',\n",
       " '---\\ntitle: \"Data Drift\"\\nslug: \"data-drift\"\\nexcerpt: \"UI Guide\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:25:14.478Z\"\\nupdatedAt: \"2023-02-14T01:18:50.009Z\"\\n---\\nModel performance can be poor if models trained on a specific dataset encounter different data in production. This is called data drift. In the **Monitor** tab for your model, Fiddler gives you a visual way to explore data drift and identify what data is drifting, when it’s drifting, and how it’s drifting. This is the first step in identifying possible model performance issues.\\n\\n![](https://files.readme.io/0d04342-Monitoring-DataDrift.png \"Monitoring-DataDrift.png\")\\n\\nYou can change the time range using the controls in the upper-right:\\n\\n![](https://files.readme.io/d5809f8-Monitoring-TimeRange.png \"Monitoring-TimeRange.png\")\\n\\n## What is being tracked?\\n\\n- **_Drift Metrics_**\\n  - **Jensen–Shannon distance (JSD)**\\n    - A distance metric calculated between the distribution of a field in the baseline dataset and that same distribution for the time period of interest.\\n    - For more information on JSD, click [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html).\\n  - **Population Stability Index (PSI)**\\n    - A drift metric based on the multinomial classification of a variable into bins or categories. The differences in each bin between the baseline and the time period of interest are then utilized to calculate it as follows:\\n\\n![](https://files.readme.io/0baeb90-psi_calculation.png \"psi_calculation.png\")\\n\\nHere, `B` is the total number of bins, `ActualProp(b)` is the proportion of counts within bin `b` from the target distribution, and `ExpectedProp(b)` is the proportion of counts within bin `b` from the reference distribution. Thus, PSI is a number that ranges from zero to infinity and has a value of zero when the two distributions exactly match.\\n\\n> 🚧 Note\\n> \\n> Since there is a possibility that a particular bin may be empty, PSI can shoot to infinity. To avoid this, PSI calculation in Fiddler is done such that each bin count is incremented with a base_count=1. Thus, there might be a slight difference in the PSI values obtained from manual calculations.\\n\\n- **_Average Values_** – The mean of a field (feature or prediction) over time. This can be thought of as an intuitive drift score.\\n- **_Drift Analytics_** – You can drill down into the features responsible for the prediction drift using the table at the bottom.\\n  - **_Feature Impact_**: The contribution of a feature to the model’s predictions, averaged over the baseline dataset. The contribution is calculated using random ablation feature impact.\\n  - **_Feature Drift_**: Drift of the feature, calculated using the drift metric of choice.\\n  - **_Prediction Drift Impact_**: A heuristic calculated using the product of the feature impact and the feature drift. The higher the score, the more this feature is likely to have contributed to the prediction drift.\\n\\nIn the Drift Analytics table, you can select a feature to see the feature distribution for both the time period under consideration and the baseline dataset. If it’s a numerical feature, you will also see a time series of the average feature value over time.\\n\\n![](https://files.readme.io/63a452',\n",
       " 'slug: \"project-structure\" \\n\\nYou can collate specific visualizations under the Project Dashboard. After visualizations are created using the Model Analytics tool, you can pin them to the dashboard, which can then be shared with others.\\n\\n![](https://files.readme.io/b7cb9ce-Chart_Dashboard.png \"Chart_Dashboard.png\")\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_',\n",
       " '---\\ntitle: \"Dashboards\"\\nslug: \"dashboards-ui\"\\nhidden: false\\ncreatedAt: \"2023-02-21T22:35:31.234Z\"\\nupdatedAt: \"2023-02-27T20:04:02.598Z\"\\n---\\n## Creating Dashboards\\n\\nTo begin using our dashboard feature, navigate to the dashboard page by clicking on \"Dashboards\" from the top-level navigation bar. On the Dashboards page, you can choose to either select from previously created dashboards or create a new one. This simple process allows you to quickly access your dashboards and begin monitoring your models\\' performance, data drift, data integrity, and traffic.\\n\\n![](https://files.readme.io/570614f-image.png)\\n\\nWhen creating a new dashboard, it\\'s important to note that each dashboard is tied to a specific project space. This means that only models and charts associated with that project can be added to the dashboard. To ensure you\\'re working within the correct project space, select the desired project space before entering the dashboard editor page, then click \"Continue.\" This will ensure that you can add relevant charts and models to your dashboard.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/ef961be-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n## Add Monitoring Chart\\n\\nOnce you’ve created a dashboard, you can add previously saved monitoring charts that display these metrics over time, making it easy to track changes and identify patterns.\\n\\n![](https://files.readme.io/b862277-image.png)\\n\\nTo create a new monitoring chart for your dashboard, simply select \"New Monitoring Chart\" from the \"Add\" dropdown menu. For more information on creating and customizing monitoring charts, check out our Monitoring Charts UI Guide.\\n\\nIf you\\'d like to add an existing chart to your dashboard, select \"Saved Charts\" to display a full list of monitoring charts that are available in your project space. This makes it easy to quickly access and add the charts you need to your dashboard for monitoring and reporting purposes.\\n\\n![](https://files.readme.io/2c3857c-image.png)\\n\\nTo further customize your dashboard, you can select the saved monitoring charts of interest by clicking on their respective cards. For instance, you might choose to add charts for Accuracy, Drift, Traffic, and Range Violation to your dashboard for a more comprehensive model overview. By adding these charts to your dashboard, you can quickly access important metrics and visualize your model\\'s performance over time, enabling you to identify trends and patterns that might require further investigation.\\n\\n## Dashboard Filters\\n\\nThere are three main filters that can be applied to all the charts within dashboards, these include date range, time zone, and bin size. \\n\\n![](https://files.readme.io/0795752-image.png)\\n\\n### Date Range\\n\\nWhen the `Default` time range is selected, the data range, time zone, and bin size that each monitoring chart was originally saved with will be applied. This enables you to create a dashboard where each chart shows a unique filter set to highlight what matters to each team. Updating the date range will unlock the time zone and bin size filters. You can select from a number of predefined ranges or choose `Custom` to select a start and end date-time.\\n\\n![](https://files.readme.io/960262c-image.png)\\n\\n### Bin Size\\n\\nBin size controls the frequency at which data is displayed on your monitoring charts. You can select from the following bin sizes: `Hour`, `Day`, `',\n",
       " 'slug: \"clientget_mutual_information\" )\",\\n      \"language\": \"python\",\\n      \"name\": \"Usage with SQL Query\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"Return Type\",\\n    \"h-1\": \"Description\",\\n    \"0-0\": \"dict\",\\n    \"0-1\": \"A dictionary containing mutual information results.\"\\n  },\\n  \"cols\": 2,\\n  \"rows\": 1\\n}\\n[/block]',\n",
       " 'slug: \"customer-churn-prediction\"  the subset of Hawaii, we see a huge performance drop.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/974b118-churn-image8--analyze-rca-4.png\",\\n        \"churn-image8--analyze-rca-4.png\",\\n        1624\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 7\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nOn the contrary, we see a good performance for the subset of data without the ‘Hawaii’. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/ee29b35-churn-image6-analyze-rca-4-1-1.png\",\\n        \"churn-image6-analyze-rca-4-1-1.png\",\\n        924\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 8\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/fc54636-churn-image9--analyze-rca-5.png\",\\n        \"churn-image9--analyze-rca-5.png\",\\n        1606\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 9\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Step 7 - Measuring the impact of the ‘numofproducts’ feature\\n\\nIn order to measure the impact of features - `numofproducts`, we can navigate back to the **Monitor** tab. We can see that the prediction drift impact is highest for `numofproducts` due to its high drift value, which means it is contributing the most to the prediction drift.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/e78d838-churn-image10-impact1.png\",\\n        \"churn-image10-impact1.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Feature Impact - 1\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe can further measure the attribution of the feature - `numofproducts` for a single data point. We can select a data point which was incorrectly predicted to not churn (false negative). We can check point explanations for a point from the **Analyze** by running a query or from the **Explain** tab. Below we check point explanations for a data point form analyze tab by clicking the **bulb** symbol from the query results. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/026d5cb-churn-image11-impact2.png\",\\n        \"churn-image11-impact2.png\",\\n        1654\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Feature Impact - 2\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe see that the feature - `numofproducts` attributes significantly towards the data point being predicted not to churn.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/65fd05d-churn-image12-impact3.png\",\\n        \"churn-image12-impact3.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Feature Impact - 3\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe have seen that the performance of the',\n",
       " 'slug: \"evaluation-ui\" Calibration Plot_**\\n  - A graph that tell us how well the model is calibrated. The plot is obtained by dividing the predictions into 10 quantile buckets (0-10th percentile, 10-20th percentile, etc.). The average predicted probability is plotted against the true observed probability for that set of points.',\n",
       " 'Custom metrics is an upcoming feature and it is currently not supported.',\n",
       " '---\\ntitle: \"Fraud Detection\"\\nslug: \"fraud-detection\"\\nexcerpt: \"How to monitor and improve your Fraud Detection ML Models using Fiddler\\'s AI Observability platform\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:06:54.951Z\"\\nupdatedAt: \"2023-08-04T23:19:48.015Z\"\\n---\\nMachine learning-based fraud detection models have been proven to be more effective than humans when it comes to detecting fraud. However, if left unattended, the performance of fraud detection models can degrade over time leading to big losses for the company and dissatisfied customers.  \\nThe **Fiddler AI Observability** platform provides a variety of tools that can be used to monitor, explain, analyze, and improve the performance of your fraud detection model.\\n\\n## Monitoring\\n\\n### Drift Detection\\n\\n- **Class-imbalanced Data** - Fraud use cases suffer from highly imbalanced data. Users can specify model weights on a global or event level to improve drift detection. Please see more information in  [Class-Imbalanced Data](https://docs.fiddler.ai/v1.3/docs/class-imbalanced-data). \\n\\n- **Feature Impact** - Tells us the contribution of features to the model\\'s prediction, averaged over the baseline dataset. The contribution is calculated using [random ablation feature impact](https://arxiv.org/pdf/1910.00174.pdf).\\n\\n- **Feature Drift** - Tells us how much a feature is drifting away from the baseline dataset for the time period of interest. For more information on how drift metrics are calculated, see [Data Drift](doc:data-drift-platform).\\n\\n- **Prediction Drift Impact** - A heuristic calculated by taking the product of Feature Impact and Feature Drift. The higher the score the more this feature contributed to the prediction value drift.\\n\\n### Performance Metrics\\n\\nAccuracy might not be a good measure of model performance in the case of fraud detection as most of the cases are non-fraud. Therefore, we use monitor metrics like: \\n\\n1. **Recall** - How many of the non-fraudulent cases were actually detected as fraud? A low recall value might lead to an increased number of cases for review even though all the fraud cases were predicted correctly.\\n2. **False Positive Rate** - Non-Fraud cases labeled as fraud, high FPR rate leads to dissatisfied customers.\\n\\n### Data Integrity\\n\\n- **Range Violations** - This metric shows the percentage of data in the selected production data that has violated the range specified in the baseline data through [`DatasetInfo`](https://api.fiddler.ai/#fdl-datasetinfo) API.\\n- **Missing Value Violations** - This metric shows the percentage of missing data for a feature in the selected production data.\\n- **Type Violations** - This metric shows the percentage of data in the selected production data that has violated the type specified in the baseline data through the DatasetInfo API.\\n\\n## Explanability\\n\\n### Point Overview\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/c7249cf-XAI21.gif\",\\n        \"XAI21.gif\",\\n        1083\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Point Overview\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nThis tab in the Fiddler AI Observability platform gives an overview for the data point selected. The prediction value for the point along with the strongest positive and negative feature attributions. We can choose from the explanation types. In the case of fraud detection, we can choose from SHAP, Fiddler SHAP, Mean',\n",
       " 'slug: \"clientupdate_model_deployment\" _CONTAINER\",\\n  active: True,\\n  image_uri: \"md-base/python/machine-learning:1.0.0\",\\n  replicas: 1,\\n  cpu: 250,\\n  memory: 512,\\n  created_by: {\\n    id: 4839,\\n    full_name: \"first_name last_name\",\\n    email: \"example_email@gmail.com\",\\n  },\\n  updated_by: {\\n    id: 4839,\\n    full_name: \"first_name last_name\",\\n    email: \"example_email@gmail.com\",\\n  },\\n  created_at: datetime(2023, 1, 27, 10, 9, 39, 793829),\\n  updated_at: datetime(2023, 1, 30, 17, 3, 17, 813865),\\n  job_uuid: UUID(\"539j9630-a69b-98d5-g496-326117174805\")\\n}\\n```',\n",
       " '---\\ntitle: \"Product Tour\"\\nslug: \"product-tour\"\\nexcerpt: \"Here\\'s a tour of our product UI!\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:09:29.819Z\"\\nupdatedAt: \"2023-08-04T23:19:01.026Z\"\\n---\\n# Video Demo\\n\\nWatch the video to learn how Fiddler AI Observability provides data science and MLOps teams with a unified platform to monitor, analyze, explain, and improve machine learning models at scale, and build trust in AI.\\n\\n\\n[block:embed]\\n{\\n  \"html\": \"<iframe class=\\\\\"embedly-embed\\\\\" src=\\\\\"//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FPENnn3YUAcg&display_name=YouTube&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DPENnn3YUAcg&image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FPENnn3YUAcg%2Fhqdefault.jpg&key=7788cb384c9f4d5dbbdbeffd9fe4b92f&type=text%2Fhtml&schema=youtube\\\\\" width=\\\\\"854\\\\\" height=\\\\\"480\\\\\" scrolling=\\\\\"no\\\\\" title=\\\\\"YouTube embed\\\\\" frameborder=\\\\\"0\\\\\" allow=\\\\\"autoplay; fullscreen\\\\\" allowfullscreen=\\\\\"true\\\\\"></iframe>\",\\n  \"url\": \"https://www.youtube.com/watch?v=PENnn3YUAcg\",\\n  \"favicon\": \"https://www.google.com/favicon.ico\",\\n  \"image\": \"http://i.ytimg.com/vi/PENnn3YUAcg/hqdefault.jpg\",\\n  \"provider\": \"youtube.com\",\\n  \"href\": \"https://www.youtube.com/watch?v=PENnn3YUAcg\",\\n  \"typeOfEmbed\": \"youtube\"\\n}\\n[/block]\\n\\n\\n# Documented UI Tour\\n\\nWhen you log in to Fiddler, you are on the Home page and you can visualize monitoring information for your models across all your projects. \\n\\n- At the top of the page, you will see donut charts for the number of triggered alerts for [Performance](doc:performance-tracking-platform), [Data Drift](doc:data-drift-platform), and [Data Integrity](doc:data-integrity-platform). \\n- To the right of the donut charts, you will find the Bookmarks as well as a Recent Job Status card that lets you keep track of long-running async jobs and whether they have failed, are in progress, or successfully completed. \\n- The [Monitoring](doc:monitoring-ui) summary table displays your models across different [projects](doc:project-architecture) along with information on their traffic, drift, and the number of triggered alerts.  \\n  ![](https://files.readme.io/e959fe5-image.png)\\n\\nOn the navigation bar at the top, next to the Home Tab, is the [Projects](doc:project-structure) Tab. You can click on the Projects tab and it lands on a page that lists all your projects contained within Fiddler. See the [Fiddler Samples](doc:product-tour#fiddler-samples)  section below for more information on these projects. You can create new projects within the UI (by clicking the “Add Project” button) or via the [Fiddler Client](ref:about-the-fiddler-client).\\n\\n![](https://files.readme.io/8ffbd1b-image.png',\n",
       " '---\\ntitle: \"Data Integrity\"\\nslug: \"data-integrity-platform\"\\nexcerpt: \"platform guide\"\\nhidden: false\\ncreatedAt: \"2022-12-19T18:33:03.797Z\"\\nupdatedAt: \"2023-08-04T23:21:25.682Z\"\\n---\\nML models are increasingly driven by complex feature pipelines and automated workflows that involve dynamic data. Data is transformed from source to model input, which can result in data inconsistencies and errors.\\n\\nThere are three types of violations that can occur at model inference: **missing feature values**, **type mismatches** (e.g. sending a float input for a categorical feature type) or **range mismatches** (e.g. sending an unknown US State for a State categorical feature).\\n\\nYou can track all these violations in the Data Integrity tab. The time series shown above tracks the violations of data integrity constraints set up for this model.\\n\\n## What is being tracked?\\n\\n![](https://files.readme.io/8a59eb0-Monitoring_DataIntegrity.png \"Monitoring_DataIntegrity.png\")\\n\\nThe time series above tracks the violations of data integrity constraints set up for this model.\\n\\n- **_Missing value violations_** — The percentage of missing value violations over all features for a given period of time.\\n- **_Type violations_** — The percentage of data type mismatch violations over all features for a given period of time.\\n- **_Range violations_** — The percentage of range mismatch violations over all features for a given period of time.\\n- **_All violating events_** — An aggregation of all the data integrity violations above for a given period of time.\\n\\n## Why is it being tracked?\\n\\n- Data integrity issues can cause incorrect data to flow into the model, which can lead to poor model performance and have a negative impact on the business or end-user experience. \\n\\n## How does it work?\\n\\nIt can be tedious to set up constraints for individual features when they number in the tens or hundreds. To avoid this, you can provide Fiddler with a baseline dataset that\\'s representative of the data you expect your model to infer on in production. This should be sampled from your model\\'s training set, and can be [uploaded to Fiddler using the Python API client](ref:clientupload_dataset).\\n\\nFiddler will automatically generate constraints based on the distribution of data in this dataset.\\n\\n- **Missing values**: If a feature has no missing values, then the data integrity violation will be set up to trigger when any missing values are seen. Similarly, if the feature has 50% of its values missing, then the data integrity violation will be set up to trigger when more than 50% of the values encountered are missing in a specified time range.\\n- **Type mismatch**: A data integrity violation will be triggered when the type of a feature value differs from what was specified for that feature in the baseline dataset.\\n- **Range mismatch**: For categorical features, a data integrity violation will be triggered when it sees any value other than the ones specified in the baseline. Similarly, for continuous variables, the violation will be triggered if the values are outside the range specified in the baseline.\\n\\n## What steps should I take with this information?\\n\\n- The visualization above informs us of the feature-wise breakdown of the violations. The raw counts of the violations are shown in parentheses.\\n- If there is a spike in violations, or an unexpected violation occurs (such as missing values for a feature that doesn’t accept a missing value), then a deeper examination of the feature pipeline may be required.\\n- You can also drill down deeper into the data by examining it in the **Analyze** tab. We can use SQL to slice and dice',\n",
       " '---\\ntitle: \"Alerts\"\\nslug: \"alerts-platform\"\\nhidden: false\\ncreatedAt: \"2023-01-27T19:53:56.493Z\"\\nupdatedAt: \"2023-08-04T23:20:46.288Z\"\\n---\\nFiddler enables users to set up alert rules to track a model\\'s health and performance over time. Fiddler alerts also enable users to dig into triggered alerts and perform root cause analysis to discover what is causing a model to degrade. Users can set up alerts using both the [Fiddler UI](https://docs.fiddler.ai/v1.6/docs/alerts-ui) and the [Fiddler API Client](https://docs.fiddler.ai/v1.6/docs/alerts-client).\\n\\n## Supported Metric Types\\n\\nYou can get alerts for the following metrics:\\n\\n- [**Data Drift**](doc:data-drift)  — Predictions and all features\\n  - Model performance can be poor if models trained on a specific dataset encounter different data in production.\\n- [**Performance**](doc:performance) \\n  - Model performance tells us how well a model is doing on its task. A poorly performing model can have significant business implications.\\n- [**Data Integrity**](doc:data-integrity)  — All features\\n  - There are three types of violations that can occur at model inference: missing feature values, type mismatches (e.g. sending a float input for a categorical feature type) or range mismatches (e.g. sending an unknown US State for a State categorical feature).\\n- [**Service Metrics**](doc:traffic-platform) \\n  - The volume of traffic received by the model over time that informs us of the overall system health.\\n\\n## Supported Comparison Types\\n\\nYou have two options for deciding when to be alerted:\\n\\n1. **Absolute** — Compare the metric to an absolute value\\n   1. e.g. if traffic for a given hour is less than 1000, then alert.\\n2. **Relative** — Compare the metric to a previous time period\\n   1. e.g. if traffic is down 10% or more than it was at the same time one week ago, then alert.\\n\\nYou can set the alert threshold in either case.\\n\\n## Alert Rule Priority\\n\\nWhether you\\'re setting up an alert rule to keep tabs on a model in a test environment, or data for production scenarios, Fiddler has you covered. Easily set the Alert Rule Priority to indicate the importance of any given Alert Rule. Users can select from Low, Medium, and High priorities. \\n\\n## Alert Rule Severity\\n\\nFor additional flexibility, users can now specify up to two threshold values, **Critical** and **Warning** severities. Critical severity is always required when setting up an Alert Rule, but Warning can be optionally set as well.\\n\\n## Why do we need alerts?\\n\\n- It’s not possible to manually track all metrics 24/7.\\n- Sensible alerts are your first line of defense, and they are meant to warn about issues in production.\\n\\n## What should I do when I receive an alert?\\n\\n- Click on the link in the email to go to the tab where the alert originated (e.g. Data Drift). \\n- Under the Monitoring tab, more information can be obtained from the drill down below the main chart.\\n- You can also examine the data in the Analyze tab. You can use SQL to slice and dice the data, and use custom visualization tools and operators to make sense of the model’s behavior within the time range under consideration.\\n\\n## Sample Alert Email\\n\\nHere\\'s a sample of an email that\\'s sent if an alert is triggered:\\n\\n!',\n",
       " 'slug: \"performance-tracking-platform\"  the ranking of the retrieved items, by discounting the relevance scores of items at lower ranks                            |\\n\\n## Why is it being tracked?\\n\\n- Model performance tells us how well a model is doing on its task. A poorly performing model can have significant business implications.\\n- The volume of decisions made on the basis of the predictions give visibility into the business impact of the model.\\n\\n## What steps should I take based on this information?\\n\\n- For decisions, if there is an increase or decrease in approvals, we can cross-check with the average prediction and prediction drift trendlines on the [Data Drift Tab](doc:data-drift). In general, the average prediction value should increase with an increase in the number of approvals, and vice-versa.\\n- For changes in model performance—again, the best way to cross-verify the results is by checking the [Data Drift Tab](doc:data-drift) ). Once you confirm that the performance issue is not due to the data, you need to assess if the change in performance is due to temporary factors, or due to longer-lasting issues.\\n- You can check if there are any lightweight changes you can make to help recover performance—for example, you could try modifying the decision threshold.\\n- Retraining the model with the latest data and redeploying it is usually the solution that yields the best results, although it may be time-consuming and expensive.\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"global-explainability\"  _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"fraud-detection\" -reset feature impact, Permutation Feature Impact.\\n\\nFor the data point chosen, ‘category’ has the highest positive attribution (35.1%), pushing the prediction value towards fraud, and ‘amt’ has the highest negative attribution(-45.8%), pushing the prediction value towards non-fraud.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/b4704f6-XAI11.png\",\\n        \"XAI11.png\",\\n        1807\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Explanation Type\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n### Feature Attribution\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/9b91f72-XAI22.gif\",\\n        \"XAI22.gif\",\\n        1078\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Feature Attribution\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nThe Feature Attribution tab gives us information about how much each feature can be attributed to the prediction value based on the Explanation Type chosen. We can also change the value of a particular feature to measure how much the prediction value changes.  \\nIn the example below we can see that on changing the value of feature ‘amt’ from 110 to 10k the prediction value changes from 0.001 to 0.577 (not fraud to fraud).\\n\\n### Feature Sensitivity\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/3fba7b9-XAI23.gif\",\\n        \"XAI23.gif\",\\n        1073\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Feature Sensitivity\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nThis tab plots the prediction value against the range of values for different features (top n user selected). We can change the value for any feature and measure the resulting prediction sensitivity plot of all other features against the initial sensitivity plot. \\n\\nOn reducing the value of the ‘amt’ feature below from 331 to 10, we can see that the final prediction sensitivity plot shows a prediction value \\\\< 0.5 for any value of ‘age’ and ‘unique_merchant_card’. This shows that a lower value for ‘amt’ will result in a prediction value close to 0 (non-fraud)\\n\\n## Make your Fraud Detections Model better with Fiddler!\\n\\nPlease refer to our [Colab Notebook](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/business-use-cases/fraud-detection/Fraud_Detection_Usecase_Fiddler.ipynb) for a walkthrough on how to get started with using Fiddler for your fraud detection use case and an interactive demo on usability.\\n\\n### Overview\\n\\nIt is often the case that a model’s performance will degrade over time. We can use the Fiddler AI Observability platform to monitor the model’s performance in production, look at various metrics and also provide explanations to predictions on various data points. In this walkthrough, we will look at a few scenarios common to a fraud model when monitoring for performance. We will show how you would:\\n\\n1. Get baseline and production data onto the Fiddler Platform\\n2. Monitor drift for various features\\n3. Monitor performance metrics associated with fraud detection like recall, false-positive rate\\n4. Monitor data integrity Issues like range violations\\n5. Provide point explanations to the mislabelled points\\n6. Get to the root cause of the issues\\n\\n### Example - Model Performance Degradation due',\n",
       " '---\\ntitle: \"Baselines\"\\nslug: \"fiddler-baselines\"\\nexcerpt: \"A baseline is a set of reference data that is used to compare the performance of our model for monitoring purposes.\"\\nhidden: false\\ncreatedAt: \"2023-01-19T22:47:23.862Z\"\\nupdatedAt: \"2023-05-09T18:32:22.276Z\"\\n---\\nA model needs a baseline dataset for comparing its performance and identifying any degradation. A baseline is a set of reference data that is used to compare with our current data. \\n\\nThe dataset that was used to train the model is often a good starting point for a baseline. For more in-depth analysis, we may want to use a specific time period or a rolling window of production events. \\n\\nIn Fiddler, **the default baseline for all monitoring metrics is the training dataset **that was associated with the model during registration. Use this default baseline if you do not anticipate any differences between training and production. [New baselines can be added to existing models using the Python client APIs](ref:add_baseline).',\n",
       " '---\\ntitle: \"Monitoring Charts\"\\nslug: \"monitoring-charts-platform\"\\nhidden: false\\ncreatedAt: \"2023-02-23T22:56:27.756Z\"\\nupdatedAt: \"2023-05-24T17:29:04.123Z\"\\n---\\nFiddler AI’s monitoring charts allow you to easily track your models and ensure that they are performing optimally. For any of your models, monitoring charts for data drift, performance, data integrity, or traffic metrics can be displayed using Fiddler Dashboards.\\n\\n## Supported Metric Types\\n\\nMonitoring charts enable you to plot one of the following metric types for a given model:\\n\\n- [**Data Drift**](doc:data-drift-platform#what-is-being-tracked)\\n  - Plot drift for up to 20 columns at once and track it using your choice of Jensen–Shannon distance (JSD) or Population Stability Index (PSI).\\n- [**Performance**](doc:performance-tracking-platform#what-is-being-tracked)\\n  - Available metrics are model dependent.\\n- [**Data Integrity Violations**](doc:data-integrity-platform#what-is-being-tracked)\\n  - Plot data integrity violations for up to 20 columns and track one of the three violations at once.\\n- [**Traffic **](doc:traffic-platform#what-is-being-tracked)\\n\\n## Key Features:\\n\\n### Multiple Charting Options\\n\\nYou can [plot up to 20 columns](doc:monitoring-charts-ui#chart-metric-queries--filters) for a model when charting data drift or data integrity metrics, allowing you to compare them side by side.\\n\\n### Downloadable CSV Data\\n\\nYou can [easily download a CSV of the raw chart data](doc:monitoring-charts-ui#breakdown-summary). This feature allows you to analyze your data further.\\n\\n### Advanced Chart Functionality\\n\\nThe monitoring charts feature offers [advanced chart functionalities ](doc:monitoring-charts-ui#chart-metric-queries--filters)  to provide you with the flexibility to customize your charts and view your data in a way that is most useful to you. Features include:\\n\\n- Zoom\\n- Dragging of time ranges\\n- Toggling between bar and line chart types\\n- Adjusting the scale between linear and log options\\n- Adjusting the range of the y-axis\\n\\n![](https://files.readme.io/9ad4867-image.png)\\n\\n\\n\\nCheck out more on the [Monitoring Charts UI Guide](doc:monitoring-charts-ui).',\n",
       " '---\\ntitle: \"Supported Browsers\"\\nslug: \"supported-browsers\"\\nexcerpt: \"Platform Guide\"\\nhidden: false\\ncreatedAt: \"2023-01-10T22:16:01.134Z\"\\nupdatedAt: \"2023-01-10T22:16:17.735Z\"\\n---\\nFiddler Product can be accessed through the following supported web browsers:\\n\\n- Google Chrome\\n- Firefox\\n- Safari\\n- Microsoft Edge',\n",
       " 'slug: \"quick-start\"  following code block to get your URL.\\n\\n\\n```python\\nprint(\\'/\\'.join([URL, \\'projects\\', PROJECT_ID, \\'models\\', MODEL_ID, \\'monitor\\']))\\n```\\n\\n*Please allow 5-10 minutes for monitoring data to populate the charts.*\\n  \\nThe following screen will be available to you upon completion.\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/8.png\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n**What\\'s Next?**\\n\\nTry the [NLP Monitoring - Quickstart Notebook](https://docs.fiddler.ai/docs/simple-nlp-monitoring-quick-start)\\n\\n---\\n\\n\\n**Questions?**  \\n  \\nCheck out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\\n\\nJoin our [community Slack](http://fiddler-community.slack.com/) to ask any questions!\\n\\nIf you\\'re still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we\\'ll get back to you shortly.\\n',\n",
       " 'slug: \"authorizing-the-client\" .ini\\nclient = fdl.FiddlerApi()\\n```',\n",
       " 'slug: \"simple-nlp-monitoring-quick-start\"  an alternative text embedding method, you can run the following cell to generate the TF-IDF vectors for this dataset.\\n\\n\\n```python\\n\\'\\'\\'\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nembedding_dimension = 300\\nvectorizer = TfidfVectorizer(sublinear_tf=True,\\n                             max_features=embedding_dimension,\\n                             min_df=0.01,\\n                             max_df=0.9,\\n                             stop_words=\\'english\\',\\n                             token_pattern=u\\'(?ui)\\\\\\\\b\\\\\\\\w*[a-z]+\\\\\\\\w*\\\\\\\\b\\')\\n\\ntfidf_sparse = vectorizer.fit_transform(source_df[\\'original_text\\'])\\nembedding_cols = vectorizer.get_feature_names_out()\\nembedding_col_names = [\\'tfidf_token_{}\\'.format(t) for t in embedding_cols]\\ntfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_sparse, columns=embedding_col_names)\\ntfidf_column_names = tfidf_df.columns.tolist()\\n\\'\\'\\'\\n```\\n\\n# 3. Train a Multiclass Classifier and Prepare Baseline and Production DataFrames\\n\\nNow we train a classifier to predict the labels assigned to each data sample. We use the logistic regression classifier from scikit-learn for this task. We split the data into train and test subsets and we use 25% of data points to train a logistic regression model.\\n\\nHere we use the first 128 components of the OpenAI embeddings as the input features to the classifier.\\n\\n\\n```python\\ninput_features = openai_column_names[0:128]\\ndf_all = pd.concat([source_df,openai_df[input_features]], axis=1)\\n```\\n\\n\\n```python\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\n\\ndf_train, df_test = train_test_split(df_all, test_size=0.75, random_state=1)\\nclf = LogisticRegression(random_state=1).fit(df_train[input_features], df_train.target)\\nclf_classes = clf.classes_\\nprediction_col_names = [\\'prob_%s\\'%c for c in clf_classes]\\n```\\n\\nUsing the logistic regression classifier for a multi-class classification problem, we get a probability for each target label. We store all the predicted class probabilities as well as the predicted target for each data point in both the training and test sets.\\n\\n\\n```python\\npredictions_train = pd.DataFrame(index=df_train.index)\\npredictions_train[\\'predicted_target\\'] = clf.predict(df_train[input_features])\\npredicted_probs = clf.predict_proba(df_train[input_features])\\nfor idx,col in enumerate(predicted_probs.T):\\n    predictions_train[prediction_col_names[idx]] = col\\n\\npredictions_test = pd.DataFrame(index=df_test.index)\\npredictions_test[\\'predicted_target\\'] = clf.predict(df_test[input_features])\\npredicted_probs = clf.predict_proba(df_test[input_features])\\nfor idx,col in enumerate(predicted_probs.T):\\n    predictions_test[prediction_col_names[idx]] = col\\n```\\n\\nFinally, we create the baseline and prodcution DataFrames by concatenating the corresponding predicion DataFrames to each of training and test DataFrames. The actual production events are later samples form the predicion DataFrame that we create here.\\n\\n\\n```python\\nbaseline_df = pd.concat([predictions_train, df_train], axis=1)\\nproduction_df = pd.concat([predictions_test, df_test], axis=1)\\n\\nacc_baseline = sum(baseline_df[\\'predicted_target\\'] == baseline_df[\\'target\\'])/baseline_df.shape[0]\\nacc_production = sum(production_df[\\'predicted_target\\'] == production_df[\\'target\\'])/production_df.shape[0]\\n\\nprint(\\'accuracy on baseline:{:.2f}\\'.format(acc_baseline))\\nprint(\\'accuracy on test data:{:.2f}\\'.format(acc_production))\\n```\\n\\n# 4. Upload Baseline Data to Fiddler\\n\\nNow we create a [DatasetInfo](https',\n",
       " '---\\ntitle: \"Authorization and Access Control\"\\nslug: \"authorization-and-access-control\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:26:44.914Z\"\\nupdatedAt: \"2023-06-14T21:25:53.499Z\"\\n---\\n## Project Roles\\n\\nEach project supports its own set of permissions for its users.\\n\\n![](https://files.readme.io/caf2bc9-project_settings.png \"project_settings.png\")\\n\\n![](https://files.readme.io/97b71c4-project_settings_add.png \"project_settings_add.png\")\\n\\nFor more details refer to [Administration Page](doc:administration-platform) in the Platform Guide.\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_',\n",
       " '---\\ntitle: \"Welcome to Fiddler\\'s Documentation!\"\\nslug: \"welcome\"\\nexcerpt: \"This is Fiddler’s AI Observability Platform Documentation. Fiddler is a pioneer in AI Observability for responsible AI. Data Science, MLOps, and LOB teams use Fiddler to monitor, explain, analyze, and improve ML models, generative AI models, and AI applications.\"\\nhidden: false\\nmetadata: \\n  title: \"Fiddler Documentation\"\\n  description: \"This is Fiddler\\'s Model Performance Management Platform Documentation. Fiddler is a pioneer in enterprise Model Performance Management. Data Science, MLOps, and business teams use Fiddler to monitor, explain, analyze, and improve their models and build trust into AI.\"\\ncreatedAt: \"2023-02-27T18:08:02.575Z\"\\nupdatedAt: \"2023-08-04T23:18:40.327Z\"\\n---\\nHere you can find a number of helpful guides to aid with onboarding. These include:\\n\\n[block:html]\\n{\\n  \"html\": \"<style>\\\\n  .index-container {\\\\n      display: grid;\\\\n      grid: auto / 50% 50%;\\\\n      grid-gap: 20px;\\\\n      max-width: 97.5%;\\\\n  }\\\\n  .index-container .index-item {\\\\n    padding: 20px;\\\\n    border: 1px solid #CCCCCC;\\\\n    border-radius: 5px;\\\\n    grid-gap: 15px;\\\\n    \\\\n}\\\\n.index-item{\\\\n  text-decoration: none !important;\\\\n  color: #000000;\\\\n }\\\\n.index-item:hover{\\\\n  color: #000000;\\\\n  border-color: #1A5EF3;\\\\n  -webkit-box-shadow: 0 2px 4px rgb(0 0 0 / 10%);\\\\n  -moz-box-shadown: 0 2px 4px rgb(0 0 0 / 10%);\\\\n  box-shadow: 0 2px 4px rgb(0 0 0 / 10%);\\\\n } \\\\n  \\\\n.index-title {\\\\n    font-size: 20px !important;\\\\n    color: #111111;\\\\n    margin-top: 0px !important;\\\\n    margin-bottom: 20px;\\\\n}\\\\n@media only screen and (max-width: 420px){\\\\n  .index-container {\\\\n    grid: auto / 100%;\\\\n  }\\\\n}\\\\n  </style>\\\\n<div class=\\\\\"index-container\\\\\">\\\\n  <a class=\\\\\"index-item\\\\\" href=\\\\\"https://docs.fiddler.ai/v1.7/docs/administration-platform\\\\\">\\\\n    <div>\\\\n\\\\t\\\\t\\\\t<h2 class=\\\\\"index-title\\\\\">Platform Guide</h2>\\\\n    \\\\t<p>How Fiddler does AI Observability and Fiddler-specific terminologies.</p>\\\\n \\\\t\\\\t</div>\\\\n  </a>\\\\n\\\\n  <a class=\\\\\"index-item\\\\\" href=\\\\\"https://docs.fiddler.ai/v1.7/docs/administration-ui\\\\\">\\\\n    <div>\\\\n      <h2 class=\\\\\"index-title\\\\\">User Interface (UI) Guide</h2>\\\\n      <p>An introduction to the product UI with screenshots that illustrate how to interface with the product.</p>\\\\n    </div>\\\\n  </a>\\\\n\\\\n  <a class=\\\\\"index-item\\\\\" href=\\\\\"https://docs.fiddler.ai/v1.7/docs/installation-and',\n",
       " '---\\ntitle: \"ML Algorithms In Fiddler\"\\nslug: \"ds\"\\nhidden: true\\ncreatedAt: \"2022-11-18T22:11:48.747Z\"\\nupdatedAt: \"2022-11-18T22:12:58.704Z\"\\n---\\n',\n",
       " '---\\ntitle: \"About Event Publication\"\\nslug: \"publish_event\"\\nhidden: false\\ncreatedAt: \"2022-05-13T14:36:29.265Z\"\\nupdatedAt: \"2022-06-14T15:30:43.091Z\"\\n---\\nEvent publication is the process of sending your model\\'s prediction logs, or events, to the Fiddler platform.  Using the [Fiddler Client](ref:about-the-fiddler-client), events can be published in batch or streaming mode.  Using these events, Fiddler will calculate metrics around feature drift, prediction drift, and model performance.  These events are also stored in Fiddler to allow for ad hoc segment analysis.  Please read the sections that follow to learn more about how to use the Fiddler Client for event publication.',\n",
       " 'slug: \"performance\" https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"fdlmulticlassattributionexplanation\" ={\\'background_dataset_size\\': 120, \\n            \\'baseline_prediction\\': 0.32470778860007266, \\n            \\'explanation_ci\\': {\\n              \\'PetalLength\\': 0.03880995606758349,\\n              \\'PetalWidth\\': 0.011875959565576195,\\n              \\'SepalLength\\': 0.007051209191437458,\\n              \\'SepalWidth\\': 0.0031140002491094245\\n            },\\n            \\'explanation_ci_level\\': 0.95,\\n            \\'model_prediction\\': 0.696459162361538\\n           }\\n    )\\n  }\\n)\\n```',\n",
       " 'slug: \"pagerduty\"  you wish to use with PagerDuty.\\n\\n![](https://files.readme.io/d9ad82e-pagerduty_fiddler_1.png \"pagerduty_fiddler_1.png\")\\n\\n\\n\\n2. Select **Monitor** → **Alerts** → **Add Alert**.\\n\\n![](https://files.readme.io/b7118f0-pagerduty_fiddler_2.png \"pagerduty_fiddler_2.png\")\\n\\n\\n\\n3. Enter the condition you would like to alert on, and under **PagerDuty Services**, select all services you would like the alert to trigger for. Additionally, select the **Severity** of this alert, and hit **Save**.\\n\\n![](https://files.readme.io/8fbffde-pagerduty_fiddler_3.png \"pagerduty_fiddler_3.png\")\\n\\n\\n\\n4. After creation, the alert will now trigger for the specified PagerDuty services.\\n\\n> 📘 Info\\n> \\n> Check out the [alerts documentation](doc:alerts-platform) for more information on setting up alerts.\\n\\n## FAQ\\n\\n**Can Fiddler integrate with multiple PagerDuty services?**\\n\\n- Yes. So long as the service is present within **Settings** → **PagerDuty Services**, anyone within your organization can select that service to be a recipient for an alert.',\n",
       " '---\\ntitle: \"Data Integrity\"\\nslug: \"data-integrity\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:25:27.271Z\"\\nupdatedAt: \"2023-07-28T18:52:17.679Z\"\\n---\\nML models are increasingly driven by complex feature pipelines and automated workflows that involve dynamic data. Data is transformed from source to model input, which can result in data inconsistencies and errors.\\n\\nThere are three types of violations that can occur at model inference: **missing feature values**, **type mismatches** (e.g. sending a float input for a categorical feature type) or **range mismatches** (e.g. sending an unknown US State for a State categorical feature).\\n\\nYou can track all these violations in the Data Integrity tab. \\n\\n## What is being tracked?\\n\\n![](https://files.readme.io/8a59eb0-Monitoring_DataIntegrity.png \"Monitoring_DataIntegrity.png\")\\n\\nThe time series above tracks the violations of data integrity constraints set up for this model.\\n\\n- **_Missing value violations_** — The percentage of missing value violations over all features for a given period of time.\\n- **_Type violations_** — The percentage of data type mismatch violations over all features for a given period of time.\\n- **_Range violations_** — The percentage of range mismatch violations over all features for a given period of time.\\n- **_All violating events_** — An aggregation of all the data integrity violations above for a given period of time.\\n\\n## Why is it being tracked?\\n\\n- Data integrity issues can cause incorrect data to flow into the model, which can lead to poor model performance and have a negative impact on the business or end-user experience. \\n\\n## How does it work?\\n\\nIt can be tedious to set up constraints for individual features when they number in the tens or hundreds. To avoid this, you can provide Fiddler with a baseline dataset that\\'s representative of the data you expect your model to infer on in production. This should be sampled from your model\\'s training set, and can be [uploaded to Fiddler using the Python API client](ref:clientupload_dataset).\\n\\nFiddler will automatically generate constraints based on the distribution of data in this dataset.\\n\\n- **Missing values**: If a feature has no missing values, then the data integrity violation will be set up to trigger when any missing values are seen. Similarly, if the feature has 50% of its values missing, then the data integrity violation will be set up to trigger when more than 50% of the values encountered are missing in a specified time range.\\n- **Type mismatch**: A data integrity violation will be triggered when the type of a feature value differs from what was specified for that feature in the baseline dataset.\\n- **Range mismatch**: For categorical features, a data integrity violation will be triggered when it sees any value other than the ones specified in the baseline. Similarly, for continuous variables, the violation will be triggered if the values are outside the range specified in the baseline.\\n\\n## What steps should I take with this information?\\n\\n- The visualization above informs us of the feature-wise breakdown of the violations. The raw counts of the violations are shown in parentheses.\\n- If there is a spike in violations, or an unexpected violation occurs (such as missing values for a feature that doesn’t accept a missing value), then a deeper examination of the feature pipeline may be required.\\n- You can also drill down deeper into the data by examining it in the **Analyze** tab. We can use SQL to slice and dice the data, and try to find the root cause of the issues.\\n\\n**Reference**\\n\\n- See our article on [_The',\n",
       " 'slug: \"simple-nlp-monitoring-quick-start\"  adding timestamps\\n    )\\n```\\n\\n# 7. Get insights\\n\\n\\n**You\\'re all done!**\\n  \\nYou can now head to Fiddler URL and start getting enhanced observability into your model\\'s performance. Run the following code block to get your URL:\\n\\n\\n\\n\\n```python\\nprint(\\'/\\'.join([URL, \\'projects\\', PROJECT_ID, \\'models\\', MODEL_ID, \\'monitor\\']))\\n```\\n\\nIn particular, you can go to the monitoring tab in your Fiddler URL and check the resulting drift chart for the TF-IDF embedding vectors. Bellow is a sceernshot of the data drift chart after running this notebook on the [Fiddler demo](https://demo.fiddler.ai/) deployment. (Annotation bubbles are not generated by the Fiddler UI.)\\n\\n\\n--------\\n**Questions?**  \\n  \\nCheck out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\\n\\nJoin our [community Slack](http://fiddler-community.slack.com/) to ask any questions!\\n\\nIf you\\'re still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we\\'ll get back to you shortly.\\n\\n\\n```python\\n\\n```\\n',\n",
       " '---\\ntitle: \"Simple Monitoring\"\\nslug: \"quick-start\"\\nexcerpt: \"Quickstart Notebook\"\\nhidden: false\\ncreatedAt: \"2022-08-10T15:11:33.699Z\"\\nupdatedAt: \"2023-03-07T21:38:01.896Z\"\\n---\\nThis guide will walk you through the basic onboarding steps required to use Fiddler for model monitoring, **using sample data provided by Fiddler**.  \\n\\n**Note**: This guide does not upload a model artifact or create a surrogate model, both of which are supported by Fiddler.  As a result, this guide won\\'t allow you to explore explainability within the platform.\\n\\nClick the following link to get started using Google Colab:\\n\\n<div class=\"colab-box\">\\n    <a href=\"https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/Fiddler_Quickstart_Simple_Monitoring.ipynb\" target=\"_blank\">\\n        <div>\\n            Open in Google Colab →\\n        </div>\\n    </a>\\n    <div>\\n            <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" />\\n    </div>\\n</div>\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]# Fiddler Simple Monitoring Quick Start Guide\\n\\nFiddler is the pioneer in enterprise Model Performance Management (MPM), offering a unified platform that enables Data Science, MLOps, Risk, Compliance, Analytics, and other LOB teams to **monitor, explain, analyze, and improve ML deployments at enterprise scale**. \\nObtain contextual insights at any stage of the ML lifecycle, improve predictions, increase transparency and fairness, and optimize business revenue.\\n\\n---\\n\\nYou can start using Fiddler ***in minutes*** by following these 7 quick steps:\\n\\n1. Imports\\n2. Connect to Fiddler\\n3. Upload a baseline dataset\\n4. Add metadata about your model with Fiddler\\n5. Set up Alerts and Notifications (Optional)\\n6. Publish production events\\n7. Get insights\\n\\n**Don\\'t have a Fiddler account? [Sign-up for a 14-day free trial](https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral).**\\n\\n## 1. Imports\\n\\n\\n```python\\n!pip install -q fiddler-client\\n\\nimport numpy as np\\nimport pandas as pd\\nimport time as time\\nimport fiddler as fdl\\n\\nprint(f\"Running client version {fdl.__version__}\")\\n```\\n\\n## 2. Connect to Fiddler\\n\\nBefore you can add information about your model with Fiddler, you\\'ll need to connect using our API client.\\n\\n\\n---\\n\\n\\n**We need a few pieces of information to get started.**\\n1. The URL you\\'re using to connect to Fiddler\\n\\n\\n```python\\nURL = \\'\\' # Make sure to include the full URL (including https://). For example, https://abc.xyz.ai\\n```\\n\\n2. Your organization ID\\n3. Your authorization token\\n\\nBoth of these can be found by clicking the URL you entered and navigating to the **Settings',\n",
       " '---\\ntitle: \"Dashboard Interactions\"\\nslug: \"dashboard-interactions\"\\nhidden: false\\nmetadata: \\n  title: \"Dashboard Interactions\"\\ncreatedAt: \"2023-02-22T18:06:29.918Z\"\\nupdatedAt: \"2023-02-22T18:47:36.624Z\"\\n---\\n## Dashboard Interactions\\n\\n### Remove a Chart\\n\\nIf you want to remove a chart from your dashboard, simply click on the \"X\" located at the top right of the chart. This will remove the chart from the dashboard, but it will still be available in the saved charts list for future use. If you change your mind and decide to add the chart back to the dashboard, you can simply find it in the saved charts list and add it back to the dashboard at any time.\\n\\n![](https://files.readme.io/91bb601-image.png)\\n\\n### Edit a Saved Chart\\n\\nTo edit a saved chart, simply click on the chart title within your dashboard. This will open the chart studio in a new tab, where you can make any necessary changes. Once you have made your changes, be sure to select the `Default` time range and then use the Dashboard refresh button to see your updated chart.\\n\\n![](https://files.readme.io/e7d5b04-image.png)\\n\\n### Zoom\\n\\nTo zoom into a chart within your dashboard, you have two different utilities at your disposal. The first one is located on the top right of the chart component, in the toolbar. After clicking on the **zoom icon**, you can drag your cursor over the data points you wish to zoom into.\\n\\n![](https://files.readme.io/e1a1218-image.png)\\n\\nYou can also use the** horizontal zoom bar **located at the base of the chart to zoom in. Once you\\'ve identified the time range you want to focus on, you can use the zoom bar to drag the range across time. For instance, if you want to analyze your data week by week over the past six months, you can use the toolbar or horizontal zoom bar to zoom in on the desired time range and then click and drag the selected range using the base horizontal zoom bar.\\n\\n![](https://files.readme.io/0a13f18-image.png)\\n\\n### Bar & Line Charts\\n\\nYou can switch between visualizing your chart as a line or a bar chart using the toolbar icons. Click on the line chart icon on the top right of the chart to switch to the line chart view. Likewise, select the bar chart icon in the toolbar to switch to the bar chart view. However,_ note that these views are only temporary and any settings you specify using the toolbar will not be saved to the dashboard._\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/31d5896-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"600px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n### Undo Chart Toolbar Changes\\n\\nYou can easily restore the changes you applied to your chart using the chart toolbar options, including zoom, switch to line chart, and switch to bar chart. The restore option, which is the last icon in the toolbar, allows you to undo any changes you made and return to the original chart configuration.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/0ec8eb6-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"600px\"\\n    }\\n  ]\\n}\\n[/block]',\n",
       " 'slug: \"alerts-platform\" [](https://files.readme.io/9dfc566-Monitor_Alert_Email_0710.png \"Monitor_Alert_Email_0710.png\")\\n\\n## Integrations\\n\\nFiddler supports the following alert notification integrations:\\n\\n- Email\\n- Slack\\n- PagerDuty\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Retrieving Events\"\\nslug: \"retrieving-events\"\\nhidden: false\\ncreatedAt: \"2022-07-06T16:22:23.142Z\"\\nupdatedAt: \"2022-07-06T16:23:39.398Z\"\\n---\\nAfter publishing events to Fiddler, you may want to retrieve them for further analysis.\\n[block:api-header]\\n{\\n  \"title\": \"Querying production data\"\\n}\\n[/block]\\nYou can query production data from the **Analyze** tab by issuing the following SQL query to Fiddler.\\n\\n```sql\\nSELECT\\n    *\\nFROM\\n    \"production.MODEL_ID\"\\n```\\n\\nThe above query will return the entire production table (all published events) for a model with a model ID of `MODEL_ID`.\\n\\n[block:api-header]\\n{\\n  \"title\": \"Querying a baseline dataset\"\\n}\\n[/block]\\nYou can query a baseline dataset that has been uploaded to Fiddler with the following SQL query.\\n\\n```sql\\nSELECT\\n    *\\nFROM\\n    \"DATASET_ID.MODEL_ID\"\\n```\\n\\nHere, this will return the entire baseline dataset that has been uploaded with an ID of `DATASET_ID` to a model with an ID of `MODEL_ID`.',\n",
       " '---\\ntitle: \"Alerting Integrations\"\\nslug: \"alerting-integrations\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:18:54.515Z\"\\nupdatedAt: \"2022-04-19T20:18:54.515Z\"\\n---\\n',\n",
       " '---\\ntitle: \"Settings\"\\nslug: \"settings\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:26:28.689Z\"\\nupdatedAt: \"2022-11-10T19:09:26.719Z\"\\n---\\n![](https://files.readme.io/d937de2-Home_Page.png \"Home_Page.png\")\\n\\nThe Settings section captures team setup, permissions, and credentials. You can access the **Settings** page from the left menu of the Fiddler UI at all times.\\n\\nThese are the key tabs in **Settings**.\\n\\n## General\\n\\nThe **General** tab shows your organization name, ID, email, and a few other details. The organization ID is needed when accessing Fiddler from the Fiddler Python API client.\\n\\n![](https://files.readme.io/3f2e734-general.png \"general.png\")\\n\\n## Access\\n\\nThe **Access** tab shows the users, teams, and invitations for everyone in the organization.\\n\\n### Users\\n\\nThe **Users** tab shows all the users that are part of this organization.\\n\\n![](https://files.readme.io/c8c5bf1-access_user.png \"access_user.png\")\\n\\n### Teams\\n\\nThe **Teams** tab shows all the teams that are part of this organization.\\n\\n![](https://files.readme.io/8cba270-access_team.png \"access_team.png\")\\n\\nYou can create a team by clicking on the plus (**`+`**) icon on the top-right.\\n\\n> 🚧 Note\\n> \\n> Only Administrators can create teams. The plus (**`+`**) icon will not be visible unless you have Administrator permissions.\\n\\n![](https://files.readme.io/b0c4c53-access_create_team.png \"access_create_team.png\")\\n\\n### Invitations\\n\\nThe **Invitations** tab shows all pending user invitations.\\n\\n![](https://files.readme.io/5cb4046-access_invitation.png \"access_invitation.png\")\\n\\nYou can invite a user by clicking on the plus (**`+`**) icon on the top-right.\\n\\n> 🚧 Note\\n> \\n> Only Administrators can invite users. The plus (**`+`**) icon will not be visible unless you have Administrator permissions.\\n\\n![](https://files.readme.io/abb030c-access_invite_user.png \"access_invite_user.png\")\\n\\n## Credentials\\n\\nThe **Credentials** tab displays user access keys. These access keys are used by Fiddler Python client for authentication. Each Administrator or Member can create a unique key by clicking on **Create Key**.\\n\\n![](https://files.readme.io/fce7911-credentials.png \"credentials.png\")\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_',\n",
       " 'slug: \"alerts-ui\"  rules across any projects and model at a glance.\\n\\n![](https://files.readme.io/ec2fde7-image.png)\\n\\nA few high level details from the alert rule definition are displayed in the table, but users can select to view the full alert definition by selecting the overflow button (⋮) on the right-hand side of any Alert Rule record and clicking `View All Details`. \\n\\n![](https://files.readme.io/0e1dbdc-image.png)\\n\\nDelete an existing alert by clicking on the overflow button (⋮) on the right-hand side of any Alert Rule record and clicking `Delete`. To make any other changes to an Alert Rule, you will need to delete the alert and create a new one with the desired specifications. \\n\\n![](https://files.readme.io/eddf05e-image.png)\\n\\n## Visualizations\\n\\nThroughout the Alert Rules, Triggered Alerts, and Home pages users will see references to the monitors they set up. These visualizations include Alert Rule priority, threshold severities, and more.\\n\\n### Alert Rule Priority\\n\\nAlert rule priority allows users to specify how important an alert rule is to their workflows, learn more on the [Alerts Platform Guide](https://docs.fiddler.ai/v1.6/docs/alerts-platform).\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/4f87100-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"300px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n### Threshold Severity\\n\\nUsers can specify Warning and Critical thresholds as additional customization on their monitors, learn more on the [Alerts Platform Guide](https://docs.fiddler.ai/v1.6/docs/alerts-platform).\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/664e72e-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"300px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n### Alert Summary\\n\\nOn the Fiddler home page, users can get a summary glance of their triggered alerts, categorized by Alert Type. This view allows users to easily navigate to their degraded models.\\n\\n![](https://files.readme.io/3f76938-image.png)\\n\\n## View Triggered Alerts on Fiddler\\n\\nThe Triggered Alerts view gives a single pane of glass experience where you can view all triggered alerts across any Project and Model. Easily apply time filters to see alerts that fired in a desired range, or customize the table to only show columns that matter the most to you. This view aggregates all triggered alerts by alert rule, where the number of times a given alert rule has been triggered is called out by the `Count` column. Explore the triggered alerts further by clicking on the `Monitor` button to further diagnose your model and data.\\n\\n![](https://files.readme.io/30a5ab5-Screen_Shot_2022-10-03_at_3.39.32_PM.png)\\n\\n## Sample Alert Email\\n\\nHere\\'s a sample of an email that\\'s sent if an alert is triggered:\\n\\n![](https://files.readme.io/9dfc566-Monitor_Alert_Email_0710.png \"Monitor_Alert_Email_0710.png\")\\n\\n## Integrations\\n\\nThe Integrations tab is a read-only view of all the integrations your Admin has enabled for use. As of today, users can configure their Alert Rules to notify them via email or Pager Duty services.\\n\\n![](https://files.readme.io/7462149-image',\n",
       " '---\\ntitle: \"Dashboards\"\\nslug: \"dashboards-platform\"\\nhidden: false\\ncreatedAt: \"2023-02-21T22:34:44.508Z\"\\nupdatedAt: \"2023-02-27T20:05:52.789Z\"\\n---\\n## Overview\\n\\nWith Fiddler, you can create comprehensive dashboards that bring together all of your monitoring data in one place. This includes monitoring charts for data drift, traffic, data integrity, and performance metrics. Adding monitoring charts to your dashboards lets you create a detailed view of your model\\'s performance. These dashboards can inform your team, management, or stakeholders, and make data-driven decisions that help improve your AI performance. \\n\\nView a list of the **[available metrics for monitoring charts here](doc:monitoring-charts-platform#supported-metric-types)**.\\n\\n## Dashboards Functionality\\n\\nDashboards offer a powerful way to analyze the overall health and performance of your models, as well as to compare multiple models. \\n\\n### Dashboard Filters\\n\\n- [Flexible filters](doc:dashboards-ui#dashboard-filters) including date range, time zone, and bin size to customize your view\\n\\n### Chart Utilities\\n\\n- [Leverage the chart toolbar ](doc:dashboard-interactions#zoom)to zoom into data and toggle between line and bar chart types\\n\\n### [Dashboard Basics](doc:dashboard-utilities)\\n\\n- Easily save, delete, or share your dashboard\\n- Click on a chart name to edit the base chart\\n- Remove and add monitoring charts to your dashboard\\n- Perform model-to-model comparison\\n- Plot drift or data integrity for multiple columns in one view\\n\\n![](https://files.readme.io/9bf5fc2-image.png)\\n\\nCheckout more on the [Dashboards UI Guide](doc:dashboards-ui).',\n",
       " 'slug: \"fairness\"  compares the pass rate of two groups.\\n\\nThe pass rate is the rate of positive outcome for a given group. It is defined as follow:\\n\\n`pass rate = passed / (num of ppl in the group)`\\n\\nIf the decisions are fair, the pass rates should be the same.\\n\\n## Group Benefit\\n\\nGroup benefit aims to measure the rate at which a particular event is predicted to occur within a subgroup compared to the rate at which it actually occurs.\\n\\nMathematically, group benefit for a given group is defined as follows:\\n\\n`Group Benefit = (TP+FP) / (TP + FN).`\\n\\nGroup benefit equality compares the group benefit between two groups. If the two groups are treated equally, the group benefit should be the same.\\n\\n## Equal Opportunity\\n\\nEqual opportunity means that all people will be treated equally or similarly and not disadvantaged by prejudices or bias.\\n\\nMathematically, equal opportunity compares the true positive rate (TPR) between two groups. TPR is the probability that an actual positive will test positive. The true positive rate is defined as follows:\\n\\n`TPR = TP/(TP+FN)`\\n\\nIf the two groups are treated equally, the TPR should be the same.\\n\\n## Intersectional Fairness\\n\\nWe believe fairness should be ensured to all subgroups of the population. We extended the classical metrics (which are defined for two classes) to multiple classes. In addition, we allow multiple protected features (e.g. race _and_ gender). By measuring fairness along overlapping dimensions, we introduce the concept of intersectional fairness.\\n\\nTo understand why we decided to go with intersectional fairness, we can take a simple example. In the figure below, we observe that equal numbers of black and white people pass. Similarly, there is an equal number of men and women passing. However, this classification is unfair because we don’t have any black women and white men that passed, and all black men and white women passed. Here, we observe bias within subgroups when we take race and gender as protected attributes.\\n\\n![](https://files.readme.io/21f6b94-intersectional_fairness.svg \"intersectional_fairness.svg\")\\n\\nThe EEOC provides examples of past intersectional discrimination/harassment cases.[<sup>\\\\[2\\\\]</sup>](#reference)\\n\\n## Model Behavior\\n\\nIn addition to the fairness metrics, we provide information about model outcomes and model performance for each subgroup. \\n\\n## Dataset Fairness\\n\\nWe also provide a section for dataset fairness, with a mutual information matrix and a label distribution. Note that this is a pre-modeling step.\\n\\n**Mutual information **gives information about existing dependence in your dataset between the protected attributes and the remaining features. We are displaying Normalized Mutual Information (NMI). This metric is symmetric, and has values between 0 and 1, where 1 means perfect dependency.\\n\\nFor more details about the implementation of the intersectional framework, please refer to this [research paper](https://arxiv.org/pdf/2101.01673.pdf).\\n\\n## Reference\\n\\n[^1]\\\\: USEEOC article on [_Discrimination By Type_](https://www.eeoc.gov/discrimination-type)  \\n[^2]\\\\:  USEEOC article on [_Intersectional Discrimination/Harassment_](https://www.eeoc.gov/initiatives/e-race/significant-eeoc-racecolor-casescovering-private-and-federal-sectors#intersectional)',\n",
       " 'slug: \"fdlmetric\" =\\'some_org_name\\',\\n           project_id=\\'project-a\\',\\n           model_id=\\'binary_classification_model-a\\',\\n           name=\\'perf-gt-5prec-1hr-1d-ago\\',\\n           alert_type=AlertType.PERFORMANCE,\\n           metric=Metric.PRECISION, <---\\n           priority=Priority.HIGH,\\n           compare_to=\\'CompareTo.TIME_PERIOD,\\n           compare_period=ComparePeriod.ONE_DAY,\\n           compare_threshold=None,\\n           raw_threshold=None,\\n           warning_threshold=0.05,\\n           critical_threshold=0.1,\\n           condition=AlertCondition.GREATER,\\n           bin_size=BinSize.ONE_HOUR)]\\n```',\n",
       " '---\\ntitle: \"Monitoring Charts UI\"\\nslug: \"monitoring-charts-ui\"\\nhidden: false\\ncreatedAt: \"2023-02-23T23:06:54.406Z\"\\nupdatedAt: \"2023-05-24T17:50:37.837Z\"\\n---\\n## Getting Started:\\n\\nTo use Fiddler AI’s monitoring charts, navigate to the Charts tab in the top-level navigation bar on the Fiddler AI platform. Choose between opening a previously saved chart or creating a new chart.\\n\\n## Create a New Monitoring Chart\\n\\nTo create a new monitoring chart, click on the Add Chart button on the top right of the screen.\\n\\n![](https://files.readme.io/2c98736-image.png)\\n\\nSearch for and select the [project](doc:project-structure) to create the chart, and press Add Chart.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/5e5cc8e-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n## Chart Functions\\n\\n![](https://files.readme.io/c5b2029-image.png)\\n\\n### Chart Properties\\n\\nBefore the first save, you can change the project space to ensure you’re focusing on the right models and data. After confirming the project selection, you can choose to name and add a description to your chart.\\n\\n### Save & Share\\n\\nManually save your chart using the Save button on the top right corner of the chart studio. Copy a link to your chart and share it with other [fiddler accounts who have access](doc:inviting-users) to the project where the chart resides.\\n\\n### Global Undo & Redo\\n\\nEasily control the following actions with the undo and redo buttons:\\n\\n- Metric query selection\\n- Time range selections\\n- Time range selections\\n- Bin size selections\\n\\nTo learn how to undo actions taken using the chart toolbar, see the Toolbar information in the next section.\\n\\n## Chart Metric Queries & Filters\\n\\n### Metric Query\\n\\nA metric query enables you to define what model to focus on, and which metrics and columns to plot on your monitoring chart. To get started with the metric query, choose a model of choice. Note: only models within the same project as your chart are accessible.\\n\\nOnce a model is selected, choose a metric type from Performance, Data Drift, Data Integrity, or Traffic metrics and relevant metrics. For example, we may choose to chart accuracy for our binary classification model. \\n\\n![](https://files.readme.io/a46e656-image.png)\\n\\n\\n\\n\\n\\n### Charting Multiple Columns\\n\\nIf you choose to chart data drift or data integrity, you can choose to plot up to 20 different columns including outputs, inputs, and metadata columns. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/e7b2019-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"350px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n### Chart Filters & Capabilities\\n\\nThere are three major chart filter capabilities, chart filters, chart toolbar, and zoom slider.  \\nThey work together to enable you to best analyze the slices of data that may be worth investigating. \\n\\n![](https://files.readme.io/f58936d-image.png)\\n\\n### Filters\\n\\nYou can customize your chart view using time range, time zone, and bin size chart filters. The data range can be one of the pre-defined time ranges or a custom range. The bin size',\n",
       " 'slug: \"monitoring-charts-ui\"  selected controls the frequency for which the data is displayed. So selecting Day will show daily data over the date range selected.\\n\\n### Toolbar\\n\\nThe charts toolbar is made up of 5 functions:\\n\\n- Drag to zoom\\n- Reset zoom\\n- Toggle to a line chart\\n- Toggle to a bar chart\\n- Undo all toolbar actions\\n\\n> 📘 Note: If the zoom reset or toolbar undo is selected, this will also undo any actions taken with the zoom slider.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/0a9224c-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n#### Zoom\\n\\nTo utilize the drag-to-zoom functionality, click on the associated icon, it should turn blue on selection. Once selected, move your mouse over the chart area and drag it to zoom into the data points of interest. If you want to return to the original view, you can leverage the Reset Zoom button, which is the icon directly to the right of the drag-to-zoom functionality.\\n\\n![](https://files.readme.io/ed8ef2b-image.png)\\n\\n#### Line & Bar Chart Toggle\\n\\nYou can switch between visualizing your chart as a line or bar chart using the toolbar icons. Click on the line chart icon on the top right of the chart to switch to the line chart view. Likewise, select the bar chart icon in the toolbar to switch to the bar chart view. However, note that these views are only temporary and any settings you specify using the toolbar will not be saved to the chart.\\n\\n![](https://files.readme.io/c8c0e79-image.png)\\n\\n#### Zoom Slider\\n\\nYou can also use the horizontal zoom bar to zoom, located at the base of the chart. Once you\\'ve identified the time range you want to focus on, you can use the zoom bar to drag the range across time. For instance, if you want to analyze your data week-by-week over the past six months, you can use the toolbar or horizontal zoom bar to zoom in on the desired time range and then click and drag the selected range using the base horizontal zoom bar.\\n\\n![](https://files.readme.io/c73c24c-image.png)\\n\\n### Breakdown Summary\\n\\nYou can easily visualize your charts\\' raw data as a table within the fiddler chart studio, or download the content as a CSV for further analysis. If you choose to chart multiple columns, as shown below, you can search for and sort by Model name, Metric name, Column name, or values for a specific date.\\n\\n![](https://files.readme.io/0ddc155-image.png)\\n\\n## Customize Tab\\n\\n The Customize tab enables users to adjust the scale and range of the y-axis on their monitoring charts. In the example below, we have adjusted the minimum value of the y-axis for the plotted traffic to make more use of the chart space. For values with large variance, logarithmic scale can be applied to more clearly analyze the chart.\\n\\n![](https://files.readme.io/1683a1f-image.png)',\n",
       " 'slug: \"cv-monitoring\" .pyplot as plt\\nimport numpy as np\\n\\nimport torch\\nimport torch.nn.functional as F\\nimport torchvision.transforms as transforms\\n\\ntorch.manual_seed(0)\\n\\nCIFAR_CLASSES = (\\n    \\'plane\\', \\'car\\', \\'bird\\', \\'cat\\',\\n    \\'deer\\', \\'dog\\', \\'frog\\',\\n    \\'horse\\', \\'ship\\', \\'truck\\',\\n)\\n\\nglobal view_fc1_output_embeds\\n\\ndef fc1_hook_func(model, input, output):\\n    global view_fc1_output_embeds\\n    view_fc1_output_embeds = output\\n\\ndef idx_to_classes(target_arr):\\n    return [CIFAR_CLASSES[int(i)] for i in target_arr]\\n\\ndef generate_embeddings(model, device, dataloader, n=100_000):\\n    \"\"\"Generate embeddings for the inout images\"\"\"\\n    with torch.no_grad():\\n        model = model.eval()\\n        fc1_module = model.fc[0]\\n        fc1_hook = fc1_module.register_forward_hook(fc1_hook_func)\\n        correct_preds = 0\\n        images_processed = 0\\n        try:\\n            for i, (inputs, labels) in enumerate(dataloader):\\n                inputs = inputs.to(device)\\n                labels = labels.to(device)\\n                outputs = model(inputs)\\n                outputs_smax = F.softmax(outputs, dim=1)\\n                _, preds = torch.max(outputs, 1)\\n                correct_preds += torch.sum(preds == labels.data)\\n                if i == 0:\\n                    fc1_embeds = view_fc1_output_embeds.cpu().detach().numpy()\\n                    output_scores = outputs_smax.cpu().detach().numpy()\\n                    target = labels.cpu().detach().numpy()\\n                else:\\n                    fc1_embeds = np.concatenate((fc1_embeds, view_fc1_output_embeds.cpu().detach().numpy()))\\n                    output_scores = np.concatenate((output_scores, outputs_smax.cpu().detach().numpy()))\\n                    target = np.concatenate((target, labels.cpu().detach().numpy()))\\n                images_processed += outputs.size(0)\\n                if images_processed >= n:\\n                    break\\n        except Exception as e:\\n            fc1_hook.remove()\\n            raise\\n    embs = deepcopy(fc1_embeds[:n])\\n    labels = idx_to_classes(target[:n]) \\n    embedding_cols = [\\'emb_\\'+str(i) for i in range(128)]\\n    baseline_embeddings = pd.DataFrame(embs, columns=embedding_cols)\\n    baseline_predictions = pd.DataFrame(output_scores[:n], columns=CIFAR_CLASSES)\\n    baseline_labels = pd.DataFrame(labels, columns=[\\'target\\'])\\n    embeddings_df = pd.concat(\\n        [baseline_embeddings, baseline_predictions, baseline_labels],\\n        axis=\\'columns\\',\\n        ignore_index=False\\n    )\\n    return embeddings_df\\n\\n\\ndef get_cifar_transforms():\\n    image_transforms = transforms.Compose(\\n        [\\n            transforms.ToTensor(),\\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\\n        ]\\n    )\\n    return image_transforms\\n\\ndef get_blur_transforms():\\n    image_transforms = transforms.Compose(\\n        [\\n            transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2)),\\n            transforms.ToTensor(),\\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\\n        ]\\n    )\\n    return image_transforms\\n\\ndef get_brightness_transforms():\\n    image_transforms = transforms.Compose(\\n        [\\n            transforms.ColorJitter(brightness=(0.4, 0.6)),\\n            transforms.ToTensor(),\\n            transforms.Normalize([0.5, 0.5, ',\n",
       " '---\\ntitle: \"CV Monitoring\"\\nslug: \"cv-monitoring\"\\nexcerpt: \"Quickstart Notebook\"\\nhidden: false\\ncreatedAt: \"2023-01-31T19:44:34.862Z\"\\nupdatedAt: \"2023-03-07T21:39:35.954Z\"\\n---\\nThis guide will walk you through the basic steps required to use Fiddler for monitoring computer vision (CV) models. In this notebook we demonstrate how to detect drift in image data using model embeddings using Fiddler\\'s unique Vector Monitoring approach.\\n\\nClick the following link to get started using Google Colab:\\n\\n<div class=\"colab-box\">\\n    <a href=\"https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/Fiddler_Quickstart_Image_Monitoring.ipynb\" target=\"_blank\">\\n        <div>\\n            Open in Google Colab →\\n        </div>\\n    </a>\\n    <div>\\n            <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" />\\n    </div>\\n</div># Monitoring Image data using Fiddler Vector Monotoring\\n\\nIn this notebook we present the steps for monitoring images. Fiddler employs a vector-based monitoring approach that can be used to monitor data drift in high-dimensional data such as NLP embeddings, images, video etc. In this notebook we demonstrate how to detect drift in image data using model embeddings.\\n\\nFiddler is the pioneer in enterprise Model Performance Management (MPM), offering a unified platform that enables Data Science, MLOps, Risk, Compliance, Analytics, and LOB teams to **monitor, explain, analyze, and improve ML deployments at enterprise scale**. \\nObtain contextual insights at any stage of the ML lifecycle, improve predictions, increase transparency and fairness, and optimize business revenue.\\n\\n---\\n\\nYou can experience Fiddler\\'s Image monitoring ***in minutes*** by following these quick steps:\\n\\n1. Connect to Fiddler\\n2. Load and generate embeddings for CIFAR-10 dataset\\n3. Upload the vectorized baseline dataset\\n4. Add metadata about your model \\n5. Inject data drift and publish production events\\n6. Get insights\\n\\n## Imports\\n\\n\\n```python\\n!pip install torch==2.0.0\\n!pip install torchvision==0.15.1\\n!pip install -q fiddler-client\\n```\\n\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nimport random\\nimport time\\nimport torch\\nimport torch.nn as nn\\nimport torchvision.transforms as transforms\\nfrom torchvision.models import resnet18, ResNet18_Weights\\nimport torchvision\\nimport requests\\n\\nimport fiddler as fdl\\nprint(f\"Running Fiddler client version {fdl.__version__}\")\\n```\\n\\n# 1. Connect to Fiddler\\n\\nBefore you can add information about your model with Fiddler, you\\'ll need to connect using our API client.\\n\\n---\\n\\n**We need a few pieces of information to get started.**\\n1. The URL you\\'re using to connect to Fiddler\\n2. Your organization ID\\n3. Your authorization token\\n\\nThe latter two of these can be found by pointing your browser to your Fiddler URL and navigating to the **Settings** page.\\n\\n\\n```python\\nURL = \\'\\'  # Make sure to include the full URL (including https://).\\nORG_ID = \\'\\'\\nAUTH_TOKEN = \\'\\'\\n```\\n\\nNow just run the following code block to connect to the Fiddler API!\\n\\n\\n```python\\nclient = fdl.FiddlerApi(\\n    url=URL,\\n    org_id=ORG_ID,\\n',\n",
       " '---\\ntitle: \"Useful Queries for Root Cause Analysis\"\\nslug: \"useful-queries-for-root-cause-analysis\"\\nexcerpt: \"This page has an examples of queries which one can use in the **Analyze** tab to perform Root Cause Analysis of an issue or look at various aspect of the data.\"\\nhidden: false\\ncreatedAt: \"2022-09-07T14:46:59.340Z\"\\nupdatedAt: \"2023-02-14T01:20:01.435Z\"\\n---\\n## 1. Count of events from the previous day\\n\\nIn order to look at how many events were published from the previous/last publishing date, we can do it in two ways - \\n\\n### i. Jump from **Monitor** tab\\n\\nThis can be done in the following steps - \\n\\n1. In the monitor tab, click on the \\'jump to last event\\' button to get to the most recent event \\n2. Select the appropriate time bin, in this case, we can select **1D bin** to get day-wise aggregated data\\n3. Once we have the data in the chart, we can select the most recent bin\\n4. Select \\'Export bin and feature to analyze\\' to jump to analyze tab\\n\\n![](https://files.readme.io/54545c9-1a.png)\\n\\n5. In the analyze tab, query will be auto-populated based on the **Monitor** tab selection\\n6. Modify the query to count the number of events from the selection \\n\\n   ```sql\\n   SELECT\\n     count(*)\\n   FROM\\n     production.\"bank_churn\"\\n   WHERE\\n     fiddler_timestamp BETWEEN \\'2022-07-20 00:00:00\\'\\n     AND \\'2022-07-20 23:59:59\\'\\n   ```\\n\\n### ii. Using `date` function in Analyze tab\\n\\nTo know how many events were published on the last publishing day, we can use `date` function of SQL  \\nUse the following query to query number of events\\n\\n```sql\\nselect\\n  *\\nfrom\\n  \"production.churn_classifier_test\"\\nwhere\\n  date(fiddler_timestamp) = (\\n    select\\n      date(max(fiddler_timestamp))\\n    from\\n      \"production.churn_classifier_test\"\\n  )\\n```\\n\\n\\n\\n![](https://files.readme.io/2676acb-2.png)\\n\\n## 2. Number of events on last day by output label\\n\\nIf we want to check how many events were published on the last day by the output class, we can use the following query \\n\\n```sql SQL\\nselect\\n  churn,\\n  count(*)\\nfrom\\n  \"production.churn_classifier_test\"\\nwhere\\n  date(fiddler_timestamp) = (\\n    select\\n      date(max(fiddler_timestamp))\\n    from\\n      \"production.churn_classifier_test\"\\n  )\\ngroup by \\n  churn\\n```\\n\\n\\n\\n![](https://files.readme.io/29e443f-3.png)\\n\\n## 3. Check events with missing values\\n\\nIf you want to check events where one of the columns is has null values, you can use the `isnull` function. \\n\\n```sql\\nSELECT\\n  *\\nFROM\\n  production.\"churn_classifier_test\"\\nWHERE\\n  isnull(\"estimatedsalary\")\\nLIMIT\\n  1000\\n```\\n\\n\\n\\n![](https://files.readme.io/43c2eac-4.png)\\n\\n## 4. Frequency by Categorical column\\n\\nWe query w.r.t to a categorical field. For example, we can count the number of events by geography which is a categorical column using the following query \\n\\n```sql\\nSELECT\\n  geography,\\n',\n",
       " 'Once you have added a model on the Fiddler platform using a specific model info object, that is fdl.ModelInfo, you cannot modify aspects such as features, inputs, outputs, model task etc. specified in the model info object. Currently, if you want to change fundamental details about a modelinfo object, then it is advised to create/add a new model with a new modelinfo object.',\n",
       " 'slug: \"bigquery-integration\"  query which will be used to import the data from BigQuery\\n\\n```python\\n#Write Query on BQ\\nQUERY = \"\"\"\\nSELECT * FROM `fiddler-bq.fiddler_test.churn_prediction_baseline` \\n  \"\"\"\\n```\\n\\n5. Read the data using the query and write the data to a pandas dataframe\\n\\n```python\\n#Run the query and write result to a pandas data frame\\nQuery_Results = bigquery_client.query(QUERY)\\nbaseline_df = Query_Results.to_dataframe()\\n```\\n\\nNow that we have data imported from BigQuery to a jupyter notebook, we can refer to the following notebooks to\\n\\n1. [Upload baseline data and onboard a model ](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/integration-examples/bigquery/Fiddler-BigQuery%20Integration%20-%20Model%20Registration.ipynb)\\n2. [Publish production events ](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/integration-examples/bigquery/Fiddler-BigQuery%20Integration%20-%20Event%20Publishing.ipynb)',\n",
       " 'slug: \"integration-with-s3\" ,\\n    Filename=OUTPUT_FILENAME\\n)\\n```',\n",
       " '---\\ntitle: \"BigQuery Integration\"\\nslug: \"bigquery-integration\"\\nhidden: false\\ncreatedAt: \"2022-05-20T18:53:49.606Z\"\\nupdatedAt: \"2023-06-13T19:09:07.072Z\"\\n---\\n## Using Fiddler on your ML data stored in BigQuery\\n\\nIn this article, we will be looking at loading data from BigQuery tables and using the data for the following tasks-\\n\\n1. Uploading baseline data to Fiddler\\n2. Onboarding a model to Fiddler and creating a surrogate\\n3. Publishing production data to Fiddler\\n\\n## Step 1 - Enable BigQuery API\\n\\nBefore looking at how to import data from BigQuery to Fiddler, we will first see how to enable BigQuery API. This can be done as follows - \\n\\n1. In the GCP platform, Go to the navigation menu -> click APIs & Services. Once you are there, click + Enable APIs and Services (Highlighted below). In the search bar, enter BigQuery API and click Enable.\\n\\n![](https://files.readme.io/75ca647-Screen_Shot_2022-05-19_at_1.26.33_PM.png \"Screen Shot 2022-05-19 at 1.26.33 PM.png\")\\n\\n![](https://files.readme.io/3dd5deb-Screen_Shot_2022-05-19_at_3.33.43_PM.png \"Screen Shot 2022-05-19 at 3.33.43 PM.png\")\\n\\n2. In order to make a request to the API enabled in Step#1, you need to create a service account and get an authentication file for your Jupyter Notebook. To do so, navigate to the Credentials tab under APIs and Services console and click Create Credentials tab, and then Service account under dropdown.\\n\\n![](https://files.readme.io/ea63eca-Screen_Shot_2022-05-19_at_3.34.24_PM.png \"Screen Shot 2022-05-19 at 3.34.24 PM.png\")\\n\\n3. Enter the Service account name and description. You can use the BigQuery Admin role under Grant this service account access to the project. Click Done. You can now see the new service account under the Credentials screen. Click the pencil icon beside the new service account you have created and click Add Key to add auth key. Please choose JSON and click CREATE. It will download the JSON file with auth key info. (Download path will be used to authenticate)\\n\\n![](https://files.readme.io/662315e-Screen_Shot_2022-05-19_at_3.39.24_PM.png \"Screen Shot 2022-05-19 at 3.39.24 PM.png\")\\n\\n## Step 2 - Import data from BigQuery\\n\\nWe will now use the generated key to connect to BigQuery tables from Jupyter Notebook. \\n\\n1. Install the following libraries in the python environment and load them to jupyter-\\n\\n- Google-cloud\\n- Google-cloud-bigquery[pandas]\\n- Google-cloud-storage\\n\\n2. Set the environment variable using the key that was generated in Step 1\\n\\n```python\\n#Set environment variables for your notebook\\nimport os\\nos.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'<path to json file>\\'\\n```\\n\\n3. Import Google cloud client and initiate BigQuery service\\n\\n```python\\n#Imports google cloud client library and initiates BQ service\\nfrom google.cloud import bigquery\\nbigquery_client = bigquery.Client()\\n```\\n\\n4. Specify the',\n",
       " 'slug: \"clientrun_feature_importance\"  dictionary containing feature importance results.\"\\n  },\\n  \"cols\": 2,\\n  \"rows\": 1\\n}\\n[/block]',\n",
       " 'slug: \"quick-start\"  your project, you should now be able to see the newly created dataset on the UI.\\n\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/6.png\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n## 4. Add metadata about your model\\n\\nNow it\\'s time to add your model with Fiddler.  We do this by defining a [ModelInfo](https://docs.fiddler.ai/reference/fdlmodelinfo) object.\\n\\n\\n---\\n\\n\\nThe [ModelInfo](https://docs.fiddler.ai/reference/fdlmodelinfo) object will contain some **information about how your model operates**.\\n  \\n*Just include:*\\n1. The **task** your model is performing (regression, binary classification, etc.)\\n2. The **target** (ground truth) column\\n3. The **output** (prediction) column\\n4. The **feature** columns\\n5. Any **metadata** columns\\n6. Any **decision** columns (these measures the direct business decisions made as result of the model\\'s prediction)\\n\\n\\n\\n```python\\n# Specify task\\nmodel_task = \\'binary\\'\\n\\nif model_task == \\'regression\\':\\n    model_task = fdl.ModelTask.REGRESSION\\n    \\nelif model_task == \\'binary\\':\\n    model_task = fdl.ModelTask.BINARY_CLASSIFICATION\\n\\nelif model_task == \\'multiclass\\':\\n    model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION\\n    \\nelif model_task == \\'ranking\\':\\n    model_task = fdl.ModelTask.RANKING\\n\\n    \\n# Specify column types\\nfeatures = [\\'geography\\', \\'gender\\', \\'age\\', \\'tenure\\', \\'balance\\', \\'numofproducts\\', \\'hascrcard\\', \\'isactivemember\\', \\'estimatedsalary\\']\\noutputs = [\\'predicted_churn\\']\\ntarget = \\'churn\\'\\ndecision_cols = [\\'decision\\']\\nmetadata_cols = [\\'customer_id\\']\\n    \\n# Generate ModelInfo\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n    dataset_info=dataset_info,\\n    dataset_id=DATASET_ID,\\n    model_task=model_task,\\n    features=features,\\n    outputs=outputs,\\n    target=target,\\n    categorical_target_class_details=\\'yes\\',\\n    decision_cols=decision_cols, # Optional\\n    metadata_cols=metadata_cols, # Optional\\n    binary_classification_threshold=0.5 # Optional\\n)\\nmodel_info\\n```\\n\\nAlmost done! Now just specify a unique model ID and use the client\\'s [add_model](https://docs.fiddler.ai/reference/clientadd_model) function to send this information to Fiddler.\\n\\n\\n```python\\nMODEL_ID = \\'churn_classifier\\'\\n\\nclient.add_model(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID,\\n    model_id=MODEL_ID,\\n    model_info=model_info,\\n)\\n```\\n\\nOn the project page, you should now be able to see the newly created model.\\n\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/7.png\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n## 5. Set up Alerts and Notifications (Optional)\\n\\nFiddler Client API function [add_alert_rule](https://dash.readme.com/project/fiddler/v1.5/refs/clientadd_alert_rule) allow creating rules to receive email and pagerduty notifications when your data or model predictions deviates from it\\'s expected behavior.\\n\\nThe rules can of **Data Drift, Performance, Data Integrity,** and **Service Metrics** types and they can be compared to **',\n",
       " 'slug: \"publishing-events-with-complex-data-formats\"  `example_project_1` project and `example_model_1` model, using the schema defined for that conditional.\\n[block:callout]\\n{\\n  \"type\": \"warning\",\\n  \"title\": \"Note\",\\n  \"body\": \"In order to use this conditional functionality, you\\'ll need to specify the `__project` and `__model` **outside** of the conditional.\"\\n}\\n[/block]',\n",
       " 'slug: \"customer-churn-prediction\" Churn-image5-analyze-rca-1.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 1\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe further check the performance of the model for our selection by selecting the **Chart Type - Slice Evaluation**.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/350aef8-Churn-image6-analyze-rca-2.png\",\\n        \"Churn-image6-analyze-rca-2.png\",\\n        1578\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 2\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nIn order to check if the change in the range violation has occurred for a subsection of data, we can plot it against the categorical variable. In our case, we can check distribution of `numofproducts` against `age` and `geography`. For this we can plot a feature correlation plot for two features by querying data and selecting **Chart type - Feature Correlation**.\\n\\nOn plotting the feature correlation plot of `gender` vs `numofprodcuts`, we observe the distribution to be similar.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/4f73274-churn-image6-analyze-rca-2-1.png\",\\n        \"churn-image6-analyze-rca-2-1.png\",\\n        512\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 3\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/aff3dbf-churn-image6-analyze-rca-2-2.png\",\\n        \"churn-image6-analyze-rca-2-2.png\",\\n        464\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 4\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nFor the sake of this example, let’s say that state of Hawaii (which is a value in the `geography` field in the data) announced that it has eased restrictions on number of loans, since loans is one of products, our hypothesis is the `numofproducts` would be higher for the state. To test this we will check the feature correlation between `geography` and `numofproducts`.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/e1c31a1-churn-image6-analyze-rca-2-3.png\",\\n        \"churn-image6-analyze-rca-2-3.png\",\\n        463\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 5\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nWe do see higher values for the state of Hawaii as compared to other states. We can further check distribution for the field `numofproducts` just for the state of Hawaii. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/6664850-churn-image7--analyze-rca-3.png\",\\n        \"churn-image7--analyze-rca-3.png\",\\n        1999\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Root Cause Analysis - 6\"\\n    }\\n  ]\\n}\\n[/block]\\n\\nOn checking performance for',\n",
       " '---\\ntitle: \"Performance Tracking\"\\nslug: \"performance-tracking-platform\"\\nexcerpt: \"Platform Guide\"\\nhidden: false\\ncreatedAt: \"2022-12-19T19:27:22.159Z\"\\nupdatedAt: \"2023-08-04T23:21:39.375Z\"\\n---\\n## What is being tracked?\\n\\n![](https://files.readme.io/4a646d4-qs_monitoring.png \"qs_monitoring.png\")\\n\\n- **_Decisions_** - The post-prediction business decisions made as a result of the model output. Decisions are calculated before [client.publish_event()](ref:clientpublish_event) (they\\'re not inferred by Fiddler). For binary classification models, a decision is usually determined using a threshold. For multi-class classification models, it\\'s usually determined using the argmax value of the model outputs.\\n\\n- **_Performance metrics_**\\n\\n| Model Task Type       | Metric                                                         | Description                                                                                                                                        |\\n| :-------------------- | :------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| Binary Classification | Accuracy                                                       | (TP + TN) / (TP + TN + FP + FN)                                                                                                                    |\\n| Binary Classification | True Positive Rate/Recall                                      | TP / (TP + FN)                                                                                                                                     |\\n| Binary Classification | False Positive Rate                                            | FP / (FP + TN)                                                                                                                                     |\\n| Binary Classification | Precision                                                      | TP / (TP + FP)                                                                                                                                     |\\n| Binary Classification | F1 Score                                                       | 2  \\\\* ( Precision \\\\*  Recall ) / ( Precision + Recall )                                                                                            |\\n| Binary Classification | AUROC                                                          | Area Under the Receiver Operating Characteristic (ROC) curve, which plots the true positive rate against the false positive rate                   |\\n| Binary Classification | Binary Cross Entropy                                           | Measures the difference between the predicted probability distribution and the true distribution                                                   |\\n| Binary Classification | Geometric Mean                                                 | Square Root of ( Precision \\\\* Recall )                                                                                                             |\\n| Binary Classification | Calibrated Threshold                                           | A threshold that balances precision and recall at a particular operating point                                                                     |\\n| Binary Classification | Data Count                                                     | The number of events where target and output are both not NULL. **_This will be used as the denominator when calculating accuracy_**.              |\\n| Binary Classification | Expected Calibration Error                                     | Measures the difference between predicted probabilities and empirical probabilities                                                                |\\n| Multi Classification  | Accuracy                                                       | (Number of correctly classified samples) / ( Data Count ). Data Count refers to the number of events where the target and output are both not NULL |\\n| Multi Classification  | Log Loss                                                       | Measures the difference between the predicted probability distribution and the true distribution, in a logarithmic scale                           |\\n| Regression            | Coefficient of determination (R-squared)                       | Measures the proportion of variance in the dependent variable that is explained by the independent variables                                       |\\n| Regression            | Mean Squared Error (MSE)                                       | Average of the squared differences between the predicted and true values                                                                           |\\n| Regression            | Mean Absolute Error (MAE)                                      | Average of the absolute differences between the predicted and true values                                                                          |\\n| Regression            | Mean Absolute Percentage Error (MAPE)                          | Average of the absolute percentage differences between the predicted and true values                                                               |\\n| Regression            | Weighted Mean Absolute Percentage Error (WMAPE)                | The weighted average of the absolute percentage differences between the predicted and true values                                                  |\\n| Ranking               | Mean Average Precision (MAP)—for binary relevance ranking only | Measures the average precision of the relevant items in the top-k results                                                                          |\\n| Ranking               | Normalized Discounted Cumulative Gain (NDCG)                   | Measures the quality of',\n",
       " 'slug: \"quick-start\" ** page.\\n\\n<table>\\n    <tr>\\n        <td><img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/1.png\" /></td>\\n        <td><img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/2.png\" /></td>\\n    </tr>\\n    <tr>\\n        <td><img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/3.png\" /></td>\\n        <td><img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/4.png\" /></td>\\n    </tr>\\n</table>\\n\\n\\n```python\\nORG_ID = \\'\\'\\nAUTH_TOKEN = \\'\\'\\n```\\n\\nNow just run the following code block to connect to the Fiddler API!\\n\\n\\n```python\\nclient = fdl.FiddlerApi(url=URL, org_id=ORG_ID, auth_token=AUTH_TOKEN)\\n```\\n\\nOnce you connect, you can create a new project by specifying a unique project ID in the client\\'s `create_project` function.\\n\\n\\n```python\\nPROJECT_ID = \\'\\'\\n\\nclient.create_project(PROJECT_ID)\\n```\\n\\nYou should now be able to see the newly created project on the UI.\\n\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/5.png\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n## 3. Upload a baseline dataset\\n\\nIn this example, we\\'ll be considering the case where we\\'re a bank and we have **a model that predicts churn for our customers**.  \\nWe want to know when our model\\'s predictions start to drift—that is, **when churn starts to increase** within our customer base.\\n  \\nIn order to get insights into the model\\'s performance, **Fiddler needs a small  sample of data that can serve as a baseline** for making comparisons with data in production.\\n\\n\\n---\\n\\n\\n*For more information on how to design a baseline dataset, [click here](https://docs.fiddler.ai/docs/designing-a-baseline-dataset).*\\n\\n\\n```python\\nPATH_TO_BASELINE_CSV = \\'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/churn_baseline.csv\\'\\n\\nbaseline_df = pd.read_csv(PATH_TO_BASELINE_CSV)\\nbaseline_df\\n```\\n\\nFiddler uses this baseline dataset to keep track of important information about your data.\\n  \\nThis includes **data types**, **data ranges**, and **unique values** for categorical variables.\\n\\n---\\n\\nYou can construct a `DatasetInfo` object to be used as **a schema for keeping track of this information** by running the following code block.\\n\\n\\n```python\\ndataset_info = fdl.DatasetInfo.from_dataframe(baseline_df, max_inferred_cardinality=100)\\ndataset_info\\n```\\n\\nThen use the client\\'s [upload_dataset](https://docs.fiddler.ai/reference/clientupload_dataset) function to send this information to Fiddler!\\n  \\n*Just include:*\\n1. A unique dataset ID\\n2. The baseline dataset as a pandas DataFrame\\n3. The [DatasetInfo](https://docs.fiddler.ai/reference/fdldatasetinfo) object you just created\\n\\n\\n```python\\nDATASET_ID = \\'churn_data\\'\\n\\nclient.upload_dataset(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID,\\n    dataset={\\n        \\'baseline\\': baseline_df\\n    },\\n    info=dataset_info\\n)\\n```\\n\\nIf you click on',\n",
       " '---\\ntitle: \"Project Structure on UI\"\\nslug: \"project-structure\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:26:33.568Z\"\\nupdatedAt: \"2023-02-03T19:45:51.093Z\"\\n---\\nSupervised machine learning involves identifying a predictive task, finding data to enable that task, and building a model using that data. Fiddler captures this workflow with project, dataset, and model entities.\\n\\n## Projects\\n\\nA project represents a machine learning task (e.g. predicting house prices, assessing creditworthiness, or detecting fraud).\\n\\nA project can contain one or more models for the ML task (e.g. LinearRegression-HousePredict, RandomForest-HousePredict).\\n\\nCreate a project by clicking on **Projects** and then clicking on **Add Project**.\\n\\n![](https://files.readme.io/8e4b429-Add_project_0710.png \"Add_project_0710.png\")\\n\\n- **_Create New Project_** — A window will pop up where you can enter the project name and click **Create**. Once the project is created, it will be displayed on the projects page.\\n\\nYou can access your projects from the Projects Page.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/82404e6-Screenshot_2022-12-27_at_1.00.15_PM.png\",\\n        null,\\n        \"Projects Page on Fiddler UI\"\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Projects Page on Fiddler UI\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n## Datasets\\n\\nA dataset in Fiddler is a data table containing features, model outputs, and a target for machine learning models. Optionally, you can also upload metadata and “decision” columns, which can be used to segment the dataset for analyses, track business decisions, and work as protected attributes in bias-related workflows. For more details refer to [Datasets](doc:project-architecture#datasets) in the Platform Guide.\\n\\nOnce you click on a particular project, you will be able to see if there are any datasets associated with the project. For example, the bank_churn project, in the following screenshot, has the bank_churn dataset. [Datasets are uploaded via the Fiddler client](ref:clientupload_dataset). \\n\\n![](https://files.readme.io/3fa7700-Screenshot_2022-12-27_at_1.05.05_PM.png)\\n\\n## Models\\n\\nA model in Fiddler represents a machine learning model. A project will have one or more models for the ML task (e.g. a project to predict house prices might contain LinearRegression-HousePredict and RandomForest-HousePredict). For further details refer to the [Models](doc:project-architecture#models) section in the Platform Guide.\\n\\n![](https://files.readme.io/e151df5-Model_Dashboard.png \"Model_Dashboard.png\")\\n\\n### Model Artifacts\\n\\nAt its most basic level, a model in Fiddler is simply a directory that contains [model artifacts](doc:artifacts-and-surrogates) such as:\\n\\n- The model file (e.g. `*.pkl`)\\n- `package.py`: A wrapper script containing all of the code needed to standardize the execution of the model.\\n\\n![](https://files.readme.io/7170489-Model_Details.png \"Model_Details.png\")\\n\\n![](https://files.readme.io/2b3d52e-Model_Details_1.png \"Model_Details_1.png\")\\n\\n## Project Dashboard',\n",
       " 'slug: \"monitoring-xai-quick-start\" raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/xai_data.png?raw=true\" /></td> \\n    </tr>\\n    <tr>\\n        <td><img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/xai_data_info.png?raw=true\" /></td>\\n    </tr> \\n</table>\\n\\n## 3. Add information about your model\\n\\nNow it\\'s time to add details about your model with Fiddler. We do so by first creating a **ModelInfo Object** that helps Fiddler understand **how your model operates**.\\n  \\n*Just include:*\\n1. The **task** your model is performing (regression, binary classification, etc.)\\n2. The **target** (ground truth) column\\n3. The **output** (prediction) column\\n4. The **feature** columns\\n5. Any **metadata** columns\\n6. Any **decision** columns (these measures the direct business decisions made as result of the model\\'s prediction)\\n\\n\\n\\n```python\\n# Specify task\\nmodel_task = \\'binary\\'\\n\\nif model_task == \\'regression\\':\\n    model_task = fdl.ModelTask.REGRESSION\\n    \\nelif model_task == \\'binary\\':\\n    model_task = fdl.ModelTask.BINARY_CLASSIFICATION\\n\\nelif model_task == \\'multiclass\\':\\n    model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION\\n\\n    \\n# Specify column types\\ntarget = \\'churn\\'\\noutputs = [\\'predicted_churn\\']\\ndecision_cols = [\\'decision\\']\\nfeatures = [\\'geography\\', \\'gender\\', \\'age\\', \\'tenure\\', \\'balance\\', \\'numofproducts\\', \\'hascrcard\\', \\'isactivemember\\', \\'estimatedsalary\\']\\n    \\n# Generate ModelInfo\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n    dataset_info=dataset_info,\\n    dataset_id=DATASET_ID,\\n    model_task=model_task,\\n    target=target,\\n    categorical_target_class_details=\\'yes\\',\\n    outputs=outputs,\\n    decision_cols=decision_cols,\\n    features=features\\n)\\nmodel_info\\n```\\n\\nAfter ModelInfo object is created to save your model information, use the client\\'s *add_model* call to add the generated details about your model. \\n\\n**Note:** You will need to specify a unique model ID.\\n\\n\\n```python\\nMODEL_ID = \\'churn_classifier\\'\\n\\nclient.add_model(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID,\\n    model_id=MODEL_ID,\\n    model_info=model_info\\n)\\n```\\n\\nOn the project page, you should now be able to see the newly created model. Notice how without uploading model or creating surrogate model, you can only explore monitoring capabilities.\\n\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/xai_add_model.png?raw=true\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n## 4. Either upload your own model or generate a surrogate model\\n\\nWith the above step, your model is added to Fiddler which means that for a given *project_id*, your given *model_id* now holds *ModelInfo* about the model you care about. \\n\\nIn order to be able to run predictions for explainability analysis, however, you will need to upload your model file. If you just want to explore the XAI capabilities without providing your model to Fiddler, you can also generate a surrogate model which tries to mimic your model based on the details provided. \\n\\nIn this quickstart, we will go with generating a surrogate model based on the',\n",
       " 'slug: \"product-tour\" )\\n\\n**Projects** represent your organization\\'s distinct AI applications or use cases. Within Fiddler, Projects house all the **Models** specific to a given application, and thus serve as a jumping-off point for the majority of Fiddler’s model monitoring and explainability features.\\n\\nGo ahead and click on the _Lending project_ to navigate to the Project Overview page.\\n\\n![](https://files.readme.io/b008f03-image.png)\\n\\nHere you can see a list of the models contained within the Lending project, as well as a project dashboard to which analyze charts can be pinned. Go ahead and click the “logreg-all” model.\\n\\n![](https://files.readme.io/f3e024d-image.png)\\n\\nFrom the Model Overview page, you can view details about the model: its metadata (schema), the files in its model directory, and its features, which are sorted by impact (the degree to which each feature influences the model’s prediction score).\\n\\nYou can then navigate to the platform\\'s core monitoring and explainability capabilities. These include:\\n\\n- **_Monitor_** — Track and configure alerts on your model’s performance, data drift, data integrity, and overall service metrics. Read the [Monitoring](doc:monitoring-platform) documentation for more details.\\n- **_Analyze_** — Analyze the behavior of your model in aggregate or with respect to specific segments of your population. Read the [Analytics](doc:analytics-ui) documentation for more details.\\n- **_Explain_** — Generate “point” or prediction-level explanations on your training or production data for insight into how each model decision was made. Read the [Explainability](doc:explainability-platform) documentation for more details.\\n- **_Evaluate_** — View your model’s performance on its training and test sets for quick validation prior to deployment. Read the [Evaluation](doc:evaluation-ui) documentation for more details.\\n\\n## Fiddler Samples\\n\\nFiddler Samples is a set of datasets and models that are preloaded into Fiddler. They represent different data types, model frameworks, and machine learning techniques. See the table below for more details.\\n\\n| **Project**   | **Model**                       | **Dataset** | **Model Framework** | **Algorithm**       | **Model Task**             | **Explanation Algos** |\\n| ------------- | ------------------------------- | ----------- | ------------------- | ------------------- | -------------------------- | --------------------- |\\n| Bank Churn    | Bank Churn                      | Tabular     | scikit-learn        | Random Forest       | Binary Classification      | Fiddler Shapley       |\\n| Heart Disease | Heart Disease                   | Tabular     | Tensorflow          |                     | Binary Classification      | Fiddler Shapley, IG   |\\n| IMDB          | Imdb Rnn                        | Text        | Tensorflow          | BiLSTM              | Binary Classfication       | Fiddler Shapley, IG   |\\n| Iris          | Iris                            | Tabular     | scikit-learn        | Logistic Regression | Multi-class Classification | Fiddler Shapley       |\\n| Lending       | Logreg-all                      | Tabular     | scikit-learn        | Logistic Regression | Binary Classification      | Fiddler Shapley       |\\n|               | Logreg-simple                   | Tabular     | scikit-learn        | Logistic Regression | Binary Classification      | Fiddler Shapley       |\\n|               | Xgboost-simple-sagemaker        | Tabular     | scikit-learn        | XGboost             | Binary Classification      | Fiddler Shapley       |\\n| Newsgroup     | Christianity',\n",
       " 'slug: \"fdldeploymentparams\" 2000               | 1200             |\\n| \\\\<400                    | 2800               | 1300             |\\n| \\\\<500                    | 2900               | 1500             |\\n\\n2. **User Uploaded guide**\\n\\nFor uploading your artifact model, refer to the table above and increase the memory number, depending on your model framework and complexity. Surrogate models use lightgbm framework. \\n\\nFor example, an NLP model for a TEXT input might need memory set at 1024 or higher and CPU at 1000.\\n\\n> 📘 Usage Reference\\n> \\n> See the usage with:\\n> \\n> - [add_model_artifact](ref:clientadd_model_artifact)\\n> - [add_model_surrogate](ref:clientadd_model_surrogate)\\n> - [update_model_artifact](ref:clientupdate_model_artifact)\\n> - [update_model_surrogate](ref:clientupdate_model_surrogate)\\n> \\n> Check more about the [Model Deployment](doc:model-deployment) feature set.',\n",
       " '---\\ntitle: \"S3 Integration\"\\nslug: \"integration-with-s3\"\\nhidden: false\\ncreatedAt: \"2022-04-19T17:40:36.681Z\"\\nupdatedAt: \"2023-02-02T20:19:53.031Z\"\\n---\\n## Pulling a dataset from S3\\n\\nYou may want to **pull a dataset directly from S3**. This may be used either to upload a baseline dataset, or to publish production traffic to Fiddler.\\n\\nYou can use the following code snippet to do so. Just fill out each of the string variables (`S3_BUCKET`, `S3_FILENAME`, etc.) with the correct information.\\n\\n```python\\nimport boto3\\nimport pandas as pd\\n\\nS3_BUCKET = \\'my_bucket\\'\\nS3_FILENAME = \\'my_baseline.csv\\'\\n\\nAWS_ACCESS_KEY_ID = \\'my_access_key\\'\\nAWS_SECRET_ACCESS_KEY = \\'my_secret_access_key\\'\\nAWS_REGION = \\'my_region\\'\\n\\nsession = boto3.session.Session(\\n    aws_access_key_id=AWS_ACCESS_KEY_ID,\\n    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\\n    region_name=AWS_REGION\\n)\\n\\ns3 = session.client(\\'s3\\')\\n\\ns3_data = s3.get_object(\\n    Bucket=S3_BUCKET,\\n    Key=S3_FILENAME\\n)[\\'Body\\']\\n\\ndf = pd.read_csv(s3_data)\\n```\\n\\n\\n\\n## Uploading the data to Fiddler\\n\\nIf your goal is to **use this data as a baseline dataset** within Fiddler, you can then proceed to upload your dataset (see [Uploading a Baseline Dataset](doc:uploading-a-baseline-dataset)).\\n\\nIf your goal is to **use this data as a batch of production traffic**, you can then proceed to publish the batch to Fiddler (see [Publishing Batches of Events](doc:publishing-batches-of-events) ). \\n\\n## What if I don’t want to hardcode my AWS credentials?\\n\\nIf you don’t want to hardcode your credentials, you can **use an AWS profile** instead. For more information on how to create an AWS profile, click [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html).\\n\\nYou can use the following code snippet to point your `boto3` session to the profile of your choosing.\\n\\n```python\\nimport boto3\\nimport pandas as pd\\n\\nS3_BUCKET = \\'my_bucket\\'\\nS3_FILENAME = \\'my_baseline.csv\\'\\n\\nAWS_PROFILE = \\'my_profile\\'\\n\\nsession = boto3.session.Session(\\n    profile_name=AWS_PROFILE\\n)\\n\\ns3 = session.client(\\'s3\\')\\n\\ns3_data = s3.get_object(\\n    Bucket=S3_BUCKET,\\n    Key=S3_FILENAME\\n)[\\'Body\\']\\n\\ndf = pd.read_csv(s3_data)\\n```\\n\\n\\n\\n## What if I don\\'t want to load the data into memory?\\n\\nIf you would rather **save the data to a disk** instead of loading it in as a pandas DataFrame, you can use the following code snippet instead.\\n\\n```python\\nimport boto3\\nimport pandas as pd\\nimport fiddler as fdl\\n\\nS3_BUCKET = \\'my_bucket\\'\\nS3_FILENAME = \\'my_baseline.csv\\'\\n\\nAWS_ACCESS_KEY_ID = \\'my_access_key\\'\\nAWS_SECRET_ACCESS_KEY = \\'my_secret_access_key\\'\\nAWS_REGION = \\'my_region\\'\\n\\nOUTPUT_FILENAME = \\'s3_data.csv\\'\\n\\nsession = boto3.session.Session(\\n    aws_access_key_id=AWS_ACCESS_KEY_ID,\\n    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\\n    region_name=AWS_REGION\\n)\\n\\ns3 = session.client(\\'s3\\')\\n\\ns3.download_file(\\n    Bucket=S3_BUCKET,\\n    Key=S3_FILENAME',\n",
       " '---\\ntitle: \"Traffic\"\\nslug: \"traffic-platform\"\\nexcerpt: \"Platform Guide\"\\nhidden: false\\ncreatedAt: \"2022-12-19T19:28:11.378Z\"\\nupdatedAt: \"2023-08-04T23:21:11.689Z\"\\n---\\nTraffic as a service metric gives you basic insights into the operational health of your ML service in production.\\n\\n![](https://files.readme.io/d2c1eaa-Screenshot_2023-02-01_at_5.13.34_PM.png)\\n\\n## What is being tracked?\\n\\n- **_Traffic_** — The volume of traffic received by the model over time.\\n\\n## Why is it being tracked?\\n\\n- Traffic is a basic high-level metric that informs us of the overall system\\'s health.\\n\\n## What steps should I take when I see an outlier?\\n\\n- A dip or spike in traffic needs to be investigated. For example, a dip could be due to a production model server going down; a spike could be an adversarial attack.\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"quick-start\" absolute** or **relative** values.\\n\\nPlease refer [our documentation](https://docs.fiddler.ai/docs/alerts) for more information on Alert Rules. \\n\\n---\\n  \\nLet\\'s set up a few Alert Rules.\\n\\nThe following API call sets up a Data Integrity type rule which triggers an email notification when published events have 2 or more range violations in any 1 day bin for the ```numofproducts``` column.\\n\\n\\n```python\\nnotifications_config = client.build_notifications_config(\\n    emails = \"name@google.com\",\\n)\\n\\nclient.add_alert_rule(\\n    name = \"Bank Churn Range Violation Alert1\",\\n    project_id = PROJECT_ID,\\n    model_id = MODEL_ID,\\n    alert_type = fdl.AlertType.DATA_INTEGRITY,\\n    metric = fdl.Metric.RANGE_VIOLATION,\\n    bin_size = fdl.BinSize.ONE_DAY, \\n    compare_to = fdl.CompareTo.RAW_VALUE,\\n    #compare_period = None,\\n    priority = fdl.Priority.HIGH,\\n    warning_threshold = 2,\\n    critical_threshold = 3,\\n    condition = fdl.AlertCondition.GREATER,\\n    column = \"numofproducts\",\\n    notifications_config = notifications_config\\n)\\n```\\n\\nThe following API call sets up a Performance type rule which triggers an email notification when precision metric is 5% higher than that from 1 hr bin one day ago.\\n\\n\\n```python\\nnotifications_config = client.build_notifications_config(\\n    emails = \"name@google.com\",\\n)\\nclient.add_alert_rule(\\n    name = \"Bank Churn Performance Alert\",\\n    project_id = PROJECT_ID,\\n    model_id = MODEL_ID,\\n    alert_type = fdl.AlertType.PERFORMANCE,\\n    metric = fdl.Metric.PRECISION,\\n    bin_size = fdl.BinSize.ONE_HOUR, \\n    compare_to = fdl.CompareTo.TIME_PERIOD,\\n    compare_period = fdl.ComparePeriod.ONE_DAY,\\n    warning_threshold = 0.05,\\n    critical_threshold = 0.1,\\n    condition = fdl.AlertCondition.GREATER,\\n    priority = fdl.Priority.HIGH,\\n    notifications_config = notifications_config\\n)\\n```\\n\\n## 6. Publish production events\\n\\nInformation about your model is added to Fiddler and now it\\'s time to start publishing some production data!  \\nFiddler will **monitor this data and compare it to your baseline to generate powerful insights into how your model is behaving**.\\n\\n\\n---\\n\\n\\nEach record sent to Fiddler is called **an event**.\\n  \\nLet\\'s load in some sample events from a CSV file.\\n\\n\\n```python\\nPATH_TO_EVENTS_CSV = \\'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/churn_events.csv\\'\\n\\nproduction_df = pd.read_csv(PATH_TO_EVENTS_CSV)\\n\\n# Shift the timestamps of the production events to be as recent as today \\nproduction_df[\\'timestamp\\'] = production_df[\\'timestamp\\'] + (int(time.time() * 1000) - production_df[\\'timestamp\\'].max())\\n```\\n\\n\\n```python\\nprint(production_df[\\'timestamp\\'])\\n```\\n\\nYou can use the client\\'s `publish_events_batch` function to start pumping data into Fiddler!\\n  \\n*Just include:*\\n1. The DataFrame containing your events\\n2. The name of the column containing event timestamps\\n\\n\\n```python\\nclient.publish_events_batch(\\n    project_id=PROJECT_ID,\\n    model_id=MODEL_ID,\\n    batch_source=production_df,\\n    timestamp_field=\\'timestamp\\',\\n    id_field=\\'customer_id\\' # Optional\\n)\\n```\\n\\n## 7. Get insights\\n  \\nNow just head to your Fiddler URL and start getting enhanced observability into your model\\'s performance.\\n\\nRun the',\n",
       " '---\\ntitle: \"Snowflake Integration\"\\nslug: \"snowflake-integration\"\\nhidden: false\\ncreatedAt: \"2022-06-22T14:51:45.373Z\"\\nupdatedAt: \"2023-06-13T19:08:09.527Z\"\\n---\\n## Using Fiddler on your ML data stored in Snowflake\\n\\nIn this article, we will be looking at loading data from Snowflake tables and using the data for the following tasks-\\n\\n1. Uploading baseline data to Fiddler\\n2. Onboarding a model to Fiddler and creating a surrogate\\n3. Publishing production data to Fiddler\\n\\n### Import data from Snowflake\\n\\nIn order to import data from Snowflake to Jupyter notebook, we will use the snowflake library, this can be installed using the following command in your Python environment.\\n\\n```python\\npip install snowflake-connector-python\\n```\\n\\nOnce the library is installed, we would require the following to establish a connection to Snowflake\\n\\n- Snowflake Warehouse\\n- Snowflake Role\\n- Snowflake Account\\n- Snowflake User\\n- Snowflake Password\\n\\nThese can be obtained from your Snowflake account under the ‘Admin’ option in the Menu as shown below or by running the queries - \\n\\n- Warehouse - select CURRENT_WAREHOUSE()\\n- Role - select CURRENT_ROLE()\\n- Account - select CURRENT_ACCOUNT()\\n\\n\\'User\\' and \\'Password\\' are the same as one used for logging into your Snowflake account.\\n\\n![](https://files.readme.io/c2f4cf4-Screen_Shot_2022-06-14_at_4.17.36_PM.png \"Screen Shot 2022-06-14 at 4.17.36 PM.png\")\\n\\nOnce you have this information, you can set up a Snowflake connector using the following code -\\n\\n```python\\n# establish Snowflake connection\\nconnection = connector.connect(user=snowflake_username, \\n                               password=snowflake_password, \\n                               account=snowflake_account, \\n                               role=snowflake_role, \\n                               warehouse=snowflake_warehouse\\n                              )\\n```\\n\\nYou can then write a custom SQL query and import the data to a pandas dataframe.\\n\\n```python\\n# sample SQL query\\nsql_query = \\'select * from FIDDLER.FIDDLER_SCHEMA.CHURN_BASELINE LIMIT 100\\'\\n\\n# create cursor object\\ncursor = connection.cursor()\\n\\n# execute SQL query inside Snowflake\\ncursor.execute(sql_query)\\n\\nbaseline_df = cursor.fetch_pandas_all()\\n```\\n\\n### Publish Production Events\\n\\nIn order to publish production events from Snowflake, we can load the data to a pandas dataframe and publish it to fiddler using _client.publish_events_batch_ api.\\n\\nNow that we have data imported from Snowflake to a jupyter notebook, we can refer to the following notebooks to\\n\\n- [Upload baseline data and onboard a model ](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/integration-examples/snowflake/Fiddler-Snowflake%20Integration%20-%20Model%20Registration.ipynb)\\n- [Publish production events](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/integration-examples/snowflake/Fiddler-Snowflake%20Integration%20-%20Event%20Publishing.ipynb)',\n",
       " '---\\ntitle: \"System Architecture\"\\nslug: \"system-architecture\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:19:53.311Z\"\\nupdatedAt: \"2023-05-18T21:09:05.870Z\"\\n---\\nFiddler deploys into your private cloud\\'s existing Kubernetes clusters, for which the minimum system requirements can be found [here](doc:technical-requirements).  Fiddler supports deployment into Kubernetes in AWS, Azure, or GCP.  All services of the Fiddler platform are containerized in order to eliminate reliance on other cloud services and to reduce the deployment and maintenance friction of the platform.  This includes storage services like object storage and databases as well as system monitoring services like Grafana.  \\n\\nUpdates to the Fiddler containers is accomplished through a shared container registry (that Fiddler is provided access to).  Updates to the containers are orchestrated using Helm charts.\\n\\nA full-stack deployment of Fiddler is shown in the diagram below. \\n\\n![](https://files.readme.io/7cbfe31-reference_architecture.png)\\n\\nThe Fiddler system components are deployed within a single namespace on a Kubernetes cluster, using the official Fiddler Helm chart.\\n\\n- Fiddler core infrastructure relies on persistent volumes provided within the Kubernetes cluster. We recommend using encrypted storage volumes wherever possible.\\n- Fiddler may be configured to utilize external infrastructure in a self-hosted environment, such as existing PostgresQL servers, but this is not required as all services are containerized by default.\\n- Full-stack \"any-prem\" Fiddler includes observability infrastructure to monitor Fiddler system health and performance. These mainstream observability components may be integrated with external observability systems to support administration in a self-hosted environment.\\n- HTTP traffic to the Fiddler system is handled by an L4 or L7 load balancer or other proxy. TLS termination should usually occur outside the Fiddler system.\\n\\nOnce the platform is running, end users can interface with the Fiddler platform using their browser, the [Fiddler Python client](ref:about-the-fiddler-client), or Fiddler\\'s RESTful APIs.',\n",
       " '---\\ntitle: \"Traffic\"\\nslug: \"traffic-ui\"\\nexcerpt: \"UI Guide\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:25:31.308Z\"\\nupdatedAt: \"2023-02-14T01:19:05.392Z\"\\n---\\nTraffic as a service metric gives you basic insights into the operational health of your model\\'s service in production.\\n\\n![](https://files.readme.io/d2c1eaa-Screenshot_2023-02-01_at_5.13.34_PM.png)\\n\\n## What is being tracked?\\n\\n- **_Traffic_** — The volume of traffic received by the model over time.\\n\\n## Why is it being tracked?\\n\\n- Traffic is a basic high-level metric that informs us of the overall model\\'s usage.\\n\\n## What steps should I take when I see an outlier?\\n\\n- A dip or spike in traffic needs to be investigated. For example, a dip could be due to a production model server going down; a spike could be an adversarial attack.\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"ML Framework Examples\"\\nslug: \"ml-framework-examples\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:13:20.434Z\"\\nupdatedAt: \"2022-04-19T20:13:20.434Z\"\\n---\\n',\n",
       " 'slug: \"client-setup\" _TOKEN\\n)\\n```\\n```python Connect the Client with self-signed certs\\nimport fiddler as fdl\\n\\nURL = \\'https://app.fiddler.ai\\'\\nORG_ID = \\'my_org\\'\\nAUTH_TOKEN = \\'p9uqlkKz1zAA3KAU8kiB6zJkXiQoqFgkUgEa1sv4u58\\'\\n\\nclient = fdl.FiddlerApi(\\n    url=URL,\\n    org_id=ORG_ID,\\n    auth_token=AUTH_TOKEN, \\n\\t\\tverify=False\\n)\\n```\\n```Text Connect the Client with Proxies\\nproxies = {\\n    \\'http\\' : \\'http://proxy.example.com:1234\\',\\n    \\'https\\': \\'https://proxy.example.com:5678\\'\\n}\\n\\nclient = fdl.FiddlerApi(\\n    url=URL,\\n    org_id=ORG_ID,\\n    auth_token=AUTH_TOKEN, \\n\\t\\tproxies=proxies\\n)\\n```\\n\\nIf you want to authenticate with Fiddler without passing this information directly into the function call, you can store it in a file named_ fiddler.ini_, which should be stored in the same directory as your notebook or script.\\n\\n```python Writing fiddler.ini\\n%%writefile fiddler.ini\\n\\n[FIDDLER]\\nurl = https://app.fiddler.ai\\norg_id = my_org\\nauth_token = p9uqlkKz1zAA3KAU8kiB6zJkXiQoqFgkUgEa1sv4u58\\n```\\n\\n\\n\\n```python Connecting the Client with a fiddler.ini file\\nclient = fdl.FiddlerApi()\\n```',\n",
       " '---\\ntitle: \"On-prem Technical Requirements\"\\nslug: \"technical-requirements\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:20:05.290Z\"\\nupdatedAt: \"2023-05-18T15:43:28.706Z\"\\n---\\n## Minimum System Requirements\\n\\nFiddler is horizontally scalable to support the throughput requirements for enormous production use-cases. The minimum system requirements below correspond to approximately 20 million inference events monitored per day (~230 EPS) for models with around 100 features, with 90 day retention.\\n\\n- **Deployment**: Kubernetes namespace in AWS, Azure or GCP\\n- **Compute**: A minimum of 96 vCPU cores\\n- **Memory**: 384Gi\\n- **Persistent volumes**: 500 Gi storage across 10 volumes \\n  - POSIX-compliant block storage\\n  - 125 MB/s recommended\\n  - 3,000 IOPS recommended\\n- **Container Registry**: Quay.io or similar\\n- **Ingress Controller**: Ingress-nginx or AWS/GCP/Azure Load Balancer Controller\\n- **DNS**: FQDN that resolves to an L4 or L7 load balancer/proxy that provides TLS termination\\n\\n## Kubernetes Cluster Requirements\\n\\nAs stated above, Fiddler requires a Kubernetes cluster to install into.  The following outlines the requirements for this K8 cluster:\\n\\n- **Node Groups**:  2 node groups -  1 for core Fiddler services, 1 for Clickhouse (Fiddler\\'s event database)\\n- **Resources**:\\n  - Fiddler :  48 vCPUs, 192 Gi\\n  - Clickhouse :  64 vCPUs, 256 Gi [tagged & tainted]\\n- **Persistent Volumes**: 500 GB (minimum) /  1 TB (recommended)\\n- **Instance Sizes**\\n\\n  | Instance Size | AWS    | Azure      | GCP        |\\n  | :------------ | :----- | :--------- | :--------- |\\n  | Minimum       | m5.4xl | Std_D16_v3 | c2d_std_16 |\\n  | Recommended   | m5.8xl | Std_D32_v3 | c2d_std_32 |',\n",
       " '---\\ntitle: \"Explainability\"\\nslug: \"explainability\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:24:31.709Z\"\\nupdatedAt: \"2023-02-14T01:19:19.993Z\"\\n---\\nThere are three topics related to Explainability to cover:\\n\\n- [Point Explainability](doc:point-explainability) \\n- [Global Explainability](doc:global-explainability) \\n- [Surrogate Models](doc:surrogate-models)\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Analytics and Evaluation\"\\nslug: \"analytics-eval-platform\"\\nexcerpt: \"Platform Guide\"\\nhidden: false\\ncreatedAt: \"2023-02-01T21:50:06.052Z\"\\nupdatedAt: \"2023-02-14T01:18:11.015Z\"\\n---\\n## Analytics\\n\\nFiddler’s industry-first model analytics tool, called Slice and Explain, allows you to perform an exploratory or targeted analysis of model behavior.\\n\\n1. **_Slice_** — Identify a selection, or slice, of data. Or, you can start with the entire dataset for global analysis.\\n2. **_Explain_** — Analyze model behavior on that slice using Fiddler’s visual explanations and other data insights.\\n\\nSlice and Explain is designed to help data scientists, model validators, and analysts drill down into a model and dataset to see global, local, or instance-level explanations for the model’s predictions.\\n\\nSlice and Explain can help you answer questions like:\\n\\n- What are the key drivers of my model output in a subsection of the data?\\n- How are the model inputs correlated to other inputs and to the output?\\n- Where is my model underperforming?\\n- How is my model performing across the classes in a protected group?\\n\\nAccess Slice and Explain from the Analyze tab for your model. Slice and Explain currently support all tabular models.\\n\\n**For details on how to use Fiddler Analytics through our interface check the [Analytics Page](doc:analytics-ui) on our UI Guide**\\n\\n## Evaluation\\n\\nModel performance evaluation is one of the key tasks in the ML model lifecycle. A model\\'s performance indicates how successful the model is at making useful predictions on data.\\n\\n**For details on how to use Fiddler Evaluation through our interface check the [Evaluation Page](doc:evaluation-ui) on our UI Guide**\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"simple-nlp-monitoring-quick-start\"  these can be found by pointing your browser to your Fiddler URL and navigating to the **Settings** page.\\n\\n\\n```python\\nURL = \\'\\'  # Make sure to include the full URL (including https://).\\nORG_ID = \\'\\'\\nAUTH_TOKEN = \\'\\'\\n```\\n\\nNext we run the following code block to connect to the Fiddler API.\\n\\n\\n```python\\nclient = fdl.FiddlerApi(\\n    url=URL,\\n    org_id=ORG_ID,\\n    auth_token=AUTH_TOKEN,\\n)\\n```\\n\\nOnce you connect, you can create a new project by specifying a unique project ID in the client\\'s `create_project` function.\\n\\n\\n```python\\nPROJECT_ID = \\'nlp_newsgroups\\'\\n\\nif not PROJECT_ID in client.list_projects():\\n    print(f\\'Creating project: {PROJECT_ID}\\')\\n    client.create_project(PROJECT_ID)\\nelse:\\n    print(f\\'Project: {PROJECT_ID} already exists\\')\\n```\\n\\n# 2. Upload the Assets and Vectorize Text Data\\n\\nNow we retrieve the 20Newsgroup dataset. This dataset is fetched from the [scikit-learn real-world datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#) and pre-processed using [this notebook](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/pre-proccessing/20newsgroups_prep_vectorization.ipynb).\\n\\n\\n```python\\nDATA_PATH = \\'https://media.githubusercontent.com/media/fiddler-labs/fiddler-examples/main/quickstart/data/\\'\\nsource_df = pd.read_csv(DATA_PATH + \\'20newsgroups_preprocessed.csv\\')\\nsource_df\\n```\\n\\n# Vectorization via Text Embeddings\\n\\n## OpenAI Embeddings\\n\\nIf you have an openai account, you can use your openai key and run the following cells to get the embedding directly from openai.\\nHowever, for this example dataset we upload the openai embeddings which are previously fetched and stored by Fiddler.\\n\\n\\n```python\\n\\'\\'\\'\\nimport openai\\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\") #Use your own OpenAI key\\nMODEL = \"text-embedding-ada-002\"\\n\\'\\'\\'\\n```\\n\\n\\n```python\\n\\'\\'\\'\\ndef get_openai_embedding_batch(df, text_col_name, batch_size, model=MODEL):\\n    if batch_size>2000:\\n        raise ValueError(\\'openai currently does not support chunks larger than 2000\\')\\n    embeddings = []\\n    for i in range(0, df.shape[0], batch_size):\\n        batch_df = df.iloc[i:i+batch_size] if i+batch_size<df.shape[0] else df.iloc[i:]\\n        response = openai.Embedding.create(\\n            input=batch_df[text_col_name].tolist(),\\n            model=model\\n        )\\n        response_embedding_list = [res[\\'embedding\\'] for res in response[\\'data\\']]\\n        embeddings += response_embedding_list\\n    embedding_col_names = [\\'openai_dim{}\\'.format(i+1) for i in range(len(embeddings[0]))]\\n    return pd.DataFrame(embeddings, columns=embedding_col_names)\\n\\'\\'\\'\\n```\\n\\n\\n```python\\n\\'\\'\\'\\nbatch_size = 2000\\ntext_col_name = \\'original_text\\'\\nopenai_df = get_openai_embedding_batch(source_df, text_col_name, batch_size, model=\"text-embedding-ada-002\")\\nopenai_column_names = openai_df.columns.tolist()\\n\\'\\'\\'\\n```\\n\\n\\n```python\\nopenai_df = pd.read_csv(DATA_PATH + \\'20newsgroups_openai_embeddings.csv\\')\\nopenai_column_names = openai_df.columns.tolist()\\n```\\n\\n## TF-IDF Embeddings\\n\\nAs')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s,r,emb = strings_ranked_by_relatedness(\"is my data safe?\", df)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b20e53-04f5-48e5-82fe-a47d8ccf7dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fef2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message, message, query_embed = ask(\"Why should I use Fiddler for monitoring my ML models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d67b0-922e-48dc-8818-6346d310aad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61dc3bd-85c4-4662-bd7c-fbac1c996772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6db109-d8b0-4907-b184-9cdb82d85d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee53c3-9efd-4b6b-a541-1111b5dce8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9bb8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876e117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ec442126",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message, message, query_embed = ask(\"Why should I use Fiddler for monitoring my ML models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "32a77501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fiddler is the pioneer in enterprise Model Performance Management (MPM), offering a unified platform that enables Data Science, MLOps, Risk, Compliance, Analytics, and other LOB teams to monitor, explain, analyze, and improve ML deployments at enterprise scale. Fiddler allows you to obtain contextual insights at any stage of the ML lifecycle, improve predictions, increase transparency and fairness, and optimize business revenue. It provides features such as monitoring model performance, detecting data drift, explaining model behavior, analyzing model fairness, and more. By using Fiddler, you can ensure that your ML models are performing as expected and make informed decisions to improve their performance.\\n\\nReference URL: [https://docs.fiddler.ai/docs/](https://docs.fiddler.ai/docs/)'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f13420d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message2, message2, query_embed2 = ask(\"How can I monitor fraud detection models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f1fdce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a397021c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To monitor fraud detection models using Fiddler's AI Observability platform, you can utilize the following tools and features:\n",
      "\n",
      "1. **Drift Detection**: Fiddler allows you to monitor drift in your fraud detection models. This includes handling class-imbalanced data, calculating feature impact, measuring feature drift, and determining prediction drift impact. You can find more information on these topics in the [Class-Imbalanced Data](https://docs.fiddler.ai/v1.3/docs/class-imbalanced-data) and [Data Drift](doc:data-drift-platform) documentation.\n",
      "\n",
      "2. **Performance Metrics**: Fiddler provides performance metrics specifically tailored for fraud detection models. These metrics include recall (detection of non-fraudulent cases as fraud) and false positive rate (non-fraud cases labeled as fraud). Monitoring these metrics helps you assess the effectiveness of your model. \n",
      "\n",
      "3. **Data Integrity**: Fiddler allows you to monitor data integrity issues in your fraud detection models. This includes checking for range violations, missing value violations, and type violations in your production data. These metrics help you identify potential issues with the quality of your data. \n",
      "\n",
      "To learn more about monitoring fraud detection models using Fiddler, you can refer to the [Fraud Detection](https://docs.fiddler.ai/docs/fraud-detection) documentation.\n",
      "\n",
      "Reference URL: [https://docs.fiddler.ai/docs/fraud-detection](https://docs.fiddler.ai/docs/fraud-detection)\n"
     ]
    }
   ],
   "source": [
    "print(response_message2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b0d0d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message2_2, message2_2, query_embed2_2 = ask2(\"How can I monitor fraud detection models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f5cfb2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To monitor fraud detection models using Fiddler's AI Observability platform, you can use the following tools and features:\n",
      "\n",
      "1. **Drift Detection**: Fiddler provides various metrics to detect drift in your fraud detection model, including class-imbalanced data, feature impact, feature drift, and prediction drift impact. You can find more information on these metrics in the [Class-Imbalanced Data](https://docs.fiddler.ai/docs/class-imbalanced-data) and [Data Drift](https://docs.fiddler.ai/docs/data-drift-platform) documentation.\n",
      "\n",
      "2. **Performance Metrics**: Accuracy may not be the best measure of model performance for fraud detection. Fiddler recommends monitoring metrics like recall (detection of non-fraudulent cases as fraud) and false positive rate (non-fraud cases labeled as fraud). These metrics can be monitored in the Fiddler AI Observability platform. \n",
      "\n",
      "3. **Data Integrity**: Fiddler allows you to monitor data integrity issues in your fraud detection model. This includes checking for range violations, missing value violations, and type violations in your production data. You can find more information on these metrics in the [Data Integrity](https://docs.fiddler.ai/docs/data-drift-platform) documentation.\n",
      "\n",
      "For more details on how to monitor fraud detection models using Fiddler, you can refer to the [Fraud Detection](https://docs.fiddler.ai/docs/fraud-detection) documentation.\n",
      "\n",
      "Reference URL: [https://docs.fiddler.ai/docs/fraud-detection](https://docs.fiddler.ai/docs/fraud-detection)\n"
     ]
    }
   ],
   "source": [
    "print(response_message2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fbd9fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message3, message3, query_embed3 = ask2(\"How can I upload events to Fiddler?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5c35d3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To upload events to Fiddler, you can use the Fiddler Client's `publish_event` API. This API allows you to send traffic from your live deployed model to Fiddler in real-time. You can include inputs, outputs, target, decisions (categorical only), and metadata in the event.\n",
      "\n",
      "Here is an example of how to use the `publish_event` API:\n",
      "\n",
      "```python\n",
      "client.publish_event(\n",
      "    project_id=PROJECT_ID,\n",
      "    model_id=MODEL_ID,\n",
      "    event=event_data,\n",
      "    event_id='event_001',\n",
      "    event_timestamp=1637344470000\n",
      ")\n",
      "```\n",
      "\n",
      "You need to replace `PROJECT_ID` and `MODEL_ID` with the actual IDs of your project and model. `event_data` should contain the data for the event you want to upload. The `event_id` is a unique identifier for the event, and `event_timestamp` is the timestamp for the event.\n",
      "\n",
      "Please note that currently there isn't a way for users to directly delete events. If you need to delete events, please contact Fiddler personnel for assistance.\n",
      "\n",
      "Reference: [Publishing Events](https://docs.fiddler.ai/docs/databricks-integration#publishing-events)\n"
     ]
    }
   ],
   "source": [
    "print(response_message3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9790eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "805b076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message4, message4, query_embed4 = ask2(\"What API to use to delete a project in Fiddler?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "89f42ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To delete a project in Fiddler, you can use the `delete_project` API. This API allows you to delete a specific project by providing the project ID as a parameter. Here is an example of how to use the API:\n",
      "\n",
      "```python\n",
      "import fiddler as fdl\n",
      "\n",
      "# Set up the Fiddler API client\n",
      "client = fdl.FiddlerApi(url=\"https://api.fiddler.ai\", org_id=\"your_org_id\", auth_token=\"your_auth_token\")\n",
      "\n",
      "# Delete the project\n",
      "client.delete_project(project_id=\"your_project_id\")\n",
      "```\n",
      "\n",
      "Please note that deleting a project will permanently remove all associated datasets, models, and other project-related data. Make sure to use this API with caution as it cannot be undone.\n"
     ]
    }
   ],
   "source": [
    "print(response_message4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "26539e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...</td>\n",
       "      <td>[-0.020456863567233086, -0.0040573012083768845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>---\\ntitle: \"Customer Churn Prediction\"\\nslug:...</td>\n",
       "      <td>[-0.006977782119065523, -0.001356987631879747,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slug: \"customer-churn-prediction\" analyze-rca-...</td>\n",
       "      <td>[-0.018147623166441917, -0.0034140129573643208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>---\\ntitle: \"Fraud Detection\"\\nslug: \"fraud-de...</td>\n",
       "      <td>[-0.007915230467915535, -0.007179588079452515,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slug: \"fraud-detection\" 3.png\",\\n        \"RCA3...</td>\n",
       "      <td>[-0.01609727181494236, -0.0010202372213825583,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>---\\ntitle: \"Uploading a scikit-learn Model Ar...</td>\n",
       "      <td>[0.0031662925612181425, 0.018320860341191292, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...</td>\n",
       "      <td>[-0.0003327192389406264, -0.021783549338579178...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Once you have added a model on the Fiddler pla...</td>\n",
       "      <td>[-0.022226542234420776, 0.011369146406650543, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Custom metrics is an upcoming feature and it i...</td>\n",
       "      <td>[-0.017716489732265472, -0.0035160724073648453...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Re-uploading in Fiddler essentially means havi...</td>\n",
       "      <td>[-0.01760503277182579, 0.011651406064629555, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    ---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...   \n",
       "1    ---\\ntitle: \"Customer Churn Prediction\"\\nslug:...   \n",
       "2    slug: \"customer-churn-prediction\" analyze-rca-...   \n",
       "3    ---\\ntitle: \"Fraud Detection\"\\nslug: \"fraud-de...   \n",
       "4    slug: \"fraud-detection\" 3.png\",\\n        \"RCA3...   \n",
       "..                                                 ...   \n",
       "200  ---\\ntitle: \"Uploading a scikit-learn Model Ar...   \n",
       "201  ---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...   \n",
       "202  Once you have added a model on the Fiddler pla...   \n",
       "203  Custom metrics is an upcoming feature and it i...   \n",
       "204  Re-uploading in Fiddler essentially means havi...   \n",
       "\n",
       "                                             embedding  \n",
       "0    [-0.020456863567233086, -0.0040573012083768845...  \n",
       "1    [-0.006977782119065523, -0.001356987631879747,...  \n",
       "2    [-0.018147623166441917, -0.0034140129573643208...  \n",
       "3    [-0.007915230467915535, -0.007179588079452515,...  \n",
       "4    [-0.01609727181494236, -0.0010202372213825583,...  \n",
       "..                                                 ...  \n",
       "200  [0.0031662925612181425, 0.018320860341191292, ...  \n",
       "201  [-0.0003327192389406264, -0.021783549338579178...  \n",
       "202  [-0.022226542234420776, 0.011369146406650543, ...  \n",
       "203  [-0.017716489732265472, -0.0035160724073648453...  \n",
       "204  [-0.01760503277182579, 0.011651406064629555, 0...  \n",
       "\n",
       "[205 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "17aaa964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a tool called Fiddler Chatbot and your purpose is to use the below documentation from the company Fiddler to answer the subsequent documentation questions. Also, if possible, give me the reference URLs according to the following instructions. The way to create the URLs is: add \"https://docs.fiddler.ai/docs/\" before the \"slug\" value of the document. For any URL references that start with \"doc:\" or \"ref:\" use its value to create a URL by adding \"https://docs.fiddler.ai/docs/\" before that value. Do not use page titles to create urls. Note that if a user asks about uploading events, it means the same as publishing events. If the answer cannot be found in the documentation, write \"I could not find an answer.\"Re-uploading in Fiddler essentially means having to delete what was uploaded or ingested earlier, making the updates you want to make, and then following the same steps as before for the specific thing you are looking to upload. So for example, if you want make changes to the baseline dataset you uploaded, you will have to delete the dataset and then make modifications for the datasetinfo object or the dataset itself and then upload the dataset again. As for events, currently there isn\\'t a way for the user to directly delete events. Please contact Fiddler personnell for the same. ---\\ntitle: \"Project Structure on UI\"\\nslug: \"project-structure\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:26:33.568Z\"\\nupdatedAt: \"2023-02-03T19:45:51.093Z\"\\n---\\nSupervised machine learning involves identifying a predictive task, finding data to enable that task, and building a model using that data. Fiddler captures this workflow with project, dataset, and model entities.\\n\\n## Projects\\n\\nA project represents a machine learning task (e.g. predicting house prices, assessing creditworthiness, or detecting fraud).\\n\\nA project can contain one or more models for the ML task (e.g. LinearRegression-HousePredict, RandomForest-HousePredict).\\n\\nCreate a project by clicking on **Projects** and then clicking on **Add Project**.\\n\\n![](https://files.readme.io/8e4b429-Add_project_0710.png \"Add_project_0710.png\")\\n\\n- **_Create New Project_** — A window will pop up where you can enter the project name and click **Create**. Once the project is created, it will be displayed on the projects page.\\n\\nYou can access your projects from the Projects Page.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/82404e6-Screenshot_2022-12-27_at_1.00.15_PM.png\",\\n        null,\\n        \"Projects Page on Fiddler UI\"\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Projects Page on Fiddler UI\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n## Datasets\\n\\nA dataset in Fiddler is a data table containing features, model outputs, and a target for machine learning models. Optionally, you can also upload metadata and “decision” columns, which can be used to segment the dataset for analyses, track business decisions, and work as protected attributes in bias-related workflows. For more details refer to [Datasets](doc:project-architecture#datasets) in the Platform Guide.\\n\\nOnce you click on a particular project, you will be able to see if there are any datasets associated with the project. For example, the bank_churn project, in the following screenshot, has the bank_churn dataset. [Datasets are uploaded via the Fiddler client](ref:clientupload_dataset). \\n\\n![](https://files.readme.io/3fa7700-Screenshot_2022-12-27_at_1.05.05_PM.png)\\n\\n## Models\\n\\nA model in Fiddler represents a machine learning model. A project will have one or more models for the ML task (e.g. a project to predict house prices might contain LinearRegression-HousePredict and RandomForest-HousePredict). For further details refer to the [Models](doc:project-architecture#models) section in the Platform Guide.\\n\\n![](https://files.readme.io/e151df5-Model_Dashboard.png \"Model_Dashboard.png\")\\n\\n### Model Artifacts\\n\\nAt its most basic level, a model in Fiddler is simply a directory that contains [model artifacts](doc:artifacts-and-surrogates) such as:\\n\\n- The model file (e.g. `*.pkl`)\\n- `package.py`: A wrapper script containing all of the code needed to standardize the execution of the model.\\n\\n![](https://files.readme.io/7170489-Model_Details.png \"Model_Details.png\")\\n\\n![](https://files.readme.io/2b3d52e-Model_Details_1.png \"Model_Details_1.png\")\\n\\n## Project Dashboard\\n\\nYou can collate specific visualizations under the Project Dashboard. After visualizations are created using the Model Analytics tool, you can pin them to the dashboard, which can then be shared with others.\\n\\n![](https://files.readme.io/b7cb9ce-Chart_Dashboard.png \"Chart_Dashboard.png\")\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_---\\ntitle: \"Project Architecture\"\\nslug: \"project-architecture\"\\nhidden: false\\ncreatedAt: \"2022-11-15T18:06:28.079Z\"\\nupdatedAt: \"2023-02-14T23:21:13.699Z\"\\n---\\nSupervised machine learning involves identifying a predictive task, finding data to enable that task, and building a model using that data. \\n\\nFiddler captures this workflow with **project**, **dataset**, and **model** entities.\\n\\n## Project\\n\\nIn Fiddler, a project is essentially a parent folder that hosts one or more **model** (s) for the ML task (e.g. A Project HousePredict for predicting house prices will LinearRegression-HousePredict, RandomForest-HousePredict).\\n\\n## Models\\n\\nA model in Fiddler represents a **placeholder** for a machine-learning model. It\\'s a placeholder because we may not need the **[model artifacts](doc:artifacts-and-surrogates#Model-Artifacts)**. Instead, we may just need adequate [information about the model](ref:fdlmodelinfo) in order to monitor model-specific data. \\n\\n> 📘 Info\\n> \\n> You can [upload your model artifacts](https://dash.readme.com/project/fiddler/v1.6/docs/uploading-model-artifacts) to Fiddler to unlock high-fidelity explainability for your model. However, it is not required. If you do not wish to upload your artifact but want to explore explainability with Fiddler, we can build a [**surrogate model**](doc:artifacts-and-surrogates#surrogate-model) on the backend to be used in place of your artifact.\\n\\n## Datasets\\n\\nA dataset in Fiddler is a data table containing [information about data](ref:fdldatasetinfo) such as **features**, **model outputs**, and a **target** for machine learning models. Optionally, you can also upload **metadata** and “**decision**” columns, which can be used to segment the dataset for analyses, track business decisions, and work as protected attributes in bias-related workflows. \\n\\nIn order to monitor **production data**, a [dataset must be uploaded](ref:clientupload_dataset) to be used as a **baseline** for making comparisons. This baseline dataset should be sampled from your model\\'s **training data**. The sample should be unbiased and should faithfully capture moments of the parent distribution. Further, values appearing in the baseline dataset\\'s columns should be representative of their entire ranges within the complete training dataset.\\n\\n**Datasets are used by Fiddler in the following ways:**\\n\\n1. As a reference for [drift calculations](doc:data-drift-platform) and [data integrity violations ](doc:data-integrity-platform)on the **[Monitor](doc:monitoring-ui)** page\\n2. To train a model to be used as a [surrogate](doc:artifacts-and-surrogates#surrogate-model) when using [`add_model_surrogate`](/reference/clientadd_model_surrogate)\\n3. For computing model performance metrics globally on the **[Evaluate](doc:evaluation-ui)** page, or on slices on the **[Analyze](doc:analytics-ui)** page\\n4. As a reference for explainability algorithms (e.g. partial dependence plots, permutation feature impact, approximate Shapley values, and ICE plots).\\n\\nBased on the above uses, _datasets with sizes much in excess of 10K rows are often unnecessary_ and can lead to excessive upload, precomputation, and query times. That being said, here are some situations where larger datasets may be desirable:\\n\\n- **Auto-modeling for tasks with significant class imbalance; or strong and complex feature interactions, possibly with deeply encoded semantics**\\n  - However, in use cases like these, most users opt to upload carefully-engineered model artifacts tailored to the specific application.\\n- **Deep segmentation analysis**\\n  - If it’s desirable to perform model analyses on very specific subpopulations (e.g. “55-year-old Canadian home-owners who have been customers between 18 and 24 months”), large datasets may be necessary to have sufficient reference representation to drive model analytics.\\n\\n> 📘 Info\\n> \\n> Datasets can be uploaded to Fiddler using the[ Python API client](doc:installation-and-setup).\\n\\n [Check the UI Guide to Visualize Project Architecture on our User Interface](doc:project-structure)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_---\\ntitle: \"Publishing Production Data\"\\nslug: \"publishing-production-data\"\\nhidden: false\\ncreatedAt: \"2022-11-18T23:28:25.348Z\"\\nupdatedAt: \"2022-12-19T19:14:28.171Z\"\\n---\\nThis Section guides you on the various ways you can provide event data to Fiddler and update and retrieve them.---\\ntitle: \"Databricks Integration\"\\nslug: \"databricks-integration\"\\nhidden: false\\ncreatedAt: \"2023-02-02T20:38:54.971Z\"\\nupdatedAt: \"2023-08-14T17:14:26.297Z\"\\n---\\nFiddler allows your team to monitor, explain and analyze your models developed and deployed in [Databricks Workspace](https://docs.databricks.com/introduction/index.html) by integrating with [MLFlow](https://docs.databricks.com/mlflow/index.html) for model asset management and utilizing Databricks Spark environment for data management. \\n\\nTo validate and monitor models built on Databricks using Fiddler, you can follow these steps:\\n\\n1. [Creating a Fiddler Project](doc:databricks-integration#creating-a-fiddler-project)\\n2. [Uploading a Baseline Dataset](doc:databricks-integration#uploading-a-baseline-dataset)\\n3. [Adding Model Information ](doc:databricks-integration#adding-model-information)\\n4. [Uploading Model Files (for Explainability)](doc:databricks-integration#uploading-model-files)\\n5. [Publishing Events](doc:databricks-integration#publishing-events)\\n   1. Batch Models \\n   2. Live Models \\n\\n## Creating a Fiddler Project\\n\\nLaunch a [Databricks notebook](https://docs.databricks.com/notebooks/index.html) from your workspace and run the following code:\\n\\n```python\\n!pip install -q fiddler-client\\nimport fiddler as fdl\\n```\\n\\nNow that you have the Fiddler library installed, you can connect to your Fiddler environment. Please use the [UI administration guide](doc:administration-ui) to help you find your Fiddler credentials.\\n\\n```python\\nURL = \"\"\\nORG_ID = \"\"\\nAUTH_TOKEN = \"\"\\nclient = fdl.FiddlerApi(url=URL, org_id=ORG_ID, auth_token=AUTH_TOKEN)\\n```\\n\\nFinally, you can set up a new project using:\\n\\n```python\\nclient.create_project(\"YOUR_PROJECT_NAME\")\\n```\\n\\n## Uploading a Baseline Dataset\\n\\nYou can grab your baseline dataset from a[ delta table](https://docs.databricks.com/getting-started/dataframes-python.html) and share it with Fiddler as a baseline dataset:\\n\\n```python\\nbaseline_dataset = spark.read.table(\"YOUR_DATASET\").select(\"*\").toPandas()\\n\\ndataset_info = fdl.DatasetInfo.from_dataframe(baseline_upload, max_inferred_cardinality=100)\\n  \\nclient.upload_dataset(\\n  project_id=PROJECT_ID,\\n  dataset_id=DATASET_ID,\\n  dataset={\\'baseline\\': baseline_upload},\\n  info=dataset_info)\\n```\\n\\n## Adding Model Information\\n\\nUsing the **[MLFlow API](https://docs.databricks.com/reference/mlflow-api.html) ** you can query the model registry and get the **model signature** which describes the inputs and outputs as a dictionary. You can use this dictionary to build out the [ModelInfo](ref:fdlmodelinfo) object required to the model to Fiddler:\\n\\n```python Python\\nmport mlflow \\n\\nmodel_uri = MlflowClient.get_model_version_download_uri(model_name, model_version) #spevify the model name and model version you want to share wth Fiddler\\n\\nmodel_info = mlflow.models.ModelSignature.to_dict(model_uri)  #MLFlow_Params_Object with model I/O info\\n```\\n\\nNow you can share the model signature with Fiddler as part of the Fiddler ModelInfo object :\\n\\n```python\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n\\tdataset_info = client.get_dataset_info(YOUR_PROJECT,YOUR_DATASET),\\n\\ttarget =  \"TARGET COLUMN\",\\n\\t#optionalArguments\\n\\tmlflow_params = fdl.MLFlowParams(mlflow.models.ModelSignature.to_dict())  \\n)\\n```\\n\\n## Uploading Model Files\\n\\nSharing your [model artifacts](https://docs.fiddler.ai/docs/uploading-model-artifacts) helps Fiddler explain your models. By leveraging the MLFlow API you can download these model files:\\n\\n```python\\nimport os  \\nimport mlflow  \\nfrom mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\\n\\nmodel_name = \"example-model-name\"  \\nmodel_stage = \"Staging\"  # Should be either \\'Staging\\' or \\'Production\\'\\n\\nmlflow.set_tracking_uri(\"databricks\")  \\nos.makedirs(\"model\", exist_ok=True)  \\nlocal_path = ModelsArtifactRepository(\\n  f\\'models:/{model_name}/{model_stage}\\').download_artifacts(\"\", dst_path=\"model\")  \\n\\nprint(f\\'{model_stage} Model {model_name} is downloaded at {local_path}\\')  \\n```\\n\\nOnce you have the model file, you can create a [package.py](doc:binary-classification-1) file in this model directory that describes how to access this model.\\n\\nFinally, you can upload all the model artifacts to Fiddler:\\n\\n```python\\nclient.add_model_artifact(  \\n    project_id=PROJECT_ID,\\n    model_id=MODEL_ID,\\n    model_dir=\\'model/\\',\\n)\\n```\\n\\nAlternatively, you can skip uploading your model and use Fiddler to generate a [surrogate model](doc:surrogate-models-client-guide) to get low-fidelity explanations for your model.\\n\\n## Publishing Events\\n\\nNow you can publish all the events from your models. You can do this in two ways:\\n\\n### Batch Models\\n\\nIf your models run batch processes with your models or your aggregate model outputs over a timeframe, then you can use the table change feed from Databricks to select only the new events and send them to Fiddler:\\n\\n```python Python\\nchanges_df = spark.read.format(\"delta\") \\\\\\n.option(\"readChangeFeed\", \"true\") \\\\\\n.option(\"startingVersion\",last_version) \\\\\\n.option(\"endingVersion\", new_version) \\\\\\n.table(\"inferences\").toPandas()\\n\\n\\nclient.publish_events_batch(\\n   project_id=PROJECT_ID,\\n   model_id=MODEL_ID,\\n   batch_source=changes_df,\\n   timestamp_field=\\'timestamp\\')\\n\\n```\\n\\n### Live Models\\n\\nFor models with live predictions or real-time applications, you can add the following code snippet to your prediction pipeline and send every event to Fiddler in real-time: \\n\\n```python Python\\nexample_event = model_output.toPandas() #turn your model\\'s ouput in a pandas datafram \\n\\nclient.publish_event(\\n    project_id=PROJECT_ID,\\n    model_id=MODEL_ID,\\n    event=example_event,\\n    event_id=\\'event_001\\',\\n    event_timestamp=1637344470000)\\n```\\n\\n_Support for Inference tables and hosted endpoints is coming soon!_\\n\\nQuestion: What API to use to delete a project in Fiddler?'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "41413c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message5, message5, query_embed5 = ask2(\"How to delete a project in Fiddler?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ca562416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a tool called Fiddler Chatbot and your purpose is to use the below documentation from the company Fiddler to answer the subsequent documentation questions. Also, if possible, give me the reference URLs according to the following instructions. The way to create the URLs is: add \"https://docs.fiddler.ai/docs/\" before the \"slug\" value of the document. For any URL references that start with \"doc:\" or \"ref:\" use its value to create a URL by adding \"https://docs.fiddler.ai/docs/\" before that value. Do not use page titles to create urls. Note that if a user asks about uploading events, it means the same as publishing events. If the answer cannot be found in the documentation, write \"I could not find an answer.\"Re-uploading in Fiddler essentially means having to delete what was uploaded or ingested earlier, making the updates you want to make, and then following the same steps as before for the specific thing you are looking to upload. So for example, if you want make changes to the baseline dataset you uploaded, you will have to delete the dataset and then make modifications for the datasetinfo object or the dataset itself and then upload the dataset again. As for events, currently there isn\\'t a way for the user to directly delete events. Please contact Fiddler personnell for the same. ---\\ntitle: \"Project Structure on UI\"\\nslug: \"project-structure\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:26:33.568Z\"\\nupdatedAt: \"2023-02-03T19:45:51.093Z\"\\n---\\nSupervised machine learning involves identifying a predictive task, finding data to enable that task, and building a model using that data. Fiddler captures this workflow with project, dataset, and model entities.\\n\\n## Projects\\n\\nA project represents a machine learning task (e.g. predicting house prices, assessing creditworthiness, or detecting fraud).\\n\\nA project can contain one or more models for the ML task (e.g. LinearRegression-HousePredict, RandomForest-HousePredict).\\n\\nCreate a project by clicking on **Projects** and then clicking on **Add Project**.\\n\\n![](https://files.readme.io/8e4b429-Add_project_0710.png \"Add_project_0710.png\")\\n\\n- **_Create New Project_** — A window will pop up where you can enter the project name and click **Create**. Once the project is created, it will be displayed on the projects page.\\n\\nYou can access your projects from the Projects Page.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/82404e6-Screenshot_2022-12-27_at_1.00.15_PM.png\",\\n        null,\\n        \"Projects Page on Fiddler UI\"\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Projects Page on Fiddler UI\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n## Datasets\\n\\nA dataset in Fiddler is a data table containing features, model outputs, and a target for machine learning models. Optionally, you can also upload metadata and “decision” columns, which can be used to segment the dataset for analyses, track business decisions, and work as protected attributes in bias-related workflows. For more details refer to [Datasets](doc:project-architecture#datasets) in the Platform Guide.\\n\\nOnce you click on a particular project, you will be able to see if there are any datasets associated with the project. For example, the bank_churn project, in the following screenshot, has the bank_churn dataset. [Datasets are uploaded via the Fiddler client](ref:clientupload_dataset). \\n\\n![](https://files.readme.io/3fa7700-Screenshot_2022-12-27_at_1.05.05_PM.png)\\n\\n## Models\\n\\nA model in Fiddler represents a machine learning model. A project will have one or more models for the ML task (e.g. a project to predict house prices might contain LinearRegression-HousePredict and RandomForest-HousePredict). For further details refer to the [Models](doc:project-architecture#models) section in the Platform Guide.\\n\\n![](https://files.readme.io/e151df5-Model_Dashboard.png \"Model_Dashboard.png\")\\n\\n### Model Artifacts\\n\\nAt its most basic level, a model in Fiddler is simply a directory that contains [model artifacts](doc:artifacts-and-surrogates) such as:\\n\\n- The model file (e.g. `*.pkl`)\\n- `package.py`: A wrapper script containing all of the code needed to standardize the execution of the model.\\n\\n![](https://files.readme.io/7170489-Model_Details.png \"Model_Details.png\")\\n\\n![](https://files.readme.io/2b3d52e-Model_Details_1.png \"Model_Details_1.png\")\\n\\n## Project Dashboard\\n\\nYou can collate specific visualizations under the Project Dashboard. After visualizations are created using the Model Analytics tool, you can pin them to the dashboard, which can then be shared with others.\\n\\n![](https://files.readme.io/b7cb9ce-Chart_Dashboard.png \"Chart_Dashboard.png\")\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_Once you have added a model on the Fiddler platform using a specific model info object, that is fdl.ModelInfo, you cannot modify aspects such as features, inputs, outputs, model task etc. specified in the model info object. Currently, if you want to change fundamental details about a modelinfo object, then it is advised to create/add a new model with a new modelinfo object.---\\ntitle: \"Project Architecture\"\\nslug: \"project-architecture\"\\nhidden: false\\ncreatedAt: \"2022-11-15T18:06:28.079Z\"\\nupdatedAt: \"2023-02-14T23:21:13.699Z\"\\n---\\nSupervised machine learning involves identifying a predictive task, finding data to enable that task, and building a model using that data. \\n\\nFiddler captures this workflow with **project**, **dataset**, and **model** entities.\\n\\n## Project\\n\\nIn Fiddler, a project is essentially a parent folder that hosts one or more **model** (s) for the ML task (e.g. A Project HousePredict for predicting house prices will LinearRegression-HousePredict, RandomForest-HousePredict).\\n\\n## Models\\n\\nA model in Fiddler represents a **placeholder** for a machine-learning model. It\\'s a placeholder because we may not need the **[model artifacts](doc:artifacts-and-surrogates#Model-Artifacts)**. Instead, we may just need adequate [information about the model](ref:fdlmodelinfo) in order to monitor model-specific data. \\n\\n> 📘 Info\\n> \\n> You can [upload your model artifacts](https://dash.readme.com/project/fiddler/v1.6/docs/uploading-model-artifacts) to Fiddler to unlock high-fidelity explainability for your model. However, it is not required. If you do not wish to upload your artifact but want to explore explainability with Fiddler, we can build a [**surrogate model**](doc:artifacts-and-surrogates#surrogate-model) on the backend to be used in place of your artifact.\\n\\n## Datasets\\n\\nA dataset in Fiddler is a data table containing [information about data](ref:fdldatasetinfo) such as **features**, **model outputs**, and a **target** for machine learning models. Optionally, you can also upload **metadata** and “**decision**” columns, which can be used to segment the dataset for analyses, track business decisions, and work as protected attributes in bias-related workflows. \\n\\nIn order to monitor **production data**, a [dataset must be uploaded](ref:clientupload_dataset) to be used as a **baseline** for making comparisons. This baseline dataset should be sampled from your model\\'s **training data**. The sample should be unbiased and should faithfully capture moments of the parent distribution. Further, values appearing in the baseline dataset\\'s columns should be representative of their entire ranges within the complete training dataset.\\n\\n**Datasets are used by Fiddler in the following ways:**\\n\\n1. As a reference for [drift calculations](doc:data-drift-platform) and [data integrity violations ](doc:data-integrity-platform)on the **[Monitor](doc:monitoring-ui)** page\\n2. To train a model to be used as a [surrogate](doc:artifacts-and-surrogates#surrogate-model) when using [`add_model_surrogate`](/reference/clientadd_model_surrogate)\\n3. For computing model performance metrics globally on the **[Evaluate](doc:evaluation-ui)** page, or on slices on the **[Analyze](doc:analytics-ui)** page\\n4. As a reference for explainability algorithms (e.g. partial dependence plots, permutation feature impact, approximate Shapley values, and ICE plots).\\n\\nBased on the above uses, _datasets with sizes much in excess of 10K rows are often unnecessary_ and can lead to excessive upload, precomputation, and query times. That being said, here are some situations where larger datasets may be desirable:\\n\\n- **Auto-modeling for tasks with significant class imbalance; or strong and complex feature interactions, possibly with deeply encoded semantics**\\n  - However, in use cases like these, most users opt to upload carefully-engineered model artifacts tailored to the specific application.\\n- **Deep segmentation analysis**\\n  - If it’s desirable to perform model analyses on very specific subpopulations (e.g. “55-year-old Canadian home-owners who have been customers between 18 and 24 months”), large datasets may be necessary to have sufficient reference representation to drive model analytics.\\n\\n> 📘 Info\\n> \\n> Datasets can be uploaded to Fiddler using the[ Python API client](doc:installation-and-setup).\\n\\n [Check the UI Guide to Visualize Project Architecture on our User Interface](doc:project-structure)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_---\\ntitle: \"Publishing Production Data\"\\nslug: \"publishing-production-data\"\\nhidden: false\\ncreatedAt: \"2022-11-18T23:28:25.348Z\"\\nupdatedAt: \"2022-12-19T19:14:28.171Z\"\\n---\\nThis Section guides you on the various ways you can provide event data to Fiddler and update and retrieve them.---\\ntitle: \"System Architecture\"\\nslug: \"system-architecture\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:19:53.311Z\"\\nupdatedAt: \"2023-05-18T21:09:05.870Z\"\\n---\\nFiddler deploys into your private cloud\\'s existing Kubernetes clusters, for which the minimum system requirements can be found [here](doc:technical-requirements).  Fiddler supports deployment into Kubernetes in AWS, Azure, or GCP.  All services of the Fiddler platform are containerized in order to eliminate reliance on other cloud services and to reduce the deployment and maintenance friction of the platform.  This includes storage services like object storage and databases as well as system monitoring services like Grafana.  \\n\\nUpdates to the Fiddler containers is accomplished through a shared container registry (that Fiddler is provided access to).  Updates to the containers are orchestrated using Helm charts.\\n\\nA full-stack deployment of Fiddler is shown in the diagram below. \\n\\n![](https://files.readme.io/7cbfe31-reference_architecture.png)\\n\\nThe Fiddler system components are deployed within a single namespace on a Kubernetes cluster, using the official Fiddler Helm chart.\\n\\n- Fiddler core infrastructure relies on persistent volumes provided within the Kubernetes cluster. We recommend using encrypted storage volumes wherever possible.\\n- Fiddler may be configured to utilize external infrastructure in a self-hosted environment, such as existing PostgresQL servers, but this is not required as all services are containerized by default.\\n- Full-stack \"any-prem\" Fiddler includes observability infrastructure to monitor Fiddler system health and performance. These mainstream observability components may be integrated with external observability systems to support administration in a self-hosted environment.\\n- HTTP traffic to the Fiddler system is handled by an L4 or L7 load balancer or other proxy. TLS termination should usually occur outside the Fiddler system.\\n\\nOnce the platform is running, end users can interface with the Fiddler platform using their browser, the [Fiddler Python client](ref:about-the-fiddler-client), or Fiddler\\'s RESTful APIs.slug: \"ranking-model\" ```\\n\\n*Please allow 3-5 minutes for monitoring data to populate the charts.*\\n\\n--------\\n**Questions?**  \\n  \\nCheck out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\\n\\nJoin our [community Slack](http://fiddler-community.slack.com/) to ask any questions!\\n\\nIf you\\'re still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we\\'ll get back to you shortly.\\n---\\ntitle: \"Deploying Fiddler\"\\nslug: \"deploying-fiddler\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:19:47.579Z\"\\nupdatedAt: \"2023-02-14T01:21:23.264Z\"\\n---\\n## Deployment Overview\\n\\nFiddler runs on most mainstream flavors and configurations of Kubernetes, including OpenShift, Rancher, AWS Elastic Kubernetes Service, Azure Managed Kubernetes Service (AKS), GCP Google Kubernetes Engine, and more.\\n\\n- **Our premises**—Fiddler is offered as a fully managed service, deployed within an isolated network and dedicated hardware in the cloud.\\n\\n- **Your premises**—Deploy Fiddler into a Kubernetes cluster running in your own cloud account or data center. Please refer to the [On-prem Technical Requirements](doc:technical-requirements#system-requirements) section for more details.\\n\\n> 📘 Info\\n> \\n> Interested in a Fiddler Cloud or on-premises deployment?  Please contact [sales@fiddler.ai](mailto:sales@fiddler.ai).\\n\\n## Deploy on cloud\\n\\nFiddler cloud deployment uses a managed Kubernetes service to deploy, scale, and manage the application. We\\'ll handle the specifics! Please contact [sales@fiddler.ai](mailto:sales@fiddler.ai)\\n\\n## Deploy on-premise\\n\\n- [Technical Requirements](doc:technical-requirements) \\n- [Installation Guide](doc:installation-guide)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]\\n\\nQuestion: How to delete a project in Fiddler?'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "33e8b2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To delete a project in Fiddler, follow these steps:\n",
      "\n",
      "1. Go to the Fiddler UI and click on the \"Projects\" tab.\n",
      "2. Find the project you want to delete and click on it to open the project details.\n",
      "3. In the project details page, click on the three-dot menu icon in the top-right corner.\n",
      "4. From the dropdown menu, select \"Delete Project\".\n",
      "5. A confirmation dialog will appear asking you to confirm the deletion. Click \"Delete\" to proceed.\n",
      "\n",
      "Please note that deleting a project will permanently remove all associated models, datasets, and other project-related data. Make sure to double-check before deleting a project as this action cannot be undone.\n",
      "\n",
      "Reference URL: [https://docs.fiddler.ai/docs/project-structure#projects](https://docs.fiddler.ai/docs/project-structure#projects)\n"
     ]
    }
   ],
   "source": [
    "print(response_message5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "638a8048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...</td>\n",
       "      <td>[-0.020456863567233086, -0.0040573012083768845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>---\\ntitle: \"Customer Churn Prediction\"\\nslug:...</td>\n",
       "      <td>[-0.006977782119065523, -0.001356987631879747,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slug: \"customer-churn-prediction\" analyze-rca-...</td>\n",
       "      <td>[-0.018147623166441917, -0.0034140129573643208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>---\\ntitle: \"Fraud Detection\"\\nslug: \"fraud-de...</td>\n",
       "      <td>[-0.007915230467915535, -0.007179588079452515,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slug: \"fraud-detection\" 3.png\",\\n        \"RCA3...</td>\n",
       "      <td>[-0.01609727181494236, -0.0010202372213825583,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>---\\ntitle: \"Uploading a scikit-learn Model Ar...</td>\n",
       "      <td>[0.0031662925612181425, 0.018320860341191292, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...</td>\n",
       "      <td>[-0.0003327192389406264, -0.021783549338579178...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Once you have added a model on the Fiddler pla...</td>\n",
       "      <td>[-0.022226542234420776, 0.011369146406650543, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Custom metrics is an upcoming feature and it i...</td>\n",
       "      <td>[-0.017716489732265472, -0.0035160724073648453...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Re-uploading in Fiddler essentially means havi...</td>\n",
       "      <td>[-0.01760503277182579, 0.011651406064629555, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    ---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...   \n",
       "1    ---\\ntitle: \"Customer Churn Prediction\"\\nslug:...   \n",
       "2    slug: \"customer-churn-prediction\" analyze-rca-...   \n",
       "3    ---\\ntitle: \"Fraud Detection\"\\nslug: \"fraud-de...   \n",
       "4    slug: \"fraud-detection\" 3.png\",\\n        \"RCA3...   \n",
       "..                                                 ...   \n",
       "200  ---\\ntitle: \"Uploading a scikit-learn Model Ar...   \n",
       "201  ---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...   \n",
       "202  Once you have added a model on the Fiddler pla...   \n",
       "203  Custom metrics is an upcoming feature and it i...   \n",
       "204  Re-uploading in Fiddler essentially means havi...   \n",
       "\n",
       "                                             embedding  \n",
       "0    [-0.020456863567233086, -0.0040573012083768845...  \n",
       "1    [-0.006977782119065523, -0.001356987631879747,...  \n",
       "2    [-0.018147623166441917, -0.0034140129573643208...  \n",
       "3    [-0.007915230467915535, -0.007179588079452515,...  \n",
       "4    [-0.01609727181494236, -0.0010202372213825583,...  \n",
       "..                                                 ...  \n",
       "200  [0.0031662925612181425, 0.018320860341191292, ...  \n",
       "201  [-0.0003327192389406264, -0.021783549338579178...  \n",
       "202  [-0.022226542234420776, 0.011369146406650543, ...  \n",
       "203  [-0.017716489732265472, -0.0035160724073648453...  \n",
       "204  [-0.01760503277182579, 0.011651406064629555, 0...  \n",
       "\n",
       "[205 rows x 2 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "146229a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s,r,emb = strings_ranked_by_relatedness(\"How to delete a project in Fiddler?\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "42eb45f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s,r,emb = strings_ranked_by_relatedness(\"How to use fiddler client to delete a project?\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ececb3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Re-uploading in Fiddler essentially means having to delete what was uploaded or ingested earlier, making the updates you want to make, and then following the same steps as before for the specific thing you are looking to upload. So for example, if you want make changes to the baseline dataset you uploaded, you will have to delete the dataset and then make modifications for the datasetinfo object or the dataset itself and then upload the dataset again. As for events, currently there isn't a way for the user to directly delete events. Please contact Fiddler personnell for the same. \",\n",
       " 'slug: \"authorizing-the-client\" .ini\\nclient = fdl.FiddlerApi()\\n```',\n",
       " '---\\ntitle: \"About the Fiddler Client\"\\nslug: \"about-the-fiddler-client\"\\nhidden: false\\ncreatedAt: \"2022-05-23T15:59:05.747Z\"\\nupdatedAt: \"2022-05-23T15:59:05.747Z\"\\n---\\nThe Fiddler Client contains many useful methods for sending and receiving data to and from the Fiddler platform.\\n\\nFiddler provides a Python Client that allows you to connect to Fiddler directly from a Python notebook or automated pipeline.\\n\\nEach client function is documented with a description, usage information, and code examples.',\n",
       " 'slug: \"product-tour\" )\\n\\n**Projects** represent your organization\\'s distinct AI applications or use cases. Within Fiddler, Projects house all the **Models** specific to a given application, and thus serve as a jumping-off point for the majority of Fiddler’s model monitoring and explainability features.\\n\\nGo ahead and click on the _Lending project_ to navigate to the Project Overview page.\\n\\n![](https://files.readme.io/b008f03-image.png)\\n\\nHere you can see a list of the models contained within the Lending project, as well as a project dashboard to which analyze charts can be pinned. Go ahead and click the “logreg-all” model.\\n\\n![](https://files.readme.io/f3e024d-image.png)\\n\\nFrom the Model Overview page, you can view details about the model: its metadata (schema), the files in its model directory, and its features, which are sorted by impact (the degree to which each feature influences the model’s prediction score).\\n\\nYou can then navigate to the platform\\'s core monitoring and explainability capabilities. These include:\\n\\n- **_Monitor_** — Track and configure alerts on your model’s performance, data drift, data integrity, and overall service metrics. Read the [Monitoring](doc:monitoring-platform) documentation for more details.\\n- **_Analyze_** — Analyze the behavior of your model in aggregate or with respect to specific segments of your population. Read the [Analytics](doc:analytics-ui) documentation for more details.\\n- **_Explain_** — Generate “point” or prediction-level explanations on your training or production data for insight into how each model decision was made. Read the [Explainability](doc:explainability-platform) documentation for more details.\\n- **_Evaluate_** — View your model’s performance on its training and test sets for quick validation prior to deployment. Read the [Evaluation](doc:evaluation-ui) documentation for more details.\\n\\n## Fiddler Samples\\n\\nFiddler Samples is a set of datasets and models that are preloaded into Fiddler. They represent different data types, model frameworks, and machine learning techniques. See the table below for more details.\\n\\n| **Project**   | **Model**                       | **Dataset** | **Model Framework** | **Algorithm**       | **Model Task**             | **Explanation Algos** |\\n| ------------- | ------------------------------- | ----------- | ------------------- | ------------------- | -------------------------- | --------------------- |\\n| Bank Churn    | Bank Churn                      | Tabular     | scikit-learn        | Random Forest       | Binary Classification      | Fiddler Shapley       |\\n| Heart Disease | Heart Disease                   | Tabular     | Tensorflow          |                     | Binary Classification      | Fiddler Shapley, IG   |\\n| IMDB          | Imdb Rnn                        | Text        | Tensorflow          | BiLSTM              | Binary Classfication       | Fiddler Shapley, IG   |\\n| Iris          | Iris                            | Tabular     | scikit-learn        | Logistic Regression | Multi-class Classification | Fiddler Shapley       |\\n| Lending       | Logreg-all                      | Tabular     | scikit-learn        | Logistic Regression | Binary Classification      | Fiddler Shapley       |\\n|               | Logreg-simple                   | Tabular     | scikit-learn        | Logistic Regression | Binary Classification      | Fiddler Shapley       |\\n|               | Xgboost-simple-sagemaker        | Tabular     | scikit-learn        | XGboost             | Binary Classification      | Fiddler Shapley       |\\n| Newsgroup     | Christianity',\n",
       " '---\\ntitle: \"Simple Monitoring\"\\nslug: \"quick-start\"\\nexcerpt: \"Quickstart Notebook\"\\nhidden: false\\ncreatedAt: \"2022-08-10T15:11:33.699Z\"\\nupdatedAt: \"2023-03-07T21:38:01.896Z\"\\n---\\nThis guide will walk you through the basic onboarding steps required to use Fiddler for model monitoring, **using sample data provided by Fiddler**.  \\n\\n**Note**: This guide does not upload a model artifact or create a surrogate model, both of which are supported by Fiddler.  As a result, this guide won\\'t allow you to explore explainability within the platform.\\n\\nClick the following link to get started using Google Colab:\\n\\n<div class=\"colab-box\">\\n    <a href=\"https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/Fiddler_Quickstart_Simple_Monitoring.ipynb\" target=\"_blank\">\\n        <div>\\n            Open in Google Colab →\\n        </div>\\n    </a>\\n    <div>\\n            <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" />\\n    </div>\\n</div>\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]# Fiddler Simple Monitoring Quick Start Guide\\n\\nFiddler is the pioneer in enterprise Model Performance Management (MPM), offering a unified platform that enables Data Science, MLOps, Risk, Compliance, Analytics, and other LOB teams to **monitor, explain, analyze, and improve ML deployments at enterprise scale**. \\nObtain contextual insights at any stage of the ML lifecycle, improve predictions, increase transparency and fairness, and optimize business revenue.\\n\\n---\\n\\nYou can start using Fiddler ***in minutes*** by following these 7 quick steps:\\n\\n1. Imports\\n2. Connect to Fiddler\\n3. Upload a baseline dataset\\n4. Add metadata about your model with Fiddler\\n5. Set up Alerts and Notifications (Optional)\\n6. Publish production events\\n7. Get insights\\n\\n**Don\\'t have a Fiddler account? [Sign-up for a 14-day free trial](https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral).**\\n\\n## 1. Imports\\n\\n\\n```python\\n!pip install -q fiddler-client\\n\\nimport numpy as np\\nimport pandas as pd\\nimport time as time\\nimport fiddler as fdl\\n\\nprint(f\"Running client version {fdl.__version__}\")\\n```\\n\\n## 2. Connect to Fiddler\\n\\nBefore you can add information about your model with Fiddler, you\\'ll need to connect using our API client.\\n\\n\\n---\\n\\n\\n**We need a few pieces of information to get started.**\\n1. The URL you\\'re using to connect to Fiddler\\n\\n\\n```python\\nURL = \\'\\' # Make sure to include the full URL (including https://). For example, https://abc.xyz.ai\\n```\\n\\n2. Your organization ID\\n3. Your authorization token\\n\\nBoth of these can be found by clicking the URL you entered and navigating to the **Settings',\n",
       " '---\\ntitle: \"Project Structure on UI\"\\nslug: \"project-structure\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:26:33.568Z\"\\nupdatedAt: \"2023-02-03T19:45:51.093Z\"\\n---\\nSupervised machine learning involves identifying a predictive task, finding data to enable that task, and building a model using that data. Fiddler captures this workflow with project, dataset, and model entities.\\n\\n## Projects\\n\\nA project represents a machine learning task (e.g. predicting house prices, assessing creditworthiness, or detecting fraud).\\n\\nA project can contain one or more models for the ML task (e.g. LinearRegression-HousePredict, RandomForest-HousePredict).\\n\\nCreate a project by clicking on **Projects** and then clicking on **Add Project**.\\n\\n![](https://files.readme.io/8e4b429-Add_project_0710.png \"Add_project_0710.png\")\\n\\n- **_Create New Project_** — A window will pop up where you can enter the project name and click **Create**. Once the project is created, it will be displayed on the projects page.\\n\\nYou can access your projects from the Projects Page.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/82404e6-Screenshot_2022-12-27_at_1.00.15_PM.png\",\\n        null,\\n        \"Projects Page on Fiddler UI\"\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Projects Page on Fiddler UI\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n## Datasets\\n\\nA dataset in Fiddler is a data table containing features, model outputs, and a target for machine learning models. Optionally, you can also upload metadata and “decision” columns, which can be used to segment the dataset for analyses, track business decisions, and work as protected attributes in bias-related workflows. For more details refer to [Datasets](doc:project-architecture#datasets) in the Platform Guide.\\n\\nOnce you click on a particular project, you will be able to see if there are any datasets associated with the project. For example, the bank_churn project, in the following screenshot, has the bank_churn dataset. [Datasets are uploaded via the Fiddler client](ref:clientupload_dataset). \\n\\n![](https://files.readme.io/3fa7700-Screenshot_2022-12-27_at_1.05.05_PM.png)\\n\\n## Models\\n\\nA model in Fiddler represents a machine learning model. A project will have one or more models for the ML task (e.g. a project to predict house prices might contain LinearRegression-HousePredict and RandomForest-HousePredict). For further details refer to the [Models](doc:project-architecture#models) section in the Platform Guide.\\n\\n![](https://files.readme.io/e151df5-Model_Dashboard.png \"Model_Dashboard.png\")\\n\\n### Model Artifacts\\n\\nAt its most basic level, a model in Fiddler is simply a directory that contains [model artifacts](doc:artifacts-and-surrogates) such as:\\n\\n- The model file (e.g. `*.pkl`)\\n- `package.py`: A wrapper script containing all of the code needed to standardize the execution of the model.\\n\\n![](https://files.readme.io/7170489-Model_Details.png \"Model_Details.png\")\\n\\n![](https://files.readme.io/2b3d52e-Model_Details_1.png \"Model_Details_1.png\")\\n\\n## Project Dashboard',\n",
       " 'slug: \"client-setup\" _TOKEN\\n)\\n```\\n```python Connect the Client with self-signed certs\\nimport fiddler as fdl\\n\\nURL = \\'https://app.fiddler.ai\\'\\nORG_ID = \\'my_org\\'\\nAUTH_TOKEN = \\'p9uqlkKz1zAA3KAU8kiB6zJkXiQoqFgkUgEa1sv4u58\\'\\n\\nclient = fdl.FiddlerApi(\\n    url=URL,\\n    org_id=ORG_ID,\\n    auth_token=AUTH_TOKEN, \\n\\t\\tverify=False\\n)\\n```\\n```Text Connect the Client with Proxies\\nproxies = {\\n    \\'http\\' : \\'http://proxy.example.com:1234\\',\\n    \\'https\\': \\'https://proxy.example.com:5678\\'\\n}\\n\\nclient = fdl.FiddlerApi(\\n    url=URL,\\n    org_id=ORG_ID,\\n    auth_token=AUTH_TOKEN, \\n\\t\\tproxies=proxies\\n)\\n```\\n\\nIf you want to authenticate with Fiddler without passing this information directly into the function call, you can store it in a file named_ fiddler.ini_, which should be stored in the same directory as your notebook or script.\\n\\n```python Writing fiddler.ini\\n%%writefile fiddler.ini\\n\\n[FIDDLER]\\nurl = https://app.fiddler.ai\\norg_id = my_org\\nauth_token = p9uqlkKz1zAA3KAU8kiB6zJkXiQoqFgkUgEa1sv4u58\\n```\\n\\n\\n\\n```python Connecting the Client with a fiddler.ini file\\nclient = fdl.FiddlerApi()\\n```',\n",
       " '---\\ntitle: \"Databricks Integration\"\\nslug: \"databricks-integration\"\\nhidden: false\\ncreatedAt: \"2023-02-02T20:38:54.971Z\"\\nupdatedAt: \"2023-08-14T17:14:26.297Z\"\\n---\\nFiddler allows your team to monitor, explain and analyze your models developed and deployed in [Databricks Workspace](https://docs.databricks.com/introduction/index.html) by integrating with [MLFlow](https://docs.databricks.com/mlflow/index.html) for model asset management and utilizing Databricks Spark environment for data management. \\n\\nTo validate and monitor models built on Databricks using Fiddler, you can follow these steps:\\n\\n1. [Creating a Fiddler Project](doc:databricks-integration#creating-a-fiddler-project)\\n2. [Uploading a Baseline Dataset](doc:databricks-integration#uploading-a-baseline-dataset)\\n3. [Adding Model Information ](doc:databricks-integration#adding-model-information)\\n4. [Uploading Model Files (for Explainability)](doc:databricks-integration#uploading-model-files)\\n5. [Publishing Events](doc:databricks-integration#publishing-events)\\n   1. Batch Models \\n   2. Live Models \\n\\n## Creating a Fiddler Project\\n\\nLaunch a [Databricks notebook](https://docs.databricks.com/notebooks/index.html) from your workspace and run the following code:\\n\\n```python\\n!pip install -q fiddler-client\\nimport fiddler as fdl\\n```\\n\\nNow that you have the Fiddler library installed, you can connect to your Fiddler environment. Please use the [UI administration guide](doc:administration-ui) to help you find your Fiddler credentials.\\n\\n```python\\nURL = \"\"\\nORG_ID = \"\"\\nAUTH_TOKEN = \"\"\\nclient = fdl.FiddlerApi(url=URL, org_id=ORG_ID, auth_token=AUTH_TOKEN)\\n```\\n\\nFinally, you can set up a new project using:\\n\\n```python\\nclient.create_project(\"YOUR_PROJECT_NAME\")\\n```\\n\\n## Uploading a Baseline Dataset\\n\\nYou can grab your baseline dataset from a[ delta table](https://docs.databricks.com/getting-started/dataframes-python.html) and share it with Fiddler as a baseline dataset:\\n\\n```python\\nbaseline_dataset = spark.read.table(\"YOUR_DATASET\").select(\"*\").toPandas()\\n\\ndataset_info = fdl.DatasetInfo.from_dataframe(baseline_upload, max_inferred_cardinality=100)\\n  \\nclient.upload_dataset(\\n  project_id=PROJECT_ID,\\n  dataset_id=DATASET_ID,\\n  dataset={\\'baseline\\': baseline_upload},\\n  info=dataset_info)\\n```\\n\\n## Adding Model Information\\n\\nUsing the **[MLFlow API](https://docs.databricks.com/reference/mlflow-api.html) ** you can query the model registry and get the **model signature** which describes the inputs and outputs as a dictionary. You can use this dictionary to build out the [ModelInfo](ref:fdlmodelinfo) object required to the model to Fiddler:\\n\\n```python Python\\nmport mlflow \\n\\nmodel_uri = MlflowClient.get_model_version_download_uri(model_name, model_version) #spevify the model name and model version you want to share wth Fiddler\\n\\nmodel_info = mlflow.models.ModelSignature.to_dict(model_uri)  #MLFlow_Params_Object with model I/O info\\n```\\n\\nNow you can share the model signature with Fiddler as part of the Fiddler ModelInfo object :\\n\\n```python\\nmodel_info =',\n",
       " '---\\ntitle: \"Publishing Production Data\"\\nslug: \"publishing-production-data\"\\nhidden: false\\ncreatedAt: \"2022-11-18T23:28:25.348Z\"\\nupdatedAt: \"2022-12-19T19:14:28.171Z\"\\n---\\nThis Section guides you on the various ways you can provide event data to Fiddler and update and retrieve them.',\n",
       " '---\\ntitle: \"Project Architecture\"\\nslug: \"project-architecture\"\\nhidden: false\\ncreatedAt: \"2022-11-15T18:06:28.079Z\"\\nupdatedAt: \"2023-02-14T23:21:13.699Z\"\\n---\\nSupervised machine learning involves identifying a predictive task, finding data to enable that task, and building a model using that data. \\n\\nFiddler captures this workflow with **project**, **dataset**, and **model** entities.\\n\\n## Project\\n\\nIn Fiddler, a project is essentially a parent folder that hosts one or more **model** (s) for the ML task (e.g. A Project HousePredict for predicting house prices will LinearRegression-HousePredict, RandomForest-HousePredict).\\n\\n## Models\\n\\nA model in Fiddler represents a **placeholder** for a machine-learning model. It\\'s a placeholder because we may not need the **[model artifacts](doc:artifacts-and-surrogates#Model-Artifacts)**. Instead, we may just need adequate [information about the model](ref:fdlmodelinfo) in order to monitor model-specific data. \\n\\n> 📘 Info\\n> \\n> You can [upload your model artifacts](https://dash.readme.com/project/fiddler/v1.6/docs/uploading-model-artifacts) to Fiddler to unlock high-fidelity explainability for your model. However, it is not required. If you do not wish to upload your artifact but want to explore explainability with Fiddler, we can build a [**surrogate model**](doc:artifacts-and-surrogates#surrogate-model) on the backend to be used in place of your artifact.\\n\\n## Datasets\\n\\nA dataset in Fiddler is a data table containing [information about data](ref:fdldatasetinfo) such as **features**, **model outputs**, and a **target** for machine learning models. Optionally, you can also upload **metadata** and “**decision**” columns, which can be used to segment the dataset for analyses, track business decisions, and work as protected attributes in bias-related workflows. \\n\\nIn order to monitor **production data**, a [dataset must be uploaded](ref:clientupload_dataset) to be used as a **baseline** for making comparisons. This baseline dataset should be sampled from your model\\'s **training data**. The sample should be unbiased and should faithfully capture moments of the parent distribution. Further, values appearing in the baseline dataset\\'s columns should be representative of their entire ranges within the complete training dataset.\\n\\n**Datasets are used by Fiddler in the following ways:**\\n\\n1. As a reference for [drift calculations](doc:data-drift-platform) and [data integrity violations ](doc:data-integrity-platform)on the **[Monitor](doc:monitoring-ui)** page\\n2. To train a model to be used as a [surrogate](doc:artifacts-and-surrogates#surrogate-model) when using [`add_model_surrogate`](/reference/clientadd_model_surrogate)\\n3. For computing model performance metrics globally on the **[Evaluate](doc:evaluation-ui)** page, or on slices on the **[Analyze](doc:analytics-ui)** page\\n4. As a reference for explainability algorithms (e.g. partial dependence plots, permutation feature impact, approximate Shapley values, and ICE plots).\\n\\nBased on the above uses, _datasets with sizes much in excess of 10K rows are often unnecessary_ and can lead to excessive upload, precomputation, and query times. That being said, here are some situations where larger datasets may be desirable:\\n\\n-',\n",
       " '---\\ntitle: \"PagerDuty Integration\"\\nslug: \"pagerduty\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:19:10.407Z\"\\nupdatedAt: \"2023-04-07T01:25:06.491Z\"\\n---\\nFiddler offers powerful alerting tools for monitoring models. By integrating with  \\nPagerDuty services, you gain the ability to trigger PagerDuty events within your monitoring  \\nworkflow.\\n\\n> 📘 \\n> \\n> If your organization has already integrated with PagerDuty, then you may skip to the [Setup: In Fiddler](#setup-in-fiddler) section to learn more about setting up PagerDuty within Fiddler.\\n\\n## Setup: In PagerDuty\\n\\n1. Within your PagerDuty Team, navigate to **Services** → **Service Directory**.\\n\\n![](https://files.readme.io/0ae47bb-pagerduty_1.png \"pagerduty_1.png\")\\n\\n\\n\\n2. Within the Service Directory:\\n   - If you are creating a new service for integration, select **+New Service** and follow the prompts to create your service.\\n   - Click the **name of the service** you want to integrate with.\\n\\n![](https://files.readme.io/956dbdf-pagerduty_2.png \"pagerduty_2.png\")\\n\\n\\n\\n3. Navigate to **Integrations** within your service, and select **Add a new integration to this service**.\\n\\n![](https://files.readme.io/ca2e4c2-pagerduty_3.png \"pagerduty_3.png\")\\n\\n\\n\\n4. Enter an **Integration Name**, and under **Integration Type** select the option **Use our API directly**. Then, select the **Add Integration** button to save your new integration. You will be redirected to the Integrations page for your service.\\n\\n![](https://files.readme.io/0f5d5ae-pagerduty_4.png \"pagerduty_4.png\")\\n\\n\\n\\n5. Copy the **Integration Key** for your new integration.\\n\\n![](https://files.readme.io/e144e08-pagerduty_5.png \"pagerduty_5.png\")\\n\\n\\n\\n## Setup: In Fiddler\\n\\n1. Within **Fiddler**, navigate to the **Settings** page, and then to the **PagerDuty Integration** menu. If your organization **already has a PagerDuty service integrated with Fiddler**, you will be able to find it in the list of services.\\n\\n![](https://files.readme.io/8de1a6b-pagerduty_setup_f_1.png \"pagerduty_setup_f_1.png\")\\n\\n\\n\\n2. If you are looking to integrate with a new service, select the **`+`** box on the top right. Then, enter the name of your service, as well as the Integration Key copied from the end of the [Setup: In PagerDuty](#setup-in-pagerduty) section above. After creation, confirm that your new entry is now in the list of available services.\\n\\n![](https://files.readme.io/9febb10-pagerduty_setup_f_2.png \"pagerduty_setup_f_2.png\")\\n\\n\\n\\n> 🚧 \\n> \\n> Creating, editing, and deleting these services is an **ADMINSTRATOR**-only privilege. Please contact an **ADMINSTRATOR** within your organization to setup any new PagerDuty services\\n\\n## PagerDuty Alerts in Fiddler\\n\\n1. Within the **Projects** page, select the model',\n",
       " 'Once you have added a model on the Fiddler platform using a specific model info object, that is fdl.ModelInfo, you cannot modify aspects such as features, inputs, outputs, model task etc. specified in the model info object. Currently, if you want to change fundamental details about a modelinfo object, then it is advised to create/add a new model with a new modelinfo object.',\n",
       " 'slug: \"explainability-with-model-artifact-quickstart-notebook\" models\\', MODEL_ID, \\'monitor\\']))\\n```\\n\\n*Please allow 3-5 minutes for monitoring data to populate the charts.*\\n\\n\\n\\n---\\n\\n\\n**Questions?**  \\n  \\nCheck out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\\n\\nIf you\\'re still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we\\'ll get back to you shortly.\\n',\n",
       " 'slug: \"quick-start\"  your project, you should now be able to see the newly created dataset on the UI.\\n\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/6.png\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n## 4. Add metadata about your model\\n\\nNow it\\'s time to add your model with Fiddler.  We do this by defining a [ModelInfo](https://docs.fiddler.ai/reference/fdlmodelinfo) object.\\n\\n\\n---\\n\\n\\nThe [ModelInfo](https://docs.fiddler.ai/reference/fdlmodelinfo) object will contain some **information about how your model operates**.\\n  \\n*Just include:*\\n1. The **task** your model is performing (regression, binary classification, etc.)\\n2. The **target** (ground truth) column\\n3. The **output** (prediction) column\\n4. The **feature** columns\\n5. Any **metadata** columns\\n6. Any **decision** columns (these measures the direct business decisions made as result of the model\\'s prediction)\\n\\n\\n\\n```python\\n# Specify task\\nmodel_task = \\'binary\\'\\n\\nif model_task == \\'regression\\':\\n    model_task = fdl.ModelTask.REGRESSION\\n    \\nelif model_task == \\'binary\\':\\n    model_task = fdl.ModelTask.BINARY_CLASSIFICATION\\n\\nelif model_task == \\'multiclass\\':\\n    model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION\\n    \\nelif model_task == \\'ranking\\':\\n    model_task = fdl.ModelTask.RANKING\\n\\n    \\n# Specify column types\\nfeatures = [\\'geography\\', \\'gender\\', \\'age\\', \\'tenure\\', \\'balance\\', \\'numofproducts\\', \\'hascrcard\\', \\'isactivemember\\', \\'estimatedsalary\\']\\noutputs = [\\'predicted_churn\\']\\ntarget = \\'churn\\'\\ndecision_cols = [\\'decision\\']\\nmetadata_cols = [\\'customer_id\\']\\n    \\n# Generate ModelInfo\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n    dataset_info=dataset_info,\\n    dataset_id=DATASET_ID,\\n    model_task=model_task,\\n    features=features,\\n    outputs=outputs,\\n    target=target,\\n    categorical_target_class_details=\\'yes\\',\\n    decision_cols=decision_cols, # Optional\\n    metadata_cols=metadata_cols, # Optional\\n    binary_classification_threshold=0.5 # Optional\\n)\\nmodel_info\\n```\\n\\nAlmost done! Now just specify a unique model ID and use the client\\'s [add_model](https://docs.fiddler.ai/reference/clientadd_model) function to send this information to Fiddler.\\n\\n\\n```python\\nMODEL_ID = \\'churn_classifier\\'\\n\\nclient.add_model(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID,\\n    model_id=MODEL_ID,\\n    model_info=model_info,\\n)\\n```\\n\\nOn the project page, you should now be able to see the newly created model.\\n\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/7.png\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n## 5. Set up Alerts and Notifications (Optional)\\n\\nFiddler Client API function [add_alert_rule](https://dash.readme.com/project/fiddler/v1.5/refs/clientadd_alert_rule) allow creating rules to receive email and pagerduty notifications when your data or model predictions deviates from it\\'s expected behavior.\\n\\nThe rules can of **Data Drift, Performance, Data Integrity,** and **Service Metrics** types and they can be compared to **',\n",
       " 'slug: \"project-architecture\"  **Auto-modeling for tasks with significant class imbalance; or strong and complex feature interactions, possibly with deeply encoded semantics**\\n  - However, in use cases like these, most users opt to upload carefully-engineered model artifacts tailored to the specific application.\\n- **Deep segmentation analysis**\\n  - If it’s desirable to perform model analyses on very specific subpopulations (e.g. “55-year-old Canadian home-owners who have been customers between 18 and 24 months”), large datasets may be necessary to have sufficient reference representation to drive model analytics.\\n\\n> 📘 Info\\n> \\n> Datasets can be uploaded to Fiddler using the[ Python API client](doc:installation-and-setup).\\n\\n [Check the UI Guide to Visualize Project Architecture on our User Interface](doc:project-structure)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_',\n",
       " 'slug: \"monitoring-xai-quick-start\" .png\" /></td>\\n        <td><img src=\"https://fiddler-nb-assets.s3.us-west-1.amazonaws.com/qs_org_id_numbered.png\" /></td>\\n    </tr>\\n    <tr>\\n        <td><img src=\"https://fiddler-nb-assets.s3.us-west-1.amazonaws.com/qs_new_key_numbered.png\" /></td>\\n        <td><img src=\"https://fiddler-nb-assets.s3.us-west-1.amazonaws.com/qs_auth_token_numbered.png\" /></td>\\n    </tr>\\n</table>\\n\\nNow just run the following code block to connect to the Fiddler API!\\n\\n\\n```python\\nclient = fdl.FiddlerApi(\\n    url=URL,\\n    org_id=ORG_ID,\\n    auth_token=AUTH_TOKEN\\n)\\n```\\n\\nOnce you connect, you can create a new project by specifying a unique project ID in the client\\'s `create_project` function.\\n\\n\\n```python\\nPROJECT_ID = \\'quickstart_xai\\'\\n\\nclient.create_project(PROJECT_ID)\\n```\\n\\nYou should now be able to see the newly created project on the UI.\\n\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/xai_project_list.png\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n## 2. Upload a baseline dataset\\n\\nIn this example, we\\'ll be considering the case where we\\'re a bank and we have **a model that predicts churn for our customers**.  \\nWe want to explain our model\\'s predictions and **understand the features that impact model predictions** the most.\\n  \\nIn order to get explainability insights, **Fiddler needs to fiddle with your model**. To do so, we need to add your model details. This includes information about the data used by your model. So, we first start with uploading a small sample of data that can serve as a baseline.\\n\\n\\n---\\n\\n\\n*For more information on how to design a baseline dataset, [click here](https://docs.fiddler.ai/pages/user-guide/data-science-concepts/monitoring/designing-a-baseline-dataset/).*\\n\\n\\n```python\\nPATH_TO_BASELINE_CSV = \\'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/churn_baseline.csv\\'\\n\\nbaseline_df = pd.read_csv(PATH_TO_BASELINE_CSV)\\nbaseline_df\\n```\\n\\nFiddler uses this baseline dataset to keep track of important information about your data.\\n  \\nThis includes **data types**, **data ranges**, and **unique values** for categorical variables.\\n\\n---\\n\\nYou can construct a `DatasetInfo` object to be used as **a schema for keeping track of this information** by running the following code block.\\n\\n\\n```python\\ndataset_info = fdl.DatasetInfo.from_dataframe(baseline_df, max_inferred_cardinality=100)\\ndataset_info\\n```\\n\\nThen use the client\\'s `upload_dataset` function to send this information to Fiddler!\\n  \\n*Just include:*\\n1. A unique dataset ID\\n2. The baseline dataset as a pandas DataFrame\\n3. The `DatasetInfo` object you just created\\n\\n\\n```python\\nDATASET_ID = \\'churn_data\\'\\n\\nclient.upload_dataset(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID,\\n    dataset={\\n        \\'baseline\\': baseline_df\\n    },\\n    info=dataset_info\\n)\\n```\\n\\nIf you click on your project, you should now be able to see the newly created dataset on the UI.\\n\\n<table>\\n    <tr>\\n        <td><img src=\"https://',\n",
       " 'slug: \"explainability-with-model-artifact-quickstart-notebook\" _ID,\\n    auth_token=AUTH_TOKEN\\n)\\n```\\n\\nOnce you connect, you can create a new project by specifying a unique project ID in the client\\'s [create_project](https://docs.fiddler.ai/reference/clientcreate_project) function.\\n\\n\\n```python\\nPROJECT_ID = \\'your_project_name\\'\\n\\nif not PROJECT_ID in client.list_projects():\\n    print(f\\'Creating project: {PROJECT_ID}\\')\\n    client.create_project(PROJECT_ID)\\nelse:\\n    print(f\\'Project: {PROJECT_ID} already exists\\')\\n```\\n\\n# 2. Upload a baseline dataset\\n\\nIn this example, we\\'ll be considering the case where we\\'re a bank and we have **a model that predicts churn for our customers**.  \\n  \\nIn order to get insights into the model\\'s performance, **Fiddler needs a small  sample of data that can serve as a baseline** for making comparisons with data in production.\\n\\n\\n---\\n\\n\\n*For more information on how to design a baseline dataset, [click here](https://docs.fiddler.ai/docs/designing-a-baseline-dataset).*\\n\\n\\n```python\\nPATH_TO_BASELINE_CSV = \\'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/churn_baseline.csv\\'\\n\\nbaseline_df = pd.read_csv(PATH_TO_BASELINE_CSV)\\nbaseline_df\\n```\\n\\nFiddler uses this baseline dataset to keep track of important information about your data.\\n  \\nThis includes **data types**, **data ranges**, and **unique values** for categorical variables.\\n\\n---\\n\\nYou can construct a [DatasetInfo](https://docs.fiddler.ai/reference/fdldatasetinfo) object to be used as **a schema for keeping track of this information** by running the following code block.\\n\\n\\n```python\\ndataset_info = fdl.DatasetInfo.from_dataframe(baseline_df, max_inferred_cardinality=100)\\ndataset_info\\n```\\n\\nThen use the client\\'s [upload_dataset](https://docs.fiddler.ai/reference/clientupload_dataset) function to send this information to Fiddler.\\n  \\n*Just include:*\\n1. A unique dataset ID\\n2. The baseline dataset as a pandas DataFrame\\n3. The `DatasetInfo` object you just created\\n\\n\\n```python\\nDATASET_ID = \\'churn_data\\'\\n\\nclient.upload_dataset(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID,\\n    dataset={\\n        \\'baseline\\': baseline_df\\n    },\\n    info=dataset_info\\n)\\n```\\n\\nWithin your Fiddler environment\\'s UI, you should now be able to see the newly created dataset within your project.\\n\\n## 3. Upload your model package\\n\\nNow it\\'s time to upload your model package to Fiddler.  To complete this step, we need to ensure we have 2 assets in a directory.  It doesn\\'t matter what this directory is called, but for this example we will call it **/model**.\\n\\n\\n```python\\nimport os\\nos.makedirs(\"model\")\\n```\\n\\n***Your model package directory will need to contain:***\\n1. A **package.py** file which explains to Fiddler how to invoke your model\\'s prediction endpoint\\n2. And the **model artifact** itself\\n3. A **requirements.txt** specifying which python libraries need by package.py\\n\\n---\\n\\n### 3.1.a  Create the **model_info** object \\n\\nThis is done by creating our [model_info](https://docs.fiddler.ai/reference/fdlmodelinfo) object.\\n\\n\\n\\n```python\\nmetadata_cols = [\\'gender\\']\\ndecision_cols = [\\'decision\\']\\nfeature_columns = [\\'creditscore\\', \\'geography\\', \\'age\\', \\'tenure\\',\\n       \\'balance\\', \\'numofproducts\\', \\'hascrcard\\', \\'',\n",
       " '---\\ntitle: \"Installation and Setup\"\\nslug: \"installation-and-setup\"\\nhidden: false\\ncreatedAt: \"2022-05-10T17:14:02.670Z\"\\nupdatedAt: \"2023-02-14T01:20:21.990Z\"\\n---\\nFiddler offers a **Python SDK client** that allows you to connect to Fiddler directly from a Jupyter notebook or automated pipeline.\\n\\n***\\n\\n\\n\\nThe client is available for download from PyPI via `pip`:\\n\\n```\\npip install fiddler-client\\n```\\n\\n\\n\\n<br>\\n\\nOnce you\\'ve installed the client, you can import the `fiddler` package into any Python script:\\n\\n```python\\nimport fiddler as fdl\\n```\\n\\n\\n\\n***\\n\\n\\n\\n> 📘 Info\\n> \\n> For detailed documentation on the client’s many features, check out the [API reference](https://api.fiddler.ai) section.\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"System Architecture\"\\nslug: \"system-architecture\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:19:53.311Z\"\\nupdatedAt: \"2023-05-18T21:09:05.870Z\"\\n---\\nFiddler deploys into your private cloud\\'s existing Kubernetes clusters, for which the minimum system requirements can be found [here](doc:technical-requirements).  Fiddler supports deployment into Kubernetes in AWS, Azure, or GCP.  All services of the Fiddler platform are containerized in order to eliminate reliance on other cloud services and to reduce the deployment and maintenance friction of the platform.  This includes storage services like object storage and databases as well as system monitoring services like Grafana.  \\n\\nUpdates to the Fiddler containers is accomplished through a shared container registry (that Fiddler is provided access to).  Updates to the containers are orchestrated using Helm charts.\\n\\nA full-stack deployment of Fiddler is shown in the diagram below. \\n\\n![](https://files.readme.io/7cbfe31-reference_architecture.png)\\n\\nThe Fiddler system components are deployed within a single namespace on a Kubernetes cluster, using the official Fiddler Helm chart.\\n\\n- Fiddler core infrastructure relies on persistent volumes provided within the Kubernetes cluster. We recommend using encrypted storage volumes wherever possible.\\n- Fiddler may be configured to utilize external infrastructure in a self-hosted environment, such as existing PostgresQL servers, but this is not required as all services are containerized by default.\\n- Full-stack \"any-prem\" Fiddler includes observability infrastructure to monitor Fiddler system health and performance. These mainstream observability components may be integrated with external observability systems to support administration in a self-hosted environment.\\n- HTTP traffic to the Fiddler system is handled by an L4 or L7 load balancer or other proxy. TLS termination should usually occur outside the Fiddler system.\\n\\nOnce the platform is running, end users can interface with the Fiddler platform using their browser, the [Fiddler Python client](ref:about-the-fiddler-client), or Fiddler\\'s RESTful APIs.',\n",
       " 'slug: \"cv-monitoring\" _ID, \\'monitor\\']))\\n```\\n\\n*Please allow 3-5 minutes for monitoring data to populate the charts.*\\n  \\nThe following screen (without the annotation bubbles) will be available to you upon completion.\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://github.com/fiddler-labs/fiddler-examples/raw/main/quickstart/images/image_monitoring_drift.png\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n\\n\\n---\\n\\n\\n**Questions?**  \\n  \\nCheck out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\\n\\nJoin our [community Slack](http://fiddler-community.slack.com/) to ask any questions!\\n\\nIf you\\'re still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we\\'ll get back to you shortly.\\n',\n",
       " 'slug: \"onboarding-a-model\" clientadd_model) to onboard your model with Fiddler.\\n\\n```python\\nMODEL_ID = \\'example_model\\'\\n\\nclient.add_model(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID,\\n    model_id=MODEL_ID,\\n    model_info=model_info\\n)\\n```',\n",
       " '---\\ntitle: \"BigQuery Integration\"\\nslug: \"bigquery-integration\"\\nhidden: false\\ncreatedAt: \"2022-05-20T18:53:49.606Z\"\\nupdatedAt: \"2023-06-13T19:09:07.072Z\"\\n---\\n## Using Fiddler on your ML data stored in BigQuery\\n\\nIn this article, we will be looking at loading data from BigQuery tables and using the data for the following tasks-\\n\\n1. Uploading baseline data to Fiddler\\n2. Onboarding a model to Fiddler and creating a surrogate\\n3. Publishing production data to Fiddler\\n\\n## Step 1 - Enable BigQuery API\\n\\nBefore looking at how to import data from BigQuery to Fiddler, we will first see how to enable BigQuery API. This can be done as follows - \\n\\n1. In the GCP platform, Go to the navigation menu -> click APIs & Services. Once you are there, click + Enable APIs and Services (Highlighted below). In the search bar, enter BigQuery API and click Enable.\\n\\n![](https://files.readme.io/75ca647-Screen_Shot_2022-05-19_at_1.26.33_PM.png \"Screen Shot 2022-05-19 at 1.26.33 PM.png\")\\n\\n![](https://files.readme.io/3dd5deb-Screen_Shot_2022-05-19_at_3.33.43_PM.png \"Screen Shot 2022-05-19 at 3.33.43 PM.png\")\\n\\n2. In order to make a request to the API enabled in Step#1, you need to create a service account and get an authentication file for your Jupyter Notebook. To do so, navigate to the Credentials tab under APIs and Services console and click Create Credentials tab, and then Service account under dropdown.\\n\\n![](https://files.readme.io/ea63eca-Screen_Shot_2022-05-19_at_3.34.24_PM.png \"Screen Shot 2022-05-19 at 3.34.24 PM.png\")\\n\\n3. Enter the Service account name and description. You can use the BigQuery Admin role under Grant this service account access to the project. Click Done. You can now see the new service account under the Credentials screen. Click the pencil icon beside the new service account you have created and click Add Key to add auth key. Please choose JSON and click CREATE. It will download the JSON file with auth key info. (Download path will be used to authenticate)\\n\\n![](https://files.readme.io/662315e-Screen_Shot_2022-05-19_at_3.39.24_PM.png \"Screen Shot 2022-05-19 at 3.39.24 PM.png\")\\n\\n## Step 2 - Import data from BigQuery\\n\\nWe will now use the generated key to connect to BigQuery tables from Jupyter Notebook. \\n\\n1. Install the following libraries in the python environment and load them to jupyter-\\n\\n- Google-cloud\\n- Google-cloud-bigquery[pandas]\\n- Google-cloud-storage\\n\\n2. Set the environment variable using the key that was generated in Step 1\\n\\n```python\\n#Set environment variables for your notebook\\nimport os\\nos.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'<path to json file>\\'\\n```\\n\\n3. Import Google cloud client and initiate BigQuery service\\n\\n```python\\n#Imports google cloud client library and initiates BQ service\\nfrom google.cloud import bigquery\\nbigquery_client = bigquery.Client()\\n```\\n\\n4. Specify the',\n",
       " 'slug: \"sagemaker-ml-integration\" AUTH_TOKEN = \\'xtu4g_lReHyEisNg23xJ8IEex0YZEZeeEbTwAsupT0U\\'\\n\\nfiddler_client = fdl.FiddlerApi(\\n    url=URL,\\n    org_id=ORG_ID,\\n    auth_token=AUTH_TOKEN\\n)\\n```\\n\\n\\n\\nThen, get the dataset info from your baseline dataset by using (client.get_dataset_info)[https://api.fiddler.ai/#client-get_dataset_info].\\n\\nAfter that, construct a model info object and save it as a `.yaml` file into the `model` directory.\\n\\n```python\\nPROJECT_ID = \\'example_project\\'\\nDATASET_ID = \\'example_data\\'\\n\\ndataset_info = fiddler_client.get_dataset_info(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID\\n)\\n\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n    dataset_info=dataset_info,\\n    dataset_id=DATASET_ID,\\n    target=\\'target_column\\',\\n    outputs=[\\'output_column\\']\\n)\\n\\nwith open(\\'model/model.yaml\\', \\'w\\') as yaml_file:\\n    yaml.dump({\\'model\\': model_info.to_dict()}, yaml_file)\\n```\\n\\n\\n\\nThe last step is to write our `package.py`.\\n\\n```python\\n%%writefile model/package.py\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom pathlib import Path\\nimport xgboost as xgb\\n\\nimport fiddler as fdl\\n\\nPACKAGE_PATH = Path(__file__).parent\\n\\nclass ModelPackage:\\n\\n    def __init__(self):\\n        \\n        self.model_path = str(PACKAGE_PATH / \\'xgboost-model\\') # This is the name of your model file within the model directory\\n        self.model = xgb.Booster()\\n        self.model.load_model(self.model_path)\\n        \\n        self.output_columns = [\\'output_column\\']\\n    \\n    def transform_input(self, input_df):\\n        return xgb.DMatrix(input_df)\\n    \\n    def predict(self, input_df):\\n        transformed_input = self.transform_input(input_df)\\n        pred = self.model.predict(transformed_input)\\n        return pd.DataFrame(pred, columns=self.output_columns)\\n    \\ndef get_model():\\n    return ModelPackage()\\n```\\n\\n\\n\\nNow, go ahead and upload!\\n\\n```python\\nMODEL_ID = \\'sagemaker_model\\'\\n\\nfiddler_client.upload_model_package(\\n    artifact_path=\\'model\\',\\n    project_id=PROJECT_ID,\\n    model_id=MODEL_ID\\n)\\n```',\n",
       " '---\\ntitle: \"Deploying Fiddler\"\\nslug: \"deploying-fiddler\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:19:47.579Z\"\\nupdatedAt: \"2023-02-14T01:21:23.264Z\"\\n---\\n## Deployment Overview\\n\\nFiddler runs on most mainstream flavors and configurations of Kubernetes, including OpenShift, Rancher, AWS Elastic Kubernetes Service, Azure Managed Kubernetes Service (AKS), GCP Google Kubernetes Engine, and more.\\n\\n- **Our premises**—Fiddler is offered as a fully managed service, deployed within an isolated network and dedicated hardware in the cloud.\\n\\n- **Your premises**—Deploy Fiddler into a Kubernetes cluster running in your own cloud account or data center. Please refer to the [On-prem Technical Requirements](doc:technical-requirements#system-requirements) section for more details.\\n\\n> 📘 Info\\n> \\n> Interested in a Fiddler Cloud or on-premises deployment?  Please contact [sales@fiddler.ai](mailto:sales@fiddler.ai).\\n\\n## Deploy on cloud\\n\\nFiddler cloud deployment uses a managed Kubernetes service to deploy, scale, and manage the application. We\\'ll handle the specifics! Please contact [sales@fiddler.ai](mailto:sales@fiddler.ai)\\n\\n## Deploy on-premise\\n\\n- [Technical Requirements](doc:technical-requirements) \\n- [Installation Guide](doc:installation-guide)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"installation-guide\" _FQDN}/grafana\" \\\\\\n      --set=hostname=${FIDDLER_FQDN}  \\\\\\n      --set=k8s.storage.className=${STORAGE_CLASS} \\\\\\n      --set=clickhouse.storage.className=${STORAGE_CLASS} \\\\\\n      --set=zookeeper.storage.className=${STORAGE_CLASS} \\\\\\n      --set=ingress.class=${INGRESS_CLASS} \\\\\\n      --wait \\\\\\n       fiddler fiddler/fiddler\\n   ```',\n",
       " '---\\ntitle: \"Monitoring\"\\nslug: \"monitoring-ui\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:24:28.175Z\"\\nupdatedAt: \"2023-02-14T01:18:31.256Z\"\\n---\\nFiddler Monitoring helps you identify issues with the performance of your ML models after deployment. Fiddler Monitoring has five main features:\\n\\n1. **Data Drift**\\n2. **Performance**\\n3. **Data Integrity**\\n4. **Service Metrics**\\n5. **Alerts**\\n\\n## Integrate with Fiddler Monitoring\\n\\nIntegrating Fiddler monitoring is a four-step process:\\n\\n1. **Upload dataset**\\n\\n   Fiddler needs a dataset to be used as a baseline for monitoring. A dataset can be uploaded to Fiddler using our UI and Python package. For more information, see:\\n\\n   - [client.upload_dataset()](ref:clientupload_dataset) \\n\\n2. **Onboard model**\\n\\n   Fiddler needs some specifications about your model in order to help you troubleshoot production issues. Fiddler supports a wide variety of model formats. For more information, see:\\n\\n   - [client.add_model()](ref:clientadd_model)\\n\\n3. **Configure monitoring for this model**\\n\\n   You will need to configure bins and alerts for your model. These will be discussed in details below.\\n\\n4. **Send traffic from your live deployed model to Fiddler**\\n\\n   Use the Fiddler SDK to send us traffic from your live deployed model.\\n\\n## Publish events to Fiddler\\n\\nIn order to send traffic to Fiddler, use the [`publish_event`](https://api.fiddler.ai/#client-publish_event) API from the Fiddler SDK. Here is a sample of the API call:\\n\\n```python Publish Event\\nimport fiddler as fdl\\n\\tfiddler_api = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=token)\\n\\t# Publish an event\\n\\tfiddler_api.publish_event(\\n\\t\\tproject_id=\\'bank_churn\\',\\n\\t\\tmodel_id=\\'bank_churn\\',\\n\\t\\tevent={\\n\\t\\t\\t\"CreditScore\": 650,      # data type: int\\n\\t\\t\\t\"Geography\": \"France\",   # data type: category\\n\\t\\t\\t\"Gender\": \"Female\",\\n\\t\\t\\t\"Age\": 45,\\n\\t\\t\\t\"Tenure\": 2,\\n\\t\\t\\t\"Balance\": 10000.0,      # data type: float\\n\\t\\t\\t\"NumOfProducts\": 1,\\n\\t\\t\\t\"HasCrCard\": \"Yes\",\\n\\t\\t\\t\"isActiveMember\": \"Yes\",\\n\\t\\t\\t\"EstimatedSalary\": 120000,\\n\\t\\t\\t\"probability_churned\": 0.105,\\n      \"churn\": 1\\n\\t\\t},\\n\\t\\tevent_id=’some_unique_id’, #optional\\n\\t\\tupdate_event=False, #optional\\n\\t\\tevent_timestamp=1511253040519 #optional\\n\\t)\\n```\\n\\n\\n\\nThe `publish_event` API can be called in real-time right after your model inference. \\n\\n> 📘 Info\\n> \\n> You can also publish events as part of a batch call after the fact using the `publish_events_batch` API (click [here](https://api.fiddler.ai/#client-publish_events_batch) for more information). In this case, you will need to send Fiddler the original event timestamps as to accurately populate the time series charts.\\n\\nFollowing is a description of all the parameters for `publish_event`:\\n\\n- `project_id`: Project ID for the project this event belongs to.\\n\\n- `model_id`: Model ID for the model this event belongs to',\n",
       " '---\\ntitle: \"Product Tour\"\\nslug: \"product-tour\"\\nexcerpt: \"Here\\'s a tour of our product UI!\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:09:29.819Z\"\\nupdatedAt: \"2023-08-04T23:19:01.026Z\"\\n---\\n# Video Demo\\n\\nWatch the video to learn how Fiddler AI Observability provides data science and MLOps teams with a unified platform to monitor, analyze, explain, and improve machine learning models at scale, and build trust in AI.\\n\\n\\n[block:embed]\\n{\\n  \"html\": \"<iframe class=\\\\\"embedly-embed\\\\\" src=\\\\\"//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FPENnn3YUAcg&display_name=YouTube&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DPENnn3YUAcg&image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FPENnn3YUAcg%2Fhqdefault.jpg&key=7788cb384c9f4d5dbbdbeffd9fe4b92f&type=text%2Fhtml&schema=youtube\\\\\" width=\\\\\"854\\\\\" height=\\\\\"480\\\\\" scrolling=\\\\\"no\\\\\" title=\\\\\"YouTube embed\\\\\" frameborder=\\\\\"0\\\\\" allow=\\\\\"autoplay; fullscreen\\\\\" allowfullscreen=\\\\\"true\\\\\"></iframe>\",\\n  \"url\": \"https://www.youtube.com/watch?v=PENnn3YUAcg\",\\n  \"favicon\": \"https://www.google.com/favicon.ico\",\\n  \"image\": \"http://i.ytimg.com/vi/PENnn3YUAcg/hqdefault.jpg\",\\n  \"provider\": \"youtube.com\",\\n  \"href\": \"https://www.youtube.com/watch?v=PENnn3YUAcg\",\\n  \"typeOfEmbed\": \"youtube\"\\n}\\n[/block]\\n\\n\\n# Documented UI Tour\\n\\nWhen you log in to Fiddler, you are on the Home page and you can visualize monitoring information for your models across all your projects. \\n\\n- At the top of the page, you will see donut charts for the number of triggered alerts for [Performance](doc:performance-tracking-platform), [Data Drift](doc:data-drift-platform), and [Data Integrity](doc:data-integrity-platform). \\n- To the right of the donut charts, you will find the Bookmarks as well as a Recent Job Status card that lets you keep track of long-running async jobs and whether they have failed, are in progress, or successfully completed. \\n- The [Monitoring](doc:monitoring-ui) summary table displays your models across different [projects](doc:project-architecture) along with information on their traffic, drift, and the number of triggered alerts.  \\n  ![](https://files.readme.io/e959fe5-image.png)\\n\\nOn the navigation bar at the top, next to the Home Tab, is the [Projects](doc:project-structure) Tab. You can click on the Projects tab and it lands on a page that lists all your projects contained within Fiddler. See the [Fiddler Samples](doc:product-tour#fiddler-samples)  section below for more information on these projects. You can create new projects within the UI (by clicking the “Add Project” button) or via the [Fiddler Client](ref:about-the-fiddler-client).\\n\\n![](https://files.readme.io/8ffbd1b-image.png',\n",
       " '---\\ntitle: \"Single Sign On with Okta\"\\nslug: \"okta-integration\"\\nhidden: false\\ncreatedAt: \"2022-08-01T15:14:37.774Z\"\\nupdatedAt: \"2023-05-18T15:21:21.423Z\"\\n---\\n## Overview\\n\\nThese instructions will help administrators configure Fiddler to be used with an existing Okta single sign on application.\\n\\n## Okta Setup:\\n\\nFirst, you must create an OIDC based application within Okta. Your application will require a callback URL during setup time. This URL will be provided to you by a Fiddler administrator. Your application should grant \"Authorization Code\" permissions to a client acting on behalf of a user. See the image below for how your setup might look like:\\n\\n![](https://files.readme.io/b7b67fe-Screen_Shot_2022-08-07_at_10.22.36_PM.png)\\n\\nThis is the stage where you can allow certain users of your organization access to Fiddler through Okta. You can use the \"Group Assignments\" field to choose unique sets of organization members to grant access to. This setup stage will also allow for Role Based Access Control (i.e. RBAC) based on specific groups using your application.\\n\\nOnce your application has been set up, a Fiddler administrator will need to receive the following information and credentials:\\n\\n- Okta domain\\n- Client ID\\n- Client Secret\\n- Okta Account Type (default or custom)\\n\\nAll of the above can be obtained from your Okta application dashboard, as shown in the pictures below:\\n\\n![](https://files.readme.io/6442827-Screen_Shot_2022-08-07_at_10.30.03_PM.png)\\n\\n![](https://files.readme.io/f1dbcf6-Screen_Shot_2022-08-07_at_10.30.15_PM.png)\\n\\nYou can also pass the above information to your Fiddler administrator via your okta.yml file. \\n\\n## Logging into Fiddler:\\n\\nOnce a Fiddler administrator has successfully set up a deployment for your organization using your given Okta credentials, you should see the “Sign in with SSO” button enabled. When this button is clicked, you should be navigated to an Okta login screen. Once successfully authenticated, and assuming you have been granted access to Fiddler through Okta, you should be able to login to Fiddler.\\n\\n![](https://files.readme.io/c96a709-Screen_Shot_2022-08-07_at_10.36.40_PM.png)\\n\\nNOTES:\\n\\n1. To be able to login with SSO, it is initially required for the user to register with Fiddler Application. Upon successful registration, the users will be able to login using SSO.\\n2. The only information Fiddler stores from Okta based logins is a user’s first name, last name, email address, and OIDC token.\\n3. Fiddler does not currently support using Okta based login through its API (see fiddler-client). In order to use an Okta based account through Fiddler\\'s API, use a valid access token which can be created and copied on the “Credentials” tab on Fiddler’s “Settings” page.',\n",
       " 'slug: \"quick-start\"  following code block to get your URL.\\n\\n\\n```python\\nprint(\\'/\\'.join([URL, \\'projects\\', PROJECT_ID, \\'models\\', MODEL_ID, \\'monitor\\']))\\n```\\n\\n*Please allow 5-10 minutes for monitoring data to populate the charts.*\\n  \\nThe following screen will be available to you upon completion.\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/8.png\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n**What\\'s Next?**\\n\\nTry the [NLP Monitoring - Quickstart Notebook](https://docs.fiddler.ai/docs/simple-nlp-monitoring-quick-start)\\n\\n---\\n\\n\\n**Questions?**  \\n  \\nCheck out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\\n\\nJoin our [community Slack](http://fiddler-community.slack.com/) to ask any questions!\\n\\nIf you\\'re still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we\\'ll get back to you shortly.\\n',\n",
       " '---\\ntitle: \"Alerts with Fiddler UI\"\\nslug: \"alerts-ui\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:25:34.901Z\"\\nupdatedAt: \"2023-04-06T22:22:10.370Z\"\\n---\\nFiddler allows you to set up [alerts](https://docs.fiddler.ai/v1.6/docs/alerts-platform) for your model. View your alerts by clicking on the Alerts tab in the navigation bar. The Alerts tab presents three views: Triggered Alerts, Alert Rules, and Integrations. Users can set up alerts using both the Fiddler UI and the Fiddler API Client. This page introduces the available alert types, and how to set up and view alerts in the Fiddler UI. For instructions about how to use the Fiddler API client for alert configuration see [Alert Configuration with Fiddler Client](doc:alerts-client).\\n\\n![](https://files.readme.io/1730387-image.png)\\n\\n## Setting up Alert Rules\\n\\nTo create a new alert using the Fiddler UI, click the **Add Alert** button on the top-right corner of any screen on the Alerts tab. \\n\\n![](https://files.readme.io/78537d3-image.png)\\n\\nIn the Alert Rule form, provide the basic information such as the desired alert name, and the project and model of interest. \\n\\n![](https://files.readme.io/8418e4f-image.png)\\n\\nNext, select the Alert Type you would like to monitor. Users can select from Performance, Data Drift, Data Integrity, or Traffic monitors. For this example, we\\'ll set up a Data Drift alert to measure distribution drift.\\n\\n![](https://files.readme.io/d51ca30-image.png)\\n\\nOnce an Alert Type is selected, users can choose a metric corresponding to the Alert Type for which to set the alert on. For our Data Drift alert, we will use JSD (Jensen–Shannon distance) as our metric. The next consideration are the bin size, which is the duration for which fiddler monitoring calculates the metric values, and the column to apply this monitor on. Let\\'s choose a 1 hour bin and the CreditScore column for this example. \\n\\n![](https://files.readme.io/9ba686e-image.png)\\n\\nNext, users can focus on the alerts comparison method. Learn more about Alert comparisons on the [Alerts Platform Guide](https://docs.fiddler.ai/v1.6/docs/alerts-platform). For our example we will select the Relative comparison option, and compare to the same time 7 days back. Users can select the alert condition as well as a Warning and Critical threshold. We will ask for an alert when the production data is greater than 10%.\\n\\n![](https://files.readme.io/cb3f4b0-image.png)\\n\\nFinally user can set the alert rules priority- how important this alert is to a customers work streams, along with how to get notified of triggered alerts. \\n\\n![](https://files.readme.io/0e75a9e-image.png)\\n\\n Last, click **Add Alert Rule** when you\\'re done. In order to create and configure alerts using the Fiddler API client see [Alert Configuration with Fiddler Client](https://docs.fiddler.ai/v1.5/docs/fiddler-ui).\\n\\n![](https://files.readme.io/72a1e8b-image.png)\\n\\n### Alert Rules Tab\\n\\nOnce an alert rule is created it can be viewed in the **Alert Rules** tab. This view enables you to view all alert',\n",
       " 'slug: \"analytics-ui\"  project dashboard, which can be shared with others.\\n\\nTo pin a chart, click on the thumbtack icon and click **Send**. If the **Update with Query** option is enabled, the pinned chart will update automatically whenever the underlying query is changed on the **Analyze** tab.\\n\\n![](https://files.readme.io/c4247d1-Pinning_Chart.png \"Pinning_Chart.png\")\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Retrieving Events\"\\nslug: \"retrieving-events\"\\nhidden: false\\ncreatedAt: \"2022-07-06T16:22:23.142Z\"\\nupdatedAt: \"2022-07-06T16:23:39.398Z\"\\n---\\nAfter publishing events to Fiddler, you may want to retrieve them for further analysis.\\n[block:api-header]\\n{\\n  \"title\": \"Querying production data\"\\n}\\n[/block]\\nYou can query production data from the **Analyze** tab by issuing the following SQL query to Fiddler.\\n\\n```sql\\nSELECT\\n    *\\nFROM\\n    \"production.MODEL_ID\"\\n```\\n\\nThe above query will return the entire production table (all published events) for a model with a model ID of `MODEL_ID`.\\n\\n[block:api-header]\\n{\\n  \"title\": \"Querying a baseline dataset\"\\n}\\n[/block]\\nYou can query a baseline dataset that has been uploaded to Fiddler with the following SQL query.\\n\\n```sql\\nSELECT\\n    *\\nFROM\\n    \"DATASET_ID.MODEL_ID\"\\n```\\n\\nHere, this will return the entire baseline dataset that has been uploaded with an ID of `DATASET_ID` to a model with an ID of `MODEL_ID`.',\n",
       " '---\\ntitle: \"ML Algorithms In Fiddler\"\\nslug: \"ds\"\\nhidden: true\\ncreatedAt: \"2022-11-18T22:11:48.747Z\"\\nupdatedAt: \"2022-11-18T22:12:58.704Z\"\\n---\\n',\n",
       " '---\\ntitle: \"Datadog Integration\"\\nslug: \"datadog-integration\"\\nhidden: false\\ncreatedAt: \"2023-06-21T15:21:52.559Z\"\\nupdatedAt: \"2023-06-21T15:51:15.017Z\"\\n---\\nFiddler offers an integration with Datadog which allows Fiddler and Datadog customers to bring their AI Observability metrics from Fiddler into their centralized Datadog dashboards.  Additionally, a Fiddler license can now be procured through the [Datadog Marketplace](https://www.datadoghq.com/blog/tag/datadog-marketplace/). This [integration](https://www.datadoghq.com/blog/monitor-machine-learning-models-fiddler/) enables you to centralize your monitoring of ML models and the applications that utilize them within one unified platform.\\n\\n## Integrating Fiddler with Datadog\\n\\nInstructions for integrating Fiddler with Datadog can be found on the \"Integrations\" section of your Datadog console.  Simply search for \"Fiddler\" and follow the installation instructions provided on the \"Configure\" tab.  Please reach out to [support@fiddler.ai](mailto:support@fiddler.ai) with any issues or questions.\\n\\n![](https://files.readme.io/3f9dcd9-Screenshot_2023-06-21_at_10.28.17_AM.png)\\n\\n![](https://files.readme.io/9fa9503-Screenshot_2023-06-21_at_10.31.54_AM.png)\\n\\n![](https://files.readme.io/218dfc2-Screenshot_2023-06-21_at_10.45.14_AM.png)',\n",
       " '---\\ntitle: \"Supported Browsers\"\\nslug: \"supported-browsers\"\\nexcerpt: \"Platform Guide\"\\nhidden: false\\ncreatedAt: \"2023-01-10T22:16:01.134Z\"\\nupdatedAt: \"2023-01-10T22:16:17.735Z\"\\n---\\nFiddler Product can be accessed through the following supported web browsers:\\n\\n- Google Chrome\\n- Firefox\\n- Safari\\n- Microsoft Edge',\n",
       " 'slug: \"explainability-with-model-artifact-quickstart-notebook\" /python/python-39:1.1.0\",  \\n                                    cpu=100,\\n                                    memory=256,\\n                                    replicas=1)\\n```\\n\\n### Finally, upload the model package directory\\n\\nOnce the model\\'s artifact is in the */model* directory along with the **pacakge.py** file and requirments.txt the model package directory can be uploaded to Fiddler.\\n\\n\\n```python\\nclient.add_model_artifact(model_dir=\\'model/\\', project_id=PROJECT_ID, model_id=MODEL_ID, deployment_params = DEPLOYMENT_PARAMETERS)\\n```\\n\\nWithin your Fiddler environment\\'s UI, you should now be able to see the newly created model.\\n\\n# 4. Publish production events\\n\\nYour model artifact is uploaded.  Now it\\'s time to start publishing some production data! \\n\\nFiddler will **monitor this data and compare it to your baseline to generate powerful insights into how your model is behaving**.  \\n\\nWith the model artifact available to Fiddler, **high-fidelity explanations are also avaialbe**.\\n\\n\\n---\\n\\n\\nEach record sent to Fiddler is called **an event**.  An event is just **a dictionary that maps column names to column values**.\\n  \\nLet\\'s load in some sample events from a CSV file.  Then we can create an artificial timestamp for the events and publish them to fiddler one by one in a streaming fashion using the Fiddler client\\'s [publish_event](https://docs.fiddler.ai/reference/clientpublish_event) function.\\n\\n\\n```python\\nPATH_TO_EVENTS_CSV = \\'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/hawaii_drift_demo_large.csv\\'\\n\\nevent_log = pd.read_csv(PATH_TO_EVENTS_CSV)\\nevent_log\\n```\\n\\n\\n```python\\nNUM_EVENTS_TO_SEND = 11500\\n\\nFIVE_MINUTES_MS = 300000\\nONE_DAY_MS = 8.64e+7\\nNUM_DAYS_BACK_TO_START=39 #set the start of the event data publishing this many days in the past\\nstart_date = round(time.time() * 1000) - (ONE_DAY_MS * NUM_DAYS_BACK_TO_START) \\nprint(datetime.datetime.fromtimestamp(start_date/1000.0))\\n```\\n\\n\\n```python\\ndef event_generator_df():\\n    for ind, row in event_log.iterrows():\\n        event_dict = dict(row)\\n        event_id = event_dict.pop(\\'event_id\\')\\n        event_time = start_date + ind * FIVE_MINUTES_MS #publish an event every FIVE_MINUTES_MS\\n        yield event_id, event_dict, event_time\\n        \\nevent_queue_df = event_generator_df()\\n\\ndef get_next_event_df():\\n    return next(event_queue_df)\\n```\\n\\n\\n```python\\nfor ind in range(NUM_EVENTS_TO_SEND):\\n    event_id_tmp, event_dict, event_time = get_next_event_df()\\n   \\n    result = client.publish_event(PROJECT_ID,\\n                                  MODEL_ID,\\n                                  event_dict,\\n                                  event_timestamp=event_time,\\n                                  event_id= event_id_tmp,\\n                                  update_event= False)\\n    \\n    readable_timestamp = datetime.datetime.fromtimestamp(event_time/1000.0)\\n    clear_output(wait = True)\\n    \\n    print(f\\'Sending {ind+1} / {NUM_EVENTS_TO_SEND} \\\\n{readable_timestamp} UTC: \\\\n{event_dict}\\')\\n    time.sleep(0.001)\\n```\\n\\n# 5. Get insights\\n\\n**You\\'re all done!**\\n  \\nNow just head to your Fiddler environment\\'s UI and start getting enhanced monitoring, analytics, and explainability.\\n\\nRun the following code block to get your URL.\\n\\n\\n```python\\nprint(\\'/\\'.join([URL, \\'projects\\', PROJECT_ID, \\'',\n",
       " '---\\ntitle: \"CV Monitoring\"\\nslug: \"cv-monitoring\"\\nexcerpt: \"Quickstart Notebook\"\\nhidden: false\\ncreatedAt: \"2023-01-31T19:44:34.862Z\"\\nupdatedAt: \"2023-03-07T21:39:35.954Z\"\\n---\\nThis guide will walk you through the basic steps required to use Fiddler for monitoring computer vision (CV) models. In this notebook we demonstrate how to detect drift in image data using model embeddings using Fiddler\\'s unique Vector Monitoring approach.\\n\\nClick the following link to get started using Google Colab:\\n\\n<div class=\"colab-box\">\\n    <a href=\"https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/Fiddler_Quickstart_Image_Monitoring.ipynb\" target=\"_blank\">\\n        <div>\\n            Open in Google Colab →\\n        </div>\\n    </a>\\n    <div>\\n            <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" />\\n    </div>\\n</div># Monitoring Image data using Fiddler Vector Monotoring\\n\\nIn this notebook we present the steps for monitoring images. Fiddler employs a vector-based monitoring approach that can be used to monitor data drift in high-dimensional data such as NLP embeddings, images, video etc. In this notebook we demonstrate how to detect drift in image data using model embeddings.\\n\\nFiddler is the pioneer in enterprise Model Performance Management (MPM), offering a unified platform that enables Data Science, MLOps, Risk, Compliance, Analytics, and LOB teams to **monitor, explain, analyze, and improve ML deployments at enterprise scale**. \\nObtain contextual insights at any stage of the ML lifecycle, improve predictions, increase transparency and fairness, and optimize business revenue.\\n\\n---\\n\\nYou can experience Fiddler\\'s Image monitoring ***in minutes*** by following these quick steps:\\n\\n1. Connect to Fiddler\\n2. Load and generate embeddings for CIFAR-10 dataset\\n3. Upload the vectorized baseline dataset\\n4. Add metadata about your model \\n5. Inject data drift and publish production events\\n6. Get insights\\n\\n## Imports\\n\\n\\n```python\\n!pip install torch==2.0.0\\n!pip install torchvision==0.15.1\\n!pip install -q fiddler-client\\n```\\n\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nimport random\\nimport time\\nimport torch\\nimport torch.nn as nn\\nimport torchvision.transforms as transforms\\nfrom torchvision.models import resnet18, ResNet18_Weights\\nimport torchvision\\nimport requests\\n\\nimport fiddler as fdl\\nprint(f\"Running Fiddler client version {fdl.__version__}\")\\n```\\n\\n# 1. Connect to Fiddler\\n\\nBefore you can add information about your model with Fiddler, you\\'ll need to connect using our API client.\\n\\n---\\n\\n**We need a few pieces of information to get started.**\\n1. The URL you\\'re using to connect to Fiddler\\n2. Your organization ID\\n3. Your authorization token\\n\\nThe latter two of these can be found by pointing your browser to your Fiddler URL and navigating to the **Settings** page.\\n\\n\\n```python\\nURL = \\'\\'  # Make sure to include the full URL (including https://).\\nORG_ID = \\'\\'\\nAUTH_TOKEN = \\'\\'\\n```\\n\\nNow just run the following code block to connect to the Fiddler API!\\n\\n\\n```python\\nclient = fdl.FiddlerApi(\\n    url=URL,\\n    org_id=ORG_ID,\\n',\n",
       " '---\\ntitle: \"Administration\"\\nslug: \"administration-platform\"\\nhidden: false\\ncreatedAt: \"2022-11-15T18:09:04.719Z\"\\nupdatedAt: \"2023-02-03T20:49:07.153Z\"\\n---\\n## Organization Roles\\n\\nFiddler access control comes with some preset roles. There are two global roles at the organizational level \\n\\n- **_ADMINISTRATOR_** — Has complete access to every aspect of the organization.\\n  - As an administrator, you can [invite users](doc:inviting-users) to the platform.\\n- **_MEMBER_** — Access is assigned at the project and model level.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/0fbfca7-roles.png\",\\n        \"roles.png\",\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"550px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n## Project Roles\\n\\nEach project supports its own set of permissions for its users.\\n\\nThere are three roles that can be assigned:\\n\\n- **_OWNER_** — Assigns super-user permissions to the user.\\n- **_WRITE_** — Allows a user to perform write operations (e.g. uploading datasets and/or models, using slice and explain, sending events to Fiddler for monitoring, etc).\\n- **_READ_** — Allows a user to perform read operations (e.g. getting project/dataset/model metadata, accessing pre-existing charts, etc.).\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/3b07b46-project_roles.png\",\\n        \"project_roles.png\",\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"550px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n**Some notes about these roles:**\\n\\n- A user who creates a project is assigned the **OWNER** role by default.\\n- A project **OWNER** or an organization **ADMINISTRATOR** can share/unshare projects with other users or teams.\\n- Only the **OWNER** only and an organization **ADMINISTRATOR** have access to a project until that project is explicitly shared with others.\\n- Project roles can be assigned to individual users or teams by the project  \\n  **OWNER** or by an organization **ADMINISTRATOR**.\\n\\n## Teams\\n\\nA team is a group of users.\\n\\n- Each user can be a member of zero or more teams.\\n- Team roles are associated with project roles (i.e. teams can be granted  \\n  **READ**, **WRITE**, and/or **OWNER** permissions for a project).\\n\\nClick [here](doc:settings#teams) for more information on teams.\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_',\n",
       " '---\\ntitle: \"Uploading model artifacts\"\\nslug: \"uploading-model-artifacts\"\\nexcerpt: \"Upload a model artifact in Fiddler\"\\nhidden: false\\ncreatedAt: \"2023-02-01T16:04:40.181Z\"\\nupdatedAt: \"2023-03-08T21:10:00.091Z\"\\n---\\nBefore uploading your model artifact into Fiddler, you need to add the model with [client.add_model](ref:clientadd_model).\\n\\nOnce you have prepared the [model artifacts directory](doc:artifacts-and-surrogates), you can upload your model using [client.add_model_artifact](ref:clientadd_model_artifact)\\n\\n```python\\nPROJECT_ID = \\'example_project\\'\\nMODEL_ID = \\'example_model\\'\\nMODEL_ARTIFACTS_DIR = Path(\\'model/\\')\\n\\nclient.add_model_artifact(\\n    model_dir=MODEL_ARTIFACTS_DIR,\\n    project_id=PROJECT_ID,\\n    model_id=MODEL_ID\\n)\\n```',\n",
       " 'slug: \"simple-nlp-monitoring-quick-start\"  adding timestamps\\n    )\\n```\\n\\n# 7. Get insights\\n\\n\\n**You\\'re all done!**\\n  \\nYou can now head to Fiddler URL and start getting enhanced observability into your model\\'s performance. Run the following code block to get your URL:\\n\\n\\n\\n\\n```python\\nprint(\\'/\\'.join([URL, \\'projects\\', PROJECT_ID, \\'models\\', MODEL_ID, \\'monitor\\']))\\n```\\n\\nIn particular, you can go to the monitoring tab in your Fiddler URL and check the resulting drift chart for the TF-IDF embedding vectors. Bellow is a sceernshot of the data drift chart after running this notebook on the [Fiddler demo](https://demo.fiddler.ai/) deployment. (Annotation bubbles are not generated by the Fiddler UI.)\\n\\n\\n--------\\n**Questions?**  \\n  \\nCheck out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\\n\\nJoin our [community Slack](http://fiddler-community.slack.com/) to ask any questions!\\n\\nIf you\\'re still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we\\'ll get back to you shortly.\\n\\n\\n```python\\n\\n```\\n',\n",
       " 'slug: \"welcome\" -setup\\\\\">\\\\n    <div>\\\\n      <h2 class=\\\\\"index-title\\\\\">Client Guide</h2>\\\\n      <p>For using Fiddler client including API references and code examples.</p>\\\\n    </div>\\\\n  </a>\\\\n\\\\n  <a class=\\\\\"index-item\\\\\" href=\\\\\"https://docs.fiddler.ai/v1.7/docs/deploying-fiddler\\\\\">\\\\n  \\\\t<div>\\\\n      <h2 class=\\\\\"index-title\\\\\">Deployment Guide</h2>\\\\n      <p>For technical details on how to deploy Fiddler on your premises or ours.</p>\\\\n  \\\\t</div>\\\\n  </a>\\\\n</div>\\\\n\"\\n}\\n[/block]\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Kafka Integration\"\\nslug: \"kafka-integration\"\\nhidden: false\\ncreatedAt: \"2023-05-23T16:48:07.206Z\"\\nupdatedAt: \"2023-05-23T19:31:26.107Z\"\\n---\\nFiddler Kafka connector is a service that connects to a [Kafka topic](https://kafka.apache.org/documentation/#intro_concepts_and_terms) containing production events for a model, and publishes the events to Fiddler.\\n\\n## Pre-requisites\\n\\nWe assume that the user has an account with Fiddler, has already created a project, uploaded a dataset and onboarded a model. We will need the [url_id, org_id,](doc:client-setup) project_id and model_id to configure the Kafka connector.\\n\\n## Installation\\n\\nThe Kafka connector runs on Kubernetes within the customer’s environment. It is packaged as a Helm chart. To install:\\n\\n```shell\\nhelm repo add fiddler https://helm.fiddler.ai/stable/\\n\\nhelm repo update\\n\\nkubectl -n kafka create secret generic fiddler-credentials --from-literal=auth=<API-KEY>\\n\\nhelm install fiddler-kafka fiddler/fiddler-kafka \\\\\\n    --devel \\\\\\n    --namespace kafka \\\\\\n    --set fiddler.url=https://<FIDDLER-URL> \\\\\\n    --set fiddler.org=<ORG> \\\\\\n    --set fiddler.project_id=<PROJECT-ID> \\\\\\n    --set fiddler.model_id=<MODEL-ID> \\\\\\n    --set fiddler.ts_field=timestamp \\\\\\n    --set fiddler.ts_format=INFER \\\\\\n    --set kafka.host=kafka \\\\\\n    --set kafka.port=9092 \\\\\\n    --set kafka.topic=<KAFKA-TOPIC> \\\\\\n    --set kafka.security_protocol=SSL \\\\\\n    --set kafka.ssl_cafile=cafile \\\\\\n    --set kafka.ssl_certfile=certfile \\\\\\n    --set kafka.ssl_keyfile=keyfile \\\\\\n    --set-string kafka.ssl_check_hostname=False\\n\\n```\\n\\nThis creates a deployment that reads events from the Kafka topic and publishes it to the configured model. The deployment can be scaled as needed. However, if the Kafka topic is not partitioned, scaling will not result in any gains.\\n\\n## Limitations\\n\\n1. The connector assumes that there is a single dedicated topic containing production events for a given model. Multiple deployments can be created, one for each model, and scaled independently.\\n2. The connector assumes that events are published as JSON serialized dictionaries of key-value pairs. Support for other formats can be added on request. As an example, a Kafka message should look like the following:\\n\\n```json\\n{\\n    “feature_1”: 20.7,\\n    “feature_2”: 45000,\\n    “feature_3”: true,\\n    “output_column”: 0.79,\\n    “target_column”: 1,\\n    “ts”: 1637344470000,\\n}\\n\\n```',\n",
       " 'slug: \"quick-start\" ** page.\\n\\n<table>\\n    <tr>\\n        <td><img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/1.png\" /></td>\\n        <td><img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/2.png\" /></td>\\n    </tr>\\n    <tr>\\n        <td><img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/3.png\" /></td>\\n        <td><img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/4.png\" /></td>\\n    </tr>\\n</table>\\n\\n\\n```python\\nORG_ID = \\'\\'\\nAUTH_TOKEN = \\'\\'\\n```\\n\\nNow just run the following code block to connect to the Fiddler API!\\n\\n\\n```python\\nclient = fdl.FiddlerApi(url=URL, org_id=ORG_ID, auth_token=AUTH_TOKEN)\\n```\\n\\nOnce you connect, you can create a new project by specifying a unique project ID in the client\\'s `create_project` function.\\n\\n\\n```python\\nPROJECT_ID = \\'\\'\\n\\nclient.create_project(PROJECT_ID)\\n```\\n\\nYou should now be able to see the newly created project on the UI.\\n\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/5.png\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n## 3. Upload a baseline dataset\\n\\nIn this example, we\\'ll be considering the case where we\\'re a bank and we have **a model that predicts churn for our customers**.  \\nWe want to know when our model\\'s predictions start to drift—that is, **when churn starts to increase** within our customer base.\\n  \\nIn order to get insights into the model\\'s performance, **Fiddler needs a small  sample of data that can serve as a baseline** for making comparisons with data in production.\\n\\n\\n---\\n\\n\\n*For more information on how to design a baseline dataset, [click here](https://docs.fiddler.ai/docs/designing-a-baseline-dataset).*\\n\\n\\n```python\\nPATH_TO_BASELINE_CSV = \\'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/churn_baseline.csv\\'\\n\\nbaseline_df = pd.read_csv(PATH_TO_BASELINE_CSV)\\nbaseline_df\\n```\\n\\nFiddler uses this baseline dataset to keep track of important information about your data.\\n  \\nThis includes **data types**, **data ranges**, and **unique values** for categorical variables.\\n\\n---\\n\\nYou can construct a `DatasetInfo` object to be used as **a schema for keeping track of this information** by running the following code block.\\n\\n\\n```python\\ndataset_info = fdl.DatasetInfo.from_dataframe(baseline_df, max_inferred_cardinality=100)\\ndataset_info\\n```\\n\\nThen use the client\\'s [upload_dataset](https://docs.fiddler.ai/reference/clientupload_dataset) function to send this information to Fiddler!\\n  \\n*Just include:*\\n1. A unique dataset ID\\n2. The baseline dataset as a pandas DataFrame\\n3. The [DatasetInfo](https://docs.fiddler.ai/reference/fdldatasetinfo) object you just created\\n\\n\\n```python\\nDATASET_ID = \\'churn_data\\'\\n\\nclient.upload_dataset(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID,\\n    dataset={\\n        \\'baseline\\': baseline_df\\n    },\\n    info=dataset_info\\n)\\n```\\n\\nIf you click on',\n",
       " '---\\ntitle: \"Routing to Fiddler (on-prem)\"\\nslug: \"routing-to-fiddler-on-prem\"\\nhidden: false\\ncreatedAt: \"2022-09-06T21:46:51.548Z\"\\nupdatedAt: \"2022-09-07T11:09:41.093Z\"\\n---\\nFiddler supports a wide range of strategies for routing HTTP traffic from end users to the Fiddler system. A typical on-prem Fiddler deployment includes an HTTP reverse proxy (Envoy) that can be configured as needed to meet your routing needs.\\n\\n![](https://files.readme.io/fd4b216-image.png)\\n\\nThe diagram above shows some of the deployment configuration options related to routing and TLS, described below. Once Fiddler is installed in your on-prem environment, you may need to take  additional steps to route TCP traffic to the Fiddler Envoy service.\\n\\n# TLS termination\\n\\nBy default, Fiddler does not perform TLS termination. We find that our customers generally have excellent opinions about how TLS should be terminated, and generally prefer to perform TLS termination using their own network machinery.\\n\\n## Terminate TLS outside of Fiddler\\n\\nIn a typical production environment, TLS termination will occur outside of Fiddler. Clear HTTP traffic should then be routed to the Fiddler Envoy service at the port specified by `envoy.publicHttpPort`. \\n\\n```\\nenvoy:\\n  terminateTLS: false\\n  publicHttpPort: \"80\"\\n```\\n\\n## Terminate TLS within Fiddler\\n\\nFiddler can be configured to perform TLS termination using an X509 server certificate and corresponding PKCS #8 private key. The TLS certificate must be valid for the FQDN via which end-users will access the Fiddler platform. Both the server certificate and private key must be available in DER format, and should be placed in a `Secret` within the namespace where Fiddler will be deployed prior to installation. For example:\\n\\n```\\nkubectl create secret tls my-tls-secret \\\\\\n    --cert=path/to/the/cert.pem \\\\\\n    --key=path/to/the/cert.key\\n```\\n\\n\\n\\nThe Fiddler Helm chart should be configured to reflect the `Secret` containing the server cert and key. TCP traffic should be routed to the port specified by `envoy.publicHttpsPort`.\\n\\n```yaml\\nenvoy:\\n  terminateTLS: true\\n  tlsSecretName: my-tls-secret\\n  serverCertKey: tls.crt\\n  privateKeyKey: tls.key\\n  publicHttpsPort: \"443\"\\n```\\n\\n\\n\\n## TLS with Ingress\\n\\nKubernetes `Ingress` [supports](https://kubernetes.io/docs/concepts/services-networking/ingress/#tls) specifying a TLS secret on a per-ingress basis. If using an `Ingress` to route traffic to Fiddler, create a `Secret` containing the DER-formatted X509 server certificate and PKCS #8 private key in the namespace where Fiddler will be deployed:\\n\\n```\\nkubectl create secret tls my-tls-secret \\\\\\n    --cert=path/to/the/cert.pem \\\\\\n    --key=path/to/the/cert.key\\n```\\n\\n\\n\\nThe Fiddler Helm chart should be configured to enable Ingress with TLS. For example:\\n\\n```\\nenvoy:\\n  createIngress: true\\n\\ningress:\\n  tls:\\n    hosts:\\n      # The FQDN where Fiddler is accessed by end users.\\n      - fiddler.acme.com\\n    secretName: my-tls-secret\\n```\\n\\n\\n\\n# Ingress\\n\\nIf the cluster where Fiddler is installed supports `Ingress`, the Fiddler Helm chart',\n",
       " '---\\ntitle: \"client.publish_event\"\\nslug: \"clientpublish_event\"\\nexcerpt: \"Publishes a single production event to Fiddler asynchronously.\"\\nhidden: false\\ncreatedAt: \"2022-05-23T19:53:24.116Z\"\\nupdatedAt: \"2023-03-09T15:54:04.906Z\"\\n---\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"Input Parameter\",\\n    \"h-1\": \"Type\",\\n    \"h-2\": \"Default\",\\n    \"h-3\": \"Description\",\\n    \"0-0\": \"project_id\",\\n    \"0-1\": \"str\",\\n    \"0-2\": \"None\",\\n    \"0-3\": \"The unique identifier for the project.\",\\n    \"1-0\": \"model_id\",\\n    \"1-1\": \"str\",\\n    \"1-2\": \"None\",\\n    \"1-3\": \"A unique identifier for the model. Must be a lowercase string between 2-30 characters containing only alphanumeric characters and underscores. Additionally, it must not start with a numeric character.\",\\n    \"2-0\": \"event\",\\n    \"2-1\": \"dict\",\\n    \"2-2\": \"None\",\\n    \"2-3\": \"A dictionary mapping field names to field values. Any fields found that are not present in the model\\'s **ModelInfo** object will be dropped from the event.\",\\n    \"3-0\": \"event_id\",\\n    \"3-1\": \"Optional [str]\",\\n    \"3-2\": \"None\",\\n    \"3-3\": \"A unique identifier for the event. If not specified, Fiddler will generate its own ID, which can be retrived using the **get_slice** API.\",\\n    \"4-0\": \"update_event\",\\n    \"4-1\": \"Optional [bool]\",\\n    \"4-2\": \"None\",\\n    \"4-3\": \"If True, will only modify an existing event, referenced by event_id. If no event is found, no change will take place.\",\\n    \"5-0\": \"event_timestamp\",\\n    \"5-1\": \"Optional [int]\",\\n    \"5-2\": \"None\",\\n    \"5-3\": \"The name of the  timestamp input field for when the event took place. The format of this timestamp is given by _timestamp_format_. If no timestamp input is provided, the current time will be used.\",\\n    \"6-0\": \"timestamp_format\",\\n    \"6-1\": \"Optional [fdl.FiddlerTimestamp]\",\\n    \"6-2\": \"fdl.FiddlerTimestamp.INFER\",\\n    \"6-3\": \"The format of the timestamp passed in _event_timestamp_. Can be one of  \\\\n- fdl.FiddlerTimestamp.INFER  \\\\n- fdl.FiddlerTimestamp.EPOCH_MILLISECONDS  \\\\n- fdl.FiddlerTimestamp.EPOCH_SECONDS  \\\\n- fdl.FiddlerTimestamp.ISO_8601\",\\n    \"7-0\": \"casting_type\",\\n    \"7-1\": \"Optional [bool]\",\\n    \"7-2\": \"False\",\\n    \"7-3\": \"If True, will try to cast the data in event to be in line with the data types defined in the model\\'s **ModelInfo** object.\",\\n    \"8-0\": \"dry_run\",\\n    \"8-1\": \"Optional [bool]\",\\n    \"8-2\": \"False\",\\n    \"8-3\": \"If True, the event will not be published, and instead a report will be',\n",
       " 'slug: \"alerts-platform\" [](https://files.readme.io/9dfc566-Monitor_Alert_Email_0710.png \"Monitor_Alert_Email_0710.png\")\\n\\n## Integrations\\n\\nFiddler supports the following alert notification integrations:\\n\\n- Email\\n- Slack\\n- PagerDuty\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"client.create_project\"\\nslug: \"clientcreate_project\"\\nexcerpt: \"Creates a project using the specified ID.\"\\nhidden: false\\ncreatedAt: \"2022-05-23T16:21:29.485Z\"\\nupdatedAt: \"2022-06-21T17:23:13.179Z\"\\n---\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"Input Parameters\",\\n    \"0-0\": \"project_id\",\\n    \"h-1\": \"Type\",\\n    \"h-2\": \"Default\",\\n    \"h-3\": \"Description\",\\n    \"0-1\": \"str\",\\n    \"0-2\": \"None\",\\n    \"0-3\": \"A unique identifier for the project. Must be a lowercase string between 2-30 characters containing only alphanumeric characters and underscores. Additionally, it must not start with a numeric character.\"\\n  },\\n  \"cols\": 4,\\n  \"rows\": 1\\n}\\n[/block]\\n\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"PROJECT_ID = \\'example_project\\'\\\\n\\\\nclient.create_project(\\\\n    project_id=PROJECT_ID\\\\n)\",\\n      \"language\": \"python\",\\n      \"name\": \"Usage\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"Return Type\",\\n    \"0-0\": \"dict\",\\n    \"h-1\": \"Description\",\\n    \"0-1\": \"A dictionary mapping project_name to the project ID string specified, once the project is successfully created.\"\\n  },\\n  \"cols\": 2,\\n  \"rows\": 1\\n}\\n[/block]\\n\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"{\\\\n    \\'project_name\\': \\'example_project\\'\\\\n}\",\\n      \"language\": \"python\",\\n      \"name\": \"Response\"\\n    }\\n  ]\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Welcome to Fiddler\\'s Documentation!\"\\nslug: \"welcome\"\\nexcerpt: \"This is Fiddler’s AI Observability Platform Documentation. Fiddler is a pioneer in AI Observability for responsible AI. Data Science, MLOps, and LOB teams use Fiddler to monitor, explain, analyze, and improve ML models, generative AI models, and AI applications.\"\\nhidden: false\\nmetadata: \\n  title: \"Fiddler Documentation\"\\n  description: \"This is Fiddler\\'s Model Performance Management Platform Documentation. Fiddler is a pioneer in enterprise Model Performance Management. Data Science, MLOps, and business teams use Fiddler to monitor, explain, analyze, and improve their models and build trust into AI.\"\\ncreatedAt: \"2023-02-27T18:08:02.575Z\"\\nupdatedAt: \"2023-08-04T23:18:40.327Z\"\\n---\\nHere you can find a number of helpful guides to aid with onboarding. These include:\\n\\n[block:html]\\n{\\n  \"html\": \"<style>\\\\n  .index-container {\\\\n      display: grid;\\\\n      grid: auto / 50% 50%;\\\\n      grid-gap: 20px;\\\\n      max-width: 97.5%;\\\\n  }\\\\n  .index-container .index-item {\\\\n    padding: 20px;\\\\n    border: 1px solid #CCCCCC;\\\\n    border-radius: 5px;\\\\n    grid-gap: 15px;\\\\n    \\\\n}\\\\n.index-item{\\\\n  text-decoration: none !important;\\\\n  color: #000000;\\\\n }\\\\n.index-item:hover{\\\\n  color: #000000;\\\\n  border-color: #1A5EF3;\\\\n  -webkit-box-shadow: 0 2px 4px rgb(0 0 0 / 10%);\\\\n  -moz-box-shadown: 0 2px 4px rgb(0 0 0 / 10%);\\\\n  box-shadow: 0 2px 4px rgb(0 0 0 / 10%);\\\\n } \\\\n  \\\\n.index-title {\\\\n    font-size: 20px !important;\\\\n    color: #111111;\\\\n    margin-top: 0px !important;\\\\n    margin-bottom: 20px;\\\\n}\\\\n@media only screen and (max-width: 420px){\\\\n  .index-container {\\\\n    grid: auto / 100%;\\\\n  }\\\\n}\\\\n  </style>\\\\n<div class=\\\\\"index-container\\\\\">\\\\n  <a class=\\\\\"index-item\\\\\" href=\\\\\"https://docs.fiddler.ai/v1.7/docs/administration-platform\\\\\">\\\\n    <div>\\\\n\\\\t\\\\t\\\\t<h2 class=\\\\\"index-title\\\\\">Platform Guide</h2>\\\\n    \\\\t<p>How Fiddler does AI Observability and Fiddler-specific terminologies.</p>\\\\n \\\\t\\\\t</div>\\\\n  </a>\\\\n\\\\n  <a class=\\\\\"index-item\\\\\" href=\\\\\"https://docs.fiddler.ai/v1.7/docs/administration-ui\\\\\">\\\\n    <div>\\\\n      <h2 class=\\\\\"index-title\\\\\">User Interface (UI) Guide</h2>\\\\n      <p>An introduction to the product UI with screenshots that illustrate how to interface with the product.</p>\\\\n    </div>\\\\n  </a>\\\\n\\\\n  <a class=\\\\\"index-item\\\\\" href=\\\\\"https://docs.fiddler.ai/v1.7/docs/installation-and',\n",
       " '---\\ntitle: \"client.delete_project\"\\nslug: \"clientdelete_project\"\\nexcerpt: \"Deletes a specified project.\"\\nhidden: false\\ncreatedAt: \"2022-05-23T16:24:42.097Z\"\\nupdatedAt: \"2023-06-07T17:34:29.882Z\"\\n---\\n| Input Parameters | Type | Default | Description                            |\\n| :--------------- | :--- | :------ | :------------------------------------- |\\n| project_id       | str  | None    | The unique identifier for the project. |\\n\\n```python Usage\\nPROJECT_ID = \\'example_project\\'\\n\\nclient.delete_project(\\n    project_id=PROJECT_ID\\n)\\n```\\n\\n| Return Type | Description                                         |\\n| :---------- | :-------------------------------------------------- |\\n| bool        | A boolean denoting whether deletion was successful. |\\n\\n```python Response\\nTrue\\n```\\n\\n\\n\\n> 🚧 Caution\\n> \\n> You cannot delete a project without deleting the datasets and the models associated with that project.',\n",
       " 'slug: \"pagerduty\"  you wish to use with PagerDuty.\\n\\n![](https://files.readme.io/d9ad82e-pagerduty_fiddler_1.png \"pagerduty_fiddler_1.png\")\\n\\n\\n\\n2. Select **Monitor** → **Alerts** → **Add Alert**.\\n\\n![](https://files.readme.io/b7118f0-pagerduty_fiddler_2.png \"pagerduty_fiddler_2.png\")\\n\\n\\n\\n3. Enter the condition you would like to alert on, and under **PagerDuty Services**, select all services you would like the alert to trigger for. Additionally, select the **Severity** of this alert, and hit **Save**.\\n\\n![](https://files.readme.io/8fbffde-pagerduty_fiddler_3.png \"pagerduty_fiddler_3.png\")\\n\\n\\n\\n4. After creation, the alert will now trigger for the specified PagerDuty services.\\n\\n> 📘 Info\\n> \\n> Check out the [alerts documentation](doc:alerts-platform) for more information on setting up alerts.\\n\\n## FAQ\\n\\n**Can Fiddler integrate with multiple PagerDuty services?**\\n\\n- Yes. So long as the service is present within **Settings** → **PagerDuty Services**, anyone within your organization can select that service to be a recipient for an alert.',\n",
       " '---\\ntitle: \"Monitoring\"\\nslug: \"monitoring-platform\"\\nhidden: false\\ncreatedAt: \"2022-11-15T18:06:49.755Z\"\\nupdatedAt: \"2023-08-04T23:20:38.662Z\"\\n---\\nFiddler Monitoring helps you identify issues with the performance of your ML models after deployment. Fiddler Monitoring has five main features:\\n\\n1. **Data Drift**\\n2. **Performance**\\n3. **Data Integrity**\\n4. **Service Metrics**\\n5. **Alerts**\\n\\n## Integrate with Fiddler Monitoring\\n\\nIntegrating Fiddler monitoring is a four-step process:\\n\\n1. **Upload dataset**\\n\\n   Fiddler needs a dataset to be used as a baseline for monitoring. A dataset can be uploaded to Fiddler using our UI and Python package. For more information, see:\\n\\n   - [client.upload_dataset()](ref:clientupload_dataset) \\n\\n2. **Onboard model**\\n\\n   Fiddler needs some specifications about your model in order to help you troubleshoot production issues. Fiddler supports a wide variety of model formats. For more information, see:\\n\\n   - [client.add_model()](ref:clientadd_model) \\n\\n3. **Configure monitoring for this model**\\n\\n   You will need to configure bins and alerts for your model. These will be discussed in detail below.\\n\\n4. **Send traffic from your live deployed model to Fiddler**\\n\\n   Use the Fiddler SDK to send us traffic from your live deployed model.\\n\\n## Publish events to Fiddler\\n\\nIn order to send traffic to Fiddler, use the [`publish_event`](ref:clientpublish_event) API from the Fiddler SDK.\\n\\nThe `publish_event` API can be called in real-time right after your model inference. \\n\\nAn event can contain the following:\\n\\n- Inputs\\n- Outputs\\n- Target\\n- Decisions (categorical only)\\n- Metadata\\n\\nThese aspects of an event can be monitored on the platform.\\n\\n> 📘 Info\\n> \\n> You can also publish events as part of a batch call after the fact using the `publish_events_batch` API (click [here](ref:clientpublish_events_batch) for more information). In this case, you will need to send Fiddler the original event timestamps as to accurately populate the time series charts.\\n\\n## Updating events\\n\\nFiddler supports [partial updates of events](doc:updating-events) for your **target** column. This can be useful when you don’t have access to the ground truth for your model at the time the model\\'s prediction is made. Other columns can only be sent at insertion time (with `update_event=False`).\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"performance\" https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"ranking-model\" if not MODEL_ID in client.list_models(project_id=PROJECT_ID):\\n    client.add_model(\\n        project_id=PROJECT_ID,\\n        dataset_id=DATASET_ID,\\n        model_id=MODEL_ID,\\n        model_info=model_info\\n    )\\nelse:\\n    print(f\\'Model: {MODEL_ID} already exists in Project: {PROJECT_ID}. Please use a different name.\\')\\n```\\n\\n### 3.b Create a Model Wrapper Script\\n\\nPackage.py is the interface between Fiddler’s backend and your model. This code helps Fiddler to understand the model, its inputs and outputs.\\n\\nYou need to implement three parts:\\n- init: Load the model, and any associated files such as feature transformers.\\n- transform: If you use some pre-processing steps not part of the model file, transform the data into a format that the model recognizes.\\n- predict: Make predictions using the model.\\n\\n\\n```python\\n%%writefile model/package.py\\n\\nimport pickle\\nfrom pathlib import Path\\nimport pandas as pd\\n\\nPACKAGE_PATH = Path(__file__).parent\\n\\nclass ModelPackage:\\n\\n    def __init__(self):\\n        \"\"\"\\n         Load the model file and any pre-processing files if needed.\\n        \"\"\"\\n        self.output_columns = [\\'score\\']\\n        \\n        with open(PACKAGE_PATH / \\'model.pkl\\', \\'rb\\') as infile:\\n            self.model = pickle.load(infile)\\n    \\n    def transform(self, input_df):\\n        \"\"\"\\n        Accepts a pandas DataFrame object containing rows of raw feature vectors. \\n        Use pre-processing file to transform the data if needed. \\n        In this example we don\\'t need to transform the data.\\n        Outputs a pandas DataFrame object containing transformed data.\\n        \"\"\"\\n        return input_df\\n    \\n    def predict(self, input_df):\\n        \"\"\"\\n        Accepts a pandas DataFrame object containing rows of raw feature vectors. \\n        Outputs a pandas DataFrame object containing the model predictions whose column labels \\n        must match the output column names in model info.\\n        \"\"\"\\n        transformed_df = self.transform(input_df)\\n        pred = self.model.predict(transformed_df)\\n        return pd.DataFrame(pred, columns=self.output_columns)\\n    \\ndef get_model():\\n    return ModelPackage()\\n```\\n\\n### 3.c Retriving the model files \\n\\nTo explain a model\\'s inner workigs we need to upload the model artifacts. We will retrive a pre-trained model from the Fiddler Repo that was trained with **lightgbm 2.3.0**\\n\\n\\n```python\\nimport urllib.request\\nurllib.request.urlretrieve(\"https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/models/ranking_model.pkl\", \"model/model.pkl\")\\n```\\n\\n### 3.d Upload the model files to Fiddler\\n\\n\\nNow as a final step in the setup you can upload the model artifact files using `add_model_artifact`. \\n   - The `model_dir` is the path for the folder containing the model file(s) and the `package.py` from ther last step.\\n\\n\\n```python\\n#Uploading Model files\\nclient.add_model_artifact(model_dir=model_dir, project_id=PROJECT_ID, model_id=MODEL_ID)\\n```\\n\\n# 5. Send Traffic For Monitoring\\n\\n### 5.a Gather and prepare Production Events\\nThis is the production log file we are going to upload in Fiddler.\\n\\n\\n```python\\ndf_logs = pd.read_csv(\\'https://media.githubusercontent.com/media/fiddler-labs/fiddler-examples/main/quickstart/data/expedia_logs.csv\\')\\ndf_logs.tail()\\n```\\n\\n\\n```python\\ndf_logs[\\'event_id\\'] = df_logs[\\'event_id\\'].apply(str)\\n#timeshift to move the data to last 29 days\\ndf_logs[\\'',\n",
       " '---\\ntitle: \"SageMaker Integration\"\\nslug: \"sagemaker-integration\"\\nhidden: false\\ncreatedAt: \"2022-05-13T14:21:38.536Z\"\\nupdatedAt: \"2023-06-13T19:12:32.529Z\"\\n---\\nThe following Python script can be used to define a AWS Lambda function that can move your SageMaker inference logs from an S3 bucket to a Fiddler environment.\\n\\n## Setup\\n\\nIn addition to pasting this code into your Lambda function, you will need to ensure the following steps are completed before the integration will work.\\n\\n1. Make sure your model is actively being served by SageMaker and that you have  [enabled data capture](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-capture.html) for your SageMaker hosted models so that your model inferences are stored in a S3 bucket as JSONL files.\\n2. Make sure you have a Fiddler trial environment and your SageMaker model is onboarded with Fiddler.  Check out our Getting Started guide for guidance on how to onboard your models.\\n3. Make sure to specify the environment variables in the “Configuration” section of your Lambda function so that the Lambda knows how to connect with your Fiddler environment and so it knows what inputs and outputs to expect in the JSONL files captured by your SageMaker model.\\n\\n![](https://files.readme.io/3b4cb21-lambda_setup.jpg \"lambda_setup.jpg\")\\n\\n4. Make sure you have set up a trigger on your Lambda function so that the function is called upon “Object creation” events in your model’s S3 bucket.\\n5. Make sure you paste the following code into your new Lambda function.\\n6. Make sure that your Lambda function references the Fiddler ARN for the Layer that encapsulates the Fiddler Python client. (`arn:aws:lambda:us-west-2:079310353266:layer:fiddler-client-0814:1`)\\n\\n## Script\\n\\n```python\\nimport fiddler as fdl\\nimport json\\nimport boto3\\nimport os\\nimport pandas as pd\\nimport sys\\nimport uuid\\nfrom urllib.parse import unquote_plus\\nimport csv\\nimport json\\nimport base64\\nfrom io import StringIO\\nfrom botocore.vendored import requests\\n\\ns3_client = boto3.client(\\'s3\\')\\nurl = os.getenv(\\'FIDDLER_URL\\')\\norg = os.getenv(\\'FIDDLER_ORG\\')\\ntoken = os.getenv(\\'FIDDLER_TOKEN\\')\\nproject = os.getenv(\\'FIDDLER_PROJECT\\')\\nmodel = os.getenv(\\'FIDDLER_MODEL\\')\\ntimestamp_field = os.getenv(\\'FIDDLER_TIMESTAMP_FIELD\\', None)  # optional arg\\nid_field = os.getenv(\\'FIDDLER_ID_FIELD\\', None)  # optional arg\\ntimestamp_format = os.getenv(\\'FIDDLER_TIMESTAMP_FORMAT\\', None)  # optional arg\\ncredentials = os.getenv(\\'FIDDLER_AWS_CREDENTIALS\\', \\'{}\\')  # optional arg, json string\\nstring_in_features = os.getenv(\\'FEATURE_INPUTS\\')\\nout_feature = os.getenv(\\'MODEL_OUTPUT\\')\\n\\ndef lambda_handler(event, context):\\n    for record in event[\\'Records\\']:\\n\\n        bucket = record[\\'s3\\'][\\'bucket\\'][\\'name\\']\\n        key = unquote_plus(record[\\'s3\\'][\\'object\\'][\\'key\\'])\\n        tmpkey = key.replace(\\'/\\', \\'\\')\\n        download_path = \\'/tmp/{}{}\\'.format(uuid.uuid4(), tmpkey)\\n        s3_client.download_file(bucket, key, download_path)\\n        parse_sagemaker_log(download_path)\\n\\n    \\n    return {\\n',\n",
       " 'slug: \"databricks-integration\"  fdl.ModelInfo.from_dataset_info(\\n\\tdataset_info = client.get_dataset_info(YOUR_PROJECT,YOUR_DATASET),\\n\\ttarget =  \"TARGET COLUMN\",\\n\\t#optionalArguments\\n\\tmlflow_params = fdl.MLFlowParams(mlflow.models.ModelSignature.to_dict())  \\n)\\n```\\n\\n## Uploading Model Files\\n\\nSharing your [model artifacts](https://docs.fiddler.ai/docs/uploading-model-artifacts) helps Fiddler explain your models. By leveraging the MLFlow API you can download these model files:\\n\\n```python\\nimport os  \\nimport mlflow  \\nfrom mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\\n\\nmodel_name = \"example-model-name\"  \\nmodel_stage = \"Staging\"  # Should be either \\'Staging\\' or \\'Production\\'\\n\\nmlflow.set_tracking_uri(\"databricks\")  \\nos.makedirs(\"model\", exist_ok=True)  \\nlocal_path = ModelsArtifactRepository(\\n  f\\'models:/{model_name}/{model_stage}\\').download_artifacts(\"\", dst_path=\"model\")  \\n\\nprint(f\\'{model_stage} Model {model_name} is downloaded at {local_path}\\')  \\n```\\n\\nOnce you have the model file, you can create a [package.py](doc:binary-classification-1) file in this model directory that describes how to access this model.\\n\\nFinally, you can upload all the model artifacts to Fiddler:\\n\\n```python\\nclient.add_model_artifact(  \\n    project_id=PROJECT_ID,\\n    model_id=MODEL_ID,\\n    model_dir=\\'model/\\',\\n)\\n```\\n\\nAlternatively, you can skip uploading your model and use Fiddler to generate a [surrogate model](doc:surrogate-models-client-guide) to get low-fidelity explanations for your model.\\n\\n## Publishing Events\\n\\nNow you can publish all the events from your models. You can do this in two ways:\\n\\n### Batch Models\\n\\nIf your models run batch processes with your models or your aggregate model outputs over a timeframe, then you can use the table change feed from Databricks to select only the new events and send them to Fiddler:\\n\\n```python Python\\nchanges_df = spark.read.format(\"delta\") \\\\\\n.option(\"readChangeFeed\", \"true\") \\\\\\n.option(\"startingVersion\",last_version) \\\\\\n.option(\"endingVersion\", new_version) \\\\\\n.table(\"inferences\").toPandas()\\n\\n\\nclient.publish_events_batch(\\n   project_id=PROJECT_ID,\\n   model_id=MODEL_ID,\\n   batch_source=changes_df,\\n   timestamp_field=\\'timestamp\\')\\n\\n```\\n\\n### Live Models\\n\\nFor models with live predictions or real-time applications, you can add the following code snippet to your prediction pipeline and send every event to Fiddler in real-time: \\n\\n```python Python\\nexample_event = model_output.toPandas() #turn your model\\'s ouput in a pandas datafram \\n\\nclient.publish_event(\\n    project_id=PROJECT_ID,\\n    model_id=MODEL_ID,\\n    event=example_event,\\n    event_id=\\'event_001\\',\\n    event_timestamp=1637344470000)\\n```\\n\\n_Support for Inference tables and hosted endpoints is coming soon!_',\n",
       " 'slug: \"cv-monitoring\" model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION\\n# name of the column that contains ground truth\\ntarget = \\'target\\'\\n\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n    dataset_info=dataset_info,\\n    dataset_id=DATASET_ID,\\n    features=embedding_cols,\\n    target=target,\\n    outputs=CIFAR_CLASSES,\\n    custom_features=[CF1],\\n    model_task=model_task,\\n    description=\\'An example to showcase monitoring Image data using model embeddings.\\',\\n    categorical_target_class_details=list(CIFAR_CLASSES),\\n)\\nmodel_info\\n```\\n\\nNow we specify a unique model ID and use the client\\'s [add_model](https://docs.fiddler.ai/reference/clientadd_model) function to send this information to Fiddler.\\n\\n\\n```python\\nMODEL_ID = \\'resnet18\\'\\n\\nclient.add_model(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID,\\n    model_id=MODEL_ID,\\n    model_info=model_info,\\n)\\n```\\n\\n# 5. Inject data drift and publish production events\\n\\nNetx, we\\'ll inject data drift in form of blurring and brightness-reduction. The following cell illustrates these transforms.\\n\\n\\n```python\\ndrift_xform_lut = {\\n    \\'original\\': None,\\n    \\'blurred\\': get_blur_transforms(),\\n    \\'brightness_reduced\\': get_brightness_transforms(),\\n}\\nfor drift_type, xform in drift_xform_lut.items():\\n    cifar_testloader = get_cifar_dataloader(train_data=False, batch_size=32, image_transforms=xform)\\n    # get some test images\\n    dataiter = iter(cifar_testloader)\\n    images, labels = dataiter.next()\\n\\n    # show images\\n    print(f\\'Image type: {drift_type}\\')\\n    imshow(torchvision.utils.make_grid(images))\\n```\\n\\n### Publish events to Fiddler\\n\\nWe\\'ll publish events over past 3 weeks. \\n\\n- Week 1: We publish CIFAR-10 test set, which would signify no distributional shift\\n- Week 2: We publish **blurred** CIFAR-10 test set \\n- Week 3: We publish **brightness reduce** CIFAR-10 test set \\n\\n\\n```python\\nimport time\\n\\nfor i, drift_type in enumerate([\\'original\\', \\'blurred\\', \\'brightness_reduced\\']):\\n    week_days = 6\\n    xform = drift_xform_lut[drift_type]\\n    cifar_testloader = get_cifar_dataloader(train_data=False, batch_size=32, image_transforms=xform)\\n    prod_df = generate_embeddings(resnet_model, device, cifar_testloader)\\n    week_offset = (2-i)*7*24*60*60*1e3 \\n    day_offset = 24*60*60*1e3\\n    print(f\\'Publishing events with {drift_type} transformation for week {i}.\\')\\n    for day in range(week_days): \\n        now = time.time() * 1000\\n        timestamp = int(now - week_offset - day*day_offset)\\n        events_df = prod_df.sample(1000)\\n        events_df[\\'timestamp\\'] = timestamp\\n        client.publish_events_batch(\\n            project_id=PROJECT_ID,\\n            model_id=MODEL_ID,\\n            batch_source=events_df,\\n            timestamp_field=\\'timestamp\\',\\n        )\\n```\\n\\n## 6. Get insights\\n\\n**You\\'re all done!**\\n  \\nYou can now head to Fiddler URL and start getting enhanced observability into your model\\'s performance.\\n\\nRun the following code block to get your URL.\\n\\n\\n```python\\nprint(\\'/\\'.join([URL, \\'projects\\', PROJECT_ID, \\'models\\', MODEL',\n",
       " '---\\ntitle: \"Administration\"\\nslug: \"administration-ui\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:26:16.156Z\"\\nupdatedAt: \"2023-02-03T20:49:54.906Z\"\\n---\\nThis section provides details on how to use the UI for:\\n\\n- Fiddler settings\\n- Inviting Users\\n- Project architecture and organization\\n- Access/Authorization details.\\n\\n\\n\\nFor platform-specific information check the [Platform Guide on Administration](doc:administration-platform).',\n",
       " '---\\ntitle: \"SageMaker ML Integration\"\\nslug: \"sagemaker-ml-integration\"\\nhidden: false\\ncreatedAt: \"2022-06-22T14:28:02.932Z\"\\nupdatedAt: \"2023-04-07T01:34:11.498Z\"\\n---\\nFiddler offers **seamless integration with Amazon SageMaker**. This guide will walk you through how you can easily upload a model trained with SageMaker into Fiddler.\\n\\n> 📘 \\n> \\n> Before proceeding with this walkthrough, make sure you have already\\n> \\n> - [Uploaded a baseline dataset](/docs/uploading-a-baseline-dataset)\\n> - Trained a model using SageMaker\\n\\n## Getting your model from S3\\n\\nIn order to download your model, navigate to the AWS console and go to SageMaker. On the left, click **\"Inference\"** and go to **\"Models\"**. Then select the model you want to upload to Fiddler.\\n\\n![](https://files.readme.io/ae27cba-sagemaker_model_select.png \"sagemaker_model_select.png\")\\n\\n\\n\\nCopy the **Model data location** to your clipboard.\\n\\n![](https://files.readme.io/be19325-sagemaker_model_location.png \"sagemaker_model_location.png\")\\n\\n\\n\\n## Downloading your model with Python\\n\\nNow, from a Python environment (Jupyter notebook or standard Python script), paste the **Model data location** you copied into a new variable.\\n\\n```python\\nMODEL_S3_LOCATION = \\'s3://fiddler-sagemaker-integration/fiddler-xgboost-sagemaker-demo/xgboost_model/output/sagemaker-xgboost-2022-06-06-15-49-54-626/output/model.tar.gz\\'\\n```\\n\\n\\n\\nThen extract the bucket name and file key into their own variables.\\n\\n```python\\nMODEL_S3_BUCKET = \\'fiddler-sagemaker-integration\\'\\nMODEL_S3_KEY = \\'fiddler-xgboost-sagemaker-demo/xgboost_model/output/sagemaker-xgboost-2022-06-06-15-49-54-626/output/model.tar.gz\\'\\n```\\n\\n\\n\\nLet\\'s also import a few packages we will be using.\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nimport boto3\\nimport tarfile\\nimport yaml\\nimport xgboost as xgb\\nimport fiddler as fdl\\n```\\n\\n\\n\\nAfter that, initialize an S3 client with AWS using `boto3`.\\n\\n```python\\nAWS_PROFILE = \\'my_profile\\'\\nAWS_REGION = \\'us-west-1\\'\\n\\nsession = boto3.session.Session(\\n    profile_name=AWS_PROFILE,\\n    region_name=AWS_REGION\\n)\\n\\ns3_client = session.client(\\'s3\\')\\n```\\n\\n\\n\\nWe\\'re ready to download! Just run the following code block.\\n\\n```python\\ns3_client.download_file(\\n    Bucket=MODEL_S3_BUCKET,\\n    Key=MODEL_S3_KEY,\\n    Filename=\\'model.tar.gz\\'\\n)\\n\\ntarfile.open(\\'model.tar.gz\\').extractall(\\'model\\')\\n```\\n\\n\\n\\nThis will save the model into a directory called `model`.\\n\\n!!! note  \\n    It\\'s important to **keep track of the name of your saved model file**. Check the `model` directory in your local filesystem to see its name.\\n\\n## Upload your model to Fiddler\\n\\nNow it\\'s time to connect to Fiddler. For more information on how this is done, see [Authorizing the Client](doc:uploading-model-artifacts).\\n\\n```python\\nURL = \\'https://app.fiddler.ai\\'\\nORG_ID = \\'my_org\\'\\n',\n",
       " 'slug: \"data-integrity-platform\"  the data, and try to find the root cause of the issues.\\n\\n**Reference**\\n\\n- See our article on [_The Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Monitoring Charts UI\"\\nslug: \"monitoring-charts-ui\"\\nhidden: false\\ncreatedAt: \"2023-02-23T23:06:54.406Z\"\\nupdatedAt: \"2023-05-24T17:50:37.837Z\"\\n---\\n## Getting Started:\\n\\nTo use Fiddler AI’s monitoring charts, navigate to the Charts tab in the top-level navigation bar on the Fiddler AI platform. Choose between opening a previously saved chart or creating a new chart.\\n\\n## Create a New Monitoring Chart\\n\\nTo create a new monitoring chart, click on the Add Chart button on the top right of the screen.\\n\\n![](https://files.readme.io/2c98736-image.png)\\n\\nSearch for and select the [project](doc:project-structure) to create the chart, and press Add Chart.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/5e5cc8e-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n## Chart Functions\\n\\n![](https://files.readme.io/c5b2029-image.png)\\n\\n### Chart Properties\\n\\nBefore the first save, you can change the project space to ensure you’re focusing on the right models and data. After confirming the project selection, you can choose to name and add a description to your chart.\\n\\n### Save & Share\\n\\nManually save your chart using the Save button on the top right corner of the chart studio. Copy a link to your chart and share it with other [fiddler accounts who have access](doc:inviting-users) to the project where the chart resides.\\n\\n### Global Undo & Redo\\n\\nEasily control the following actions with the undo and redo buttons:\\n\\n- Metric query selection\\n- Time range selections\\n- Time range selections\\n- Bin size selections\\n\\nTo learn how to undo actions taken using the chart toolbar, see the Toolbar information in the next section.\\n\\n## Chart Metric Queries & Filters\\n\\n### Metric Query\\n\\nA metric query enables you to define what model to focus on, and which metrics and columns to plot on your monitoring chart. To get started with the metric query, choose a model of choice. Note: only models within the same project as your chart are accessible.\\n\\nOnce a model is selected, choose a metric type from Performance, Data Drift, Data Integrity, or Traffic metrics and relevant metrics. For example, we may choose to chart accuracy for our binary classification model. \\n\\n![](https://files.readme.io/a46e656-image.png)\\n\\n\\n\\n\\n\\n### Charting Multiple Columns\\n\\nIf you choose to chart data drift or data integrity, you can choose to plot up to 20 different columns including outputs, inputs, and metadata columns. \\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/e7b2019-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"350px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n### Chart Filters & Capabilities\\n\\nThere are three major chart filter capabilities, chart filters, chart toolbar, and zoom slider.  \\nThey work together to enable you to best analyze the slices of data that may be worth investigating. \\n\\n![](https://files.readme.io/f58936d-image.png)\\n\\n### Filters\\n\\nYou can customize your chart view using time range, time zone, and bin size chart filters. The data range can be one of the pre-defined time ranges or a custom range. The bin size',\n",
       " '---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-setup\"\\nhidden: false\\ncreatedAt: \"2022-05-13T14:41:57.721Z\"\\nupdatedAt: \"2023-03-08T20:54:52.508Z\"\\n---\\nThe Client object is used to communicate with Fiddler.  In order to use the client, you\\'ll need to provide authentication details as shown below.\\n\\nFor more information, see [Authorizing the Client](doc:authorizing-the-client).\\n\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"Parameter\",\\n    \"h-1\": \"Type\",\\n    \"h-2\": \"Default\",\\n    \"h-3\": \"Description\",\\n    \"0-0\": \"url\",\\n    \"0-1\": \"str\",\\n    \"0-2\": \"None\",\\n    \"0-3\": \"The URL used to connect to Fiddler\",\\n    \"1-0\": \"org_id\",\\n    \"1-1\": \"str\",\\n    \"1-2\": \"None\",\\n    \"1-3\": \"The organization ID for a Fiddler instance. Can be found on the General tab of the Settings page.\",\\n    \"2-0\": \"auth_token\",\\n    \"2-1\": \"str\",\\n    \"2-2\": \"None\",\\n    \"2-3\": \"The authorization token used to authenticate with Fiddler. Can be found on the Credentials tab of the Settings page.\",\\n    \"3-0\": \"proxies\",\\n    \"3-1\": \"Optional [dict]\",\\n    \"3-2\": \"None\",\\n    \"3-3\": \"A dictionary containing proxy URLs.\",\\n    \"4-0\": \"verbose\",\\n    \"4-1\": \"Optional [bool]\",\\n    \"4-2\": \"False\",\\n    \"4-3\": \"If True, client calls will be logged verbosely.\",\\n    \"5-0\": \"verify\",\\n    \"5-1\": \"Optional  \\\\n[bool]\",\\n    \"5-2\": \"True\",\\n    \"5-3\": \"If False, client will allow self-signed SSL certificates from the Fiddler server environment.  If True, the SSL certificates need to be signed by a certificate authority (CA).\"\\n  },\\n  \"cols\": 4,\\n  \"rows\": 6,\\n  \"align\": [\\n    \"left\",\\n    \"left\",\\n    \"left\",\\n    \"left\"\\n  ]\\n}\\n[/block]\\n\\n> 🚧 Warning\\n> \\n> If verbose is set to **True**, all information required for debugging will be logged, including the authorization token.\\n\\n> 📘 Info\\n> \\n> To maximize compatibility, **please ensure that your client version matches the server version for your Fiddler instance.**\\n> \\n> When you connect to Fiddler using the code on the right, you\\'ll receive a notification if there is a version mismatch between the client and server.\\n> \\n> You can install a specific version of fiddler-client using pip:  \\n> `pip install fiddler-client==X.X.X`\\n\\n```python Connect the Client\\nimport fiddler as fdl\\n\\nURL = \\'https://app.fiddler.ai\\'\\nORG_ID = \\'my_org\\'\\nAUTH_TOKEN = \\'p9uqlkKz1zAA3KAU8kiB6zJkXiQoqFgkUgEa1sv4u58\\'\\n\\nclient = fdl.FiddlerApi(\\n    url=URL,\\n    org_id=ORG_ID,\\n    auth_token=AUTH',\n",
       " '---\\ntitle: \"NLP Monitoring\"\\nslug: \"simple-nlp-monitoring-quick-start\"\\nexcerpt: \"Quickstart Notebook\"\\nhidden: false\\ncreatedAt: \"2022-08-15T23:29:02.913Z\"\\nupdatedAt: \"2023-03-10T18:28:53.429Z\"\\n---\\nThis guide will walk you through the basic steps required to use Fiddler for monitoring NLP models. A multi-class classifier is applied to the 20newsgroup dataset and the text embeddings are monitored using Fiddler\\'s unique Vector Monitoring approach.\\n\\nClick the following link to get started using Google Colab:\\n\\n<div class=\"colab-box\">\\n    <a href=\"https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/Fiddler_Quickstart_NLP_OpenAI_Monitoring.ipynb\" target=\"_blank\">\\n        <div>\\n            Open in Google Colab →\\n        </div>\\n    </a>\\n    <div>\\n            <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" />\\n    </div>\\n</div># Monitoring a Multiclass Classifier Model for NLP Data with Fiddler\\nUnstructured data such as text are usually represented as high-dimensional vectors when processed by ML models. In this example notebook we present how [Fiddler Vector Monitoring](https://www.fiddler.ai/blog/monitoring-natural-language-processing-and-computer-vision-models-part-1) can be used to monitor NLP models using a text classification use case.\\n\\nFollowing the steps in this notebook you can monitor different embedding models. We use the 20Newsgroups dataset and train a multi-class classifier that is applied to vector embeddings of text documents. \\n\\nAfter running a few pre-processing steps on the 20Newsgroups dataset (see [this notebook](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/pre-proccessing/20newsgroups_prep_vectorization.ipynb)), we create example embeddings using OpenAI and TF-IDF vectorization methods, and we train a logistic regression model to predict the text categories. Then we publish the labled data to Fiddler for monitoring. \\n\\nWe monitor this model at production time and assess the performance of Fiddler\\'s vector monitoring by manufacturing synthetc drift via sampling from specific text categories at different deployment time intervals.\\n\\n---\\n\\nNow we perform the following steps to demonstrate how Fiddler NLP monitoring works: \\n\\n1. Connect to Fiddler and Create a Project\\n2. Upload the Assets and Vectorize Text Data\\n3. Train a Multiclass Classifier\\n3. Upload Baseline Data to Fiddler\\n4. Add Metadata About the Model \\n5. Manufacture Synthetic Data Drift and Publish Production Events\\n6. Get insights\\n\\n## Imports\\n\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\nimport random\\nimport os\\n```\\n\\n# 1. Connect to Fiddler and Create a Project\\n\\nFirst we install and import the Fiddler Python client.\\n\\n\\n```python\\n!pip install -q fiddler-client\\nimport fiddler as fdl\\nprint(f\"Running client version {fdl.__version__}\")\\n```\\n\\nBefore you can add information about your model with Fiddler, you\\'ll need to connect using our API client.\\n\\n---\\n\\n**We need a few pieces of information to get started.**\\n1. The URL you\\'re using to connect to Fiddler\\n2. Your organization ID\\n3. Your authorization token\\n\\nThe latter two of',\n",
       " '---\\ntitle: \"Explainability with Model Artifact\"\\nslug: \"explainability-with-model-artifact-quickstart-notebook\"\\nexcerpt: \"Quickstart Notebook\"\\nhidden: false\\ncreatedAt: \"2022-12-13T22:00:20.384Z\"\\nupdatedAt: \"2023-03-07T21:41:03.902Z\"\\n---\\nThis guide will walk you through the basic steps required to onboard a model in Fiddler with its model artifact.  When Fiddler is provided with the actual model artifact, it can produce high-fidelity explanations. In contrast, models within Fiddler that use a surrogate model or no model artifact at all provide approximative explainability or no explainability at all.\\n\\nClick the following link to get started using Google Colab:\\n\\n<div class=\"colab-box\">\\n    <a href=\"https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/Fiddler_Quickstart_Add_Model_Artifact.ipynb\" target=\"_blank\">\\n        <div>\\n            Open in Google Colab →\\n        </div>\\n    </a>\\n    <div>\\n            <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" />\\n    </div>\\n</div># Adding a Model Artifact\\n\\nIn this notebook, we present the steps for onboarding a model with its model artifact.  When Fiddler is provided with your real model artifact, it can produce high-fidelity explanations.  In contrast, models within Fiddler that use a surrogate model or no model artifact at all provide approximative explainability or no explainability at all.\\n\\nFiddler is the pioneer in enterprise Model Performance Management (MPM), offering a unified platform that enables Data Science, MLOps, Risk, Compliance, Analytics, and LOB teams to **monitor, explain, analyze, and improve ML deployments at enterprise scale**. \\nObtain contextual insights at any stage of the ML lifecycle, improve predictions, increase transparency and fairness, and optimize business revenue.\\n\\n---\\n\\nYou can experience Fiddler\\'s NLP monitoring ***in minutes*** by following these five quick steps:\\n\\n1. Connect to Fiddler\\n2. Upload a baseline dataset\\n3. Upload a model package directory containing the **1) package.py and 2) model artifact**\\n4. Publish production events\\n5. Get insights (including high-fidelity explainability, or XAI!)\\n\\n# 0. Imports\\n\\n\\n```python\\n!pip install -q fiddler-client\\n\\nimport fiddler as fdl\\nimport pandas as pd\\nimport yaml\\nimport datetime\\nimport time\\nfrom IPython.display import clear_output\\n\\nprint(f\"Running Fiddler client version {fdl.__version__}\")\\n```\\n\\n# 1. Connect to Fiddler\\n\\nBefore you can add information about your model with Fiddler, you\\'ll need to connect using our Python client.\\n\\n---\\n\\n**We need a few pieces of information to get started.**\\n1. The URL you\\'re using to connect to Fiddler\\n2. Your organization ID\\n3. Your authorization token\\n\\nThe latter two of these can be found by pointing your browser to your Fiddler URL and navigating to the **Settings** page.\\n\\n\\n```python\\nURL = \\'\\'  # Make sure to include the full URL (including https://).\\nORG_ID = \\'\\'\\nAUTH_TOKEN = \\'\\'\\n```\\n\\nNow just run the following code block to connect the client to your Fiddler environment.\\n\\n\\n```python\\nclient = fdl.FiddlerApi(\\n    url=URL,\\n    org_id=ORG',\n",
       " 'slug: \"explainability-with-model-artifact-quickstart-notebook\" isactivemember\\',\\n       \\'estimatedsalary\\']\\n\\n\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n    dataset_info=client.get_dataset_info(PROJECT_ID, DATASET_ID),\\n    target=\\'churn\\', \\n    categorical_target_class_details=\\'yes\\',\\n    features=feature_columns,\\n    decision_cols = decision_cols,\\n    metadata_cols = metadata_cols,\\n    outputs=[\\'predicted_churn\\'],\\n    display_name=\\'Random Forest Model\\',\\n    description=\\'This is models customer bank churn\\'\\n)\\n\\nmodel_info\\n```\\n\\n### 3.1.b Add Model Information to Fiddler\\n\\n\\n```python\\nMODEL_ID = \\'customer_churn_rf\\'\\n\\nclient.add_model(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID,\\n    model_id=MODEL_ID,\\n    model_info=model_info\\n)\\n```\\n\\n### 3.2 Create the **package.py** file\\n\\nThe contents of the cell below will be written into our ***package.py*** file.  This is the step that will be most unique based on model type, framework and use case.  The model\\'s ***package.py*** file also allows for preprocessing transformations and other processing before the model\\'s prediction endpoint is called.  For more information about how to create the ***package.py*** file for a variety of model tasks and frameworks, please reference the [Uploading a Model Artifact](https://docs.fiddler.ai/docs/uploading-a-model-artifact#packagepy-script) section of the Fiddler product documentation.\\n\\n\\n```python\\n%%writefile model/package.py\\n\\nimport pandas as pd\\nfrom pathlib import Path\\nimport os\\nfrom sklearn.ensemble import RandomForestClassifier\\nimport pickle as pkl\\n\\n \\nPACKAGE_PATH = Path(__file__).parent\\nTARGET = \\'churn\\'\\nPREDICTION = \\'predicted_churn\\'\\n\\nclass Random_Forest:\\n\\n\\n    def __init__(self, model_path, output_column=None):\\n        \"\"\"\\n        :param model_path: The directory where the model is saved.\\n        :param output_column: list of column name(s) for the output.\\n        \"\"\"\\n        self.model_path = model_path\\n        self.output_column = output_column\\n        \\n       \\n        file_path = os.path.join(self.model_path, \\'model.pkl\\')\\n        with open(file_path, \\'rb\\') as file:\\n            self.model = pkl.load(file)\\n    \\n    \\n    def predict(self, input_df):\\n        return pd.DataFrame(\\n            self.model.predict_proba(input_df.loc[:, input_df.columns != TARGET])[:,1], \\n            columns=self.output_column)\\n    \\n\\ndef get_model():\\n    return Random_Forest(model_path=PACKAGE_PATH, output_column=[PREDICTION])\\n```\\n\\n### 3.3  Ensure your model\\'s artifact is in the **/model** directory\\n\\nMake sure your model artifact (*e.g. the model .pkl file*) is also present in the model package directory as well as any dependencies called out in a *requirements.txt* file.  The following cell will move this model\\'s pkl file and requirements.txt file into our */model* directory.\\n\\n\\n```python\\nimport urllib.request\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/models/model.pkl\", \"model/model.pkl\")\\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/models/requirements.txt\", \"model/requirements.txt\")\\n```\\n\\n### 3.4 Define Model Parameters \\n\\nThis is done by creating our [DEPLOYMENT_PARAMETERS](https://docs.fiddler.ai/reference/fdldeploymentparams) object.\\n\\n\\n```python\\nDEPLOYMENT_PARAMETERS = fdl.DeploymentParams(image_uri=\"md-base',\n",
       " '---\\ntitle: \"Airflow Integration\"\\nslug: \"airflow-integration\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:18:03.854Z\"\\nupdatedAt: \"2023-06-13T19:13:41.297Z\"\\n---\\nApache Airflow is an open source platform ETL platform to manage company’s complex  \\nworkflows. Companies are increasingly integrating their ML models pipeline into Airflow DAGs to manage and monitor all the components of their ML model system.\\n\\nBy integrating Fiddler into an existing Airflow DAG, you will be able to train, manage, and onboard your models while  actively monitoring performance, data quality, and troubleshooting degradations across your models.\\n\\nFiddler can be easily integrated into your existing airflow DAG for ML model pipeline. A notebook which is used for publishing events can be orchestrated to run as a part of your airflow DAG using a ‘Papermill Operator’.\\n\\n## Steps for the walkthrough\\n\\n1. Setup airflow on your local or docker, these steps can be followed. [Link](https://airflow.apache.org/docs/apache-airflow/stable/start/index.html)\\n\\n2. Add your jupyter notebook containing the code for publishing to your airflow home directory. In this example we will use the 2 different notebooks - \\n\\n   a. [Notebook to onboard ML model to Fiddler platform](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/integration-examples/airflow/notebooks/Fiddler_Churn_Model_Registration.ipynb)\\n\\n   b. [Notebook to push production events to Fiddler platform](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/integration-examples/airflow/notebooks/Fiddler_Churn_Event_Publishing.ipynb)\\n\\n3. Add an orchestration code to your airflow directory, airflow will pick up the orchestration code and construct a DAG as defined. The orchestration code contains the ‘papermill operator’ to orchestrate the jupyter notebooks which will be used to onboard models and publish events to Fiddler. Please refer to our [orchestration codes](https://github.com/fiddler-labs/fiddler-samples/tree/master/content_root/tutorial/integration-examples/airflow/DAGs).\\n\\n4. The run interval can be set up in orchestration code as ‘schedule_interval’ in the DAG class. This interval can be based on the frequency of training and inference of your ML model.\\n\\n5. Once the DAGs are set up it can be monitored on the UI. Below we can see dummy DAGs have been set up with placeholder nodes for ‘data preparation ETL’ and ‘model training/inference’. We have two DAGs - \\n\\n   a. To set up Fiddler model registration after preparing baseline data (training pipeline)\\n\\n   b. To publish events to Fiddler after data preparation and ML model inference (inference pipeline)\\n\\n## Label Update\\n\\nAn important business use case is integrating Fiddler’s ‘Label Update’ as a part of your ML workflow using Airflow. Label update can be used to update the ground truth feature in your data. This can be done using the ‘\\u200b\\u200bpublish_event’ api, passing the event, event_id parameters, and making the update_event parameter as ‘True’.  \\nThe code to update label can be found in the [notebook](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/integration-examples/airflow/notebooks/Fiddler',\n",
       " 'slug: \"global-explainability\"  _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Snowflake Integration\"\\nslug: \"snowflake-integration\"\\nhidden: false\\ncreatedAt: \"2022-06-22T14:51:45.373Z\"\\nupdatedAt: \"2023-06-13T19:08:09.527Z\"\\n---\\n## Using Fiddler on your ML data stored in Snowflake\\n\\nIn this article, we will be looking at loading data from Snowflake tables and using the data for the following tasks-\\n\\n1. Uploading baseline data to Fiddler\\n2. Onboarding a model to Fiddler and creating a surrogate\\n3. Publishing production data to Fiddler\\n\\n### Import data from Snowflake\\n\\nIn order to import data from Snowflake to Jupyter notebook, we will use the snowflake library, this can be installed using the following command in your Python environment.\\n\\n```python\\npip install snowflake-connector-python\\n```\\n\\nOnce the library is installed, we would require the following to establish a connection to Snowflake\\n\\n- Snowflake Warehouse\\n- Snowflake Role\\n- Snowflake Account\\n- Snowflake User\\n- Snowflake Password\\n\\nThese can be obtained from your Snowflake account under the ‘Admin’ option in the Menu as shown below or by running the queries - \\n\\n- Warehouse - select CURRENT_WAREHOUSE()\\n- Role - select CURRENT_ROLE()\\n- Account - select CURRENT_ACCOUNT()\\n\\n\\'User\\' and \\'Password\\' are the same as one used for logging into your Snowflake account.\\n\\n![](https://files.readme.io/c2f4cf4-Screen_Shot_2022-06-14_at_4.17.36_PM.png \"Screen Shot 2022-06-14 at 4.17.36 PM.png\")\\n\\nOnce you have this information, you can set up a Snowflake connector using the following code -\\n\\n```python\\n# establish Snowflake connection\\nconnection = connector.connect(user=snowflake_username, \\n                               password=snowflake_password, \\n                               account=snowflake_account, \\n                               role=snowflake_role, \\n                               warehouse=snowflake_warehouse\\n                              )\\n```\\n\\nYou can then write a custom SQL query and import the data to a pandas dataframe.\\n\\n```python\\n# sample SQL query\\nsql_query = \\'select * from FIDDLER.FIDDLER_SCHEMA.CHURN_BASELINE LIMIT 100\\'\\n\\n# create cursor object\\ncursor = connection.cursor()\\n\\n# execute SQL query inside Snowflake\\ncursor.execute(sql_query)\\n\\nbaseline_df = cursor.fetch_pandas_all()\\n```\\n\\n### Publish Production Events\\n\\nIn order to publish production events from Snowflake, we can load the data to a pandas dataframe and publish it to fiddler using _client.publish_events_batch_ api.\\n\\nNow that we have data imported from Snowflake to a jupyter notebook, we can refer to the following notebooks to\\n\\n- [Upload baseline data and onboard a model ](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/integration-examples/snowflake/Fiddler-Snowflake%20Integration%20-%20Model%20Registration.ipynb)\\n- [Publish production events](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/integration-examples/snowflake/Fiddler-Snowflake%20Integration%20-%20Event%20Publishing.ipynb)',\n",
       " 'slug: \"monitoring-xai-quick-start\" raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/xai_data.png?raw=true\" /></td> \\n    </tr>\\n    <tr>\\n        <td><img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/xai_data_info.png?raw=true\" /></td>\\n    </tr> \\n</table>\\n\\n## 3. Add information about your model\\n\\nNow it\\'s time to add details about your model with Fiddler. We do so by first creating a **ModelInfo Object** that helps Fiddler understand **how your model operates**.\\n  \\n*Just include:*\\n1. The **task** your model is performing (regression, binary classification, etc.)\\n2. The **target** (ground truth) column\\n3. The **output** (prediction) column\\n4. The **feature** columns\\n5. Any **metadata** columns\\n6. Any **decision** columns (these measures the direct business decisions made as result of the model\\'s prediction)\\n\\n\\n\\n```python\\n# Specify task\\nmodel_task = \\'binary\\'\\n\\nif model_task == \\'regression\\':\\n    model_task = fdl.ModelTask.REGRESSION\\n    \\nelif model_task == \\'binary\\':\\n    model_task = fdl.ModelTask.BINARY_CLASSIFICATION\\n\\nelif model_task == \\'multiclass\\':\\n    model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION\\n\\n    \\n# Specify column types\\ntarget = \\'churn\\'\\noutputs = [\\'predicted_churn\\']\\ndecision_cols = [\\'decision\\']\\nfeatures = [\\'geography\\', \\'gender\\', \\'age\\', \\'tenure\\', \\'balance\\', \\'numofproducts\\', \\'hascrcard\\', \\'isactivemember\\', \\'estimatedsalary\\']\\n    \\n# Generate ModelInfo\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n    dataset_info=dataset_info,\\n    dataset_id=DATASET_ID,\\n    model_task=model_task,\\n    target=target,\\n    categorical_target_class_details=\\'yes\\',\\n    outputs=outputs,\\n    decision_cols=decision_cols,\\n    features=features\\n)\\nmodel_info\\n```\\n\\nAfter ModelInfo object is created to save your model information, use the client\\'s *add_model* call to add the generated details about your model. \\n\\n**Note:** You will need to specify a unique model ID.\\n\\n\\n```python\\nMODEL_ID = \\'churn_classifier\\'\\n\\nclient.add_model(\\n    project_id=PROJECT_ID,\\n    dataset_id=DATASET_ID,\\n    model_id=MODEL_ID,\\n    model_info=model_info\\n)\\n```\\n\\nOn the project page, you should now be able to see the newly created model. Notice how without uploading model or creating surrogate model, you can only explore monitoring capabilities.\\n\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/xai_add_model.png?raw=true\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n## 4. Either upload your own model or generate a surrogate model\\n\\nWith the above step, your model is added to Fiddler which means that for a given *project_id*, your given *model_id* now holds *ModelInfo* about the model you care about. \\n\\nIn order to be able to run predictions for explainability analysis, however, you will need to upload your model file. If you just want to explore the XAI capabilities without providing your model to Fiddler, you can also generate a surrogate model which tries to mimic your model based on the details provided. \\n\\nIn this quickstart, we will go with generating a surrogate model based on the',\n",
       " '---\\ntitle: \"About Event Publication\"\\nslug: \"publish_event\"\\nhidden: false\\ncreatedAt: \"2022-05-13T14:36:29.265Z\"\\nupdatedAt: \"2022-06-14T15:30:43.091Z\"\\n---\\nEvent publication is the process of sending your model\\'s prediction logs, or events, to the Fiddler platform.  Using the [Fiddler Client](ref:about-the-fiddler-client), events can be published in batch or streaming mode.  Using these events, Fiddler will calculate metrics around feature drift, prediction drift, and model performance.  These events are also stored in Fiddler to allow for ad hoc segment analysis.  Please read the sections that follow to learn more about how to use the Fiddler Client for event publication.',\n",
       " 'slug: \"data-integrity\"  Rise of MLOps Monitoring_](https://www.fiddler.ai/blog/the-rise-of-mlops-monitoring)\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " '---\\ntitle: \"On-prem Technical Requirements\"\\nslug: \"technical-requirements\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:20:05.290Z\"\\nupdatedAt: \"2023-05-18T15:43:28.706Z\"\\n---\\n## Minimum System Requirements\\n\\nFiddler is horizontally scalable to support the throughput requirements for enormous production use-cases. The minimum system requirements below correspond to approximately 20 million inference events monitored per day (~230 EPS) for models with around 100 features, with 90 day retention.\\n\\n- **Deployment**: Kubernetes namespace in AWS, Azure or GCP\\n- **Compute**: A minimum of 96 vCPU cores\\n- **Memory**: 384Gi\\n- **Persistent volumes**: 500 Gi storage across 10 volumes \\n  - POSIX-compliant block storage\\n  - 125 MB/s recommended\\n  - 3,000 IOPS recommended\\n- **Container Registry**: Quay.io or similar\\n- **Ingress Controller**: Ingress-nginx or AWS/GCP/Azure Load Balancer Controller\\n- **DNS**: FQDN that resolves to an L4 or L7 load balancer/proxy that provides TLS termination\\n\\n## Kubernetes Cluster Requirements\\n\\nAs stated above, Fiddler requires a Kubernetes cluster to install into.  The following outlines the requirements for this K8 cluster:\\n\\n- **Node Groups**:  2 node groups -  1 for core Fiddler services, 1 for Clickhouse (Fiddler\\'s event database)\\n- **Resources**:\\n  - Fiddler :  48 vCPUs, 192 Gi\\n  - Clickhouse :  64 vCPUs, 256 Gi [tagged & tainted]\\n- **Persistent Volumes**: 500 GB (minimum) /  1 TB (recommended)\\n- **Instance Sizes**\\n\\n  | Instance Size | AWS    | Azure      | GCP        |\\n  | :------------ | :----- | :--------- | :--------- |\\n  | Minimum       | m5.4xl | Std_D16_v3 | c2d_std_16 |\\n  | Recommended   | m5.8xl | Std_D32_v3 | c2d_std_32 |',\n",
       " '---\\ntitle: \"Explainability with a Surrogate Model\"\\nslug: \"monitoring-xai-quick-start\"\\nexcerpt: \"Quickstart Notebook\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:06:49.318Z\"\\nupdatedAt: \"2023-03-07T21:40:20.934Z\"\\n---\\nThis guide will walk you through the basic onboarding steps required to use Fiddler for model production monitoring and explainability with a model surrogate, **using data provided by Fiddler**.\\n\\nClick the following link to get started using Google Colab:\\n\\n<div class=\"colab-box\">\\n    <a href=\"https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/Fiddler_Quickstart_Surrogate_XAI.ipynb\" target=\"_blank\">\\n        <div>\\n            Open in Google Colab →\\n        </div>\\n    </a>\\n    <div>\\n            <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" />\\n    </div>\\n</div>\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\\\\\" alt=\\\\\"Fiddler Free Trial\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]# Fiddler Quick Start Guide for Explainability (XAI) with Surrogate Models\\n\\nFiddler is not only a powerful observability tool for monitoring the health of your ML models in production but also an explainability tool to peak into your black box models. With the ability to **point explain** and **global explain** your model, Fiddler provides powerful visualizations that can explain your model\\'s behavior. \\n\\n\\n---\\n\\n\\nYou can start exploring Fiddler\\'s XAI capabilities by following these five quick steps:\\n\\n1. Connect to Fiddler\\n2. Upload a baseline dataset\\n3. Add your model details to Fiddler\\n4. Either upload a model artifact or use Fiddler generated surrogate model\\n5. Get insights\\n\\n**Don\\'t have a Fiddler account? [Sign-up for a 14-day free trial](https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral).**\\n\\n## 0. Imports\\n\\n\\n```python\\n!pip install -q fiddler-client;\\n\\nimport numpy as np\\nimport pandas as pd\\nimport fiddler as fdl\\nimport time as time\\n\\nprint(f\"Running client version {fdl.__version__}\")\\n```\\n\\n## 1. Connect to Fiddler\\n\\nBefore you can register your model with Fiddler, you\\'ll need to connect using our API client.\\n\\n\\n---\\n\\n\\n**We need a few pieces of information to get started.**\\n1. The URL you\\'re using to connect to Fiddler\\n\\n\\n```python\\nURL = \\'\\' # Make sure to include the full URL (including https://).\\n```\\n\\n\\n```python\\nORG_ID = \\'\\'\\nAUTH_TOKEN = \\'\\'\\n```\\n\\n2. Your organization ID\\n3. Your authorization token\\n\\nBoth of these can be found by clicking the URL you entered and navigating to the **Settings** page.\\n\\n<table>\\n    <tr>\\n        <td><img src=\"https://fiddler-nb-assets.s3.us-west-1.amazonaws.com/qs_settings_page_numbered',\n",
       " '---\\ntitle: \"Ranking Monitoring Example\"\\nslug: \"ranking-model\"\\nhidden: false\\ncreatedAt: \"2023-06-16T21:38:41.066Z\"\\nupdatedAt: \"2023-06-23T01:12:25.782Z\"\\n---\\nThis notebook will show you how Fiddler enables monitoring and explainability for a Ranking model. This notebook uses a dataset from Expedia that includes shopping and purchase data with information on price competitiveness. The data are organized around a set of “search result impressions”, or the ordered list of hotels that the user sees after they search for a hotel on the Expedia website.\\n\\nClick the following link to try it now with Google Colab:\\n\\n<div class=\"colab-box\">\\n    <a href=\"https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/Fiddler_Quickstart_Ranking_Model.ipynb\" target=\"_blank\">\\n        <div>\\n            Open in Google Colab →\\n        </div>\\n    </a>\\n    <div>\\n            <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" />\\n    </div>\\n</div># Fiddler Ranking Model Quick Start Guide\\n\\nFiddler offer the ability for your teams to observe you ranking models to understand thier performance and catch issues like data drift before they affect your applications.\\n\\n# Quickstart: Expedia Search Ranking\\nThe following dataset is coming from Expedia. It includes shopping and purchase data as well as information on price competitiveness. The data are organized around a set of “search result impressions”, or the ordered list of hotels that the user sees after they search for a hotel on the Expedia website. In addition to impressions from the existing algorithm, the data contain impressions where the hotels were randomly sorted, to avoid the position bias of the existing algorithm. The user response is provided as a click on a hotel. From: https://www.kaggle.com/c/expedia-personalized-sort/overview\\n\\n# 0. Imports\\n\\n\\n```python\\nimport pandas as pd\\nimport lightgbm as lgb\\nimport numpy as np\\nimport time as time\\nimport datetime\\n```\\n\\n# 1. Connect to Fiddler and Create a Project\\nFirst we install and import the Fiddler Python client.\\n\\n\\n```python\\n!pip install -q fiddler-client\\nimport fiddler as fdl\\nprint(f\"Running client version {fdl.__version__}\")\\n```\\n\\nBefore you can add information about your model with Fiddler, you\\'ll need to connect using our API client.\\n\\n---\\n\\n**We need a few pieces of information to get started.**\\n1. The URL you\\'re using to connect to Fiddler\\n2. Your organization ID\\n3. Your authorization token\\n\\nThe latter two of these can be found by pointing your browser to your Fiddler URL and navigating to the **Settings** page.\\n\\n\\n```python\\nURL = \\'\\'  # Make sure to include the full URL (including https://).\\nORG_ID = \\'\\'\\nAUTH_TOKEN = \\'\\'\\n```\\n\\nNext we run the following code block to connect to the Fiddler API.\\n\\n\\n```python\\nclient = fdl.FiddlerApi(url=URL, org_id=ORG_ID, auth_token=AUTH_TOKEN)\\n```\\n\\nOnce you connect, you can create a new project by specifying a unique project ID in the client\\'s `create_project` function.\\n\\n\\n```python\\nPROJECT_ID = \\'search_ranking\\'\\n\\nif not PROJECT_ID in client.list_projects():\\n    print(f\\'Creating project: {PROJECT_ID}\\')\\n    client.create_project(PROJECT_ID',\n",
       " 'slug: \"project-structure\" \\n\\nYou can collate specific visualizations under the Project Dashboard. After visualizations are created using the Model Analytics tool, you can pin them to the dashboard, which can then be shared with others.\\n\\n![](https://files.readme.io/b7cb9ce-Chart_Dashboard.png \"Chart_Dashboard.png\")\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_',\n",
       " '---\\ntitle: \"Class Imbalance Monitoring Example\"\\nslug: \"class-imbalance-monitoring-example\"\\nhidden: false\\ncreatedAt: \"2023-05-08T13:42:46.086Z\"\\nupdatedAt: \"2023-05-08T13:42:46.086Z\"\\n---\\nMany ML use cases, like fraud detection and facial recognition, suffer from what is known as the _class imbalance problem_. This problem exists where a vast majority of the inferences seen by the model belong to only one class, known as the majority class. This makes detecting drift in the minority class very difficult as the \"signal\" is completely outweighed by the shear number of inferences seen in the majority class. \\n\\nThis guide showcases how Fiddler uses a class weighting parameter to deal with this problem. This notebook will onboard two identical models -- one without class imbalance weighting and one with class imbalance weighting -- to illustrate how drift signals in the minority class are easier to detect once properly amplified by Fiddler\\'s unique class weighting approach..\\n\\nClick the following link to get started using Google Colab:\\n\\n<div class=\"colab-box\">\\n    <a href=\"https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/Fiddler_Quickstart_Imbalanced_Data.ipynb\" target=\"_blank\">\\n        <div>\\n            Open in Google Colab →\\n        </div>\\n    </a>\\n    <div>\\n            <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" />\\n    </div>\\n</div># Fiddler Quickstart notebook for a Class Imbalance Example\\n\\nMany ML use cases, like fraud detection and facial recognition, suffer from what is known as the class imbalance problem.  This problem exists where a vast majority of the inferences seen by the model belong to only one class, known as the majority class.  This makes detecting drift in the minority class very difficult as the \"signal\" is completely outweighed by the shear number of inferences seen in the majority class.  The following notebook showcases how Fiddler uses a class weighting paramater to deal with this problem. This notebook will onboard two identical models -- one without class imbalance weighting and one with class imbalance weighting -- to illustrate how drift signals in the minority class are easier to detect once properly amplified by Fiddler\\'s unique class weighting approach.\\n\\n1. Connect to Fiddler\\n2. Upload a baseline dataset for a fraud detection use case\\n3. Onboard two fraud models to Fiddler -- one with class weighting and one without\\n4. Publish production events to both models with synthetic drift in the minority class\\n5. Get Insights -- compare the two onboarding approaches in Fiddler\\n\\n## 0. Imports\\n\\n\\n```python\\n!pip install -q fiddler-client;\\n\\nimport numpy as np\\nimport pandas as pd\\nimport fiddler as fdl\\nimport sklearn\\nimport datetime\\nimport time\\n\\nprint(f\"Running client version {fdl.__version__}\")\\n\\nRANDOM_STATE = 42\\n```\\n\\n## 1. Connect to Fiddler\\n\\n\\n```python\\nURL = \\'\\'  # Make sure to include the full URL (including https://).\\nORG_ID = \\'\\'\\nAUTH_TOKEN = \\'\\'\\n\\nPROJECT_ID = \\'imbalance_cc_fraud\\'\\nMODEL_ID = \\'imbalance_cc_fraud\\'\\nDATASET_ID = \\'imbalance_cc_fraud_baseline\\'\\n\\nclient = fdl.FiddlerApi(\\n    url=URL,\\n    org_id=ORG_ID,\\n    auth_token=AUTH_TOKEN\\n)\\n```\\n\\n\\n```python\\n# Create a new project within Fidd',\n",
       " '---\\ntitle: \"client.unshare_project\"\\nslug: \"clientunshare_project\"\\nexcerpt: \"Unshares a project with a user or team.\"\\nhidden: false\\ncreatedAt: \"2022-05-25T15:29:55.881Z\"\\nupdatedAt: \"2022-06-21T17:25:38.641Z\"\\n---\\n[block:callout]\\n{\\n  \"type\": \"info\",\\n  \"title\": \"Info\",\\n  \"body\": \"Administrators and project owners can unshare any project with any user. If you lack the required permissions to unshare a project, contact your organization administrator.\"\\n}\\n[/block]\\n\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"Input Paraemter\",\\n    \"h-1\": \"Type\",\\n    \"h-2\": \"Default\",\\n    \"h-3\": \"Description\",\\n    \"0-0\": \"project_id\",\\n    \"0-1\": \"str\",\\n    \"0-2\": \"None\",\\n    \"0-3\": \"The unique identifier for the project.\",\\n    \"1-0\": \"role\",\\n    \"1-1\": \"str\",\\n    \"1-2\": \"None\",\\n    \"1-3\": \"The permissions role being revoked. Can be one of\\\\n- \\'READ\\'\\\\n- \\'WRITE\\'\\\\n- \\'OWNER\\'\",\\n    \"2-0\": \"user_name\",\\n    \"2-1\": \"Optional [str]\",\\n    \"2-2\": \"None\",\\n    \"2-3\": \"A username with which the project will be revoked. Typically an email address.\",\\n    \"3-0\": \"team_name\",\\n    \"3-1\": \"Optional [str]\",\\n    \"3-2\": \"None\",\\n    \"3-3\": \"A team with which the project will be revoked.\"\\n  },\\n  \"cols\": 4,\\n  \"rows\": 4\\n}\\n[/block]\\n\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"PROJECT_ID = \\'example_project\\'\\\\n\\\\nclient.unshare_project(\\\\n    project_name=PROJECT_ID,\\\\n    role=\\'READ\\',\\\\n    user_name=\\'user@example.com\\'\\\\n)\",\\n      \"language\": \"python\",\\n      \"name\": \"Usage\"\\n    }\\n  ]\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Settings\"\\nslug: \"settings\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:26:28.689Z\"\\nupdatedAt: \"2022-11-10T19:09:26.719Z\"\\n---\\n![](https://files.readme.io/d937de2-Home_Page.png \"Home_Page.png\")\\n\\nThe Settings section captures team setup, permissions, and credentials. You can access the **Settings** page from the left menu of the Fiddler UI at all times.\\n\\nThese are the key tabs in **Settings**.\\n\\n## General\\n\\nThe **General** tab shows your organization name, ID, email, and a few other details. The organization ID is needed when accessing Fiddler from the Fiddler Python API client.\\n\\n![](https://files.readme.io/3f2e734-general.png \"general.png\")\\n\\n## Access\\n\\nThe **Access** tab shows the users, teams, and invitations for everyone in the organization.\\n\\n### Users\\n\\nThe **Users** tab shows all the users that are part of this organization.\\n\\n![](https://files.readme.io/c8c5bf1-access_user.png \"access_user.png\")\\n\\n### Teams\\n\\nThe **Teams** tab shows all the teams that are part of this organization.\\n\\n![](https://files.readme.io/8cba270-access_team.png \"access_team.png\")\\n\\nYou can create a team by clicking on the plus (**`+`**) icon on the top-right.\\n\\n> 🚧 Note\\n> \\n> Only Administrators can create teams. The plus (**`+`**) icon will not be visible unless you have Administrator permissions.\\n\\n![](https://files.readme.io/b0c4c53-access_create_team.png \"access_create_team.png\")\\n\\n### Invitations\\n\\nThe **Invitations** tab shows all pending user invitations.\\n\\n![](https://files.readme.io/5cb4046-access_invitation.png \"access_invitation.png\")\\n\\nYou can invite a user by clicking on the plus (**`+`**) icon on the top-right.\\n\\n> 🚧 Note\\n> \\n> Only Administrators can invite users. The plus (**`+`**) icon will not be visible unless you have Administrator permissions.\\n\\n![](https://files.readme.io/abb030c-access_invite_user.png \"access_invite_user.png\")\\n\\n## Credentials\\n\\nThe **Credentials** tab displays user access keys. These access keys are used by Fiddler Python client for authentication. Each Administrator or Member can create a unique key by clicking on **Create Key**.\\n\\n![](https://files.readme.io/fce7911-credentials.png \"credentials.png\")\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_',\n",
       " 'slug: \"product-tour\"  Atheism Classifier | Text        | scikit-learn        | Random Forest       | Binary Classification      | Fiddler Shapley       |\\n| Wine Quality  | Linear Model Wine Regressor     | Tabular     | scikit-learn        | Elastic Net         | Regression                 | Fiddler Shapley       |\\n|               | DNN Wine Regressor              | Tabular     | Tensorflow          |                     | Regression                 | Fiddler Shapley       |\\n\\nSee the [README](https://github.com/fiddler-labs/fiddler-examples) for more information.\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_\\n\\n[block:html]\\n{\\n  \"html\": \"<div class=\\\\\"fiddler-cta\\\\\">\\\\n<a class=\\\\\"fiddler-cta-link\\\\\" href=\\\\\"https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\\\\\"><img src=\\\\\"https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\\\\\" alt=\\\\\"Fiddler Demo\\\\\"></a>\\\\n</div>\"\\n}\\n[/block]',\n",
       " 'slug: \"monitoring-xai-quick-start\"  information (ModelInfo) provided above.\\n\\n\\n```python\\nclient.add_model_surrogate(\\n    project_id=PROJECT_ID,\\n    model_id=MODEL_ID\\n)\\n```\\n\\n\\n```python\\nPATH_TO_EVENTS_CSV = \\'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/churn_events.csv\\'\\n\\nproduction_df = pd.read_csv(PATH_TO_EVENTS_CSV)\\n# Shift the timestamps of the production events to be as recent as today \\nproduction_df[\\'timestamp\\'] = production_df[\\'timestamp\\'] + (int(time.time() * 1000) - production_df[\\'timestamp\\'].max())\\n```\\n\\n\\n```python\\nclient.publish_events_batch(\\n    project_id=PROJECT_ID,\\n    model_id=MODEL_ID,\\n    batch_source=production_df,\\n    timestamp_field=\\'timestamp\\',\\n    id_field=\\'customer_id\\' # Optional\\n)\\n```\\n\\n## 5. Get insights\\n\\n**You\\'re all done!**\\n  \\nYou can head to your Fiddler URL and start getting enhanced monitoring and explainability into the surrogate model.\\n\\nRun the following code block to get your URL.\\n\\n\\n```python\\nprint(\\'/\\'.join([URL, \\'projects\\', PROJECT_ID, \\'models\\', MODEL_ID, \\'explain\\']))\\n```\\n\\nThe following screen will be available to you upon completion.\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/xai_explain_tab.png?raw=true\" />\\n        </td>\\n    </tr>\\n</table>\\n\\nHit the run buttong and data will be populated. Hit the \"bulb\" icon and you will see point explanations.\\n<table>\\n    <tr>\\n        <td>\\n            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/xai_shap_explanations.png?raw=true\" />\\n        </td>\\n    </tr>\\n</table>\\n\\n\\n\\nYou can also run explanations from the client\\n\\n\\n```python\\n#slice to run explanation on\\nexplain_df = production_df[1:2]\\nexplain_df\\n```\\n\\n\\n```python\\nexplanation = client.run_explanation(\\n    project_id=PROJECT_ID,\\n    model_id=MODEL_ID,\\n    dataset_id=DATASET_ID,\\n    df=explain_df\\n)\\n```\\n\\n\\n```python\\nexplanation\\n```\\n\\n\\n```python\\nfeature_importance = client.run_feature_importance(\\n    project_id=PROJECT_ID,\\n    model_id=MODEL_ID,\\n    dataset_id=DATASET_ID\\n)\\n```\\n\\n\\n```python\\nfeature_importance\\n```\\n\\n\\n\\n---\\n\\n\\n**Questions?**  \\n  \\nCheck out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\\n\\nIf you\\'re still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we\\'ll get back to you shortly.\\n',\n",
       " '---\\ntitle: \"client.delete_model\"\\nslug: \"clientdelete_model\"\\nexcerpt: \"Deletes a model from a project.\"\\nhidden: false\\ncreatedAt: \"2022-05-23T19:31:30.675Z\"\\nupdatedAt: \"2023-06-15T15:41:26.869Z\"\\n---\\nFor more information, see [Uploading a Model Artifact](doc:uploading-model-artifacts).\\n\\n| Input Parameter | Type | Default | Description                            |\\n| :-------------- | :--- | :------ | :------------------------------------- |\\n| project_id      | str  | None    | The unique identifier for the project. |\\n| model_id        | str  | None    | A unique identifier for the model      |\\n\\n```python Usage\\nPROJECT_ID = \\'example_project\\'\\nMODEL_ID = \\'example_model\\'\\n\\nclient.delete_model(\\n    project_id=PROJECT_ID,\\n    model_id=MODEL_ID\\n)\\n```',\n",
       " '---\\ntitle: \"Inviting Users\"\\nslug: \"inviting-users\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:07:27.927Z\"\\nupdatedAt: \"2023-04-11T19:31:56.030Z\"\\n---\\n## Invite a user to Fiddler\\n\\n> 🚧 \\n> \\n> To invite a user to Fiddler, you will need [Administrator permissions](doc:authorization-and-access-control). If you do not have access to an Administrator account, please contact your server administrator.\\n\\nInviting a user is easy. From anywhere on the Fiddler UI, just follow these four steps:\\n\\n1. Go to the **Settings** page.\\n2. Click on the **Access** tab.\\n3. Click on the **Invitations** section.\\n4. Click on the plus icon on the right.\\n\\n![](https://files.readme.io/3bd55c1-invite_a_user.png \"invite_a_user.png\")\\n\\nWhen you click on the plus icon, an invite popup screen will appear as follows:\\n\\n![](https://files.readme.io/8e3806f-Screen_Shot_2023-04-11_at_12.27.32_PM.png)\\n\\nOnce the invitation has been sent, the user should receive a signup link at the email provided.\\n\\n## Getting an invitation link\\n\\nIn the case where the email address is not associated with an inbox, you can get the invite link by clicking **Copy invite link** after the invitation has been created.\\n\\n![](https://files.readme.io/25b8659-get_invite_link.png \"get_invite_link.png\")\\n\\n## What if I\\'m using SSO?\\n\\nWhether you are using normal sign-on or single sign-on, **the process for inviting users is the same**.\\n\\nIf using SSO, a user should still sign up using their invitation link. Once they have created their account, their SSO login will be enabled.',\n",
       " '---\\ntitle: \"Updating model artifacts\"\\nslug: \"updating-model-artifacts\"\\nexcerpt: \"Update a model already in Fiddler (surrogate or user artifact model)\"\\nhidden: false\\ncreatedAt: \"2023-02-01T15:55:08.912Z\"\\nupdatedAt: \"2023-03-08T21:10:47.961Z\"\\n---\\nIf you need to update a model artifact already uploaded in Fiddler, you can use the `client.update_model_artifact` function. This allows you to replace a surrogate model or your own uploaded model.\\n\\nOnce you have prepared the [model artifacts directory](doc:artifacts-and-surrogates), you can update your model using [client.update_model_artifact](ref:clientupdate_model_artifact)\\n\\n```python\\nPROJECT_ID = \\'example_project\\'\\nMODEL_ID = \\'example_model\\'\\nMODEL_ARTIFACTS_DIR = Path(\\'model/\\')\\n\\nclient.update_model_artifact(\\n    artifact_dir=MODEL_ARTIFACTS_DIR,\\n    project_id=PROJECT_ID,\\n    model_id=MODEL_ID\\n)\\n```',\n",
       " '---\\ntitle: \"client.delete_baseline\"\\nslug: \"delete_baseline\"\\nhidden: false\\ncreatedAt: \"2022-11-03T16:49:42.846Z\"\\nupdatedAt: \"2023-05-09T18:32:06.552Z\"\\n---\\nDeletes an existing baseline from a project\\n\\n| Input Parameter | Type   | Required | Description                            |\\n| :-------------- | :----- | :------- | :------------------------------------- |\\n| project_id      | string | Yes      | The unique identifier for the project  |\\n| model_id        | string | Yes      | The unique identifier for the model    |\\n| baseline_id     | string | Yes      | The unique identifier for the baseline |\\n\\n```python Usage\\nPROJECT_NAME = \\'example_project\\'\\nMODEL_NAME = \\'example_model\\'\\nBASELINE_NAME = \\'example_preconfigured\\'\\n\\n\\nclient.delete_baseline(\\n  project_id=PROJECT_NAME,\\n  model_id=MODEL_NAME,\\n  baseline_id=BASELINE_NAME,\\n)\\n```',\n",
       " '---\\ntitle: \"Authorization and Access Control\"\\nslug: \"authorization-and-access-control\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:26:44.914Z\"\\nupdatedAt: \"2023-06-14T21:25:53.499Z\"\\n---\\n## Project Roles\\n\\nEach project supports its own set of permissions for its users.\\n\\n![](https://files.readme.io/caf2bc9-project_settings.png \"project_settings.png\")\\n\\n![](https://files.readme.io/97b71c4-project_settings_add.png \"project_settings_add.png\")\\n\\nFor more details refer to [Administration Page](doc:administration-platform) in the Platform Guide.\\n\\n[^1]\\\\: _Join our [community Slack](https://www.fiddler.ai/slackinvite) to ask any questions_',\n",
       " '---\\ntitle: \"client.upload_model_package\"\\nslug: \"clientupload_model_package\"\\nexcerpt: \"Registers a model with Fiddler and uploads a model artifact to be used for explainability and fairness capabilities.\"\\nhidden: false\\ncreatedAt: \"2022-05-23T19:21:34.380Z\"\\nupdatedAt: \"2023-04-06T22:44:00.734Z\"\\n---\\n> 🚧 Deprecated\\n> \\n> This client method is being deprecated and will not be supported in future versions of the client.  Please use _client.add_model_artifact()_ going forward.\\n\\nFor more information, see [Uploading a Model Artifact](doc:uploading-model-artifacts).\\n\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"Input Parameter\",\\n    \"h-1\": \"Type\",\\n    \"h-2\": \"Default\",\\n    \"h-3\": \"Description\",\\n    \"0-0\": \"project_id\",\\n    \"0-1\": \"str\",\\n    \"0-2\": \"None\",\\n    \"0-3\": \"The unique identifier for the project.\",\\n    \"1-0\": \"model_id\",\\n    \"1-1\": \"str\",\\n    \"1-2\": \"None\",\\n    \"1-3\": \"A unique identifier for the model.\",\\n    \"2-0\": \"artifact_path\",\\n    \"2-1\": \"pathlib.Path\",\\n    \"2-2\": \"None\",\\n    \"2-3\": \"A path to the directory containing all of the model files needed to run the model.\",\\n    \"3-0\": \"deployment_type\",\\n    \"3-1\": \"Optional ramet\",\\n    \"3-2\": \"\\'predictor\\'\",\\n    \"3-3\": \"The type of deployment for the model. Can be one of  \\\\n_ \\'predictor\\' — Just a predict endpoint is exposed.  \\\\n_ \\'executor\\' — The model\\'s internals are exposed.\",\\n    \"4-0\": \"image_uri\",\\n    \"4-1\": \"Optional ramet\",\\n    \"4-2\": \"None\",\\n    \"4-3\": \"A URI of the form \\'/:\\'. If specified, the image will be used to create a new runtime to serve the model.\",\\n    \"5-0\": \"namespace\",\\n    \"5-1\": \"Optional ramet\",\\n    \"5-2\": \"\\'default\\'\",\\n    \"5-3\": \"The Kubernetes namespace to use for the newly created runtime. image_uri must be specified.\",\\n    \"6-0\": \"port\",\\n    \"6-1\": \"Optional ramet\",\\n    \"6-2\": \"5100\",\\n    \"6-3\": \"The port to use for the newly created runtime. image_uri must be specified.\",\\n    \"7-0\": \"replicas\",\\n    \"7-1\": \"Optional ramet\",\\n    \"7-2\": \"1\",\\n    \"7-3\": \"The number of replicas running the model. image_uri must be specified.\",\\n    \"8-0\": \"cpus\",\\n    \"8-1\": \"Optional ramet\",\\n    \"8-2\": \"0.25\",\\n    \"8-3\": \"The number of CPU cores reserved per replica. image_uri must be specified.\",\\n    \"9-0\": \"memory\",\\n    \"9-1\": \"Optional ramet\",\\n    \"9-2\": \"\\'128m\\'\",\\n    \"9-3\": \"The amount of memory reserved per replica. image_uri must be specified.\",\\n    \"10-0\": \"gpus\",\\n    \"10',\n",
       " '---\\ntitle: \"On-prem Installation Guide\"\\nslug: \"installation-guide\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:20:10.097Z\"\\nupdatedAt: \"2022-08-19T17:44:14.367Z\"\\n---\\nFiddler can run on most mainstream flavors of Kubernetes, provided that a suitable [storage class](https://kubernetes.io/docs/concepts/storage/storage-classes/) is available to provide POSIX-compliant block storage (see [On-prem Technical Requirements](technical-requirements)).\\n\\nBefore you start\\n----------------\\n\\n- Create a namespace where Fiddler will be deployed, or request that a namespace/project be created for you by the team that administers your Kubernetes cluster.\\n  ```text\\n  [~] kubectl create ns my-fiddler-ns\\n  ```\\n\\n- Identify the name of the storage class(es) that you will use for Fiddler\\'s block storage needs. Consult the team that administers your Kubernetes cluster for guidance if you are not sure which class to use.\\n  ```\\n  [~] kubectl get storageclass\\n  NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\\n  gp2 (default)   kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  96d\\n  ```\\n\\n- If using Kubernetes [ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) to route traffic to Fiddler, identify the name of the ingress class that should be used. Consult the team that administers your Kubernetes cluster for guidance if you are not sure which class to use.\\n  ```\\n  [~] kubectl get ingressclass\\n  NAME    CONTROLLER             PARAMETERS   AGE\\n  nginx   k8s.io/ingress-nginx   <none>       39d\\n  ```\\n\\nQuick-start any-prem deployment\\n-------------------------------\\n\\nFollow the steps below for a quick-start deployment of Fiddler on your Kubernetes cluster suitable for demonstration purposes. This configuration assumes that an ingress controller is available the cluster.\\n\\n1. Create a `Secret` for pulling images from the Fiddler container registry using the YAML manifest provided to you.\\n\\n   - Verify that the name of the secret is `fiddler-pull-secret`  \\n     ```yaml\\n     apiVersion: v1\\n     kind: Secret\\n     metadata:\\n       name: fiddler-pull-secret\\n     data:\\n       .dockerconfigjson: [REDACTED]\\n     type: kubernetes.io/dockerconfigjson\\n     ```\\n\\n   - Create the secret in the namespace where Fiddler will be deployed.\\n\\n     ```\\n     [~] kubectl -n my-fiddler-ns apply -f fiddler-pull-secret.yaml\\n     ```\\n\\n2. Deploy Fiddler using Helm.\\n\\n   ```\\n   [~] helm repo add fiddler https://helm.fiddler.ai/stable/fiddler\\n   [~] helm repo update\\n   [~] export STORAGE_CLASS=<my-storage-class>\\n   [~] export INGRESS_CLASS=<my-ingress-class>\\n   [~] export FIDDLER_FQDN=fiddler.acme.com\\n   [~] helm upgrade -i -n my-fiddler-ns \\\\\\n      -f https://helm.fiddler.ai/stable/samples/v2.yaml \\\\\\n      -f https://helm.fiddler.ai/stable/samples/anyprem.yaml  \\\\\\n      --set=\"grafana.grafana\\\\.ini.server.root_url=https://${FIDDLER',\n",
       " '---\\ntitle: \"Uploading a Baseline Dataset\"\\nslug: \"uploading-a-baseline-dataset\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:07:03.211Z\"\\nupdatedAt: \"2022-06-08T15:32:37.948Z\"\\n---\\nTo upload a baseline dataset to Fiddler, you can use the [`client.upload_dataset`](https://api.fiddler.ai/#client-upload_dataset) API. Let\\'s walk through a simple example of how this can be done.\\n\\n***\\n\\nThe first step is to load your baseline dataset into a pandas DataFrame.\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"import pandas as pd\\\\n\\\\ndf = pd.read_csv(\\'example_dataset.csv\\')\",\\n      \"language\": \"python\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:api-header]\\n{\\n  \"title\": \"Creating a DatasetInfo object\"\\n}\\n[/block]\\nThen, you\\'ll need to create a [fdl.DatasetInfo()](ref:fdldatasetinfo) object that can be used to **define the schema for your dataset**.\\n\\nThis schema can be inferred from your DataFrame using the [fdl.DatasetInfo.from_dataframe()](ref:fdldatasetinfofrom_dataframe) function.\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"dataset_info = fdl.DatasetInfo.from_dataframe(df)\",\\n      \"language\": \"python\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:callout]\\n{\\n  \"type\": \"info\",\\n  \"title\": \"Info\",\\n  \"body\": \"In the case that you have **categorical columns in your dataset that are encoded as strings**, you can use the `max_inferred_cardinality` argument.\\\\n    \\\\nThis argument specifies a threshold for unique values in a column. Any column with fewer than `max_inferred_cardinality` unique values will be converted to[fdl.DataType.CATEGORY](ref:fdldatatype)  type.\"\\n}\\n[/block]\\n\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \" dataset_info = fdl.DatasetInfo.from_dataframe(\\\\n        df=df,\\\\n        max_inferred_cardinality=1000\\\\n    )\",\\n      \"language\": \"python\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n[block:api-header]\\n{\\n  \"title\": \"Uploading your dataset\"\\n}\\n[/block]\\nOnce you have your [fdl.DatasetInfo()](ref:fdldatasetinfo) object, you can make any **necessary adjustments** before upload (see [Customizing Your Dataset Schema](doc:customizing-your-dataset-schema) ).\\n\\nWhen you\\'re ready, the dataset can be uploaded using [client.upload_dataset()](ref:clientupload_dataset).\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"PROJECT_ID = \\'example_project\\'\\\\nDATASET_ID = \\'example_dataset\\'\\\\n\\\\nclient.upload_dataset(\\\\n    project_id=PROJECT_ID,\\\\n    dataset_id=DATASET_ID,\\\\n    dataset={\\\\n        \\'baseline\\': df\\\\n    },\\\\n    info=dataset_info\\\\n)\",\\n      \"language\": \"python\"\\n    }\\n  ]\\n}\\n[/block]',\n",
       " '---\\ntitle: \"client.share_project\"\\nslug: \"clientshare_project\"\\nexcerpt: \"Shares a project with a user or team.\"\\nhidden: false\\ncreatedAt: \"2022-05-25T15:28:34.938Z\"\\nupdatedAt: \"2022-06-21T17:25:31.249Z\"\\n---\\n[block:callout]\\n{\\n  \"type\": \"info\",\\n  \"title\": \"Info\",\\n  \"body\": \"Administrators can share any project with any user. If you lack the required permissions to share a project, contact your organization administrator.\"\\n}\\n[/block]\\n\\n[block:parameters]\\n{\\n  \"data\": {\\n    \"h-0\": \"Input Paraemter\",\\n    \"h-1\": \"Type\",\\n    \"h-2\": \"Default\",\\n    \"h-3\": \"Description\",\\n    \"0-0\": \"project_id\",\\n    \"0-1\": \"str\",\\n    \"0-2\": \"None\",\\n    \"0-3\": \"The unique identifier for the project.\",\\n    \"1-0\": \"role\",\\n    \"1-1\": \"str\",\\n    \"1-2\": \"None\",\\n    \"1-3\": \"The permissions role being shared. Can be one of\\\\n- \\'READ\\'\\\\n- \\'WRITE\\'\\\\n- \\'OWNER\\'\",\\n    \"2-0\": \"user_name\",\\n    \"2-1\": \"Optional [str]\",\\n    \"2-2\": \"None\",\\n    \"2-3\": \"A username with which the project will be shared. Typically an email address.\",\\n    \"3-0\": \"team_name\",\\n    \"3-1\": \"Optional [str]\",\\n    \"3-2\": \"None\",\\n    \"3-3\": \"A team with which the project will be shared.\"\\n  },\\n  \"cols\": 4,\\n  \"rows\": 4\\n}\\n[/block]\\n\\n[block:code]\\n{\\n  \"codes\": [\\n    {\\n      \"code\": \"PROJECT_ID = \\'example_project\\'\\\\n\\\\nclient.share_project(\\\\n    project_name=PROJECT_ID,\\\\n    role=\\'READ\\',\\\\n    user_name=\\'user@example.com\\'\\\\n)\",\\n      \"language\": \"python\",\\n      \"name\": \"Usage\"\\n    }\\n  ]\\n}\\n[/block]',\n",
       " '---\\ntitle: \"Uploading a TensorFlow SavedModel Model Artifact\"\\nslug: \"tensorflow-savedmodel\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:13:41.335Z\"\\nupdatedAt: \"2023-04-07T01:28:00.666Z\"\\n---\\n> 🚧 Note\\n> \\n> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](doc:uploading-model-artifacts).\\n\\nSuppose you would like to upload a model artifact for a **TensorFlow (SavedModel) model**.\\n\\nFollowing is an example of what the `package.py` script may look like.\\n\\n```python\\nimport pickle\\nfrom pathlib import Path\\nimport pandas as pd\\nimport tensorflow as tf\\n\\nPACKAGE_PATH = Path(__file__).parent\\n\\nOUTPUT_COLUMN = [\\'probability_over_50k\\']\\n\\nclass MyModel:\\n\\n    def __init__(self):\\n        \\n        # Load the model\\n        self.model = tf.keras.models.load_model(PACKAGE_PATH / \\'saved_model\\')\\n\\n    def predict(self, input_df):\\n        \\n        # Store predictions in a DataFrame\\n        return pd.DataFrame(self.model.predict(input_df), columns=OUTPUT_COLUMN)\\n\\ndef get_model():\\n    return MyModel()\\n```',\n",
       " '---\\ntitle: \"Alerts with Fiddler Client\"\\nslug: \"alerts-client\"\\nhidden: false\\ncreatedAt: \"2022-10-25T16:49:32.709Z\"\\nupdatedAt: \"2023-04-12T18:11:53.062Z\"\\n---\\nThe complete user guide for alerts and setting up alert rules in the Fiddler UI is provided [here](doc:alerts-ui). In addition to using the Fiddler UI, users have the flexibility to set up alert rules using the Fiddler API client. In particular, the Fiddler client enables the following workflows:\\n\\n- Add alert rules\\n- Delete alert rules\\n- Get the list of all alert rules\\n- Get the list of triggered alerts\\n\\nIn this document we present examples of how to use the Fiddler client for different alert rule tasks.\\n\\n## Add an Alert Rule\\n\\nThe Fiddler client can be used to create a variety of alert rules. Rules can be of **Data Drift**, **Performance**, **Data Integrity**, and **Service Metrics ** types and they can be compared to absolute or to relative values.\\n\\n### Notifications\\n\\nBefore creating a new alert rule, users choose the type of the notification that will be leveraged by Fiddler when an alert is raised. Currently Fiddler client supports email and PagerDuty services as notifications. To create a notification configuration we call the [build_notifications_config()](ref:clientbuild_notifications_config) API. For example, the following code snippet creates a notification configuration using a comma separated list of email addresses.\\n\\n```python python\\nnotifications_config_emails = client.build_notifications_config(\\n  emails = \"username_1@email.com,username_2@email.com\"\\n)\\n```\\n\\n\\n\\nTo create a notification configuration using both email addresses and pager duty.\\n\\n```python python\\nnotifications_config = client.build_notifications_config(\\n  emails = \"username_1@email.com,username_2@email.com\"\\n  pagerduty_services = \\'pagerduty_service_1,\"pagerduty_service_2\",\\n  pagerduty_severity = \\'critical\\'\\n)\\n```\\n\\n\\n\\n### Example 1: Data Integrity Alert Rule to compare against a raw value\\n\\nNow let\\'s sets up a Data Integrity alert rule which triggers an email notification when published events have 5% null values in any 1 hour bin for the _age_ column. Notice compare_to = \\'raw_value\\'. The [add_alert_rule()](ref:clientadd_alert_rule) API is used to create alert rules.\\n\\n```python\\nclient.add_alert_rule(\\n    name = \"age-null-1hr\",\\n    project_id = PROJECT_ID,\\n    model_id = MODEL_ID,\\n    alert_type = fdl.AlertType.DATA_INTEGRITY,\\n    metric = fdl.Metric.MISSING_VALUE,\\n    bin_size = fdl.BinSize.ONE_HOUR, \\n    compare_to = fdl.CompareTo.RAW_VALUE,\\n    warning_threshold = 5,\\n    critical_threshold = 10,\\n    condition = fdl.AlertCondition.GREATER,\\n    column = \"age\",\\n    priority = fdl.Priority.HIGH,\\n    notifications_config = notifications_config\\n)\\n```\\n\\n\\n\\nPlease note, the possible values for bin_size are \\'one_hour\\', \\'one_day\\', and \\'seven_days\\'. When  alert_type is \\'data_integrity\\', use one of \\'missing_value\\', \\'range_violation\\', or \\'type_violation\\' for metric type. \\n\\n### Example 2: Performance Alert Rule to compare against a previous time window\\n\\nAnd the following API call sets up a Performance alert rule which triggers an email notification when precision metric is 5% higher than that from 1 hr bin one day',\n",
       " 'slug: \"streaming-live-events\"  called in real-time right after your model inference. \\n\\n> 📘 Info\\n> \\n> You can also publish events as part of a batch call after the fact using the `publish_events_batch` API (click [here](https://api.fiddler.ai/#client-publish_events_batch) for more information). In this case, you will need to send Fiddler the original event timestamps as to accurately populate the time series charts.\\n\\nFollowing is a description of all the parameters for `publish_event`:\\n\\n- `project_id`: Project ID for the project this event belongs to.\\n\\n- `model_id`: Model ID for the model this event belongs to.\\n\\n- `event`: The actual event as an array. The event can contain:\\n\\n  - Inputs\\n  - Outputs\\n  - Target\\n  - Decisions (categorical only)\\n  - Metadata\\n\\n- `event_id`: A user-generated unique event ID that Fiddler can use to join inputs/outputs to targets/decisions/metadata sent later as an update.\\n\\n- `update_event`: A flag indicating if the event is a new event (insertion) or an update to an existing event. When updating an existing event, it\\'s required that the user sends an `event_id`.\\n\\n- `event_timestamp`: The timestamp at which the event (or update) occurred, represented as a UTC timestamp in milliseconds. When updating an existing event, use the time of the update, i.e., the time the target/decision were generated and not when the model predictions were made.',\n",
       " '---\\ntitle: \"Authorizing the Client\"\\nslug: \"authorizing-the-client\"\\nhidden: false\\ncreatedAt: \"2022-04-19T17:18:59.729Z\"\\nupdatedAt: \"2023-01-23T19:54:08.750Z\"\\n---\\nIn order to use the client, you’ll need to provide some **authorization details**.\\n\\nSpecifically, there are three pieces of information that are required:\\n\\n- The [URL](#finding-your-url) you are connecting to\\n- Your [organization ID](#finding-your-organization-id)\\n- An [authorization token](#finding-your-authorization-token) for your user\\n\\nThis information can be provided in **two ways**:\\n\\n1. As arguments to the client when it\\'s instantiated (see [`fdl.FiddlerApi`](https://api.fiddler.ai/#fdl-fiddlerapi))\\n2. In a configuration file (see [`fiddler.ini`](#authorizing-via-configuration-file))\\n\\n## Finding your URL\\n\\nThe URL should point to **wherever Fiddler has been deployed** for your organization.\\n\\nIf using Fiddler’s managed cloud service, it should be of the form  \\n\\n```\\nhttps://app.fiddler.ai\\n```\\n\\n\\n\\n## Finding your organization ID\\n\\nTo find your organization ID, navigate to the **Settings** page. Your organization ID will be immediately available on the **General** tab.\\n\\n![](https://files.readme.io/2c7de6e-finding_your_org_id.png \"finding_your_org_id.png\")\\n\\n## Finding your authorization token\\n\\nTo find your authorization token, first navigate to the **Settings** page. Then click **Credentials** and **Create Key**.\\n\\n![](https://files.readme.io/ea51e6a-finding_your_auth_token.png \"finding_your_auth_token.png\")\\n\\n## Connecting the Client\\n\\nOnce you\\'ve located the URL, the org_id and the authorization token, you can connect the Fiddler client to your environment.\\n\\n```python Connect the Client\\nURL = \\'https://app.fiddler.ai\\'\\nORG_ID = \\'my_org\\'\\nAUTH_TOKEN = \\'9AYWiqwxe2hnCAePxg-uEWJUDYRZIZKBSBpx0TvItnw\\' # not a valid token\\n\\n# Connect to the Fiddler client\\nclient = fdl.FiddlerApi(\\n    url=URL,\\n    org_id=ORG_ID,\\n    auth_token=AUTH_TOKEN\\n)\\n```\\n\\n\\n\\n## Authorizing via configuration file\\n\\nIf you would prefer not to send authorization details as arguments to [`fdl.FiddlerApi`](https://api.fiddler.ai/#fdl-fiddlerapi), you can specify them in a **configuration file** called `fiddler.ini`.\\n\\nThe file should be **located in the same directory as the script or notebook** that initializes the [`fdl.FiddlerApi`](https://api.fiddler.ai/#fdl-fiddlerapi) object.\\n\\n***\\n\\n\\n\\nThe syntax should follow the below example:\\n\\n```python fiddler.ini\\n[FIDDLER]\\nurl = https://app.fiddler.ai\\norg_id = my_org\\nauth_token = xtu4g_lReHyEisNg23xJ8IEex0YZEZeeEbTwAsupT0U\\n```\\n\\n\\n\\nThen you can initialize the [`fdl.FiddlerApi`](https://api.fiddler.ai/#fdl-fiddlerapi) object without any arguments, and Fiddler will automatically detect the `fiddler.ini` file:\\n\\n```python Instantiate with fiddler',\n",
       " 'slug: \"alerts-ui\"  rules across any projects and model at a glance.\\n\\n![](https://files.readme.io/ec2fde7-image.png)\\n\\nA few high level details from the alert rule definition are displayed in the table, but users can select to view the full alert definition by selecting the overflow button (⋮) on the right-hand side of any Alert Rule record and clicking `View All Details`. \\n\\n![](https://files.readme.io/0e1dbdc-image.png)\\n\\nDelete an existing alert by clicking on the overflow button (⋮) on the right-hand side of any Alert Rule record and clicking `Delete`. To make any other changes to an Alert Rule, you will need to delete the alert and create a new one with the desired specifications. \\n\\n![](https://files.readme.io/eddf05e-image.png)\\n\\n## Visualizations\\n\\nThroughout the Alert Rules, Triggered Alerts, and Home pages users will see references to the monitors they set up. These visualizations include Alert Rule priority, threshold severities, and more.\\n\\n### Alert Rule Priority\\n\\nAlert rule priority allows users to specify how important an alert rule is to their workflows, learn more on the [Alerts Platform Guide](https://docs.fiddler.ai/v1.6/docs/alerts-platform).\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/4f87100-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"300px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n### Threshold Severity\\n\\nUsers can specify Warning and Critical thresholds as additional customization on their monitors, learn more on the [Alerts Platform Guide](https://docs.fiddler.ai/v1.6/docs/alerts-platform).\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/664e72e-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"300px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n### Alert Summary\\n\\nOn the Fiddler home page, users can get a summary glance of their triggered alerts, categorized by Alert Type. This view allows users to easily navigate to their degraded models.\\n\\n![](https://files.readme.io/3f76938-image.png)\\n\\n## View Triggered Alerts on Fiddler\\n\\nThe Triggered Alerts view gives a single pane of glass experience where you can view all triggered alerts across any Project and Model. Easily apply time filters to see alerts that fired in a desired range, or customize the table to only show columns that matter the most to you. This view aggregates all triggered alerts by alert rule, where the number of times a given alert rule has been triggered is called out by the `Count` column. Explore the triggered alerts further by clicking on the `Monitor` button to further diagnose your model and data.\\n\\n![](https://files.readme.io/30a5ab5-Screen_Shot_2022-10-03_at_3.39.32_PM.png)\\n\\n## Sample Alert Email\\n\\nHere\\'s a sample of an email that\\'s sent if an alert is triggered:\\n\\n![](https://files.readme.io/9dfc566-Monitor_Alert_Email_0710.png \"Monitor_Alert_Email_0710.png\")\\n\\n## Integrations\\n\\nThe Integrations tab is a read-only view of all the integrations your Admin has enabled for use. As of today, users can configure their Alert Rules to notify them via email or Pager Duty services.\\n\\n![](https://files.readme.io/7462149-image',\n",
       " '---\\ntitle: \"Uploading a TensorFlow HDF5 Model Artifact\"\\nslug: \"tensorflow-hdf5\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:14:00.253Z\"\\nupdatedAt: \"2023-04-07T01:28:22.422Z\"\\n---\\n> 🚧 Note\\n> \\n> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](doc:uploading-model-artifacts).\\n\\nSuppose you would like to upload a model artifact for a **TensorFlow (HDF5) model**.\\n\\nFollowing is an example of what the `package.py` script may look like.\\n\\n```python\\nimport pickle\\nfrom pathlib import Path\\nimport pandas as pd\\nimport tensorflow as tf\\n\\nPACKAGE_PATH = Path(__file__).parent\\n\\nOUTPUT_COLUMN = [\\'probability_over_50k\\']\\n\\nclass MyModel:\\n\\n    def __init__(self):\\n        \\n        # Load the model\\n        self.model = tf.keras.models.load_model(PACKAGE_PATH / \\'model.h5\\')\\n\\n    def predict(self, input_df):\\n        \\n        # Store predictions in a DataFrame\\n        return pd.DataFrame(self.model.predict(input_df), columns=OUTPUT_COLUMN)\\n\\ndef get_model():\\n    return MyModel()\\n```',\n",
       " '---\\ntitle: \"Dashboard Utilities\"\\nslug: \"dashboard-utilities\"\\nhidden: false\\ncreatedAt: \"2023-02-22T18:05:38.134Z\"\\nupdatedAt: \"2023-02-22T18:08:05.939Z\"\\n---\\n## Dashboards Utilities\\n\\n### Dashboard Name\\n\\nTo rename your dashboard, simply click on the \"Untitled Dashboard\" title on the top-left corner of the dashboard studio. This will allow you to give your dashboard a more descriptive name that reflects its purpose and contents, making it easier to find and manage among your other dashboards.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/5006167-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\nOnce you\\'ve clicked on the \"Untitled Dashboard\" title to rename your dashboard, simply type in the desired name and hit \"Enter\" on your keyboard to save the new name.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/7ec6e36-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/453ed10-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\nIf you change your mind and want to discard the changes, simply click anywhere on the page outside of the name box. This will cancel the renaming process and leave the dashboard name as it was before.\\n\\n### Save, Copy Link, and Delete\\n\\nYou can easily manage your dashboard by using the control panel located on the top left of the dashboard studio. This panel allows you to save your dashboard, copy a link to it, or delete it entirely. By using these controls, you can easily share your dashboard with others or remove it from your collection if it is no longer needed.\\n\\n![](https://files.readme.io/17c9043-image.png)\\n\\n#### Save\\n\\nIt\\'s important to note that dashboards are not automatically saved, so you\\'ll need to manually save your dashboard in order to lock in the current charts and filters. Once you\\'ve made the desired changes to your dashboard, simply click the \"Save\" button to save your progress. This will also enable you to share or delete your dashboard as needed. By saving your dashboard frequently, you can ensure that you never lose important information or data visualizations.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/c624c13-image.png\",\\n        null,\\n        \"\"\\n      ],\\n      \"align\": \"center\",\\n      \"sizing\": \"400px\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n\\n\\n#### Copy Link\\n\\nIf you want to share your dashboard with other users on Fiddler, the first step is to ensure that they have access to the project that the dashboard belongs to. Once you\\'ve confirmed that they have access, you can easily share the dashboard by copying the dashboard link and sending it to them. This makes it simple to collaborate and share insights with others who are working on the same project or who have an interest in your findings. Note that you can\\'t share a dashboard link until you\\'ve saved the dashboard.\\n\\n[block:image]\\n{\\n  \"images\": [\\n   ',\n",
       " '---\\ntitle: \"About Projects\"\\nslug: \"about-projects\"\\nhidden: false\\ncreatedAt: \"2022-05-23T16:10:39.711Z\"\\nupdatedAt: \"2022-06-13T20:14:54.951Z\"\\n---\\nProjects are **used to organize your models and datasets**. Each project can represent a machine learning task (e.g. predicting house prices, assessing creditworthiness, or detecting fraud).\\n\\nA project **can contain one or more models** (e.g. lin_reg_house_predict, random_forest_house_predict).\\n\\nFor more information on projects, click [here](doc:project-structure).',\n",
       " '---\\ntitle: \"About Models\"\\nslug: \"about-models\"\\nhidden: false\\ncreatedAt: \"2022-05-23T19:03:52.998Z\"\\nupdatedAt: \"2022-12-13T22:54:17.166Z\"\\n---\\nA model is a **representation of your machine learning model**. Each model must have an associated dataset to be used as a baseline for monitoring, explainability, and fairness capabilities.\\n\\nYou **do not need to upload your model artifact in order to onboard your model**, but doing so will significantly improve the quality of explanations generated by Fiddler.',\n",
       " 'slug: \"ranking-model\" )\\nelse:\\n    print(f\\'Project: {PROJECT_ID} already exists\\')\\n```\\n\\n# 2. Upload the Baseline Dataset\\n\\nNow we retrieve the Expedia Dataset as a baseline for this model.\\n\\n\\n```python\\ndf = pd.read_csv(\"https://media.githubusercontent.com/media/fiddler-labs/fiddler-examples/main/quickstart/data/expedia_baseline_data.csv\")\\ndf.head()\\n```\\n\\nFiddler uses this baseline dataset to keep track of important information about your data.\\n  \\nThis includes **data types**, **data ranges**, and **unique values** for categorical variables.\\n\\n---\\n\\nYou can construct a `DatasetInfo` object to be used as **a schema for keeping track of this information** by running the following code block.\\n\\n\\n```python\\ndataset_info = fdl.DatasetInfo.from_dataframe(df=df, max_inferred_cardinality=100)\\ndataset_info\\n```\\n\\nThen use the client\\'s [upload_dataset](https://docs.fiddler.ai/reference/clientupload_dataset) function to send this information to Fiddler!\\n  \\n*Just include:*\\n1. A unique dataset ID\\n2. The baseline dataset as a pandas DataFrame\\n3. The [DatasetInfo](https://docs.fiddler.ai/reference/fdldatasetinfo) object you just created\\n\\n\\n```python\\nDATASET_ID = \\'expedia_data\\'\\nclient.upload_dataset(project_id=PROJECT_ID,\\n                      dataset={\\'baseline\\': df},\\n                      dataset_id=DATASET_ID,\\n                      info=dataset_info)\\n```\\n\\n# 3. Share Model Metadata and Upload the Model\\n\\n\\n```python\\n#create model directory to sotre your model files\\nimport os\\nmodel_dir = \"model\"\\nos.makedirs(model_dir)\\n```\\n\\n### 3.a Adding model metadata to Fiddler\\nTo add a Ranking model you must specify the ModelTask as `RANKING` in the model info object.  \\n\\nAdditionally, you must provide the `group_by` argument that corresponds to the query search id. This `group_by` column should be present either in:\\n- `features` : if it is used to build and run the model\\n- `metadata_cols` : if not used by the model \\n\\nOptionally, you can give a `ranking_top_k` number (default is 50). This will be the number of results within each query to take into account while computing the performance metrics in monitoring.  \\n\\nUnless the prediction column was part of your baseline dataset, you must provide the minimum and maximum values predictions can take in a dictionary format (see below).  \\n\\nIf your target is categorical (string), you need to provide the `categorical_target_class_details` argument. If your target is numerical and you don\\'t specify this argument, Fiddler will infer it.   \\n\\nThis will be the list of possible values for the target **ordered**. The first element should be the least relevant target level, the last element should be the most relevant target level.\\n\\n\\n```python\\ntarget = \\'click_bool\\'\\nfeatures = list(df.drop(columns=[\\'click_bool\\', \\'score\\']).columns)\\n\\nmodel_info = fdl.ModelInfo.from_dataset_info(\\n    dataset_info=client.get_dataset_info(project_id=PROJECT_ID, dataset_id=DATASET_ID),\\n    target=target,\\n    features=features,\\n    input_type=fdl.ModelInputType.TABULAR,\\n    model_task=fdl.ModelTask.RANKING,\\n    outputs={\\'score\\':[-5.0, 3.0]},\\n    group_by=\\'srch_id\\',\\n    ranking_top_k=20,\\n    categorical_target_class_details=[0, 1]\\n)\\n\\n# inspect model info and modify as needed\\nmodel_info\\n```\\n\\n\\n```python\\nMODEL_ID = \\'expedia_model\\'\\n\\n',\n",
       " '---\\ntitle: \"Point Explainability\"\\nslug: \"point-explainability-platform\"\\nhidden: false\\ncreatedAt: \"2022-11-18T22:57:20.106Z\"\\nupdatedAt: \"2023-08-04T23:22:22.870Z\"\\n---\\nFiddler provides powerful visualizations that can explain your model\\'s behavior. These explanations can be queried at an individual prediction level in the **Explain** tab, at a model level in the **Analyze** tab or within the monitoring context in the **Monitor** tab.\\n\\nExplanations are available in the UI for structured (tabular) and natural language (NLP) models. They are also supported via API using the [fiddler-client](https://pypi.org/project/fiddler-client/) Python package. Explanations are available for both production and baseline queries.\\n\\nFiddler’s explanations are interactive — you can change feature inputs and immediately view an updated prediction and explanation. We have productized several popular **explanation methods** to work fast and at scale:\\n\\n- SHAP and Fiddler SHAP, using Kernel SHAP implementations, are game-theory based methods. They work for all models, because they only require the ability to ask a model for predictions.\\n- Integrated Gradients, which is particularly performant for deep learning models with a large number of inputs. It requires the model’s prediction to be mathematically differentiable, and a prediction gradient must be made available to Fiddler.\\n- Tree SHAP, is not enabled by default but can be used for Tree-based model. This is a faster and model-specific method to approximate Shapley values.\\n\\nThese methods are discussed in more detail below.\\n\\nIn addition to the previous out of the box explanation methods, Fiddler allows to bring your own explanation method. This can be customized in your model’s `package.py` wrapper script.\\n\\n## Tabular Models\\n\\nFor tabular models, Fiddler’s Point Explanation tool shows how any given model prediction can be attributed to its individual input features.\\n\\nThe following is an example of an explanation for a model predicting the likelihood of customer churn:\\n\\n![](https://files.readme.io/b8e4f81-Tabular_Explain.png \"Tabular_Explain.png\")\\n\\nA brief tour of the features above:\\n\\n- **_Explanation Method_**: The explanation method is selected from the **Explanation Type** dropdown.\\n\\n- **_Input Vector_**: The far left column contains the input vector. Each input can be adjusted.\\n\\n- **_Model Prediction_**: The box in the upper-left shows the model’s prediction for this input vector.\\n\\n  - If the model produces multiple outputs (e.g. probabilities in a multiclass classifier), you can click on the prediction field to select and explain any of the output components. This can be particularly useful when diagnosing misclassified examples.\\n\\n- **_Feature Attributions_**: The colored bars on the right represent how the prediction is attributed to the individual feature inputs.\\n\\n  - A positive value (blue bar) indicates a feature is responsible for driving the prediction in the positive direction.\\n  - A negative value (red bar) is responsible for driving the prediction in a negative direction.\\n\\n- **_Baseline Prediction_**: The thin colored line just above the bars shows the difference between the baseline prediction and the model prediction. The specifics of the baseline calculation vary with the explanation method, but usually it\\'s approximately the mean prediction of the training/reference data distribution (i.e. the dataset specified when importing the model into Fiddler). The baseline prediction represents a typical model prediction.\\n\\n**Two numbers** accompany each feature’s attribution bar in the UI.\\n\\n',\n",
       " 'slug: \"fdlmetric\"  the ROC Curve\",\\n    \"12-0\": \"fdl.Metric.F1_SCORE\",\\n    \"12-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION)\",\\n    \"12-2\": \"F1 score\",\\n    \"13-0\": \"fdl.Metric.ECE\",\\n    \"13-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.BINARY_CLASSIFICATION)\",\\n    \"13-2\": \"Expected Calibration Error\",\\n    \"14-0\": \"fdl.Metric.R2\",\\n    \"14-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.REGRESSION)\",\\n    \"14-2\": \"R Squared\",\\n    \"15-0\": \"fdl.Metric.MSE\",\\n    \"15-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.REGRESSION)\",\\n    \"15-2\": \"Mean squared error\",\\n    \"16-0\": \"fdl.Metric.MAPE\",\\n    \"16-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.REGRESSION)\",\\n    \"16-2\": \"Mean Absolute Percentage Error\",\\n    \"17-0\": \"fdl.Metric.WMAPE\",\\n    \"17-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.REGRESSION)\",\\n    \"17-2\": \"Weighted Mean Absolute Percentage Error\",\\n    \"18-0\": \"fdl.Metric.MAE\",\\n    \"18-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.REGRESSION)\",\\n    \"18-2\": \"Mean Absolute Error\",\\n    \"19-0\": \"fdl.Metric.LOG_LOSS\",\\n    \"19-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.MULTICLASS_CLASSIFICATION)\",\\n    \"19-2\": \"Log Loss\",\\n    \"20-0\": \"fdl.Metric.MAP\",\\n    \"20-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.RANKING)\",\\n    \"20-2\": \"Mean Average Precision\",\\n    \"21-0\": \"fdl.Metric.MEAN_NDCG\",\\n    \"21-1\": \"fdl.AlertType.PERFORMANCE  \\\\n(fdl.ModelTask.RANKING)\",\\n    \"21-2\": \"Normalized Discounted Cumulative Gain\"\\n  },\\n  \"cols\": 3,\\n  \"rows\": 22,\\n  \"align\": [\\n    \"left\",\\n    \"left\",\\n    \"left\"\\n  ]\\n}\\n[/block]\\n\\n```coffeescript Usage\\nimport fiddler as fdl\\n\\nclient.add_alert_rule(\\n    name = \"perf-gt-5prec-1hr-1d-ago\",\\n    project_name = \\'project-a\\',\\n    model_name = \\'binary_classification_model-a\\',\\n    alert_type = fdl.AlertType.PERFORMANCE,\\n    metric = fdl.Metric.PRECISION, <----\\n    bin_size = fdl.BinSize.ONE_HOUR, \\n    compare_to = fdl.CompareTo.TIME_PERIOD,\\n    compare_period = fdl.ComparePeriod.ONE_DAY,\\n    warning_threshold = 0.05,\\n    critical_threshold = 0.1,\\n    condition = fdl.AlertCondition.GREATER,\\n    priority = fdl.Priority.HIGH,\\n    notifications_config = notifications_config\\n)\\n```\\n```coffeescript Outputs\\n[AlertRule(alert_rule_uuid=\\'9b8711fa-735e-4a72-977c-c4c8b16543ae\\',\\n           organization_name')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "df6aadea",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message5_2, message5_2, query_embed5_2 = ask2(\"How to delete a project in Fiddler?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "57ced683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a tool called Fiddler Chatbot and your purpose is to use the below documentation from the company Fiddler to answer the subsequent documentation questions. Also, if possible, give me the reference URLs according to the following instructions. The way to create the URLs is: add \"https://docs.fiddler.ai/docs/\" before the \"slug\" value of the document. For any URL references that start with \"doc:\" or \"ref:\" use its value to create a URL by adding \"https://docs.fiddler.ai/docs/\" before that value. Do not use page titles to create urls. Note that if a user asks about uploading events, it means the same as publishing events.  If the answer cannot be found in the documentation, write \"I could not find an answer.\"Re-uploading in Fiddler essentially means having to delete what was uploaded or ingested earlier, making the updates you want to make, and then following the same steps as before for the specific thing you are looking to upload. So for example, if you want make changes to the baseline dataset you uploaded, you will have to delete the dataset and then make modifications for the datasetinfo object or the dataset itself and then upload the dataset again. As for events, currently there isn\\'t a way for the user to directly delete events. Please contact Fiddler personnell for the same. slug: \"product-tour\" )\\n\\n**Projects** represent your organization\\'s distinct AI applications or use cases. Within Fiddler, Projects house all the **Models** specific to a given application, and thus serve as a jumping-off point for the majority of Fiddler’s model monitoring and explainability features.\\n\\nGo ahead and click on the _Lending project_ to navigate to the Project Overview page.\\n\\n![](https://files.readme.io/b008f03-image.png)\\n\\nHere you can see a list of the models contained within the Lending project, as well as a project dashboard to which analyze charts can be pinned. Go ahead and click the “logreg-all” model.\\n\\n![](https://files.readme.io/f3e024d-image.png)\\n\\nFrom the Model Overview page, you can view details about the model: its metadata (schema), the files in its model directory, and its features, which are sorted by impact (the degree to which each feature influences the model’s prediction score).\\n\\nYou can then navigate to the platform\\'s core monitoring and explainability capabilities. These include:\\n\\n- **_Monitor_** — Track and configure alerts on your model’s performance, data drift, data integrity, and overall service metrics. Read the [Monitoring](doc:monitoring-platform) documentation for more details.\\n- **_Analyze_** — Analyze the behavior of your model in aggregate or with respect to specific segments of your population. Read the [Analytics](doc:analytics-ui) documentation for more details.\\n- **_Explain_** — Generate “point” or prediction-level explanations on your training or production data for insight into how each model decision was made. Read the [Explainability](doc:explainability-platform) documentation for more details.\\n- **_Evaluate_** — View your model’s performance on its training and test sets for quick validation prior to deployment. Read the [Evaluation](doc:evaluation-ui) documentation for more details.\\n\\n## Fiddler Samples\\n\\nFiddler Samples is a set of datasets and models that are preloaded into Fiddler. They represent different data types, model frameworks, and machine learning techniques. See the table below for more details.\\n\\n| **Project**   | **Model**                       | **Dataset** | **Model Framework** | **Algorithm**       | **Model Task**             | **Explanation Algos** |\\n| ------------- | ------------------------------- | ----------- | ------------------- | ------------------- | -------------------------- | --------------------- |\\n| Bank Churn    | Bank Churn                      | Tabular     | scikit-learn        | Random Forest       | Binary Classification      | Fiddler Shapley       |\\n| Heart Disease | Heart Disease                   | Tabular     | Tensorflow          |                     | Binary Classification      | Fiddler Shapley, IG   |\\n| IMDB          | Imdb Rnn                        | Text        | Tensorflow          | BiLSTM              | Binary Classfication       | Fiddler Shapley, IG   |\\n| Iris          | Iris                            | Tabular     | scikit-learn        | Logistic Regression | Multi-class Classification | Fiddler Shapley       |\\n| Lending       | Logreg-all                      | Tabular     | scikit-learn        | Logistic Regression | Binary Classification      | Fiddler Shapley       |\\n|               | Logreg-simple                   | Tabular     | scikit-learn        | Logistic Regression | Binary Classification      | Fiddler Shapley       |\\n|               | Xgboost-simple-sagemaker        | Tabular     | scikit-learn        | XGboost             | Binary Classification      | Fiddler Shapley       |\\n| Newsgroup     | Christianity---\\ntitle: \"Project Structure on UI\"\\nslug: \"project-structure\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:26:33.568Z\"\\nupdatedAt: \"2023-02-03T19:45:51.093Z\"\\n---\\nSupervised machine learning involves identifying a predictive task, finding data to enable that task, and building a model using that data. Fiddler captures this workflow with project, dataset, and model entities.\\n\\n## Projects\\n\\nA project represents a machine learning task (e.g. predicting house prices, assessing creditworthiness, or detecting fraud).\\n\\nA project can contain one or more models for the ML task (e.g. LinearRegression-HousePredict, RandomForest-HousePredict).\\n\\nCreate a project by clicking on **Projects** and then clicking on **Add Project**.\\n\\n![](https://files.readme.io/8e4b429-Add_project_0710.png \"Add_project_0710.png\")\\n\\n- **_Create New Project_** — A window will pop up where you can enter the project name and click **Create**. Once the project is created, it will be displayed on the projects page.\\n\\nYou can access your projects from the Projects Page.\\n\\n[block:image]\\n{\\n  \"images\": [\\n    {\\n      \"image\": [\\n        \"https://files.readme.io/82404e6-Screenshot_2022-12-27_at_1.00.15_PM.png\",\\n        null,\\n        \"Projects Page on Fiddler UI\"\\n      ],\\n      \"align\": \"center\",\\n      \"caption\": \"Projects Page on Fiddler UI\"\\n    }\\n  ]\\n}\\n[/block]\\n\\n## Datasets\\n\\nA dataset in Fiddler is a data table containing features, model outputs, and a target for machine learning models. Optionally, you can also upload metadata and “decision” columns, which can be used to segment the dataset for analyses, track business decisions, and work as protected attributes in bias-related workflows. For more details refer to [Datasets](doc:project-architecture#datasets) in the Platform Guide.\\n\\nOnce you click on a particular project, you will be able to see if there are any datasets associated with the project. For example, the bank_churn project, in the following screenshot, has the bank_churn dataset. [Datasets are uploaded via the Fiddler client](ref:clientupload_dataset). \\n\\n![](https://files.readme.io/3fa7700-Screenshot_2022-12-27_at_1.05.05_PM.png)\\n\\n## Models\\n\\nA model in Fiddler represents a machine learning model. A project will have one or more models for the ML task (e.g. a project to predict house prices might contain LinearRegression-HousePredict and RandomForest-HousePredict). For further details refer to the [Models](doc:project-architecture#models) section in the Platform Guide.\\n\\n![](https://files.readme.io/e151df5-Model_Dashboard.png \"Model_Dashboard.png\")\\n\\n### Model Artifacts\\n\\nAt its most basic level, a model in Fiddler is simply a directory that contains [model artifacts](doc:artifacts-and-surrogates) such as:\\n\\n- The model file (e.g. `*.pkl`)\\n- `package.py`: A wrapper script containing all of the code needed to standardize the execution of the model.\\n\\n![](https://files.readme.io/7170489-Model_Details.png \"Model_Details.png\")\\n\\n![](https://files.readme.io/2b3d52e-Model_Details_1.png \"Model_Details_1.png\")\\n\\n## Project Dashboardslug: \"authorizing-the-client\" .ini\\nclient = fdl.FiddlerApi()\\n```Once you have added a model on the Fiddler platform using a specific model info object, that is fdl.ModelInfo, you cannot modify aspects such as features, inputs, outputs, model task etc. specified in the model info object. Currently, if you want to change fundamental details about a modelinfo object, then it is advised to create/add a new model with a new modelinfo object.---\\ntitle: \"Publishing Production Data\"\\nslug: \"publishing-production-data\"\\nhidden: false\\ncreatedAt: \"2022-11-18T23:28:25.348Z\"\\nupdatedAt: \"2022-12-19T19:14:28.171Z\"\\n---\\nThis Section guides you on the various ways you can provide event data to Fiddler and update and retrieve them.---\\ntitle: \"PagerDuty Integration\"\\nslug: \"pagerduty\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:19:10.407Z\"\\nupdatedAt: \"2023-04-07T01:25:06.491Z\"\\n---\\nFiddler offers powerful alerting tools for monitoring models. By integrating with  \\nPagerDuty services, you gain the ability to trigger PagerDuty events within your monitoring  \\nworkflow.\\n\\n> 📘 \\n> \\n> If your organization has already integrated with PagerDuty, then you may skip to the [Setup: In Fiddler](#setup-in-fiddler) section to learn more about setting up PagerDuty within Fiddler.\\n\\n## Setup: In PagerDuty\\n\\n1. Within your PagerDuty Team, navigate to **Services** → **Service Directory**.\\n\\n![](https://files.readme.io/0ae47bb-pagerduty_1.png \"pagerduty_1.png\")\\n\\n\\n\\n2. Within the Service Directory:\\n   - If you are creating a new service for integration, select **+New Service** and follow the prompts to create your service.\\n   - Click the **name of the service** you want to integrate with.\\n\\n![](https://files.readme.io/956dbdf-pagerduty_2.png \"pagerduty_2.png\")\\n\\n\\n\\n3. Navigate to **Integrations** within your service, and select **Add a new integration to this service**.\\n\\n![](https://files.readme.io/ca2e4c2-pagerduty_3.png \"pagerduty_3.png\")\\n\\n\\n\\n4. Enter an **Integration Name**, and under **Integration Type** select the option **Use our API directly**. Then, select the **Add Integration** button to save your new integration. You will be redirected to the Integrations page for your service.\\n\\n![](https://files.readme.io/0f5d5ae-pagerduty_4.png \"pagerduty_4.png\")\\n\\n\\n\\n5. Copy the **Integration Key** for your new integration.\\n\\n![](https://files.readme.io/e144e08-pagerduty_5.png \"pagerduty_5.png\")\\n\\n\\n\\n## Setup: In Fiddler\\n\\n1. Within **Fiddler**, navigate to the **Settings** page, and then to the **PagerDuty Integration** menu. If your organization **already has a PagerDuty service integrated with Fiddler**, you will be able to find it in the list of services.\\n\\n![](https://files.readme.io/8de1a6b-pagerduty_setup_f_1.png \"pagerduty_setup_f_1.png\")\\n\\n\\n\\n2. If you are looking to integrate with a new service, select the **`+`** box on the top right. Then, enter the name of your service, as well as the Integration Key copied from the end of the [Setup: In PagerDuty](#setup-in-pagerduty) section above. After creation, confirm that your new entry is now in the list of available services.\\n\\n![](https://files.readme.io/9febb10-pagerduty_setup_f_2.png \"pagerduty_setup_f_2.png\")\\n\\n\\n\\n> 🚧 \\n> \\n> Creating, editing, and deleting these services is an **ADMINSTRATOR**-only privilege. Please contact an **ADMINSTRATOR** within your organization to setup any new PagerDuty services\\n\\n## PagerDuty Alerts in Fiddler\\n\\n1. Within the **Projects** page, select the model---\\ntitle: \"System Architecture\"\\nslug: \"system-architecture\"\\nhidden: false\\ncreatedAt: \"2022-04-19T20:19:53.311Z\"\\nupdatedAt: \"2023-05-18T21:09:05.870Z\"\\n---\\nFiddler deploys into your private cloud\\'s existing Kubernetes clusters, for which the minimum system requirements can be found [here](doc:technical-requirements).  Fiddler supports deployment into Kubernetes in AWS, Azure, or GCP.  All services of the Fiddler platform are containerized in order to eliminate reliance on other cloud services and to reduce the deployment and maintenance friction of the platform.  This includes storage services like object storage and databases as well as system monitoring services like Grafana.  \\n\\nUpdates to the Fiddler containers is accomplished through a shared container registry (that Fiddler is provided access to).  Updates to the containers are orchestrated using Helm charts.\\n\\nA full-stack deployment of Fiddler is shown in the diagram below. \\n\\n![](https://files.readme.io/7cbfe31-reference_architecture.png)\\n\\nThe Fiddler system components are deployed within a single namespace on a Kubernetes cluster, using the official Fiddler Helm chart.\\n\\n- Fiddler core infrastructure relies on persistent volumes provided within the Kubernetes cluster. We recommend using encrypted storage volumes wherever possible.\\n- Fiddler may be configured to utilize external infrastructure in a self-hosted environment, such as existing PostgresQL servers, but this is not required as all services are containerized by default.\\n- Full-stack \"any-prem\" Fiddler includes observability infrastructure to monitor Fiddler system health and performance. These mainstream observability components may be integrated with external observability systems to support administration in a self-hosted environment.\\n- HTTP traffic to the Fiddler system is handled by an L4 or L7 load balancer or other proxy. TLS termination should usually occur outside the Fiddler system.\\n\\nOnce the platform is running, end users can interface with the Fiddler platform using their browser, the [Fiddler Python client](ref:about-the-fiddler-client), or Fiddler\\'s RESTful APIs.\\n\\nQuestion: How to delete a project in Fiddler?'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message5_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "faf86d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To delete a project in Fiddler, follow these steps:\\n\\n1. Go to the Projects page in Fiddler.\\n2. Find the project you want to delete.\\n3. Click on the three-dot menu icon (⋮) next to the project name.\\n4. Select the \"Delete\" option from the menu.\\n5. Confirm the deletion when prompted.\\n\\nPlease note that deleting a project will permanently remove all associated models, datasets, and other project-related data. Make sure to double-check before deleting a project as this action cannot be undone.\\n\\nReference URL: [https://docs.fiddler.ai/docs/project-structure#projects](https://docs.fiddler.ai/docs/project-structure#projects)'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message5_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "87aa0116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...</td>\n",
       "      <td>[-0.01489181537181139, -0.0028792002703994513,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slug: \"client-setup\" _TOKEN\\n)\\n```\\n```python...</td>\n",
       "      <td>[-0.021182812750339508, -0.004293339792639017,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>---\\ntitle: \"Customer Churn Prediction\"\\nslug:...</td>\n",
       "      <td>[-0.007564195431768894, -0.01073946338146925, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>slug: \"customer-churn-prediction\" /bb02793-chu...</td>\n",
       "      <td>[0.0021712256129831076, -0.004808966536074877,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slug: \"customer-churn-prediction\" Churn-image5...</td>\n",
       "      <td>[0.001282965182326734, 6.48050699965097e-05, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>---\\ntitle: \"Uploading a scikit-learn Model Ar...</td>\n",
       "      <td>[0.0031662925612181425, 0.018320860341191292, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...</td>\n",
       "      <td>[-0.0003570110129658133, -0.021801473572850227...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>Once you have added a model on the Fiddler pla...</td>\n",
       "      <td>[-0.022226542234420776, 0.011369146406650543, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>Custom metrics is an upcoming feature and it i...</td>\n",
       "      <td>[-0.017716489732265472, -0.0035160724073648453...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Re-uploading in Fiddler essentially means havi...</td>\n",
       "      <td>[-0.01760503277182579, 0.011651406064629555, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>305 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    ---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...   \n",
       "1    slug: \"client-setup\" _TOKEN\\n)\\n```\\n```python...   \n",
       "2    ---\\ntitle: \"Customer Churn Prediction\"\\nslug:...   \n",
       "3    slug: \"customer-churn-prediction\" /bb02793-chu...   \n",
       "4    slug: \"customer-churn-prediction\" Churn-image5...   \n",
       "..                                                 ...   \n",
       "300  ---\\ntitle: \"Uploading a scikit-learn Model Ar...   \n",
       "301  ---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...   \n",
       "302  Once you have added a model on the Fiddler pla...   \n",
       "303  Custom metrics is an upcoming feature and it i...   \n",
       "304  Re-uploading in Fiddler essentially means havi...   \n",
       "\n",
       "                                             embedding  \n",
       "0    [-0.01489181537181139, -0.0028792002703994513,...  \n",
       "1    [-0.021182812750339508, -0.004293339792639017,...  \n",
       "2    [-0.007564195431768894, -0.01073946338146925, ...  \n",
       "3    [0.0021712256129831076, -0.004808966536074877,...  \n",
       "4    [0.001282965182326734, 6.48050699965097e-05, 0...  \n",
       "..                                                 ...  \n",
       "300  [0.0031662925612181425, 0.018320860341191292, ...  \n",
       "301  [-0.0003570110129658133, -0.021801473572850227...  \n",
       "302  [-0.022226542234420776, 0.011369146406650543, ...  \n",
       "303  [-0.017716489732265472, -0.0035160724073648453...  \n",
       "304  [-0.01760503277182579, 0.011651406064629555, 0...  \n",
       "\n",
       "[305 rows x 2 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c54d7b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message5_2, message5_2, query_embed5_2 = ask2(\"How to delete a project?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "20ac2f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To delete a project in Fiddler, you can use the `client.delete_project` method. Here is an example of how to use it:\\n\\n```python\\nPROJECT_ID = 'example_project'\\n\\nclient.delete_project(\\n    project_id=PROJECT_ID\\n)\\n```\\n\\nPlease note that you cannot delete a project without deleting the datasets and models associated with that project. For more information, you can refer to the [client.delete_project documentation](https://docs.fiddler.ai/docs/clientdelete_project).\""
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message5_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "07faeac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...</td>\n",
       "      <td>[-0.01489181537181139, -0.0028792002703994513,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slug: \"client-setup\" _TOKEN\\n)\\n```\\n```python...</td>\n",
       "      <td>[-0.021182812750339508, -0.004293339792639017,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>---\\ntitle: \"Customer Churn Prediction\"\\nslug:...</td>\n",
       "      <td>[-0.007564195431768894, -0.01073946338146925, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>slug: \"customer-churn-prediction\" /bb02793-chu...</td>\n",
       "      <td>[0.0021712256129831076, -0.004808966536074877,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slug: \"customer-churn-prediction\" Churn-image5...</td>\n",
       "      <td>[0.001282965182326734, 6.48050699965097e-05, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>---\\ntitle: \"Uploading a scikit-learn Model Ar...</td>\n",
       "      <td>[0.0031662925612181425, 0.018320860341191292, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...</td>\n",
       "      <td>[-0.0003570110129658133, -0.021801473572850227...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>Once you have added a model on the Fiddler pla...</td>\n",
       "      <td>[-0.022226542234420776, 0.011369146406650543, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>Custom metrics is an upcoming feature and it i...</td>\n",
       "      <td>[-0.017716489732265472, -0.0035160724073648453...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Re-uploading in Fiddler essentially means havi...</td>\n",
       "      <td>[-0.01760503277182579, 0.011651406064629555, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>305 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    ---\\ntitle: \"fdl.FiddlerApi\"\\nslug: \"client-se...   \n",
       "1    slug: \"client-setup\" _TOKEN\\n)\\n```\\n```python...   \n",
       "2    ---\\ntitle: \"Customer Churn Prediction\"\\nslug:...   \n",
       "3    slug: \"customer-churn-prediction\" /bb02793-chu...   \n",
       "4    slug: \"customer-churn-prediction\" Churn-image5...   \n",
       "..                                                 ...   \n",
       "300  ---\\ntitle: \"Uploading a scikit-learn Model Ar...   \n",
       "301  ---\\ntitle: \"client.get_slice\"\\nslug: \"clientg...   \n",
       "302  Once you have added a model on the Fiddler pla...   \n",
       "303  Custom metrics is an upcoming feature and it i...   \n",
       "304  Re-uploading in Fiddler essentially means havi...   \n",
       "\n",
       "                                             embedding  \n",
       "0    [-0.01489181537181139, -0.0028792002703994513,...  \n",
       "1    [-0.021182812750339508, -0.004293339792639017,...  \n",
       "2    [-0.007564195431768894, -0.01073946338146925, ...  \n",
       "3    [0.0021712256129831076, -0.004808966536074877,...  \n",
       "4    [0.001282965182326734, 6.48050699965097e-05, 0...  \n",
       "..                                                 ...  \n",
       "300  [0.0031662925612181425, 0.018320860341191292, ...  \n",
       "301  [-0.0003570110129658133, -0.021801473572850227...  \n",
       "302  [-0.022226542234420776, 0.011369146406650543, ...  \n",
       "303  [-0.017716489732265472, -0.0035160724073648453...  \n",
       "304  [-0.01760503277182579, 0.011651406064629555, 0...  \n",
       "\n",
       "[305 rows x 2 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "72a2ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s,r,emb = strings_ranked_by_relatedness(\"How to delete project?\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "136f7888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "49c5d127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Fiddler provides the capability to validate models. You can view your model's performance on its training and test sets for quick validation prior to deployment. For more details, you can refer to the [Evaluation](https://docs.fiddler.ai/docs/evaluation-ui) documentation.\n"
     ]
    }
   ],
   "source": [
    "r = ask2(\"Does Fiddler validate models?\")[0]\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1bafbf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8d85e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s,r,emb = strings_ranked_by_relatedness(\"When should I not use Fiddler?\", df, top_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "870ce4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44679b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59abadec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b95b2349",
   "metadata": {},
   "source": [
    "### Testing if string chunking is lossless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = df[2:3][\"text\"].values[0]\n",
    "num_tokens(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_test = chunked_string(test_string,EMBEDDING_MODEL,1000)\n",
    "recovered = ''.join(chunks_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638fba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered == test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9551911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chunks = []\n",
    "chunk_embedding = []\n",
    "for i , row in df.iterrows():\n",
    "    if num_tokens(row[\"text\"]) > 2000:\n",
    "        df.drop(index= i, inplace=True)\n",
    "        chunked_list = chunked_string(row[\"text\"])\n",
    "        for chunk in chunked_list:\n",
    "            response = openai.Embedding.create(model=EMBEDDING_MODEL, input=chunk)\n",
    "            chunk_embedding.append(response[\"data\"][0][\"embedding\"])\n",
    "            new_chunks.append(chunk)\n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c1efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=[]\n",
    "for i in range(len(modified_docs)):\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=modified_docs[i])\n",
    "    embeddings.append(response[\"data\"][0][\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2f807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4808743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38beb33a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
