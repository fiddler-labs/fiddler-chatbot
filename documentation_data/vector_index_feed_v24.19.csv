text
"# Table of contents

## Overview

* [Welcome](README.md)
* [Product Tour](Overview/product-tour.md)

## Doc Chatbot

* [Fiddler Chat](DOC_CHATBOT/fiddler-chat.md)

## Quickstart Notebooks

* [LLM Monitoring - Simple](QuickStart_Notebooks/simple-llm-monitoring.md)
* [ML Monitoring - Simple](QuickStart_Notebooks/quick-start.md)
* [ML Monitoring - NLP Inputs](QuickStart_Notebooks/simple-nlp-monitoring-quick-start.md)
* [ML Monitoring - CV Inputs](QuickStart_Notebooks/cv-monitoring.md)
* [ML Monitoring - Class Imbalance](QuickStart_Notebooks/class-imbalance-monitoring-example.md)
* [ML Monitoring - Model Versions](QuickStart_Notebooks/ml-monitoring-model-versions.md)
* [ML Monitoring - Ranking](QuickStart_Notebooks/ranking-model.md)
* [ML Monitoring -"
" Feature Impact](QuickStart_Notebooks/user-defined-feature-impact.md)

## Product Guide

* [Product Concepts](product-guide/product-concepts.md)
* [Monitoring](product-guide/monitoring-platform/README.md)
  * [Alerts](product-guide/monitoring-platform/alerts-platform.md)
  * [Class Imbalanced Data](product-guide/monitoring-platform/class-imbalanced-data.md)
  * [Custom Metrics](product-guide/monitoring-platform/custom-metrics.md)
  * [Dashboards](product-guide/monitoring-platform/dashboards-platform.md)
  * [Data Drift](product-guide/monitoring-platform/data-drift-platform.md)
  * [Data Integrity](product-guide/monitoring-platform/data-integrity-platform.md)
  * [Embedding Visualization With Umap](product-guide/monitoring-platform/embedding-visualization-with-umap.md)
  * [Fiddler Query Language](product-guide/monitoring-platform/fiddler-query-language"
".md)
  * [Model Versions](product-guide/monitoring-platform/model-versions.md)
  * [Monitoring Charts](product-guide/monitoring-platform/monitoring-charts-platform.md)
  * [Performance Tracking](product-guide/monitoring-platform/performance-tracking-platform.md)
  * [Segments](product-guide/monitoring-platform/segments.md)
  * [Statistics](product-guide/monitoring-platform/statistics.md)
  * [Vector Monitoring](product-guide/monitoring-platform/vector-monitoring-platform.md)
* [LLM Monitoring](product-guide/llm-monitoring/README.md)
  * [LLM Based Metrics](product-guide/llm-monitoring/llm-based-metrics.md)
  * [Embedding Visualizations for LLM Monitoring](product-guide/llm-monitoring/embedding-visualization-with-umap.md)
  * [Selecting Enrichments](product-guide/llm-monitoring/selecting-enrichments.md)
  * ["
"Enrichments (Private Preview)](product-guide/llm-monitoring/enrichments-private-preview.md)
* [Explainability](product-guide/explainability/README.md)
  * [Model: Artifacts, Package, Surrogate](product-guide/explainability/artifacts-and-surrogates.md)
  * [Global Explainability](product-guide/explainability/global-explainability-platform.md)
  * [Point Explainability](product-guide/explainability/point-explainability-platform.md)
  * [Flexible Model Deployment](product-guide/explainability/flexible-model-deployment/README.md)
    * [On Prem Manual Flexible Model Deployment Xai](product-guide/explainability/flexible-model-deployment/on-prem-manual-flexible-model-deployment-xai.md)
* [Fairness](product-guide/fairness.md)
* [Analytics](product-guide/analytics-eval-platform.md)
* [Model Task Types](product-guide/task-types.md)
* [Administration]("
"product-guide/administration-platform.md)
* [Supported Browsers](product-guide/supported-browsers.md)

## UI Guide

* [Administration](UI_Guide/administration-ui/README.md)
  * [Adding Users](UI_Guide/administration-ui/inviting-users.md)
  * [Authorization and Access Control](UI_Guide/administration-ui/authorization-and-access-control.md)
  * [Application Settings](UI_Guide/administration-ui/settings.md)
* [Monitoring](UI_Guide/monitoring-ui/README.md)
  * [Alerts With Fiddler UI](UI_Guide/monitoring-ui/alerts-with-fiddler-ui.md)
  * [Custom Metrics](UI_Guide/monitoring-ui/custom-metrics.md)
  * [Data Drift](UI_Guide/monitoring-ui/data-drift.md)
  * [Data Integrity](UI_Guide/monitoring-ui/data-integrity.md)
  * [Embedding Visualization Chart Creation](UI"
"_Guide/monitoring-ui/embedding-visualization-chart-creation.md)
  * [Monitoring Charts UI](UI_Guide/monitoring-ui/monitoring-charts-ui.md)
  * [Performance](UI_Guide/monitoring-ui/performance.md)
  * [Segments](UI_Guide/monitoring-ui/segments.md)
  * [Traffic UI](UI_Guide/monitoring-ui/traffic-ui.md)
* [Explainability](UI_Guide/explainability-ui-giude/README.md)
  * [Global Explainability](UI_Guide/explainability-ui-giude/global-explainability.md)
  * [Point Explainability](UI_Guide/explainability-ui-giude/point-explainability.md)
  * [Surrogate Models](UI_Guide/explainability-ui-giude/surrogate-models.md)
* [Analytics](UI_Guide/analytics-ui/README.md)
  * [Events Table in RCA]("
"UI_Guide/analytics-ui/data-table-in-rca.md)
  * [Feature Analytics Creation](UI_Guide/analytics-ui/feature-analytics-chart.md)
  * [Metric Card Creation](UI_Guide/analytics-ui/metric-card.md)
  * [Performance Charts Creation](UI_Guide/analytics-ui/performance-charts-creation.md)
  * [Performance Charts Visualization](UI_Guide/analytics-ui/performance-charts-visualization.md)
* [Dashboards](UI_Guide/dashboards-ui/README.md)
  * [Dashboard Interactions](UI_Guide/dashboards-ui/dashboard-interactions.md)
  * [Dashboard Utilities](UI_Guide/dashboards-ui/dashboard-utilities.md)

## Client Guide

* [Installation and Setup](Client_Guide/installation-and-setup.md)
* [Create a Project and Onboard a Model for Observation](Client_Guide/create-a-project-and-model.md)
* [Publishing Inferences](Client_Guide/publishing"
"-inferences.md)
* [Creating a Baseline Dataset](Client_Guide/creating-a-baseline-dataset.md)
* [Customizing your Model Schema](Client_Guide/customizing-your-model-schema.md)
* [Specifying Custom Missing Value Representations](Client_Guide/specifying-custom-missing-value-representations.md)
* [Adding a Surrogate Model](Client_Guide/surrogate-models-client-guide.md)
* [Uploading Model Artifacts](Client_Guide/uploading-model-artifacts.md)
* [Updating Model Artifacts](Client_Guide/updating-model-artifacts.md)
* [ML Framework Examples](Client_Guide/ml-framework-examples/README.md)
  * [Scikit Learn](Client_Guide/ml-framework-examples/scikit-learn.md)
  * [Tensorflow Hdf5](Client_Guide/ml-framework-examples/tensorflow-hdf5.md)
  * [Tensorflow Savedmodel](Client_Guide/ml-framework-examples/tensorflow"
"-savedmodel.md)
  * [Xgboost](Client_Guide/ml-framework-examples/xgboost.md)
* [Model Task Examples](Client_Guide/model-task-examples/README.md)
  * [Binary Classification](Client_Guide/model-task-examples/binary-classification-1.md)
  * [Multiclass Classification](Client_Guide/model-task-examples/multiclass-classification-1.md)
  * [Regression](Client_Guide/model-task-examples/regression.md)
  * [Uploading A Ranking Model Artifact](Client_Guide/model-task-examples/uploading-a-ranking-model-artifact.md)
* [Publishing Production Data](Client_Guide/publishing-production-data/README.md)
  * [Publishing Batches Of Events](Client_Guide/publishing-production-data/publishing-batches-of-events.md)
  * [Ranking Events](Client_Guide/publishing-production-data/ranking-events.md)
  * [Retrieving Events]("
"client-guide/publishing-production-data/retrieving-events.md)
  * [Streaming Live Events](Client_Guide/publishing-production-data/streaming-live-events.md)
  * [Updating Events](Client_Guide/publishing-production-data/updating-events.md)
  * [Using Custom Timestamps](client-guide/publishing-production-data/using-custom-timestamps.md)
* [Alerts with Fiddler Client](Client_Guide/alerts-with-fiddler-client.md)

## Deployment Guide

* [Deploying Fiddler](Deployment_Guide/deploying-fiddler/README.md)
  * [System Architecture](Deployment_Guide/deploying-fiddler/system-architecture.md)
  * [On-prem Technical Requirements](Deployment_Guide/deploying-fiddler/technical-requirements.md)
  * [On-prem Installation Guide](Deployment_Guide/deploying-fiddler/installation-guide.md)
  * [Routing To Fiddler On-pre"
"m](Deployment_Guide/deploying-fiddler/routing-to-fiddler-on-prem.md)
* [Okta Integration](Deployment_Guide/okta-integration.md)
* [SSO with Azure AD](Deployment_Guide/single-sign-on-with-azure-ad.md)

## Illustrative Walkthroughs

* [Customer Churn Prediction](Illustrative_Walkthroughs/customer-churn-prediction.md)
* [Fraud Detection](Illustrative_Walkthroughs/fraud-detection.md)

## Integrations

* [Data Pipeline Integrations](INTEGRATIONS/data-pipeline-integrations/README.md)
  * [Airflow Integration](INTEGRATIONS/data-pipeline-integrations/airflow-integration.md)
  * [BigQuery Integration](INTEGRATIONS/data-pipeline-integrations/bigquery-integration.md)
  * [Integration With S3](INTEGRATIONS/data-pipeline-integrations/integration-with-s3.md)
  * ["
"Kafka Integration](INTEGRATIONS/data-pipeline-integrations/kafka-integration.md)
  * [Sagemaker Integration](INTEGRATIONS/data-pipeline-integrations/sagemaker-integration.md)
  * [Snowflake Integration](INTEGRATIONS/data-pipeline-integrations/snowflake-integration.md)
* [ML Platform Integrations](INTEGRATIONS/ml-platform-integrations/README.md)
  * [Databricks Integration](INTEGRATIONS/ml-platform-integrations/databricks-integration.md)
  * [Datadog Integration](INTEGRATIONS/ml-platform-integrations/datadog-integration.md)
  * [ML Flow Integration](INTEGRATIONS/ml-platform-integrations/ml-flow-integration.md)
* [Alerting Integrations](INTEGRATIONS/alerting-integrations/README.md)
  * [PagerDuty Integration](INTEGRATIONS/alerting-integrations/pagerduty.md)

## API Integration

* [REST"
" API](API_Guidelines/README.md)
  * [Projects](API_Guidelines/projects.md)
  * [Model](API_Guidelines/model.md)
  * [File Upload](API_Guidelines/file-upload.md)
  * [Custom Metrics](API_Guidelines/custom-metrics.md)
  * [Segments](API_Guidelines/segments.md)
  * [Baseline](API_Guidelines/baseline.md)
  * [Jobs](API_Guidelines/jobs.md)
  * [Alert Rules](API_Guidelines/alert-rules.md)
  * [Environment](API_Guidelines/environment.md)
  * [Explainability](API_Guidelines/explainability.md)
  * [Server Info](API_Guidelines/server-info.md)
  * [Events](api-integration/api_guidelines/events.md)

## Python Client 3 X

* [About Client 3.x](Python_Client_3-x/about-client-3x.md)
"
"* [API Methods 3.x](Python_Client_3-x/api-methods-30.md)
* [Upgrade from 2.x To 3.x](Python_Client_3-x/upgrade-from-2x-to-3x.md)

## History

* [Release Notes](release-notes/README.md)
* [Python Client History](python-client-history.md)
* [Compatibility Matrix](Version_History/compatibility-matrix.md)
* [Product Maturity Definitions](feature-maturity-definitions.md)
"
"---
title: Welcome
slug: welcome
excerpt: >-
  This is Fiddler‚Äôs AI Observability Documentation. Fiddler is a pioneer in AI
  Observability for responsible AI. Data Science, MLOps, and LOB teams use
  Fiddler to monitor, explain, analyze, and improve ML models, generative AI
  models, and AI applications.
metadata:
  title: Welcome | Fiddler Docs
  description: >-
    This document provides links to helpful guides for onboarding, including a
    platform guide, user interface guide, client guide, and deployment guide.
  image: []
  robots: index
createdAt: Mon Feb 27 2023 18:08:02 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Dec 06 2024 00:26:39 GMT+0000 (Coordinated Universal Time)
icon: hand-wave
---

This is Fiddler‚Äôs AI Observability Documentation. 
Fiddler is a pioneer in AI Observability for responsible AI. 
Data Science, MLOps, and LOB teams use Fiddler to monitor, explain, analyze, and improve ML models, generative AI models, and AI applications.

{% hint style=""info"" %}

The Fiddler AI Observability Platform is now available within Amazon SageMaker AI, a part of SageMaker Unified Studio. 
This native integration enables SageMaker customers to use Fiddler to monitor ML models privately and securely, all without leaving Amazon SageMaker AI.

Learn more about the new Amazon SageMaker AI with Fiddler native integration offering [here](https://www.fiddler.ai/blog/fiddler-delivers-native-enterprise-grade-ai-observability-to-amazon-sagemaker-ai-customers).

{% endhint %}

Here you can find a number of helpful guides to aid with onboarding. These include:

<table data-card-size=""large"" data-view=""cards""><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type=""content-ref""></th></tr></thead><tbody><tr><td><p><strong>Product Guide</strong></p><p>How Fiddler does AI Observability and Fiddler-specific terminologies.</p></td><td></td><td></td><td><a href=""product-guide/monitoring-platform/"">monitoring-platform</a></td></tr><tr><td><p><strong>User Interface (UI) Guide</strong></p><p>An introduction to the product UI with screenshots that illustrate how to interface with the product.</p></td><td></td><td></td><td><a href=""UI_Guide/administration-ui/"">administration-ui</a></td></tr><tr><td><p><strong>Client Guide</strong></p><p>For using Fiddler client including API references and code examples.</p></td><td></td><td></td><td><a href=""Client_Guide/installation-and-setup.md"">installation-and-setup.md</a></td></tr><tr><td><p><strong>Deployment Guide</strong></p><p>For technical details on how to deploy Fiddler on your premises or ours.</p></td><td></td><td></td><td><a href=""Deployment_Guide/deploying-fiddler/"">deploying-fiddler</a></td></tr></tbody></table>

{% include "".gitbook/includes/main-doc-footer.md"" %}
"
"# Python Client History

## 3.x Client Version

***

### 3.7
#### **3.7.0**

Release highlights:

* **Robustness via retrying**: this release introduces a persistent HTTP request retrying strategy to enhance fault tolerance in view of transient network problems and retryable HTTP request errors. You can take control of the maximum duration for which an HTTP request is retried by setting the environment variable `FIDDLER_CLIENT_RETRY_MAX_DURATION_SECONDS`.

* **AWS SageMaker authentication support**: to enable that, install version 2.236.0+ of the [AWS Python SageMaker SDK](https://pypi.org/project/sagemaker/). Then, before calling `init()`, set the  environment variable `AWS_PARTNER_APP_AUTH` to `true` and set `AWS_PARTNER_APP_ARN`/`AWS_PARTNER_APP_URL` to meaningful values.

* **Logging improvements**: messages are now emitted to `stderr` instead of"
" `stdout`. Only if the calling context does not configure a root logger this library will actively declare a handler for its own log messages (this automation can be disabled by setting `auto_attach_log_handler=False` during `init()`).

Compatibility changes:

* Pydantic 2.x is now supported (and compatibility with pydantic 1.x has been retained).
* Support for Python 3.8 has been dropped.


API surface additions:

* Introduced `Project.get_or_create()` to reduce code required for instantiating a project.
* Introduced `model.remove_column()`  to allow for removing a column from a model object.

Fixes:

* A transient error during a job status update does not prematurely terminate waiting for a job anymore.
* GET requests do not contain the `Content-Type` header anymore.




### 3.6
#### **3.6.0**

* **Removed**
  * The `get_slice` and `download_slice` methods are removed. Please use `download"
"_data` to retrieve some data.
  * The `get_mutual_info` method is removed.
  * The `SqlSliceQueryDataSource` option is removed from explain, feature impact and importance. Please use the `DatasetDataSource` instead or the UI.

### 3.5
#### **3.5.0**

* **New Features**
  * New `download_data` method, to download a slice of data given an environment, time range and segment. Resulted file can be downloaded either as a CSV or a Parquet file.

### 3.4
#### **3.4.0**

* **Removed**
  * The `get_fairness` method is removed. Please use charts and custom metrics to track / compute fairness metrics on your model.


### 3.3

#### **3.3.2**

* **Modifications**
  * Fixed the error while setting notification config for alert rule.

***

#### **3.3.1**

*"
" **Modifications**
  * Added validation while adding notifications to alert rules.
  * Upgraded dependencies to resolve known vulnerabilities - deepdiff, mypy, pytest, pytest-mock, python-decouple, types-requests and types-simplejson.

***

#### **3.3.0**

* **New Features**
  * Introduced `upload_feature_impact()` method to upload or update feature impact manually.

***

### 3.2

#### **3.2.0**

* **New Features**
  * Introduced evaluation delay in Alerts Rule.
    * Optional `evaluation_delay` parameter added to `AlertRule.__init__` method.
    * It is used to introduce a delay in the evaluation of the alert.

* **Modifications**
  * Fix windows file permission error bug with publish method.

***

### 3.1

#### **3.1.2**

* **Modifications**
  * Adds support to get schema of Column object by `fdl.Column`

####"
" **3.1.1**

* **Modifications**
  * Updated `pydantic` and `typing-extensions` dependencies to support Python 3.12.

#### **3.1.0**

* **New Features**
  * Introduced the native support for model versions.
    * Optional `version` parameter added to `Model`, `Model.from_data`, `Model.from_name` methods.
    * New `duplicate()` method to seamlessly create new version from existing model.
    * Optional `name` parameter added to `Model.list` to offer the ability to list all the versions of a model.

***

### 3.0

#### **3.0.5**

* **New Features**
  * Allowed usage of `group_by()` to form the grouped data for ranking models.

#### **3.0.4**

* **Modifications**
  * Return Job in ModelDeployment update.

#### **3.0.3**

* **New Features**
  * Added"
" `Webhook.from_name()`
* **Modifications**
  * Import path fix for packtools.

#### **3.0.2**

* **Modifications**
  * Fix pydantic issue with typing-extensions versions > 4.5.0

#### **3.0.1**

* **New Features**
  * General
    * Moving all functions of client to an Object oriented approach
    * Methods return resource object or a deserialized object wherever possible.
    * Support to search model, project, dataset, baselines by their names using `from_name()` method.
    * List methods will return iterator which handles pagination internally.
  * Data
    * Concept of environments was introduced.
    * Ability to download slice data to a parquet file.
    * Publish dataframe as stream instead of batch.
    * New methods for baselines.
    * Multiple datasets can be added to a single model. Ability to choose which dataset to use for computing feature impact /"
" importance, surrogate generation etc.
    * Model can be added without dataset.
    * Ability to generate schema for a model.
    * Model delete is async and returns job details.
    * Added cached properties for `model`: `datasets`, `model_deployment`.
  * Alerts
    * New methods for alerts: `enable_notification`, `disable_notification`, `set_notification_config` and `get_notification_config`.
  * Explainability
    * New methods in explainability: `precompute_feature_impact`, `precompute_feature_importance`, `get_precomputed_feature_importance`, `get_precomputed_feature_impact`, `precompute_predictions`.
    * Decoupled model artifact / surrogate upload and feature impact / importance pre-computation.
* **Modifications**
  * All IDs will be UUIDs instead of strings
  * Dataset delete is not allowed anymore
"
"# Product Feature Maturity Definitions

## Overview

Fiddler ships new features rapidly.
To maintain a fast development pace and ensure a stable customer experience, we define clear stages for feature maturity.
Evolution from one stage to the other is quality-bound, not time-bound.
Features evolve through the following stages.

## Generally Available (GA)

The GA stage is when a feature is not only ready for production but the SLA is guaranteed.
GA features are fully supported by our team and are backed by our comprehensive support plans, providing customers with confidence in reliability, security, and performance.

## Public Preview

Public Preview features are reasonably mature and ready for broader testing, accessible to all customers.
This allows us to gather data about the feature in the wild, including important customer feedback.
While not formally bound by our SLA, our team responds promptly to issues.
APIs and functionality may‚Äîbut are not likely to‚Äîchange.

### Criteria

* Has a feature flag that is on by default for all customers.
* Available to all customers.
* Does not break or degrade existing functionality when enabled.  (Does not degrade existing SLOs.)
* Docs are published publicly but noted as ‚Äúpreview‚Äù or ‚Äúpublic preview‚Äù.
* Breaking changes must be minimal and noted in the release notes.  Also, Field should message and support the change to heavy users directly.
* Expectation that it WILL land in the product after some unspecified amount of time.
* SLO, but no SLA.

## Private Preview

Private Preview features are early-stage and available to select design partners only.
During this stage, no Service Level Agreement (SLA) is provided. Instead, our engineering team collaborates directly with partners to iterate on the solution, making changes as needed based on feedback during business hours.
Features, including their APIs, will change rapidly; they might also be abandoned entirely.  

Why [design partners](https://a16z.com/a-framework-for-finding-a-design-partner/)?
Having a handful of transparent, engaged users or prospective customers can guide you through your first iteration of the product‚Äôs functionality, user experience, pricing and packaging, and more.
Their critiques should help you build something useful and usable for your broader customer base.
To inquire about private preview features, please reach out to [sales@fiddler.ai](mailto:sales@fiddler.ai).

### Criteria

* Has a feature flag that is **turned off by default**.
* Only available to select customers (as decided by Eng + PM).
  * Can do a POV with Private Preview with Eng+PM agreement (supporting team and leadership must approve).
  * Can market, if desired.
* Does not break or degrade existing functionality when disabled.
* May cause unexpected side-effects when enabled.
* Docs are not published, or are published in some not publicly-accessible place.
* Supported by the engineering team directly.
* Breaking changes can be communicated via Slack, not in release notes.
* No SLA or SLO.

## Solutions Engineering

Some solutions are custom-crafted by our Solutions Engineering Team.
These features, while useful and in production, are not officially supported by our overall product SLA.
Talk directly with your SE or CSM for clarification as to any custom-developed tools fall under.
"
"---
title: Single Sign On with Okta
slug: okta-integration
excerpt: ''
createdAt: Mon Aug 01 2022 15:14:37 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# Okta Integration

### Overview

These instructions will help administrators configure Fiddler to be used with an existing Okta single sign on application.

### Okta Setup:

First, you must create an OIDC based application within Okta. Your application will require a callback URL during setup time. This URL will be provided to you by a Fiddler administrator. Your application should grant ""Authorization Code"" permissions to a client acting on behalf of a user. See the image below for how your setup might look like:

![](../.gitbook/assets/b7b67fe-Screen\_Shot\_2022-08-07\_"
"at\_10.22.36\_PM.png)

This is the stage where you can allow certain users of your organization access to Fiddler through Okta. You can use the ""Group Assignments"" field to choose unique sets of organization members to grant access to. This setup stage will also allow for Role Based Access Control (i.e. RBAC) based on specific groups using your application.

Once your application has been set up, a Fiddler administrator will need to receive the following information and credentials:

* Okta domain
* Client ID
* Client Secret
* Okta Account Type (default or custom)

All of the above can be obtained from your Okta application dashboard, as shown in the pictures below:

![](../.gitbook/assets/6442827-Screen\_Shot\_2022-08-07\_at\_10.30.03\_PM.png)

![](../.gitbook/assets/f1dbcf6-Screen\_Shot\_202"
"2-08-07\_at\_10.30.15\_PM.png)

You can also pass the above information to your Fiddler administrator via your okta.yml file.

### Deployment instructions

**Step 1** Create a `<secret-filename>.yaml` file with the following template

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: fiddler-sso-okta-credentials
  namespace: <NAMESPACE_NAME>
data:
  sso-okta-issuer: <OKTA_ISSUER> # https://<okta_domain>/oauth2/default
  sso-okta-authorize-url: <AUTHORIZE_URL> # https://<okta_domain>/oauth2/default/v1/authorize
  sso-okta-token-url: <TOKEN_URL> # https://<okta_domain>/oauth2/default/v1/token
  sso-okta-user-info-url: <USER_INFO_URL> # https://"
"<okta_domain>/oauth2/default/v1/userinfo
  sso-okta-client-id: <CLIENT_ID>
  sso-okta-client-secret: <CLIENT_SECRET>
  sso-okta-domain: <DOMAIN> # your okta domain
  authorization-type: <AUTHORIZATION_TYPE> # default
type: Opaque
```

> üìò All the values must be base64 encoded
>
> In mac you can run `echo -n ""string to be encoded"" | base64` to get the encoded value

> üìò Do not use doubles quotes
>
> Don‚Äôt use doubles quotes anywhere in values in above yaml. In above example, it is written set to ‚Äútrue‚Äù - the value is true and not ‚Äútrue‚Äù.

**Step 2** Update the k8s secret in the namespace of that cluster using the above file.

```shell
kubectl apply -f <secret-filename>.yaml -n fiddler
```

**"
"Step 3** Update the Helm variable `fiddler.auth.sso.provider` and `fiddler.auth.sso.azuread.secretName` with `azuread` and `fiddler-sso-azuread-credentials` value. If you are using the helm values file, use the following settings.

```yaml
fiddler:  
  auth:  
    sso:  
      provider: okta  
      azuread:  
        secretName: fiddler-sso-okta-credentials
```

> üìò Once the deployments are updated, the new SSO settings will be applied.

### Logging into Fiddler:

Once a Fiddler administrator has successfully set up a deployment for your organization using your given Okta credentials, you should see the ‚ÄúSign in with SSO‚Äù button enabled. When this button is clicked, you should be navigated to an Okta login screen. Once successfully authenticated, and assuming you have been granted access to F"
"iddler through Okta, you should be able to login to Fiddler.

![](../.gitbook/assets/c96a709-Screen\_Shot\_2022-08-07\_at\_10.36.40\_PM.png)

NOTES:

1. To be able to login with SSO, it is initially required for the user to register with Fiddler Application. Upon successful registration, the users will be able to login using SSO.
2. The only information Fiddler stores from Okta based logins is a user‚Äôs first name, last name, email address, and OIDC token.
3. Fiddler does not currently support using Okta based login through its API (see fiddler-client). In order to use an Okta based account through Fiddler's API, use a valid access token which can be created and copied on the ‚ÄúCredentials‚Äù tab on Fiddler‚Äôs ‚ÄúSettings‚Äù page.

{% include ""../.git"
"book/includes/main-doc-dev-footer.md"" %}

"
"---
title: Single Sign on with Azure AD
slug: single-sign-on-with-azure-ad
excerpt: Configure Azure SSO with Fiddler
createdAt: Tue Jan 23 2024 06:02:09 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 25 2024 16:51:51 GMT+0000 (Coordinated Universal Time)
---

# SSO with Azure AD

### Prerequisite

Set up [OIDC](https://learn.microsoft.com/en-us/power-apps/developer/data-platform/walkthrough-register-app-azure-active-directory) configuration within Azure by selecting the type as Web and with the redirect URI pointing to your deployment, as seen in the image below.

**Redirect URL** - `{base_url}/api/sso/azuread/callback`

![](../.gitbook/assets/ce5d081-Azure\_SSO\_registration.png)

Once the registration is successful, create a new client secret and copy"
" the secret value immediately after it is created without refreshing the page.

![](../.gitbook/assets/96042ac-Azure\_add\_new\_client\_secret.png)

> üöß Be careful
>
> You will not be able to access the `client secret` later because it is shown ONCE and not repeated

### Creating a new client secret

![Masked client secret value](../.gitbook/assets/1f8d791-Azure\_Client\_Secret.png)

Masked client secret value

### Setting up token permissions to the application

![Token Permissions](../.gitbook/assets/e40da63-Screenshot\_2024-04-15\_at\_12.21.03\_PM.png)

Token Permissions

### Setting up API permissions to the application

![Application Permissions](../.gitbook/assets/5e2f00e-Screenshot\_2024-04-15\_at\_12.22.12\_PM.png)

Application"
" Permissions

In `Authentication`, fill the details as shown below

![Application Page Updates](../.gitbook/assets/00e5989-Screenshot\_2024-04-15\_at\_12.24.07\_PM.png)

Application Page Updates

Up until this point, our application configuration is complete. The following section now deals with Fiddler side of changes.

### Configure Azure SSO with Fiddler

The following details are required to configure Azure SSO with Fiddler:

* OpenID Connect metadata document `sso-azuread-identity-metadata`
* Application (client) ID `sso-azuread-client-id`
* Newly created client secret `sso-azuread-client-secret`

![OpenID Connect metadata Document can be found under Endpoints under the overview section.](../.gitbook/assets/1d18e08-Azure\_SSO\_config\_details.png)

OpenID Connect metadata Document can be found under Endpoints under the"
" overview section.

The following details can be obtained from the `OpenID Connect metadata document` URI.

* Response Types Supported `sso-azuread-response-type`
* Response Modes Supported `sso-azuread-response-mode`
* Issuer `sso-azuread-issuer`
* Scopes Supported `sso-azuread-scope`

### Deployment instructions

**Step 1** Create a `<secret-filename>.yaml` file with the following template

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: fiddler-sso-azuread-credentials
  namespace: <NAMESPACE_NAME>
data:
  sso-azuread-identity-metadata: <IDENTITY_METADATA_URL>
  sso-azuread-client-id: <CLIENT_ID>
  sso-azuread-response-type: <RESPONSE_TYPE> # set to ""code id_token""
  sso-azuread-response-mode: <RESPONSE MODE> # set"
" to ""form_post""
  sso-azuread-client-secret: <CLIENT_SECRET>
  sso-azuread-validate-issuer: <VALIDATE_ISSUER> # set to ""true""
  sso-azuread-issuer: <ISSUER_URL>
  sso-azuread-scope: <SCOPES> # set to ""openid,offline_access,profile,email""
type: Opaque
```

> üìò All the values must be base64 encoded
>
> In mac you can run `echo -n ""string to be encoded"" | base64` to get the encoded value

> üìò Do not use doubles quotes
>
> Don‚Äôt use doubles quotes anywhere in values in above yaml. In above example, it is written set to ‚Äútrue‚Äù - the value is true and not ‚Äútrue‚Äù.

**Step 2** Update the k8s secret in the namespace of that cluster using the above file.

```shell
kubectl apply"
" -f <secret-filename>.yaml -n fiddler
```

**Step 3** Update the Helm variable `fiddler.auth.sso.provider` and `fiddler.auth.sso.azuread.secretName` with `azuread` and `fiddler-sso-azuread-credentials` value. If you are using the helm values file, use the following settings.

```yaml
fiddler:  
  auth:  
    sso:  
      provider: azuread  
      azuread:  
        secretName: fiddler-sso-azuread-credentials
```

> üìò Once the deployments are updated, the new SSO settings will be applied.

{% include ""../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Routing to Fiddler (on-prem)
slug: routing-to-fiddler-on-prem
excerpt: ''
createdAt: Tue Sep 06 2022 21:46:51 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# Routing To Fiddler On-prem

Fiddler supports a wide range of strategies for routing HTTP traffic from end users to the Fiddler system. A typical on-prem Fiddler deployment includes an HTTP reverse proxy (Envoy) that can be configured as needed to meet your routing needs.

![](../../.gitbook/assets/fd4b216-image.png)

The diagram above shows some of the deployment configuration options related to routing and TLS, described below. Once Fiddler is installed in your on-prem environment, you may need to take additional steps to route TCP traffic to"
" the Fiddler Envoy service.

## TLS termination

By default, Fiddler does not perform TLS termination. We find that our customers generally have excellent opinions about how TLS should be terminated, and generally prefer to perform TLS termination using their own network machinery.

### Terminate TLS outside of Fiddler

In a typical production environment, TLS termination will occur outside of Fiddler. Clear HTTP traffic should then be routed to the Fiddler Envoy service at the port specified by `envoy.publicHttpPort`.

```
envoy:
  terminateTLS: false
  publicHttpPort: ""80""
```

### Terminate TLS within Fiddler

Fiddler can be configured to perform TLS termination using an X509 server certificate and corresponding PKCS #8 private key. The TLS certificate must be valid for the FQDN via which end-users will access the Fiddler platform. Both the server certificate and private key must be available in DER format, and should"
" be placed in a `Secret` within the namespace where Fiddler will be deployed prior to installation. For example:

```
kubectl create secret tls my-tls-secret \
    --cert=path/to/the/cert.pem \
    --key=path/to/the/cert.key
```

The Fiddler Helm chart should be configured to reflect the `Secret` containing the server cert and key. TCP traffic should be routed to the port specified by `envoy.publicHttpsPort`.

```yaml
envoy:
  terminateTLS: true
  tlsSecretName: my-tls-secret
  serverCertKey: tls.crt
  privateKeyKey: tls.key
  publicHttpsPort: ""443""
```

### TLS with Ingress

Kubernetes `Ingress` [supports](https://kubernetes.io/docs/concepts/services-networking/ingress/#tls) specifying a TLS secret on a per-ingress basis. If using an `Ingress` to route traffic to Fiddler"
", create a `Secret` containing the DER-formatted X509 server certificate and PKCS #8 private key in the namespace where Fiddler will be deployed:

```
kubectl create secret tls my-tls-secret \
    --cert=path/to/the/cert.pem \
    --key=path/to/the/cert.key
```

The Fiddler Helm chart should be configured to enable Ingress with TLS. For example:

```
envoy:
  createIngress: true

ingress:
  tls:
    hosts:
      # The FQDN where Fiddler is accessed by end users.
      - fiddler.acme.com
    secretName: my-tls-secret
```

## Ingress

If the cluster where Fiddler is installed supports `Ingress`, the Fiddler Helm chart can be configured to create an `Ingress` resource that points to its HTTP reverse proxy.

For example, a configuration for use with the Kubernetes NGINX ingress controller"
" might look like:

```yaml
envoy:
  createIngress: true

ingress:
  class: nginx
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/proxy-body-size: ""10240m""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""3600""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""3600""
```

## ClusterIP service

By default, the Fiddler HTTP reverse proxy (Envoy) is exposed as a `ClusterIP` service within the namespace where Fiddler is installed.

To control the port(s) on the `Service` where traffic is handled, set `envoy.publicHttpPort` and/or `envoy.publicHttpsPort` in the Fiddler Helm configuration. For example:

```yaml
envoy:
  serviceType: ClusterIP
  publicHttpPort:"
" ""8080""
  publicHttpsPort: ""8443""
```

## LoadBalancer service

Fiddler can be exposed direcly outside the cluster via a `LoadBalancer` service, if supported. In AWS EKS, for example, this would result in the creation of a Network Load Balancer. See above for details on TLS termination within Fiddler.

```yaml
envoy:
  serviceType: LoadBalancer
  publicHttpPort: ""80""
  publicHttpsPort: ""443""
```

## Headless service

In cases where a more advanced service mesh or service discovery mechanism is used, it may be desirable to expose Fiddler via a headless service. For example:

```yaml
envoy:
  headlessService: true
```

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: System Architecture
slug: system-architecture
excerpt: ''
createdAt: Tue Apr 19 2022 20:19:53 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 15 2024 21:49:11 GMT+0000 (Coordinated Universal Time)
---

# System Architecture

Fiddler deploys into your private cloud's existing Kubernetes clusters, for which the minimum system requirements can be found [here](technical-requirements.md). Fiddler supports AWS, Azure, or GCP managed Kubernetes services.

Updates to the Fiddler containers is accomplished through a shared container registry (that Fiddler is provided access to). Updates to the containers are orchestrated using Helm charts.

A full-stack deployment of Fiddler is shown in the diagram below.

![Fiddler system architecture](../../.gitbook/assets/fiddler\_system\_architecture\_diagram.png)

The Fiddler system components are deployed within a single namespace on a Kubernetes cluster, using the official Fiddler Helm chart.

* Fiddler core infrastructure relies on persistent volumes provided within the Kubernetes cluster. We recommend using encrypted storage volumes.
* Fiddler may be configured to utilize external infrastructure in a self-hosted environment, such as existing PostgresQL servers, but this is not required as all services are containerized by default.
* Full-stack ""any-prem"" Fiddler includes observability infrastructure to monitor Fiddler system health and performance. These mainstream observability components may be integrated with external observability systems to support administration in a self-hosted environment.
* HTTP traffic to the Fiddler system is handled by an L4 or L7 load balancer or other proxy. TLS termination should usually occur outside the Fiddler system.

Once the platform is running, end users can interface with the Fiddler platform using their browser, the [Fiddler Python client](../../Python\_Client\_3-x/about-client-3x.md), or Fiddler's RESTful APIs.

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: On-prem Installation Guide
slug: installation-guide
excerpt: ''
createdAt: Tue Apr 19 2022 20:20:10 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# On-prem Installation Guide

Fiddler can run on most mainstream flavors of Kubernetes, provided that a suitable [storage class](https://kubernetes.io/docs/concepts/storage/storage-classes/) is available to provide POSIX-compliant block storage (see [On-prem Technical Requirements](technical-requirements.md)).

### Before you start

*   Create a namespace where Fiddler will be deployed, or request that a namespace/project be created for you by the team that administers your Kubernetes cluster.

    ```
    [~] kubectl create ns my-fiddler-ns
    ```
*   Identify the name of the storage class(es)"
" that you will use for Fiddler's block storage needs. Consult the team that administers your Kubernetes cluster for guidance if you are not sure which class to use.

    ```
    [~] kubectl get storageclass
    NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
    gp2 (default)   kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  96d
    ```
*   If using Kubernetes [ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) to route traffic to Fiddler, identify the name of the ingress class that should be used. Consult the team that administers your Kubernetes cluster for guidance if you are not sure which class to use.

    ```
    [~] kubectl get ingressclass
    NAME    CONTROLLER             PARAMETERS   AGE
    nginx   k8s.io/ingress-"
"nginx   <none>       39d
    ```

### Quick-start any-prem deployment

Follow the steps below for a quick-start deployment of Fiddler on your Kubernetes cluster suitable for demonstration purposes. This configuration assumes that an ingress controller is available the cluster.

1. Create a `Secret` for pulling images from the Fiddler container registry using the YAML manifest provided to you.
   *   Verify that the name of the secret is `fiddler-pull-secret`

       ```yaml
       apiVersion: v1
       kind: Secret
       metadata:
         name: fiddler-pull-secret
       data:
         .dockerconfigjson: [REDACTED]
       type: kubernetes.io/dockerconfigjson
       ```
   *   Create the secret in the namespace where Fiddler will be deployed.

       ```
       [~] kubectl -n my-fiddler-ns apply -f fiddler-pull-secret.yaml
      "
" ```
2.  Deploy Fiddler using Helm.

    ```
    [~] helm repo add fiddler https://helm.fiddler.ai/stable/fiddler
    [~] helm repo update
    [~] export STORAGE_CLASS=<my-storage-class>
    [~] export INGRESS_CLASS=<my-ingress-class>
    [~] export FIDDLER_HOSTNAME=fiddler
    [~] export FIDDLER_DOMAIN_NAME=acme.com
    [~] helm upgrade -i -n my-fiddler-ns \
       -f https://helm.fiddler.ai/stable/samples/v2.yaml \
       -f https://helm.fiddler.ai/stable/samples/anyprem.yaml  \
       --set=""grafana.grafana\.ini.server.root_url=https://${FIDDLER_FQDN}/grafana"" \
       --set=global.hostname=${FIDDLER_HOSTNAME}  \
"
"       --set=global.domain=${FIDDLER_DOMAIN_NAME} \
       --set=global.k8s.storageClass=${STORAGE_CLASS} \
       --set=ingestion.api.volumeClaimTemplates.eventLogs.resources.storageClass=${STORAGE_CLASS} \
       --set=fiddler.ingress.class=${INGRESS_CLASS} \
       --wait \
        fiddler fiddler/fiddler
    ```

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Deploying Fiddler
slug: deploying-fiddler
excerpt: ''
createdAt: Tue Apr 19 2022 20:19:47 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Dec 08 2023 22:54:04 GMT+0000 (Coordinated Universal Time)
---

# Deploying Fiddler

### Deployment Overview

Fiddler runs on most mainstream flavors and configurations of Kubernetes, including OpenShift, Rancher, AWS Elastic Kubernetes Service, Azure Managed Kubernetes Service (AKS), GCP Google Kubernetes Engine, and more.

* **Our premises**‚ÄîFiddler is offered as a fully managed service, deployed within an isolated network and dedicated hardware in the cloud.
* **Your premises**‚ÄîDeploy Fiddler into a Kubernetes cluster running in your own cloud account or data center. Please refer to the [On-prem Technical Requirements](technical-requirements.md) section for more details.

> üìò Info
>
> Interested in a Fiddler Cloud or on-premises deployment? Please contact [sales@fiddler.ai](mailto:sales@fiddler.ai).

### Deploy on cloud

Fiddler cloud deployment uses a managed Kubernetes service to deploy, scale, and manage the application. We'll handle the specifics! Please contact [sales@fiddler.ai](mailto:sales@fiddler.ai)

### Deploy on-premise

* [Technical Requirements](technical-requirements.md)
* [Installation Guide](installation-guide.md)

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: On-prem Technical Requirements
slug: technical-requirements
excerpt: ''
createdAt: Tue Apr 19 2022 20:20:05 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# On-prem Technical Requirements

### Minimum System Requirements

Fiddler is horizontally scalable to support the throughput requirements for enormous production use-cases. The minimum system requirements below correspond to approximately 20 million inference events monitored per day (\~230 EPS) for models with around 100 features, with 90 day retention.

* **Deployment**: Kubernetes namespace in AWS, Azure or GCP
* **Compute**: A minimum of 96 vCPU cores
* **Memory**: 384Gi
* **Persistent volumes**: 500 Gi storage across 10 volumes
  * POSIX-compliant block storage
  * 125 MB/s recommended
  * 3,000 IOPS recommended
* **Container Registry**: Quay.io or similar
* **Ingress Controller**: Ingress-nginx or AWS/GCP/Azure Load Balancer Controller
* **DNS**: FQDN that resolves to an L4 or L7 load balancer/proxy that provides TLS termination

### Kubernetes Cluster Requirements

As stated above, Fiddler requires a Kubernetes cluster to install into. The following outlines the requirements for this K8 cluster:

* **Node Groups**: 2 node groups - 1 for core Fiddler services, 1 for Clickhouse (Fiddler's event database)
* **Resources**:
  * Fiddler : 48 vCPUs, 192 Gi
  * Clickhouse : 64 vCPUs, 256 Gi \[tagged & tainted]
* **Persistent Volumes**: 500 GB (minimum) / 1 TB (recommended)
*   **Instance Sizes**

    | Instance Size | AWS    | Azure        | GCP          |
    | ------------- | ------ | ------------ | ------------ |
    | Minimum       | m5.4xl | Std\_D16\_v3 | c2d\_std\_16 |
    | Recommended   | m5.8xl | Std\_D32\_v3 | c2d\_std\_32 |

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: ML Monitoring - User-defined Feature Impact
slug: user-defined-feature-impact-quick-start
excerpt: Quickstart Notebook
metadata:
  title: 'Quickstart: User-defined Feature Impact | Fiddler Docs'
  description: >-
    This document provides a guide on using Fiddler's feature impact upload API
    to supply your own feature impact values for your Fiddler model.
  image: []
  robots: index
createdAt: Tue Nov 19 2024 14:00:57 GMT+0000 (Coordinated Universal Time)
updatedAt: Tue Nov 19 2024 14:00:57 GMT+0000 (Coordinated Universal Time)
icon: notebook
---

# ML Monitoring - Feature Impact

### User-defined Feature Impact Upload

This guide will walk you through the steps needed to upload your model's existing feature impact values to your Fiddler model. This notebook uses the same example model leveraged in the [ML"
" Monitoring - Simple](quick-start.md) quick start. If you have already run that notebook and the model exists in your Fiddler instance, then you may skip the setup steps in this guide as noted in the instructions.

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_User_Defined_Feature_Impact.ipynb)

<div align=""left""><figure><img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" alt=""Google Colab"" width=""188""><figcaption></figcaption></figure></div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_User_Defined_Feature_Impact.ipynb).

{% include ""../.git"
"book/includes/main-doc-footer.md"" %}

# Fiddler User-Defined Feature Impact Quick Start Guide

In this notebook we demonstrate how to upload your own precomputed feature impact values to a Fiddler model. Previous versions of Fiddler required you create either a surrogate or user model artifact with which to calculate the feature impact values within Fiddler. Both surrogate and user model artifact require extra steps when onboarding a model and may be unnecessary if the feature impact values already exist. 


---

The documentation for the user-defined feature impact upload API can be found online [here](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact).

User-Defined Feature Impact is supported on Fiddler version 24.12+ using Fiddler Python client API versions 3.3 and higher.

**Please note that you may skip Steps #2 - #5 and resume at [Step #6](#section_06)** if"
" you have already run Fiddler's [Simple Monitoring Quick Start Guide](https://docs.fiddler.ai/quickstart-notebooks/quick-start) and used the default values and sample data.

1. Connect to Fiddler - Initialization, create a project
2. Load a Data Sample
3. Define Your Model Specifications
4. Set a Model Task
5. Add Your Model
6. Upload Your Feature Impact Values

# 0. Imports


```python
%pip install -q fiddler-client

import pandas as pd
import fiddler as fdl

print(f""Running client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using our Python client API.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2"
". Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'  # If the project already exists, the notebook will create the model under the existing project.
MODEL_NAME = 'bank_churn_simple_monitoring'

# Sample data hosted on GitHub
PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/churn_data_sample.csv'
PATH_TO_FI_VALUES = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact_scores.json"
"'
PATH_TO_FI_VALUES_UPDATED = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact_scores_alt.json'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and call the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project"
" with id = {project.id} and name = {project.name}')
```

## 2. Load a Data Sample

In this example, we'll be considering the case where we're a bank and we have **a model that predicts churn for our customers**.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
column_list = sample_data_df.columns
sample_data_df
```

## 3. Define Your Model Specifications

In order to add your model to Fiddler, simply create a ModelSpec object with information about what each column of your data sample should used for.

Fiddler supports four column types:
1. **Inputs**
2. **Outputs** (Model predictions)
3. **Target** (Ground truth values)
4. **Metadata**


```python
input_columns = list(
"
"    column_list.drop(['predicted_churn', 'churn', 'customer_id', 'timestamp'])
)
model_spec = fdl.ModelSpec(
    inputs=input_columns,
    outputs=['predicted_churn'],
    targets=[
        'churn'
    ],  # Note: only a single Target column is allowed, use metadata columns and custom metrics for additional targets
    metadata=['customer_id', 'timestamp'],
)
id_column = (
    'customer_id'  # Indicates which column is your unique identifier for each event
)
timestamp_column = (
    'timestamp'  # Indicates which column is your timestamp for each event
)
```

## 4. Set a Model Task

Fiddler supports a variety of model tasks. In this case, we're adding a binary classification model.

For this, we'll create a ModelTask object and an additional ModelTaskParams object to specify the ordering of our positive and negative labels.

*For a detailed breakdown of all supported model tasks, click"
" here.*


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

task_params = fdl.ModelTaskParams(target_class_order=['no', 'yes'])
```

## 5. Add Your Model

Create a Model object and publish it to Fiddler, passing in
1. Your data sample
2. Your ModelSpec object
3. Your ModelTask and ModelTaskParams objects
4. Your ID and timestamp columns


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params,
    event_id_col=id_column,
    event_ts_col=timestamp_column,
)

model.create()
print(f'New model created with id = {model.id} and name = {model.name}')
```

## 6. Upload your feature impact values

**Note:** If skipping Steps #2 -"
" #5 because the Simple Monitoring Quick Start model already exists, you will still need to instantiate the fdl.Model object. Uncomment the next cell and run it.



```python
# model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)  # Load the model
# model 
```

Uploading your own feature impact values requires:

1. A Python dict containing each input column defined in your Model's schema and its numeric value
2. A local reference to the fdl.Model

In this example, the feature impact scores are stored as JSON so first they are converted to a dict after reading from the JSON file.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi_values_dict, update=False
)
feature_impacts
```

Feature impact values can be updated at any"
" time simply by setting the `update` parameter to True when calling [upload_feature_impact()](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact). The change takes effect immediately.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES_UPDATED, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi_values_dict, update=True
)
feature_impacts
```
# Fiddler User-Defined Feature Impact Quick Start Guide

In this notebook we demonstrate how to upload your own precomputed feature impact values to a Fiddler model. Previous versions of Fiddler required you create either a surrogate or user model artifact with which to calculate the feature impact values within Fiddler. Both surrogate and user model artifact require extra steps when onboarding a model and may be unnecessary if the feature impact values already exist."
" 


---

The documentation for the user-defined feature impact upload API can be found online [here](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact).

User-Defined Feature Impact is supported on Fiddler version 24.12+ using Fiddler Python client API versions 3.3 and higher.

**Please note that you may skip Steps #2 - #5 and resume at [Step #6](#section_06)** if you have already run Fiddler's [Simple Monitoring Quick Start Guide](https://docs.fiddler.ai/quickstart-notebooks/quick-start) and used the default values and sample data.

1. Connect to Fiddler - Initialization, create a project
2. Load a Data Sample
3. Define Your Model Specifications
4. Set a Model Task
5. Add Your Model
6. Upload Your Feature Impact Values

# 0. Imports


```python
%"
"pip install -q fiddler-client

import pandas as pd
import fiddler as fdl

print(f""Running client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using our Python client API.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'  # If"
" the project already exists, the notebook will create the model under the existing project.
MODEL_NAME = 'bank_churn_simple_monitoring'

# Sample data hosted on GitHub
PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/churn_data_sample.csv'
PATH_TO_FI_VALUES = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact_scores.json'
PATH_TO_FI_VALUES_UPDATED = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact_scores_alt.json'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld"
".Project constructor and call the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

## 2. Load a Data Sample

In this example, we'll be considering the case where we're a bank and we have **a model that predicts churn for our customers**.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV"
")
column_list = sample_data_df.columns
sample_data_df
```

## 3. Define Your Model Specifications

In order to add your model to Fiddler, simply create a ModelSpec object with information about what each column of your data sample should used for.

Fiddler supports four column types:
1. **Inputs**
2. **Outputs** (Model predictions)
3. **Target** (Ground truth values)
4. **Metadata**


```python
input_columns = list(
    column_list.drop(['predicted_churn', 'churn', 'customer_id', 'timestamp'])
)
model_spec = fdl.ModelSpec(
    inputs=input_columns,
    outputs=['predicted_churn'],
    targets=[
        'churn'
    ],  # Note: only a single Target column is allowed, use metadata columns and custom metrics for additional targets
    metadata=['customer_id', 'timestamp'],
)
id_column = (
    'customer_id'  # Indicates which column is your"
" unique identifier for each event
)
timestamp_column = (
    'timestamp'  # Indicates which column is your timestamp for each event
)
```

## 4. Set a Model Task

Fiddler supports a variety of model tasks. In this case, we're adding a binary classification model.

For this, we'll create a ModelTask object and an additional ModelTaskParams object to specify the ordering of our positive and negative labels.

*For a detailed breakdown of all supported model tasks, click here.*


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

task_params = fdl.ModelTaskParams(target_class_order=['no', 'yes'])
```

## 5. Add Your Model

Create a Model object and publish it to Fiddler, passing in
1. Your data sample
2. Your ModelSpec object
3. Your ModelTask and ModelTaskParams objects
4. Your ID and timestamp columns


```python
model = fdl"
".Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params,
    event_id_col=id_column,
    event_ts_col=timestamp_column,
)

model.create()
print(f'New model created with id = {model.id} and name = {model.name}')
```

## 6. Upload your feature impact values

**Note:** If skipping Steps #2 - #5 because the Simple Monitoring Quick Start model already exists, you will still need to instantiate the fdl.Model object. Uncomment the next cell and run it.



```python
# model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)  # Load the model
# model 
```

Uploading your own feature impact values requires:

1. A Python dict containing each input column defined in your Model's schema and its numeric value
2. A local reference"
" to the fdl.Model

In this example, the feature impact scores are stored as JSON so first they are converted to a dict after reading from the JSON file.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi_values_dict, update=False
)
feature_impacts
```

Feature impact values can be updated at any time simply by setting the `update` parameter to True when calling [upload_feature_impact()](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact). The change takes effect immediately.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES_UPDATED, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi_values_dict"
", update=True
)
feature_impacts
```
# Fiddler User-Defined Feature Impact Quick Start Guide

In this notebook we demonstrate how to upload your own precomputed feature impact values to a Fiddler model. Previous versions of Fiddler required you create either a surrogate or user model artifact with which to calculate the feature impact values within Fiddler. Both surrogate and user model artifact require extra steps when onboarding a model and may be unnecessary if the feature impact values already exist. 


---

The documentation for the user-defined feature impact upload API can be found online [here](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact).

User-Defined Feature Impact is supported on Fiddler version 24.12+ using Fiddler Python client API versions 3.3 and higher.

**Please note that you may skip Steps #2 - #5 and resume at [Step #6](#section_06"
")** if you have already run Fiddler's [Simple Monitoring Quick Start Guide](https://docs.fiddler.ai/quickstart-notebooks/quick-start) and used the default values and sample data.

1. Connect to Fiddler - Initialization, create a project
2. Load a Data Sample
3. Define Your Model Specifications
4. Set a Model Task
5. Add Your Model
6. Upload Your Feature Impact Values

# 0. Imports


```python
%pip install -q fiddler-client

import pandas as pd
import fiddler as fdl

print(f""Running client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using our Python client API.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler"
"
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'  # If the project already exists, the notebook will create the model under the existing project.
MODEL_NAME = 'bank_churn_simple_monitoring'

# Sample data hosted on GitHub
PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/churn_data_sample.csv'
PATH_TO_FI_VALUES = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact"
"_scores.json'
PATH_TO_FI_VALUES_UPDATED = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact_scores_alt.json'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and call the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded"
" existing project with id = {project.id} and name = {project.name}')
```

## 2. Load a Data Sample

In this example, we'll be considering the case where we're a bank and we have **a model that predicts churn for our customers**.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
column_list = sample_data_df.columns
sample_data_df
```

## 3. Define Your Model Specifications

In order to add your model to Fiddler, simply create a ModelSpec object with information about what each column of your data sample should used for.

Fiddler supports four column types:
1. **Inputs**
2. **Outputs** (Model predictions)
3. **Target** (Ground truth values)
4. **Metadata**


```python
input_columns ="
" list(
    column_list.drop(['predicted_churn', 'churn', 'customer_id', 'timestamp'])
)
model_spec = fdl.ModelSpec(
    inputs=input_columns,
    outputs=['predicted_churn'],
    targets=[
        'churn'
    ],  # Note: only a single Target column is allowed, use metadata columns and custom metrics for additional targets
    metadata=['customer_id', 'timestamp'],
)
id_column = (
    'customer_id'  # Indicates which column is your unique identifier for each event
)
timestamp_column = (
    'timestamp'  # Indicates which column is your timestamp for each event
)
```

## 4. Set a Model Task

Fiddler supports a variety of model tasks. In this case, we're adding a binary classification model.

For this, we'll create a ModelTask object and an additional ModelTaskParams object to specify the ordering of our positive and negative labels.

*For a detailed breakdown of all supported model tasks"
", click here.*


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

task_params = fdl.ModelTaskParams(target_class_order=['no', 'yes'])
```

## 5. Add Your Model

Create a Model object and publish it to Fiddler, passing in
1. Your data sample
2. Your ModelSpec object
3. Your ModelTask and ModelTaskParams objects
4. Your ID and timestamp columns


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params,
    event_id_col=id_column,
    event_ts_col=timestamp_column,
)

model.create()
print(f'New model created with id = {model.id} and name = {model.name}')
```

## 6. Upload your feature impact values

**Note:** If skipping Steps #"
"2 - #5 because the Simple Monitoring Quick Start model already exists, you will still need to instantiate the fdl.Model object. Uncomment the next cell and run it.



```python
# model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)  # Load the model
# model 
```

Uploading your own feature impact values requires:

1. A Python dict containing each input column defined in your Model's schema and its numeric value
2. A local reference to the fdl.Model

In this example, the feature impact scores are stored as JSON so first they are converted to a dict after reading from the JSON file.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi_values_dict, update=False
)
feature_impacts
```

Feature impact values can be updated"
" at any time simply by setting the `update` parameter to True when calling [upload_feature_impact()](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact). The change takes effect immediately.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES_UPDATED, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi_values_dict, update=True
)
feature_impacts
```
# Fiddler User-Defined Feature Impact Quick Start Guide

In this notebook we demonstrate how to upload your own precomputed feature impact values to a Fiddler model. Previous versions of Fiddler required you create either a surrogate or user model artifact with which to calculate the feature impact values within Fiddler. Both surrogate and user model artifact require extra steps when onboarding a model and may be unnecessary if the feature impact values already"
" exist. 


---

The documentation for the user-defined feature impact upload API can be found online [here](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact).

User-Defined Feature Impact is supported on Fiddler version 24.12+ using Fiddler Python client API versions 3.3 and higher.

**Please note that you may skip Steps #2 - #5 and resume at [Step #6](#section_06)** if you have already run Fiddler's [Simple Monitoring Quick Start Guide](https://docs.fiddler.ai/quickstart-notebooks/quick-start) and used the default values and sample data.

1. Connect to Fiddler - Initialization, create a project
2. Load a Data Sample
3. Define Your Model Specifications
4. Set a Model Task
5. Add Your Model
6. Upload Your Feature Impact Values

# 0. Imports


```python"
"
%pip install -q fiddler-client

import pandas as pd
import fiddler as fdl

print(f""Running client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using our Python client API.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples' "
" # If the project already exists, the notebook will create the model under the existing project.
MODEL_NAME = 'bank_churn_simple_monitoring'

# Sample data hosted on GitHub
PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/churn_data_sample.csv'
PATH_TO_FI_VALUES = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact_scores.json'
PATH_TO_FI_VALUES_UPDATED = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact_scores_alt.json'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in"
" the fld.Project constructor and call the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

## 2. Load a Data Sample

In this example, we'll be considering the case where we're a bank and we have **a model that predicts churn for our customers**.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python
sample_data_df = pd.read_csv(PATH_TO"
"_SAMPLE_CSV)
column_list = sample_data_df.columns
sample_data_df
```

## 3. Define Your Model Specifications

In order to add your model to Fiddler, simply create a ModelSpec object with information about what each column of your data sample should used for.

Fiddler supports four column types:
1. **Inputs**
2. **Outputs** (Model predictions)
3. **Target** (Ground truth values)
4. **Metadata**


```python
input_columns = list(
    column_list.drop(['predicted_churn', 'churn', 'customer_id', 'timestamp'])
)
model_spec = fdl.ModelSpec(
    inputs=input_columns,
    outputs=['predicted_churn'],
    targets=[
        'churn'
    ],  # Note: only a single Target column is allowed, use metadata columns and custom metrics for additional targets
    metadata=['customer_id', 'timestamp'],
)
id_column = (
    'customer_id'  # Indicates which column"
" is your unique identifier for each event
)
timestamp_column = (
    'timestamp'  # Indicates which column is your timestamp for each event
)
```

## 4. Set a Model Task

Fiddler supports a variety of model tasks. In this case, we're adding a binary classification model.

For this, we'll create a ModelTask object and an additional ModelTaskParams object to specify the ordering of our positive and negative labels.

*For a detailed breakdown of all supported model tasks, click here.*


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

task_params = fdl.ModelTaskParams(target_class_order=['no', 'yes'])
```

## 5. Add Your Model

Create a Model object and publish it to Fiddler, passing in
1. Your data sample
2. Your ModelSpec object
3. Your ModelTask and ModelTaskParams objects
4. Your ID and timestamp columns


```python
model ="
" fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params,
    event_id_col=id_column,
    event_ts_col=timestamp_column,
)

model.create()
print(f'New model created with id = {model.id} and name = {model.name}')
```

## 6. Upload your feature impact values

**Note:** If skipping Steps #2 - #5 because the Simple Monitoring Quick Start model already exists, you will still need to instantiate the fdl.Model object. Uncomment the next cell and run it.



```python
# model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)  # Load the model
# model 
```

Uploading your own feature impact values requires:

1. A Python dict containing each input column defined in your Model's schema and its numeric value
2. A"
" local reference to the fdl.Model

In this example, the feature impact scores are stored as JSON so first they are converted to a dict after reading from the JSON file.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi_values_dict, update=False
)
feature_impacts
```

Feature impact values can be updated at any time simply by setting the `update` parameter to True when calling [upload_feature_impact()](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact). The change takes effect immediately.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES_UPDATED, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi"
"_values_dict, update=True
)
feature_impacts
```
"
"---
title: ML Monitoring - CV Inputs
slug: cv-monitoring
metadata:
  title: 'Quickstart: CV Monitoring | Fiddler Docs'
  description: >-
    This document is a guide on using Fiddler for monitoring computer vision
    models and detecting drift in image data using Fiddler's Vector Monitoring
    approach.
  robots: index
icon: notebook
---

# ML Monitoring - CV Inputs

This guide will walk you through the basic steps required to use Fiddler for monitoring computer vision (CV) models. In this notebook we demonstrate how to detect drift in image data using model embeddings using Fiddler's unique Vector Monitoring approach.

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Image_Monitoring.ipynb)

<div align=""left"">

<figure><img src"
"=""https://colab.research.google.com/img/colab_favicon_256px.png"" alt=""Google Colab"" width=""188""><figcaption></figcaption></figure>

</div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Image_Monitoring.ipynb).

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

# Monitoring Image data using Fiddler Vector Monitoring

In this notebook we present the steps for monitoring images. Fiddler employs a vector-based monitoring approach that can be used to monitor data drift in high-dimensional data such as NLP embeddings, images, video etc. In this notebook we demonstrate how to detect drift in image data using model embeddings and determine the cause of that drift.

Fiddler is the pioneer in enterprise Model Performance Management (MPM), offering a unified platform that enables Data Science, MLOps"
", Risk, Compliance, Analytics, and LOB teams to **monitor, explain, analyze, and improve ML deployments at enterprise scale**.
Obtain contextual insights at any stage of the ML lifecycle, improve predictions, increase transparency and fairness, and optimize business revenue.

---

You can experience Fiddler's Image monitoring ***in minutes*** by following these quick steps:

1. Connect to Fiddler
2. Load and generate embeddings for CIFAR-10 dataset
3. Upload the vectorized baseline dataset
4. Add metadata about your model
5. Inject data drift and publish production events
6. Get insights

## Imports


```python
!pip install torch==2.0.0
!pip install torchvision==0.15.1
!pip install -q fiddler-client
```


```python
import io
import numpy as np
import pandas as pd
import random
import time
import torch
import torch.nn as nn
import torchvision.transforms"
" as transforms
from torchvision.models import resnet18, ResNet18_Weights
import requests

import fiddler as fdl
print(f""Running Fiddler client version {fdl.__version__}"")
```

## 2. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using our API client.


---


**We need a few pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

These can be found by navigating to the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https://).
TOKEN = ''
```

Now just run the following code block to connect to the Fiddler API!


```python
fdl.init(
    url=URL,
    token=TOKEN
)
```

Once you connect, you can"
" create a new project by calling a Project's `create` method.


```python
PROJECT_NAME = 'image_monitoring'

project = fdl.Project(
    name=PROJECT_NAME
)

project.create()
```

## 2. Generate Embeddings for CIFAR-10 data

In this example, we'll use the popular CIFAR-10 classification dataset and a model based on Resnet-18 architecture. For the purpose of this example we have pre-trained the model.
  
In order to compute data and prediction drift, **Fiddler needs a sample of data that can serve as a baseline** for making comparisons with data in production. When it comes to computing distributional shift for images, Fiddler relies on the model's intermediate representations also known as activations or embeddings. You can read more about our approach [here](https://www.fiddler.ai/blog/monitoring-natural-language-processing-and-computer-vision-models-part-1).

In the the following cells we"
"'ll extract these embeddings.


```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Device to be used: {device}')
```

Let us load the pre-trained model


```python
MODEL_URL='https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/models/resnet18_cifar10_epoch5.pth'
MODEL_PATH='resnet18_cifar10_epoch5.pth'

def load_model(device):
    """"""Loads the pre-trained CIFAR-10 model""""""
    model = resnet18()
    model.fc = nn.Sequential(
        nn.Linear(512, 128),
        nn.ReLU(),
        nn.Linear(128, 10),
    )

    r = requests.get(MODEL_URL)
    with open(MODEL_PATH,'wb') as f:
        f.write(r.content)

    model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(device)))
    model.to(device)
"
"    return model

resnet_model = load_model(device)

```

We'll load four tranches of [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) data for this example.  ""reference"" ‚Äì corresponding to train-time reference data, and three ""production"" sets with diffrent transformations applied to simulate drift from the model's training data. Note that running the cell below will download the CIFAR-10 data and load them using torch's dataloaders.


```python
BATCH_SIZE = 32

transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),
    ])

DATA_BASE_URL = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/cv_monitoring/'

# download file from URL
DATA_URLS ="
" {
 'reference' : DATA_BASE_URL + 'reference/image_data.npz',
'production_1': DATA_BASE_URL + 'production_1/image_data.npz', # Undrifted
'production_2': DATA_BASE_URL + 'production_2/image_data.npz', # Blurred
'production_3': DATA_BASE_URL + 'production_3/image_data.npz'} # Darkened


def get_dataloader(dataset):
  response = requests.get(DATA_URLS[dataset])
  data = np.load(io.BytesIO(response.content))

  images = [transform(x) for x in data['arr_0']]
  labels = data['arr_1']

  tuple_list = list(zip(images, labels))

  return torch.utils.data.DataLoader(
      tuple_list,
      batch_size=BATCH_SIZE,
      shuffle=False,
      num_workers=2
  )

# let's test it
get_dataloader('reference')
```

***In the cell below we define functions that"
" will extract the 128-dimensional embedding from the FC1 layer of the model and package them in a dataframe along with predictions, ground-truth labels, and image URLs***


```python
from copy import deepcopy
import matplotlib.pyplot as plt
import numpy as np

import torch
import torch.nn.functional as F
import torchvision.transforms as transforms

torch.manual_seed(0)

CIFAR_CLASSES = (
    'plane', 'car', 'bird', 'cat',
    'deer', 'dog', 'frog',
    'horse', 'ship', 'truck',
)

global view_fc1_output_embeds

def fc1_hook_func(model, input, output):
    global view_fc1_output_embeds
    view_fc1_output_embeds = output

def idx_to_classes(target_arr):
    return [CIFAR_CLASSES[int(i)] for i in target_arr]

def generate_embeddings(model, device, dataset_name):
    """"""Generate embeddings for the inout images""""""

    dataloader"
" = get_dataloader(dataset_name)

    fc1_embeds = []
    output_scores = []
    target = []

    with torch.no_grad():
        model = model.eval()
        fc1_module = model.fc[0]
        fc1_hook = fc1_module.register_forward_hook(fc1_hook_func)
        correct_preds = 0
        total_preds = 0

        try:
            for inputs, labels in dataloader:
                inputs = inputs.to(device)
                labels = labels.to(device)
                outputs = model(inputs)
                outputs_smax = F.softmax(outputs, dim=1)
                _, preds = torch.max(outputs, 1)
                correct_preds += torch.sum(preds == labels.data).cpu().numpy()
                total_preds += len(inputs)

                fc1_embeds.append(view_fc1_output_embeds.cpu().detach().numpy())
                output_scores.append(outputs_smax.cpu().detach().numpy())
                target.append(labels.cpu().detach().numpy())

            fc1_embed"
"s = np.concatenate(fc1_embeds)
            output_scores = np.concatenate(output_scores)
            target = np.concatenate(target)

        except Exception as e:
            fc1_hook.remove()
            raise

        print(f'{correct_preds}/{total_preds}: {100*correct_preds/total_preds:5.1f}% correct predictions.')

    embs = deepcopy(fc1_embeds)
    labels = idx_to_classes(target)
    embedding_cols = ['emb_'+str(i) for i in range(128)]
    baseline_embeddings = pd.DataFrame(embs, columns=embedding_cols)

    columns_to_combine = baseline_embeddings.columns
    baseline_embeddings = baseline_embeddings.apply(lambda row: row[columns_to_combine].tolist(), axis=1).to_frame()
    baseline_embeddings = baseline_embeddings.rename(columns={baseline_embeddings.columns[0]: 'embeddings'})

    baseline_predictions = pd.DataFrame(output_scores, columns=CIFAR_CLASSES)
    baseline_labels = pd.DataFrame(labels, columns=['target'])
    embeddings_df ="
" pd.concat(
        [baseline_embeddings, baseline_predictions, baseline_labels],
        axis='columns',
        ignore_index=False
    )

    embeddings_df['image_url'] = embeddings_df.apply(lambda row:DATA_BASE_URL + dataset_name + '/' + str(row.name) + '.png', axis=1)


    return embeddings_df

```

We'll now extract the embeddings for training data which will serve as baseline for monitoring.


```python
sample_df = generate_embeddings(resnet_model, device, 'reference')
sample_df.head()
```

# 4. Add metadata about the model

Next we must tell Fiddler a bit more about our model.  This is done by by creating defining some information about our model's task, inputs, output, target and which features form the image embedding and then creating a `Model` object.

Let's first define our Image vector using the API below.


```python
image_embedding_feature = fdl.ImageEmbedding(
    name='image_feature',
"
"    source_column='image_url',
    column='embeddings',
)
```

Now let's define a `ModelSpec` object with information about the columns in our data sample.


```python
model_spec = fdl.ModelSpec(
    inputs=['embeddings'],
    outputs=CIFAR_CLASSES,
    targets=['target'],
    metadata=['image_url'],
    custom_features=[image_embedding_feature],
)

timestamp_column = 'timestamp'
```

Then let's specify some information about the model task.


```python
model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION

task_params = fdl.ModelTaskParams(target_class_order=list(CIFAR_CLASSES))
```

Then we create a `Model` schema using the example data.


```python
MODEL_NAME = 'resnet18'

model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=fdl.Project.from_name(PROJECT_NAME).id,
    source=sample_df,
    spec=model_spec,
   "
" task=model_task,
    task_params=task_params,
    event_ts_col=timestamp_column
)

model.create()
```

Additionally, let's publish the baseline data so we can use it as a reference for measuring drift and to compare production data with in Embedding Visualization for root-cause analysis.


```python
model.publish(
    source=sample_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name='train_time_reference',
)
```

# 5. Publish events to Fiddler
We'll publish events over past 3 weeks.

- Week 1: We publish CIFAR-10 test set, which would signify no distributional shift
- Week 2: We publish **blurred** CIFAR-10 test set
- Week 3: We publish **brightness reduced** CIFAR-10 test set


```python
for i, dataset_name in enumerate(['production_1', 'production_2', 'production_3']):
   "
" week_days = 6
    prod_df = generate_embeddings(resnet_model, device, dataset_name)
    week_offset = (2-i)*7*24*60*60*1e3
    day_offset = 24*60*60*1e3
    print(f'Publishing events from {dataset_name} transformation for week {i+1}.')
    for day in range(week_days):
        now = time.time() * 1000
        timestamp = int(now - week_offset - day*day_offset)
        events_df = prod_df.sample(1000)
        events_df['timestamp'] = timestamp
        model.publish(events_df)
```

## 6. Get insights

**You're all done!**
  
You can now head to your Fiddler URL and start getting enhanced observability into your model's performance.

Fiddler can now track your image drift over time based on the embedding vectors of the images published into the platform.

"
"While we saw performace degrade for the various drifted data sets in the notebook when publishing (which can also be plotted in the Monitoring chart), embedding drift doesn't require ground-truth labels and often serves as a useful proxy that can indicate a problem or degraded performance.  Data drift is indicated by nonzero Jensen-Shannon Distance measured with respect to a reference sample.

Please visit your Fiddler environment upon completion to check this out for yourself.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/image_monitoring_2024_12_1.png"" />
        </td>
    </tr>
</table>

In order to identify the root cause of data drift, Fiddler allows you to ""drill-down"" into time windows where embedding drift is measured.  As indicated in blue in the image above, by selecting a time bin and clicking the ""Embeddings"""
" button, you'll be take to an Embedding Visualization chart where data from that time window is compared against reference data (e.g. `train_time_reference` that we published above) in an interactive 3D UMAP plot.

In the image below, the embedded images in the drifted time period are semantically distinct to your model from those in the train-time sample.  By investigating these differenes, it's easy to determine that the third ""production"" sample is systematically darkened.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/image_monitoring_2024_12_2.png"" />
        </td>
    </tr>
</table>



---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http"
"://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
title: LLM Monitoring - Simple
slug: simple-llm-monitoring
createdAt: Mon Feb 26 2024 20:06:46 GMT+0000 (Coordinated Universal Time)
updatedAt: Tue Apr 16 2024 14:59:37 GMT+0000 (Coordinated Universal Time)
icon: notebook
---

# LLM Monitoring - Simple

This guide will walk you through the basic onboarding steps required to use Fiddler for monitoring of LLM applications, **using sample data provided by Fiddler**.

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_LLM_Chatbot.ipynb)&#x20;

<div align=""left"">

<figure><img src=""https://colab.research.google.com/img/colab_favicon_256px"
".png"" alt=""Google Colab"" width=""188""><figcaption></figcaption></figure>

</div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_LLM_Chatbot.ipynb).

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

# Fiddler LLM Application Quick Start Guide

Fiddler is the pioneer in enterprise AI Observability, offering a unified platform that enables all model stakeholders to monitor model performance and to investigate the true source of model degredation.  Fiddler's AI Observability platform supports both traditional ML models as well as Generative AI applications.  This guide walks you through how to onboard an LLM chatbot application that is built using a RAG architecture.

---

You can start using Fiddler ***in minutes*** by following these 8 quick steps:

1. Connect to Fiddler"
"
2. Create a Fiddler Project
3. Load a Data Sample
4. Opt-in to Specific Fiddler LLM Enrichments
5. Add Information About the LLM Application
6. Publish Production Events

Get insights!

## 0. Imports


```python
%pip install -q fiddler-client

import time as time

import numpy as np
import pandas as pd
import fiddler as fdl

print(f""Running Fiddler Python client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your LLM Application with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on"
" the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'  # If the project already exists, the notebook will create the model under the existing project.
MODEL_NAME = 'fiddler_rag_llm_chatbot'

# Sample data hosted on GitHub
PATH_TO_SAMPLE_CSV = 'https://media.githubusercontent.com/media/fiddler-labs/fiddler-examples/main/quickstart/data/v3/chatbot_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://media.githubusercontent.com/media/fiddler-labs/fiddler-examples/main/quickstart/data/v3/chatbot_production_data.csv'
```

Now just run the following code block to connect to the F"
"iddler API!


```python
fdl.init(url=URL, token=TOKEN)
```

## 2. Create a Fiddler Project

Once you connect, you can create a new project by specifying a unique project name in the `Project` constructor and calling the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

## 3. Load a Data Sample

In this example, we'll be onboarding data in order to observe our **Fiddler chatbot application**"
".
  
In order to get insights into the LLM Applications's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.
Let's use a file with some historical prompts, source docs, and responses from our chatbot for the sample.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
sample_data_df
```

Fiddler will uses this data sample to keep track of important information about your data.  This includes **data types**, **data ranges**, and **unique values** for categorical variables.

## 4. Opt-in to Specific Fiddler LLM Enrichments

After picking a sample of our chatbot's prompts and responses, we can request that Fiddler execute a series of enrichment services that can ""score"" our prompts and responses for a variety of insights.  These enrichment services can detect AI safety issues like PII leakage, hallucinations, toxicity, and more.  We can also"
" opt-in for enrichment services like embedding generation which will allow us to track prompt and response outliers and drift. A full description of these enrichments can be found [here](https://docs.fiddler.ai/platform-guide/llm-monitoring/enrichments-private-preview).

---

Let's define the enrichment services we'd like to use.  Here we will opt in for embedding generation for our prompts, responses and source docs.  Additionally, let's opt in for PII detection, outlier detection through centroid distance metrics, and some other text-based evaluation scores.


```python
fiddler_backend_enrichments = [
    # prompt enrichment
    fdl.TextEmbedding(
        name='Prompt TextEmbedding',
        source_column='question',
        column='Enrichment Prompt Embedding',
        n_tags=10,
    ),
    # response enrichment
    fdl.TextEmbedding(
        name='Response TextEmbedding',
        source_column='response',
        column='Enrichment"
" Response Embedding',
        n_tags=10,
    ),
    # rag document enrichments
    fdl.TextEmbedding(
        name='Source Docs TextEmbedding',
        source_column='source_docs',
        column='Enrichment Source Docs Embedding',
        n_tags=10,
    ),
    # safety
    fdl.Enrichment(
        name='FTL Safety',
        enrichment='ftl_prompt_safety',
        columns=['question', 'response'],
    ),
    # hallucination
    fdl.Enrichment(
        name='Faithfulness',
        enrichment='ftl_response_faithfulness',
        columns=['source_docs', 'response'],
        config={'context_field': 'source_docs', 'response_field': 'response'},
    ),
    # text quality
    fdl.Enrichment(
        name='Enrichment QA TextStat',
        enrichment='textstat',
        columns=['question', 'response'],
        config={
            'statistics': [
               "
" 'char_count',
                'flesch_reading_ease',
                'flesch_kincaid_grade',
            ]
        },
    ),
    fdl.Enrichment(
        name='Enrichment QA Sentiment',
        enrichment='sentiment',
        columns=['question', 'response'],
    ),
    # PII detection
    fdl.Enrichment(
        name='Rag PII', enrichment='pii', columns=['question'], allow_list=['fiddler']
    ),
]
```

## 5.  Add Information About the LLM application

Now it's time to onboard information about our LLM application to Fiddler.  We do this by defining a `ModelSpec` object.


---


The `ModelSpec` object will contain some **information about how your LLM application operates**.
  
*Just include:*
1. The **input/output** columns.  For a LLM application, these are just the raw inputs and outputs of"
" our LLM application.
2. Any **metadata** columns.
3. The **custom features** which contain the configuration of the enrichments we opted for.

We'll also want a few other pieces of information:
1. The **task** your model or LLM application is performing (LLM, regression, binary classification, not set, etc.)
2. Which column to use to read timestamps from.


```python
model_spec = fdl.ModelSpec(
    inputs=['question', 'response', 'source_docs'],
    metadata=['session_id', 'comment', 'timestamp', 'feedback'],
    custom_features=fiddler_backend_enrichments,
)

model_task = fdl.ModelTask.LLM

timestamp_column = 'timestamp'
```

Then just publish all of this to Fiddler by configuring a `Model` object to represent your LLM application in Fiddler.


```python
llm_application = fdl.Model.from_data(
    source=sample_data_df,
    name"
"=MODEL_NAME,
    project_id=project.id,
    spec=model_spec,
    task=model_task,
    event_ts_col=timestamp_column,
    max_cardinality=5,
)
```

Now call the `create` method to publish it to Fiddler!


```python
llm_application.create()
print(
    f'New model created with id = {llm_application.id} and name = {llm_application.name}'
)
```

## 6. Publish Production Events

Information about your LLM application is now onboarded to Fiddler. It's time to start publishing some production data!  


---


Each record sent to Fiddler is called **an event**.  Events simply contain the inputs and outputs of a predictive model or LLM application.
  
Let's load in some sample events (prompts and responses) from a CSV file.


```python
llm_events_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Timeshifting the timestamp column"
" in the events file so the events are as recent as today
llm_events_df['timestamp'] = pd.to_datetime(llm_events_df['timestamp'])
time_diff = pd.Timestamp.now().normalize() - llm_events_df['timestamp'].max()
llm_events_df['timestamp'] += time_diff

llm_events_df
```

You can use the `Model.publish` function to start pumping data into Fiddler!
  
Just pass in the DataFrame containing your events.


```python
# Define the number of rows per chunk
chunk_size = 10

# Splitting the DataFrame into smaller chunks
for start in range(0, llm_events_df.shape[0], chunk_size):
    df_chunk = llm_events_df.iloc[start : start + chunk_size]
    llm_application.publish(df_chunk)
```

# Get insights

**You're all done!**
  
You can now head to your Fiddler environment and start getting enhanced observability into your LLM application"
"'s performance.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/LLM_chatbot_UMAP.png"" />
        </td>
    </tr>
</table>

**What's Next?**

Try the [ML Monitoring - Quick Start Guide](https://docs.fiddler.ai/quickstart-notebooks/quick-start)

---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.


```python

```
"
"---
title: ML Monitoring - Simple
slug: quick-start
metadata:
  title: 'Quickstart: Simple Monitoring | Fiddler Docs'
  description: >-
    This document provides a guide for using Fiddler for model monitoring using
    sample data provided by Fiddler.
  robots: index
icon: notebook
---

# ML Monitoring - Simple

This guide will walk you through the basic onboarding steps required to use Fiddler for model monitoring, **using sample data provided by Fiddler**.

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Simple_Monitoring.ipynb)

<div align=""left"">

<figure><img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" alt=""Google Colab"" width=""188"
"""><figcaption></figcaption></figure>

</div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Simple_Monitoring.ipynb).

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

# Fiddler Simple Monitoring Quick Start Guide

Fiddler is the pioneer in enterprise Model Performance Management (MPM), offering a unified platform that enables Data Science, MLOps, Risk, Compliance, Analytics, and other LOB teams to **monitor, analyze, and improve ML deployments at enterprise scale**.
Obtain contextual insights at any stage of the ML lifecycle, improve predictions, increase transparency and fairness, and optimize business revenue.

---

You can start using Fiddler ***in minutes*** by following these quick steps:

1. Connect to Fiddler
2. Load a Data Sample
3. Define the Model Specifications
4. Set"
" the Model Task
5. Create a Model
6. Set Up Alerts **(Optional)**
7. Create a Custom Metric **(Optional)**
8. Create a Segment **(Optional)**
9. Publish a Pre-production Baseline **(Optional)**
10. Configure a Rolling Baseline **(Optional)**
11. Publish Production Events

# 0. Imports


```python
%pip install -q fiddler-client

import time as time

import pandas as pd
import fiddler as fdl

print(f'Running Fiddler Python client version {fdl.__version__}')
```

## 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your"
" authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'  # If the project already exists, the notebook will create the model under the existing project.
MODEL_NAME = 'bank_churn_simple_monitoring'

STATIC_BASELINE_NAME = 'baseline_dataset'
ROLLING_BASELINE_NAME = 'rolling_baseline_1week'

# Sample data hosted on GitHub
PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/churn_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/f"
"iddler-examples/main/quickstart/data/v3/churn_production_data.csv'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and calling the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

You should now be"
" able to see the newly created project in the Fiddler UI.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/simple_monitoring_1.png"" />
        </td>
    </tr>
</table>

## 2. Load a Data Sample

In this example, we'll be considering the case where we're a bank and we have **a model that predicts churn for our customers**.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
column_list  = sample_data_df.columns
sample_data_df
```

## 3. Define the Model Specifications

In order to create a model in Fiddler, create a ModelSpec object with information about what each column of your"
" data sample should used for.

Fiddler supports four column types:
1. **Inputs**
2. **Outputs** (Model predictions)
3. **Target** (Ground truth values)
4. **Metadata**


```python
input_columns = list(
    column_list.drop(['predicted_churn', 'churn', 'customer_id', 'timestamp'])
)
```


```python
model_spec = fdl.ModelSpec(
    inputs=input_columns,
    outputs=['predicted_churn'],
    targets=['churn'],  # Note: only a single Target column is allowed, use metadata columns and custom metrics for additional targets
    metadata=['customer_id', 'timestamp'],
)
```

If you have columns in your ModelSpec which denote **prediction IDs or timestamps**, then Fiddler can use these to power its analytics accordingly.

Let's call them out here and use them when configuring the Model in step 5.


```python
id_column = 'customer_id'
timestamp_column = 'timestamp"
"'
```

## 4. Set the Model Task

Fiddler supports a variety of model tasks. In this case, we're adding a binary classification model.

For this, we'll create a ModelTask object and an additional ModelTaskParams object to specify the ordering of our positive and negative labels.

*For a detailed breakdown of all supported model tasks, click [here](https://docs.fiddler.ai/product-guide/task-types).*


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

task_params = fdl.ModelTaskParams(target_class_order=['no', 'yes'])
```

## 5. Create a Model

Create a Model object and publish it to Fiddler, passing in
1. Your data sample
2. The ModelSpec object
3. The ModelTask and ModelTaskParams objects
4. The ID and timestamp columns


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id="
"project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params,
    event_id_col=id_column,
    event_ts_col=timestamp_column,
)

model.create()
print(f'New model created with id = {model.id} and name = {model.name}')
```

On the project page, you should now be able to see the newly onboarded model with its model schema.

<table>
    <tr>
        <td>
            <img src=""https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/images/simple_monitoring_3.png?raw=true"" />
        </td>
    </tr>
</table>

<table>
    <tr>
        <td>
            <img src=""https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/images/simple_monitoring_4.png?raw=true"" />
        </td>
    </tr"
">
</table>

## 6. Set Up Alerts (Optional)

Fiddler allows creating alerting rules when your data or model predictions deviate from expected behavior.

The alert rules can compare metrics to **absolute** or **relative** values.

Please refer to [our documentation](https://docs.fiddler.ai/client-guide/alerts-with-fiddler-client) for more information on Alert Rules.

---
  
Let's set up some alert rules.

The following API call sets up a Data Integrity type rule which triggers an email notification when published events have 2 or more range violations in any 1 day bin for the `numofproducts` column.


```python
alert_rule_1 = fdl.AlertRule(
    name='Bank Churn Range Violation Alert',
    model_id=model.id,
    metric_id='range_violation_count',
    bin_size=fdl.BinSize.DAY,
    compare_to=fdl.CompareTo.RAW_VALUE,
    priority=fdl.Priority"
".HIGH,
    warning_threshold=2,
    critical_threshold=3,
    condition=fdl.AlertCondition.GREATER,
    columns=['numofproducts'],
)

alert_rule_1.create()
print(
    f'New alert rule created with id = {alert_rule_1.id} and name = {alert_rule_1.name}'
)

# Set notification configuration for the alert rule, a single email address for this simple example
alert_rule_1.set_notification_config(emails=['name@google.com'])
```

Let's add a second alert rule.

This one sets up a Performance type rule which triggers an email notification when precision metric is 5% higher than that from 1 hr bin one day ago.


```python
alert_rule_2 = fdl.AlertRule(
    name='Bank Churn Performance Alert',
    model_id=model.id,
    metric_id='precision',
    bin_size=fdl.BinSize.HOUR,
    compare_to=fdl.CompareTo.TIME_PERIOD,
"
"    compare_bin_delta=24,  # Multiple of the bin size
    condition=fdl.AlertCondition.GREATER,
    warning_threshold=0.05,
    critical_threshold=0.1,
    priority=fdl.Priority.HIGH,
)

alert_rule_2.create()
print(
    f'New alert rule created with id = {alert_rule_2.id} and name = {alert_rule_2.name}'
)

# Set notification configuration for the alert rule, a single email address for this simple example
alert_rule_2.set_notification_config(emails=['name@google.com'])
```

## 7. Create a Custom Metric (Optional)

Fiddler's [Custom Metric API](https://docs.fiddler.ai/python-client-3-x/api-methods-30#custommetric) enables user-defined formulas for custom metrics.  Custom metrics will be tracked over time and can be used in Charts and Alerts just like the many out of the box metrics provided by F"
"iddler.  Custom metrics can also be managed in the Fiddler UI.

Please refer to [our documentation](https://docs.fiddler.ai/product-guide/monitoring-platform/custom-metrics) for more information on Custom Metrics.

---
  
Let's create an example custom metric.


```python
custom_metric = fdl.CustomMetric(
    name='Lost Revenue',
    model_id=model.id,
    description='A metric to track revenue lost for each false positive prediction.',
    definition=""""""sum(if(fp(),1,0) * -100)"""""",  # This is an excel like formula which adds -$100 for each false positive predicted by the model
)

custom_metric.create()
print(
    f'New custom metric created with id = {custom_metric.id} and name = {custom_metric.name}'
)
```

## 8. Create a Segment (Optional)
Fiddler's [Segment API](https://docs.fiddler.ai/python-client-3-x/api-methods"
"-30#segments) enables defining named cohorts/sub-segments in your production data. These segments can be tracked over time, added to charts, and alerted upon. Segments can also be managed in the Fiddler UI.

Please refer to our [documentation](https://docs.fiddler.ai/product-guide/monitoring-platform/segments) for more information on the creation and management of segments.

Let's create a segment to track customers from Hawaii for a specific age range.


```python
segment = fdl.Segment(
    name='Hawaii Customers between 30 and 60',
    model_id=model.id,
    description='Hawaii Customers between 30 and 60',
    definition=""(age<60 or age>30) and geography=='Hawaii'"",
)

segment.create()
print(f'New segment created with id = {segment.id} and name = {segment.name}')
```

## 9. Publish a Static Baseline (Optional)

Since Fiddler already knows how"
" to process data for your model, we can now add a **baseline dataset**.

You can think of this as a static dataset which represents **""golden data,""** or the kind of data your model expects to receive.

Then, once we start sending production data to Fiddler, you'll be able to see **drift scores** telling you whenever it starts to diverge from this static baseline.

***

Let's publish our **original data sample** as a pre-production dataset. This will automatically add it as a baseline for the model.


*For more information on how to design your baseline dataset, [click here](https://docs.fiddler.ai/client-guide/creating-a-baseline-dataset).*


```python
baseline_publish_job = model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {"
"baseline_publish_job.id}'
)

# Uncomment the line below to wait for the job to finish, otherwise it will run in the background.
# You can check the status on the Jobs page in the Fiddler UI or use the job ID to query the job status via the API.
# baseline_publish_job.wait()
```

## 10. Configure a Rolling Baseline (Optional)

Fiddler also allows you to configure a baseline based on **past production data.**

This means instead of looking at a static slice of data, it will look into past production events and use what it finds for drift calculation.

Please refer to [our documentation](https://docs.fiddler.ai/client-guide/creating-a-baseline-dataset) for more information on Baselines.

---
  
Let's set up a rolling baseline that will allow us to calculate drift relative to production data from 1 week back.


```python
rolling_baseline = fdl.Baseline(
    model_id=model.id,
    name="
"ROLLING_BASELINE_NAME,
    type_=fdl.BaselineType.ROLLING,
    environment=fdl.EnvType.PRODUCTION,
    window_bin_size=fdl.WindowBinSize.DAY,  # Size of the sliding window
    offset_delta=7,  # How far back to set our window (multiple of window_bin_size)
)

rolling_baseline.create()
print(
    f'New rolling baseline created with id = {rolling_baseline.id} and name = {rolling_baseline.name}'
)
```

## 11. Publish Production Events

Finally, let's send in some production data!


Fiddler will **monitor this data and compare it to your baseline to generate powerful insights into how your model is behaving**.


---


Each record sent to Fiddler is called **an event**.
  
Let's load some sample events from a CSV file.


```python
production_data_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be"
" as recent as today
production_data_df['timestamp'] = production_data_df['timestamp'] + (
    int(time.time() * 1000) - production_data_df['timestamp'].max()
)
production_data_df
```


```python
production_publish_job = model.publish(production_data_df)

print(
    f'Initiated production environment data upload with Job ID = {production_publish_job.id}'
)

# Uncomment the line below to wait for the job to finish, otherwise it will run in the background.
# You can check the status on the Jobs page in the Fiddler UI or use the job ID to query the job status via the API.
# production_publish_job.wait()
```

# Get Insights
  
Return to your Fiddler environment to get enhanced observability into your model's performance.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/simple_monitor"
"ing_5.png"" />
        </td>
    </tr>
</table>

**What's Next?**

Try the [LLM Monitoring - Quick Start Notebook](https://docs.fiddler.ai/quickstart-notebooks/simple-llm-monitoring)

---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
title: ML Monitoring - NLP Inputs
slug: simple-nlp-monitoring-quick-start
excerpt: Quickstart Notebook
metadata:
  title: 'Quickstart: NLP Monitoring | Fiddler Docs'
  description: >-
    This document provides a guide on using Fiddler to monitor NLP models by
    applying a multi-class classifier to the 20newsgroup dataset and monitoring
    text embeddings using Fiddler's Vector Monitoring approach.
  image: []
  robots: index
createdAt: Mon Aug 15 2022 23:29:02 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 14:00:57 GMT+0000 (Coordinated Universal Time)
icon: notebook
---

# ML Monitoring - NLP Inputs

This guide will walk you through the basic steps required to use Fiddler for monitoring NLP models. A multi-class classifier is applied to the 20"
"newsgroup dataset and the text embeddings are monitored using Fiddler's unique Vector Monitoring approach.

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_NLP_Multiclass_Monitoring.ipynb)

<div align=""left"">

<figure><img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" alt=""Google Colab"" width=""188""><figcaption></figcaption></figure>

</div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_NLP_Multiclass_Monitoring.ipynb).

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

# Monitoring a Multiclass Classifier Model with Text Inputs
Unstructured data"
" such as text are usually represented as high-dimensional vectors when processed by ML models. In this example notebook we present how [Fiddler Vector Monitoring](https://www.fiddler.ai/blog/monitoring-natural-language-processing-and-computer-vision-models-part-1) can be used to monitor NLP models using a text classification use case.

Following the steps in this notebook you can see how to onboard models that deal with unstructured text inputs. In this example, we use the 20Newsgroups dataset and train a multi-class classifier that is applied to vector embeddings of text documents.

We monitor this model at production time and assess the performance of Fiddler's vector monitoring by manufacturing synthetic drift via sampling from specific text categories at different deployment time intervals.

---

Now we perform the following steps to demonstrate how Fiddler NLP monitoring works:

1. Connect to Fiddler
2. Load a Data Sample
3. Define the Model Specifications
4. Create a Model"
"
5. Publish a Static Baseline (Optional)
6. Publish Production Events

Get insights!

## 0. Imports


```python

%pip install -q fiddler-client

import time as time

import numpy as np
import pandas as pd
import fiddler as fdl

print(f""Running Fiddler Python client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g"
". 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'
MODEL_NAME = 'nlp_newsgroups_multiclass'

STATIC_BASELINE_NAME = 'baseline_dataset'

# Sample data hosted on GitHub
PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/nlp_text_multiclassifier_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://media.githubusercontent.com/media/fiddler-labs/fiddler-examples/main/quickstart/data/v3/nlp_text_multiclassifier_production_data.csv'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new"
" project by specifying a unique project name in the fld.Project constructor and calling the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

## 2. Load a Data Sample

Now we retrieve the 20Newsgroup dataset. This dataset is fetched from the [scikit-learn real-world datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#) and pre-processed using [this notebook](https://colab.re"
"search.google.com/github/fiddler-labs/fiddler-examples/blob/main/pre-proccessing/20newsgroups_prep_vectorization.ipynb).  For simplicity sake, we have stored a copy in our GitHub repo.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
sample_data_df
```

## 3. Define the Model Specifications

Next we should tell Fiddler a bit more about the model by creating a `ModelSpec` object that specifies the model's inputs, outputs, targets, and metadata, along with other information such as the enrichments we want performed on our model's data.

### Instruct Fiddler to generate embeddings for our unstructured model input

Fiddler offers a powerful set of enrichment services that we can use to enhance how we monitor our model's performance.  In this example, we instruct Fiddler to generate embeddings for our unstructured text.  These generated embeddings are a numerical vector that represent the"
" content and context of our unstructured input field, _original_text_.  These embeddings then power Fiddler's vector monitoring capability for detecting drift.

Before creating a `ModelSpec` object, we define a custom feature using the [fdl.Enrichment()](https://docs.fiddler.ai/python-client-3-x/api-methods-30#fdl.enrichment-private-preview) API. When creating an enrichment, a name must be assigned to the custom feature using the `name` argument. Each enrichment appears in the monitoring tab in the Fiddler UI with this assigned name. Finally, the default clustering setup can be modified by passing the number of cluster centroids to the `n_clusters` argument.

Here we define an [embedding fdl.Enrichment](https://docs.fiddler.ai/python-client-3-x/api-methods-30#embedding-private-preview) and then use that embedding enrichment to create a [fdl.TextEmbedding](https://docs.fidd"
"ler.ai/python-client-3-x/api-methods-30#customfeature) input that can be used to track drift and to be plotted in Fiddler's UMAP visualizations.


```python
fiddler_backend_enrichments = [
    fdl.Enrichment(
        name='Enrichment Text Embedding',
        enrichment='embedding',
        columns=['original_text'],
    ),
    fdl.TextEmbedding(
        name='Original TextEmbedding',
        source_column='original_text',
        column='Enrichment Text Embedding',
        n_clusters=6,
    ),
]
```


```python
model_spec = fdl.ModelSpec(
    inputs=['original_text'],
    outputs=[col for col in sample_data_df.columns if col.startswith('prob_')],
    targets=['target'],
    metadata=['n_tokens', 'string_size', 'timestamp'],
    custom_features=fiddler_backend_enrichments,
)
```

If you have columns in your ModelSpec which denote"
" **prediction IDs or timestamps**, then Fiddler can use these to power its analytics accordingly. Here we are just noting the column with the event datetime.


```python
timestamp_column = 'timestamp'
```

Fiddler supports a variety of model tasks. In this case, we're setting the task type to multi-class classification to inform Fiddler that is the type of model we are monitoring.


```python
model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION

task_params = fdl.ModelTaskParams(
    target_class_order=[col[5:] for col in model_spec.outputs]
)
```

## 4. Create a Model

Once we've specified information about our model, we can onboard (add) it to Fiddler by calling `Model.create`.


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model"
"_task,
    task_params=task_params,
    event_ts_col=timestamp_column,
)

model.create()
print(f'New model created with id = {model.id} and name = {model.name}')
```

## 5. Publish a Static Baseline (Optional)

Since Fiddler already knows how to process data for your model, we can now add a **baseline dataset**.

You can think of this as a static dataset which represents **""golden data,""** or the kind of data your model expects to receive. 

Note : The data given during the model creation is purely for schema inference and not a default baseline.

Then, once we start sending production data to Fiddler, you'll be able to see **drift scores** telling you whenever it starts to diverge from this static baseline.

***

Let's publish our **original data sample** as a pre-production dataset. This will automatically add it as a baseline for the model.


*For more information on how to"
" design your baseline dataset, [click here](https://docs.fiddler.ai/client-guide/creating-a-baseline-dataset).*


```python
baseline_publish_job = model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {baseline_publish_job.id}'
)

# Uncomment the line below to wait for the job to finish, otherwise it will run in the background.
# You can check the status on the Jobs page in the Fiddler UI or use the job ID to query the job status via the API.
# baseline_publish_job.wait()
```

## 6. Publish Production Events

Now let's publish some sample production events into Fiddler. For the timestamps in the middle of the event dataset, we've over-sampled from certain topics which introduces synthetic drift. This oversampling to inject drift allows"
" us to better illustrate how Fiddler's vector monitoring approach detects drift in our unstructured inputs.


```python
production_data_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be as recent as today
production_data_df['timestamp'] = production_data_df['timestamp'] + (
    int(time.time() * 1000) - production_data_df['timestamp'].max()
)
production_data_df
```

Next, let's publish events our events with the synthetic drift


```python
production_publish_job = model.publish(production_data_df)

print(
    f'Initiated production environment data upload with Job ID = {production_publish_job.id}'
)

# Uncomment the line below to wait for the job to finish, otherwise it will run in the background.
# You can check the status on the Jobs page in the Fiddler UI or use the job ID to query the job status via the API.
# production_publish_job.wait()
```

#"
" Get Insights


**You're all done!**
  
Head to the Fiddler UI to start getting enhanced observability into your model's performance.



In particular, you can go to your model's default dashboard in Fiddler and check out the resulting drift chart . The first image below is a screenshot of the automatically generated drift chart which shows the drift of the original text. The second image is a custom drift chart with traffic details at an hourly time bin. (Annotation bubbles are not generated by the Fiddler UI.)

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/nlp_multiiclass_drift_2.png""  />
        </td>
    </tr>
</table>

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/n"
"lp_multiiclass_drift.png""  />
        </td>
    </tr>
</table>
"
"---
title: ML Monitoring - Ranking
slug: ranking-model
excerpt: ''
metadata:
  title: 'Quickstart: Ranking Monitoring | Fiddler Docs'
  description: >-
    This document explains how Fiddler enables monitoring for a Ranking model
    using a dataset from Expedia that includes shopping and purchase data with
    information on price competitiveness.
  robots: index
icon: notebook
---

# ML Monitoring - Ranking

This notebook will show you how Fiddler enables monitoring for a Ranking model. This notebook uses a public dataset from Expedia that includes shopping and purchase data with information on price competitiveness. The data are organized around a set of ‚Äúsearch result impressions‚Äù, or the ordered list of hotels that the user sees after they search for a hotel on the Expedia website.

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest"
"/Fiddler_Quickstart_Ranking_Model.ipynb)

<div align=""left"">

<figure><img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" alt=""Google Colab"" width=""188""><figcaption></figcaption></figure>

</div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Ranking_Model.ipynb).

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

# Fiddler Ranking Model Quick Start Guide

Fiddler enables your teams to monitor and evaluate model rankings, providing insights into model performance and detecting issues like data drift before they impact your applications.

### Example: Expedia Search Ranking
The following dataset contains Expedia shopping and purchase data as well as information on price competitiveness. The data are organized around a set of ‚Äúsearch result impressions‚Äù, or the ordered"
" list of hotels that the user sees after they search for a hotel on the Expedia website. In addition to impressions from the existing algorithm, the data contain impressions where the hotels were randomly sorted, to avoid the position bias of the existing algorithm. The user response is provided as a click on a hotel. From: https://www.kaggle.com/c/expedia-personalized-sort/overview

1. Connect to Fiddler
2. Load a Data Sample
3. Define the Model Specifications and Model Task
4. Create a Model
5. Publish a Pre-production Baseline
6. Publish Production Events

## 0. Imports


```python
%pip install -q fiddler-client

import time as time

import pandas as pd
import fiddler as fdl

print(f""Running Fiddler Python client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your model"
" with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
# Constants for the default example, change as needed to create your own versions
PROJECT_NAME = 'quickstart_examples'
MODEL_NAME = 'search_ranking_example'
DATASET_NAME = 'expedia_dataset'

PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main"
"/quickstart/data/v3/expedia_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/expedia_logs.csv'
```

Next we use these credentials to connect to the Fiddler API.


```python
fdl.init(url=URL, token=TOKEN)
```

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and calling the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id ="
" {project.id} and name = {project.name}')
```

## 2. Load a Data Sample

Now we retrieve the Expedia Dataset as a data sample for this model.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
sample_data_df.head()
```

Fiddler uses this data sample to keep track of important information about your data.
  
This includes **data types**, **data ranges**, and **unique values** for categorical variables.

## 3. Define the Model Specifications and Model Task

To add a Ranking model you must specify the ModelTask as `RANKING` in the model info object.  

Additionally, you must provide the `group_by` argument that corresponds to the query search id. This `group_by` column should be present either in:
- `features` : if it is used to build and run the model
- `metadata_cols` : if not used by the model

Optionally, you can give a `"
"ranking_top_k` number (default is 50). This will be the number of results within each query to take into account while computing the performance metrics in monitoring.  

Unless the prediction column was part of your baseline dataset, you must provide the minimum and maximum values predictions can take in a dictionary format (see below).  

If your target is categorical (string), you need to provide the `target_class_order` argument. If your target is numerical and you don't specify this argument, Fiddler will infer it.   

This will be the list of possible values for the target **ordered**. The first element should be the least relevant target level, the last element should be the most relevant target level.


```python
model_spec = fdl.ModelSpec(
    inputs=list(
        sample_data_df.drop(
            columns=[
                'binary_relevance',
                'score',
                'graded_relevance',
                'position',
                'timestamp',
            ]
        ).columns
    ),
   "
" outputs=['score'],
    targets=['binary_relevance'],
    metadata=['timestamp', 'graded_relevance', 'position'],
)
```

If you have columns in your ModelSpec which denote **prediction IDs or timestamps**, then Fiddler can use these to power its analytics accordingly.

Let's call them out here and use them when configuring the Model in step 5.


```python
# id_column = '' # Optional: Specify the name of the ID column if you have one
timestamp_column = 'timestamp'
```


```python
model_task = fdl.ModelTask.RANKING

task_params = fdl.ModelTaskParams(
    group_by='srch_id', top_k=20, target_class_order=[0, 1]
)
```

## 4. Create a Model


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model"
"_task,
    task_params=task_params,
    event_ts_col=timestamp_column,
)

model.create()
print(f'New model created with id = {model.id} and name = {model.name}')
```

## 5. Publish a Pre-production Baseline **(Optional)**

We can publish the data sample from earlier to add it as a baseline.

For ranking, we need to ingest all events from a given query or search ID together. To do that, we need to transform the data to a grouped format.  
You can use the `group_by` utility function to perform this transformation.


```python
sample_df_grouped = fdl.utils.helpers.group_by(
    df=sample_data_df, group_by_col='srch_id'
)

baseline_publish_job = model.publish(
    source=sample_df_grouped,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=DATASET_NAME,
)
print(
    f'Initiated pre-production environment data"
" upload with Job ID = {baseline_publish_job.id}'
)

# Uncomment the line below to wait for the job to finish, otherwise it will run in the background.
# You can check the status on the Jobs page in the Fiddler UI or use the job ID to query the job status via the API.
# baseline_publish_job.wait()
```

# 6. Publish Events For Monitoring

Fiddler will **monitor this data and compare it to your baseline to generate powerful insights into how your model is behaving**.


---


Each record sent to Fiddler is called **an event**.
  
Let's load some sample events from a CSV file.


```python
production_data_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be as recent as today
production_data_df['timestamp'] = production_data_df['timestamp'] + (
    int(time.time() * 1000) - production_data_df['timestamp'].max()
)
production"
"_data_df
```

Again, let's group the data before sending it to Fiddler.


```python
df_logs_grouped = fdl.utils.helpers.group_by(
    df=production_data_df, group_by_col='srch_id'
)
df_logs_grouped
```

Now publish the events


```python
production_publish_job = model.publish(df_logs_grouped)

print(
    f'Initiated production environment data upload with Job ID = {production_publish_job.id}'
)

# Uncomment the line below to wait for the job to finish, otherwise it will run in the background.
# You can check the status on the Jobs page in the Fiddler UI or use the job ID to query the job status via the API.
# production_publish_job.wait()
```

# Get insights


**You're all done!**
  
You can now head to your Fiddler environment and start getting enhanced observability into your model's performance.

<table>
    <tr>
        <"
"td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/ranking_model_1.png"" />
        </td>
    </tr>
</table>

--------
**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
icon: notebook
---

# ML Monitoring - Model Versions

This guide will walk you through how you can use Model Versions feature in setting up multiple versions of the same model, **using sample data provided by Fiddler**.

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Model_Versions.ipynb)

<div align=""left"">

<figure><img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" alt=""Google Colab"" width=""188""><figcaption></figcaption></figure>

</div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Model_Versions.ipynb).

{% include ""../.gitbook/includes/main-doc-footer.md"
""" %}

# Model Versions

In this notebook, we present the steps for creating addtional versions of a model.  When a model is onboarded to Fiddler it is considered version 1 by default. To make signifacant changes to an existing model, such as altering the model schema, a new version of the model must be created. A model can have as many versions as desired and each can be live simultaneously or retained for historical viewing. 

This notebook is an example of how changes can be made in a `ModelSchema` and how Fiddler maintains them using versioning.

---

Model versioning docs can be referenced [here](https://docs.fiddler.ai/product-guide/monitoring-platform/model-versions) 

Model Versions are supported on Fiddler Python client version 3.1.0 and above using Python version 3.10+.

You can experience Fiddler's Model Versioning in minutes by following these quick steps:

1"
". Connect to Fiddler
2. Load a Data Sample
3. Create a Model: first version with no ModelTask
4. Second Version: target class and binary classification task & defined threshold
5. Third Version: change the datatype of a column and delete a column 
6. Fourth Version: change the column names
7. Fifth version: update column value range
8. Update Version Name
9. Delete a Model Version

# 0. Imports


```python
%pip install -q fiddler-client

import time as time

import pandas as pd
import fiddler as fdl

print(f""Running Fiddler Python client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1."
" The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'
MODEL_NAME = 'bank_churn_model_versions'
DATASET_NAME = 'baseline_dataset'

PATH_TO_SAMPLE_CSV = ""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/churn_data_sample.csv""
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or"
" Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and calling the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

## 2. Load a Data Sample

Load the sample dataset, store the list of columns, and create a subset of input columns (model features) for later use.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
column_list = sample_data_df.columns
input_columns"
" = list(
    column_list.drop(['predicted_churn', 'churn', 'customer_id', 'timestamp'])
)

sample_data_df
```

## 3. Create a Model

Create the first version of model in the project with NOT_SET task

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/model_versions_1.png"" />
        </td>
    </tr>
</table>


```python
# Note the model version label is semantic and can be set to any desired alphanumeric string
# **** rules? ****
version_v1 = 'v1'

# Define the model specification, the role each column plays in the Fiddler model
model_spec = fdl.ModelSpec(
    inputs=input_columns,
    outputs=['predicted_churn'],
    targets=['churn'],
    metadata=['customer_id', 'timestamp'],
    decisions=[],
    custom_features"
"=[],
)

try:
    model_v1 = fdl.Model.from_name(
        name=MODEL_NAME, project_id=project.id, version=version_v1
    )

    print(
        f'Loaded existing model with id = {model_v1.id}, name = {model_v1.name} and version = {model_v1.version}'
    )
except fdl.NotFound:
    model_v1 = fdl.Model.from_data(
        source=sample_data_df,
        name=MODEL_NAME,
        version=version_v1,
        project_id=project.id,
        spec=model_spec,
        task=fdl.ModelTask.NOT_SET,  # this sets the modeltask as NOT SET
    )

    model_v1.create()  # this creates the model
    print(
        f'New model created with id = {model_v1.id}, name = {model_v1.name} and version = {model_v1.version}'
    )
```

## 4."
" Second Version

Add a second Model version with binary classification task.

Update the version and provide target class and binary classification task & threshold.



```python
version_v2 = 'v2'

task_params = fdl.ModelTaskParams(
    binary_classification_threshold=0.5,
    target_class_order=['no', 'yes'],
)

try:
    model_v2 = fdl.Model.from_name(
        name=MODEL_NAME, project_id=project.id, version=version_v2
    )

    print(
        f'Loaded existing model with id = {model_v2.id}, name = {model_v2.name} and version = {model_v2.version}'
    )
except fdl.NotFound:
    model_v2 = model_v1.duplicate(version=version_v2)
    model_v2.task_params = task_params
    model_v2.task = fdl.ModelTask.BINARY_CLASSIFICATION

    model_v2.create()
    print(
        f'New model created with id"
" = {model_v2.id}, name = {model_v2.name} and version = {model_v2.version}'
    )
```

## 5. Third Version

For this third version of the Model we are:
1. Removing the input parameter ""tenure""
2. Changing the datatype of column ""geography"" from Category to String


```python
version_v3 = 'v3'

try:
    model_v3 = fdl.Model.from_name(
        name=MODEL_NAME, project_id=project.id, version=version_v3
    )

    print(
        f'Loaded existing model with id = {model_v3.id}, name = {model_v3.name} and version = {model_v3.version}'
    )
except fdl.NotFound:
    model_v3 = model_v2.duplicate(version=version_v3)

    # Remove the ""tenure"" column from the Model
    del model_v3.schema[
        'tenure'
    ]"
"  # this deletes the tenure column from the Model schema and subsequently the inputs
    input_columns.remove('tenure')
    model_v3.spec.inputs = input_columns

    # Categorical column ""hascrcard"" is currently numerical, changing it to categorical
    model_v3.schema['hascrcard'].min = (
        None  # Removing min and mix of a numerical column before changing datatype
    )
    model_v3.schema['hascrcard'].max = None
    model_v3.schema['hascrcard'].data_type = fdl.DataType.BOOLEAN
    model_v3.schema['hascrcard'].categories = [True, False]

    model_v3.create()
    print(
        f'New model created with id = {model_v3.id}, name = {model_v3.name} and version = {model_v3.version}'
    )
```

## 6. Fourth Version

Add a fourth version with a change in schema by changing the name of"
" the columns


```python
version_v4 = 'v4'

try:
    model_v4 = fdl.Model.from_name(
        name=MODEL_NAME, project_id=project.id, version=version_v4
    )

    print(
        f'Loaded existing model with id = {model_v4.id}, name = {model_v4.name} and version = {model_v4.version}'
    )
except fdl.NotFound:
    model_v4 = model_v3.duplicate(version=version_v4)
    model_v4.schema['age'].name = 'Age'  # we are renaming the column names
    model_v4.schema['creditscore'].name = 'CreditScore'
    model_v4.schema['geography'].name = 'Geography'
    model_v4.schema['balance'].name = 'BalanceNew'
    model_v4.schema['numofproducts'].name = 'NumOfProducts'
    model_v4.schema['hascrcard'].name ="
" 'HasCrCard'
    model_v4.schema['isactivemember'].name = 'IsActiveMember'
    model_v4.schema['estimatedsalary'].name = 'EstimatedSalary'
    model_v4.spec.inputs = [
        'CreditScore',
        'Geography',
        'Age',
        'BalanceNew',
        'NumOfProducts',
        'HasCrCard',
        'IsActiveMember',
        'EstimatedSalary',
    ]

    model_v4.create()
    print(
        f'New model created with id = {model_v4.id}, name = {model_v4.name} and version = {model_v4.version}'
    )
```

## 7. Fifth Version

Add a fifth version with where the schema is changing by increasing the max limit of the balance field.


```python
version_v5 = 'v5'

try:
    model_v5 = fdl.Model.from_name(
        name=MODEL_NAME, project_id=project.id, version=version_v"
"5
    )
    print(
        f'Loaded existing model with id = {model_v5.id}, name = {model_v5.name} and version = {model_v5.version}'
    )
except fdl.NotFound as e:
    model_v5 = model_v4.duplicate(version=version_v5)
    model_v5.schema['Age'].min = (
        18  # This sets the min and max of the age column, overriding what was inferred from the sample data
    )
    model_v5.schema['Age'].max = 85

    model_v5.schema['BalanceNew'].max = (
        1250000  # This sets the max value for the balance column, overriding what was inferred from the sample data
    )

    model_v5.create()
    print(
        f'New model created with id = {model_v5.id}, name = {model_v5.name} and version = {model_v5.version}'
    )
```

"
"## 8. Update version name


```python
model_v4.version = 'v4-old'  # Rename the existing version name to 'v4-old'
model_v4.update()

print(f'Model version updated to: {model_v4.version}')
```

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/model_versions_3.png"" />
        </td>
    </tr>
</table>

## 9. Delete Model Version

Delete version v5 of the Model


```python
model_delete_job = model_v5.delete()  # this deletes a specified version of the model

# Uncomment the line below to wait for the job to finish, otherwise it will run in the background.
# You can check the status on the Jobs page in the Fiddler UI or use the job ID to query the job status via the API.
# model"
"_delete_job.wait()
```



---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
title: ML Monitoring - Class Imbalance
slug: class-imbalance-monitoring-example
metadata:
  title: 'Quickstart: Class Imbalance Monitoring | Fiddler Docs'
  description: >-
    This document discusses the class imbalance problem in machine learning and
    how Fiddler uses a class weighting parameter to address it, showcasing the
    difference in detecting drift signals in the minority class.
  robots: index
icon: notebook
---

# ML Monitoring - Class Imbalance

Many ML use cases, like fraud detection and facial recognition, suffer from what is known as the _class imbalance problem_. This problem exists where a vast majority of the inferences seen by the model belong to only one class, known as the majority class. This makes detecting drift in the minority class very difficult as the ""signal"" is completely outweighed by the sheer number of inferences seen in the majority class.

This guide showcases how Fiddler uses a class weighting parameter to deal"
" with this problem. This notebook will onboard two identical models -- one without class imbalance weighting and one with class imbalance weighting -- to illustrate how drift signals in the minority class are easier to detect once properly amplified by Fiddler's unique class weighting approach..

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Imbalanced_Data.ipynb)

<div align=""left"">

<figure><img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" alt=""Google Colab"" width=""188""><figcaption></figcaption></figure>

</div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Imbalanced_Data.ipynb).

{% include ""../.gitbook"
"/includes/main-doc-footer.md"" %}

# Fiddler Quick Start Class Imbalance Guide

Many ML use cases, like fraud detection and facial recognition, suffer from what is known as the class imbalance problem.  This problem exists where a vast majority of the inferences seen by the model belong to only one class, known as the majority class.  This makes detecting drift in the minority class very difficult as the ""signal"" is completely outweighed by the large number of inferences seen in the majority class.  The following notebook showcases how Fiddler uses a class weighting paramater to deal with this problem. This notebook will onboard two identical models -- one without class imbalance weighting and one with class imbalance weighting -- to illustrate how drift signals in the minority class are easier to detect once properly amplified by Fiddler's unique class weighting approach.

1. Connect to Fiddler
2. Load a Data Sample
3. Create Both Model Versions
4. Publish Static Baselines
5"
". Publish Production Events
6. Compare the Two Models

## 0. Imports


```python
%pip install -q fiddler-client;

import time

import sklearn
import numpy as np
import pandas as pd
import fiddler as fdl

print(f""Running Fiddler Python client version {fdl.__version__}"")
```

# 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name"
".fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'
MODEL_NAME = 'imbalance_cc_fraud'
MODEL_NAME_WEIGHTED = 'imbalance_cc_fraud_weighted'
STATIC_BASELINE_NAME = 'baseline_dataset'

PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_production_data.csv'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the"
" fld.Project constructor and call the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

# 2. Load a Data Sample

In this example, we'll be looking at a fraud detection use case.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python

sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
sample_data_df
```


```python
sample_data_df"
"['Class'].value_counts()

print(
    'Percentage of minority class: {}%'.format(
        round(
            sample_data_df['Class'].value_counts()[1] * 100 / sample_data_df.shape[0], 4
        )
    )
)
```

# 3. Create Both Model Versions

Now, we will create two models:
1. One model with class weight parameters
2. One model without class weight parameters

Below, we first create a `ModelSpec` object which is common between the two. 


```python
model_spec = fdl.ModelSpec(
    inputs=set(sample_data_df.columns) - set(['Class', 'prediction_score', 'timestamp']),
    outputs=['prediction_score'],
    targets=['Class'],
    metadata=['timestamp']
)
```

If you have columns in your ModelSpec which denote **prediction IDs or timestamps**, then Fiddler can use these to power its analytics accordingly.

Let's call them out here and use them when configuring"
" the Model.


```python
# id_column = '' # Optional: Specify the name of the ID column if you have one
timestamp_column = 'timestamp'
```

Define the weighted and unweighted versions of the model task parameters


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

# Weighted Model Task Params
task_params_weighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
    class_weights=sklearn.utils.class_weight.compute_class_weight(
        class_weight=""balanced"",
        classes=np.unique(sample_data_df[""Class""]),
        y=sample_data_df[""Class""],
    ).tolist(),
)

# Unweighted Model Task Params aka default Model Task Params
task_params_unweighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
)
```

Now, we onboard (create) the two models to Fidd"
"ler -- the first without any class weights and the second with defined class weights.


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params_unweighted,
    event_ts_col=timestamp_column
)

model.create()
print(f'New unweighted model created with id = {model.id} and name = {model.name}')

weighted_model = fdl.Model.from_data(
    name=MODEL_NAME_WEIGHTED,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params_weighted,
    event_ts_col=timestamp_column
)

weighted_model.create()
print(f'New weighted model created with id = {weighted_model.id} and name = {weighted_model.name}')

```

# 4. Publish Static Baselines

Since Fidd"
"ler already knows how to process data for your models, we can now add a **baseline dataset**.

You can think of this as a static dataset which represents **""golden data,""** or the kind of data your model expects to receive.

Then, once we start sending production data to Fiddler, you'll be able to see **drift scores** telling you whenever it starts to diverge from this static baseline.

***

Let's publish our **original data sample** as a pre-production dataset. This will automatically add it as a baseline for each model.


*For more information on how to design your baseline dataset, [click here](https://docs.fiddler.ai/client-guide/creating-a-baseline-dataset).*


```python
baseline_publish_job = model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with"
" Job ID = {baseline_publish_job.id}'
)

baseline_publish_job_weighted = weighted_model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {baseline_publish_job_weighted.id}'
)

# Uncomment the lines below to wait for the jobs to finish, otherwise they will run in the background.
# You can check the statuses on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# baseline_publish_job.wait()
# baseline_publish_job_weighted.wait()
```

# 5. Publish Production Events 

Publish the same events to both models with synthetic drift in the minority class


```python
production_data_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be as recent as today
production_data_df['timestamp']"
" = production_data_df['timestamp'] + (
    int(time.time() * 1000) - production_data_df['timestamp'].max()
)
production_data_df
```


```python
print(
    ""Percentage of minority class: {}%"".format(
        round(
            production_data_df[""Class""].value_counts()[1] * 100 / production_data_df.shape[0], 4
        )
    )
)
```

We see that the percentage of minority class in production data is > 3 times than that of baseline data. This should create a big drift in the predictions.

We will now publish the same production/event data for both of the models -- the one with class weights and the one without class weights.


```python
production_publish_job = model.publish(production_data_df)

print(f'For Model: {model.name} - initiated production environment data upload with Job ID = {production_publish_job.id}')

production_publish_job_weighted = weighted_model.publish(production_data_df"
")

print(f'For Model: {weighted_model.name} - initiated production environment data upload with Job ID = {production_publish_job_weighted.id}')

# Uncomment the lines below to wait for the jobs to finish, otherwise they will run in the background.
# You can check the statuses on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# production_publish_job.wait()
# production_publish_job_weighted.wait()
```

# 5. Compare the Two Models

**You're all done!**


In the Fiddler UI, we can see the model without the class weights defined the output/prediction drift in the minority class is very hard to detect (`<=0.05`) because it is obsured by the overwhelming volume of events in the majority class.  If we declare class weights, then we see a higher drift which is a more accurate respresentation of the production data where the ratio of minority is class is"
" 3x.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/imabalance_data_1.png"" />
        </td>
    </tr>
</table>

**What's Next?**

Try the [LLM Monitoring - Quick Start Notebook](https://docs.fiddler.ai/quickstart-notebooks/simple-llm-monitoring)

---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
# Fiddler Quick Start Class Imbalance Guide

"
"Many ML use cases, like fraud detection and facial recognition, suffer from what is known as the class imbalance problem.  This problem exists where a vast majority of the inferences seen by the model belong to only one class, known as the majority class.  This makes detecting drift in the minority class very difficult as the ""signal"" is completely outweighed by the large number of inferences seen in the majority class.  The following notebook showcases how Fiddler uses a class weighting paramater to deal with this problem. This notebook will onboard two identical models -- one without class imbalance weighting and one with class imbalance weighting -- to illustrate how drift signals in the minority class are easier to detect once properly amplified by Fiddler's unique class weighting approach.

1. Connect to Fiddler
2. Load a Data Sample
3. Create Both Model Versions
4. Publish Static Baselines
5. Publish Production Events
6. Compare the Two Models

## 0. Imports


```"
"python
%pip install -q fiddler-client;

import time

import sklearn
import numpy as np
import pandas as pd
import fiddler as fdl

print(f""Running Fiddler Python client version {fdl.__version__}"")
```

# 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed"
" to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'
MODEL_NAME = 'imbalance_cc_fraud'
MODEL_NAME_WEIGHTED = 'imbalance_cc_fraud_weighted'
STATIC_BASELINE_NAME = 'baseline_dataset'

PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_production_data.csv'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and call the `create()` method. If the project already exists, it will"
" load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

# 2. Load a Data Sample

In this example, we'll be looking at a fraud detection use case.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python

sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
sample_data_df
```


```python
sample_data_df['Class'].value_counts()

print(
    'Percentage of minority class: {}%'.format"
"(
        round(
            sample_data_df['Class'].value_counts()[1] * 100 / sample_data_df.shape[0], 4
        )
    )
)
```

# 3. Create Both Model Versions

Now, we will create two models:
1. One model with class weight parameters
2. One model without class weight parameters

Below, we first create a `ModelSpec` object which is common between the two. 


```python
model_spec = fdl.ModelSpec(
    inputs=set(sample_data_df.columns) - set(['Class', 'prediction_score', 'timestamp']),
    outputs=['prediction_score'],
    targets=['Class'],
    metadata=['timestamp']
)
```

If you have columns in your ModelSpec which denote **prediction IDs or timestamps**, then Fiddler can use these to power its analytics accordingly.

Let's call them out here and use them when configuring the Model.


```python
# id_column = '' # Optional: Specify the name of the"
" ID column if you have one
timestamp_column = 'timestamp'
```

Define the weighted and unweighted versions of the model task parameters


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

# Weighted Model Task Params
task_params_weighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
    class_weights=sklearn.utils.class_weight.compute_class_weight(
        class_weight=""balanced"",
        classes=np.unique(sample_data_df[""Class""]),
        y=sample_data_df[""Class""],
    ).tolist(),
)

# Unweighted Model Task Params aka default Model Task Params
task_params_unweighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
)
```

Now, we onboard (create) the two models to Fiddler -- the first without any class weights and the second with defined class weights.


```python
"
"model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params_unweighted,
    event_ts_col=timestamp_column
)

model.create()
print(f'New unweighted model created with id = {model.id} and name = {model.name}')

weighted_model = fdl.Model.from_data(
    name=MODEL_NAME_WEIGHTED,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params_weighted,
    event_ts_col=timestamp_column
)

weighted_model.create()
print(f'New weighted model created with id = {weighted_model.id} and name = {weighted_model.name}')

```

# 4. Publish Static Baselines

Since Fiddler already knows how to process data for your models, we can now add a **baseline dataset"
"**.

You can think of this as a static dataset which represents **""golden data,""** or the kind of data your model expects to receive.

Then, once we start sending production data to Fiddler, you'll be able to see **drift scores** telling you whenever it starts to diverge from this static baseline.

***

Let's publish our **original data sample** as a pre-production dataset. This will automatically add it as a baseline for each model.


*For more information on how to design your baseline dataset, [click here](https://docs.fiddler.ai/client-guide/creating-a-baseline-dataset).*


```python
baseline_publish_job = model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {baseline_publish_job.id}'
)

baseline_publish_job_weighted = weighted_model.publish"
"(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {baseline_publish_job_weighted.id}'
)

# Uncomment the lines below to wait for the jobs to finish, otherwise they will run in the background.
# You can check the statuses on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# baseline_publish_job.wait()
# baseline_publish_job_weighted.wait()
```

# 5. Publish Production Events 

Publish the same events to both models with synthetic drift in the minority class


```python
production_data_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be as recent as today
production_data_df['timestamp'] = production_data_df['timestamp'] + (
    int(time.time() * 1000)"
" - production_data_df['timestamp'].max()
)
production_data_df
```


```python
print(
    ""Percentage of minority class: {}%"".format(
        round(
            production_data_df[""Class""].value_counts()[1] * 100 / production_data_df.shape[0], 4
        )
    )
)
```

We see that the percentage of minority class in production data is > 3 times than that of baseline data. This should create a big drift in the predictions.

We will now publish the same production/event data for both of the models -- the one with class weights and the one without class weights.


```python
production_publish_job = model.publish(production_data_df)

print(f'For Model: {model.name} - initiated production environment data upload with Job ID = {production_publish_job.id}')

production_publish_job_weighted = weighted_model.publish(production_data_df)

print(f'For Model: {weighted_model.name} - initiated production environment data upload with"
" Job ID = {production_publish_job_weighted.id}')

# Uncomment the lines below to wait for the jobs to finish, otherwise they will run in the background.
# You can check the statuses on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# production_publish_job.wait()
# production_publish_job_weighted.wait()
```

# 5. Compare the Two Models

**You're all done!**


In the Fiddler UI, we can see the model without the class weights defined the output/prediction drift in the minority class is very hard to detect (`<=0.05`) because it is obsured by the overwhelming volume of events in the majority class.  If we declare class weights, then we see a higher drift which is a more accurate respresentation of the production data where the ratio of minority is class is 3x.

<table>
    <tr>
        <td>
            <img src="""
"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/imabalance_data_1.png"" />
        </td>
    </tr>
</table>

**What's Next?**

Try the [LLM Monitoring - Quick Start Notebook](https://docs.fiddler.ai/quickstart-notebooks/simple-llm-monitoring)

---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
# Fiddler Quick Start Class Imbalance Guide

Many ML use cases, like fraud detection and facial recognition, suffer from what is known as the"
" class imbalance problem.  This problem exists where a vast majority of the inferences seen by the model belong to only one class, known as the majority class.  This makes detecting drift in the minority class very difficult as the ""signal"" is completely outweighed by the large number of inferences seen in the majority class.  The following notebook showcases how Fiddler uses a class weighting paramater to deal with this problem. This notebook will onboard two identical models -- one without class imbalance weighting and one with class imbalance weighting -- to illustrate how drift signals in the minority class are easier to detect once properly amplified by Fiddler's unique class weighting approach.

1. Connect to Fiddler
2. Load a Data Sample
3. Create Both Model Versions
4. Publish Static Baselines
5. Publish Production Events
6. Compare the Two Models

## 0. Imports


```python
%pip install -q fiddler-client;

import time

import sklearn
import"
" numpy as np
import pandas as pd
import fiddler as fdl

print(f""Running Fiddler Python client version {fdl.__version__}"")
```

# 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'
MODEL_NAME"
" = 'imbalance_cc_fraud'
MODEL_NAME_WEIGHTED = 'imbalance_cc_fraud_weighted'
STATIC_BASELINE_NAME = 'baseline_dataset'

PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_production_data.csv'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and call the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = f"
"dl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

# 2. Load a Data Sample

In this example, we'll be looking at a fraud detection use case.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python

sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
sample_data_df
```


```python
sample_data_df['Class'].value_counts()

print(
    'Percentage of minority class: {}%'.format(
        round(
            sample_data_df['Class'].value_counts()[1] * 100"
" / sample_data_df.shape[0], 4
        )
    )
)
```

# 3. Create Both Model Versions

Now, we will create two models:
1. One model with class weight parameters
2. One model without class weight parameters

Below, we first create a `ModelSpec` object which is common between the two. 


```python
model_spec = fdl.ModelSpec(
    inputs=set(sample_data_df.columns) - set(['Class', 'prediction_score', 'timestamp']),
    outputs=['prediction_score'],
    targets=['Class'],
    metadata=['timestamp']
)
```

If you have columns in your ModelSpec which denote **prediction IDs or timestamps**, then Fiddler can use these to power its analytics accordingly.

Let's call them out here and use them when configuring the Model.


```python
# id_column = '' # Optional: Specify the name of the ID column if you have one
timestamp_column = 'timestamp'
```

Define the weighted and"
" unweighted versions of the model task parameters


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

# Weighted Model Task Params
task_params_weighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
    class_weights=sklearn.utils.class_weight.compute_class_weight(
        class_weight=""balanced"",
        classes=np.unique(sample_data_df[""Class""]),
        y=sample_data_df[""Class""],
    ).tolist(),
)

# Unweighted Model Task Params aka default Model Task Params
task_params_unweighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
)
```

Now, we onboard (create) the two models to Fiddler -- the first without any class weights and the second with defined class weights.


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project"
".id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params_unweighted,
    event_ts_col=timestamp_column
)

model.create()
print(f'New unweighted model created with id = {model.id} and name = {model.name}')

weighted_model = fdl.Model.from_data(
    name=MODEL_NAME_WEIGHTED,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params_weighted,
    event_ts_col=timestamp_column
)

weighted_model.create()
print(f'New weighted model created with id = {weighted_model.id} and name = {weighted_model.name}')

```

# 4. Publish Static Baselines

Since Fiddler already knows how to process data for your models, we can now add a **baseline dataset**.

You can think of this as a static dataset which represents **""golden data,""**"
" or the kind of data your model expects to receive.

Then, once we start sending production data to Fiddler, you'll be able to see **drift scores** telling you whenever it starts to diverge from this static baseline.

***

Let's publish our **original data sample** as a pre-production dataset. This will automatically add it as a baseline for each model.


*For more information on how to design your baseline dataset, [click here](https://docs.fiddler.ai/client-guide/creating-a-baseline-dataset).*


```python
baseline_publish_job = model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {baseline_publish_job.id}'
)

baseline_publish_job_weighted = weighted_model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION"
",
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {baseline_publish_job_weighted.id}'
)

# Uncomment the lines below to wait for the jobs to finish, otherwise they will run in the background.
# You can check the statuses on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# baseline_publish_job.wait()
# baseline_publish_job_weighted.wait()
```

# 5. Publish Production Events 

Publish the same events to both models with synthetic drift in the minority class


```python
production_data_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be as recent as today
production_data_df['timestamp'] = production_data_df['timestamp'] + (
    int(time.time() * 1000) - production_data_df['timestamp'].max()
)
production_data_df
```


```python
"
"print(
    ""Percentage of minority class: {}%"".format(
        round(
            production_data_df[""Class""].value_counts()[1] * 100 / production_data_df.shape[0], 4
        )
    )
)
```

We see that the percentage of minority class in production data is > 3 times than that of baseline data. This should create a big drift in the predictions.

We will now publish the same production/event data for both of the models -- the one with class weights and the one without class weights.


```python
production_publish_job = model.publish(production_data_df)

print(f'For Model: {model.name} - initiated production environment data upload with Job ID = {production_publish_job.id}')

production_publish_job_weighted = weighted_model.publish(production_data_df)

print(f'For Model: {weighted_model.name} - initiated production environment data upload with Job ID = {production_publish_job_weighted.id}')

# Uncomment the lines below to wait for"
" the jobs to finish, otherwise they will run in the background.
# You can check the statuses on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# production_publish_job.wait()
# production_publish_job_weighted.wait()
```

# 5. Compare the Two Models

**You're all done!**


In the Fiddler UI, we can see the model without the class weights defined the output/prediction drift in the minority class is very hard to detect (`<=0.05`) because it is obsured by the overwhelming volume of events in the majority class.  If we declare class weights, then we see a higher drift which is a more accurate respresentation of the production data where the ratio of minority is class is 3x.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart"
"/images/imabalance_data_1.png"" />
        </td>
    </tr>
</table>

**What's Next?**

Try the [LLM Monitoring - Quick Start Notebook](https://docs.fiddler.ai/quickstart-notebooks/simple-llm-monitoring)

---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
# Fiddler Quick Start Class Imbalance Guide

Many ML use cases, like fraud detection and facial recognition, suffer from what is known as the class imbalance problem.  This problem exists where a vast majority of the inferences seen by the"
" model belong to only one class, known as the majority class.  This makes detecting drift in the minority class very difficult as the ""signal"" is completely outweighed by the large number of inferences seen in the majority class.  The following notebook showcases how Fiddler uses a class weighting paramater to deal with this problem. This notebook will onboard two identical models -- one without class imbalance weighting and one with class imbalance weighting -- to illustrate how drift signals in the minority class are easier to detect once properly amplified by Fiddler's unique class weighting approach.

1. Connect to Fiddler
2. Load a Data Sample
3. Create Both Model Versions
4. Publish Static Baselines
5. Publish Production Events
6. Compare the Two Models

## 0. Imports


```python
%pip install -q fiddler-client;

import time

import sklearn
import numpy as np
import pandas as pd
import fiddler as fdl

print(f"
"""Running Fiddler Python client version {fdl.__version__}"")
```

# 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'
MODEL_NAME = 'imbalance_cc_fraud'
MODEL_NAME_WEIGHTED = 'imbalance_cc_fraud"
"_weighted'
STATIC_BASELINE_NAME = 'baseline_dataset'

PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_production_data.csv'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and call the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id ="
" {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

# 2. Load a Data Sample

In this example, we'll be looking at a fraud detection use case.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python

sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
sample_data_df
```


```python
sample_data_df['Class'].value_counts()

print(
    'Percentage of minority class: {}%'.format(
        round(
            sample_data_df['Class'].value_counts()[1] * 100 / sample_data_df.shape[0], 4
        )
    )
)
```

#"
" 3. Create Both Model Versions

Now, we will create two models:
1. One model with class weight parameters
2. One model without class weight parameters

Below, we first create a `ModelSpec` object which is common between the two. 


```python
model_spec = fdl.ModelSpec(
    inputs=set(sample_data_df.columns) - set(['Class', 'prediction_score', 'timestamp']),
    outputs=['prediction_score'],
    targets=['Class'],
    metadata=['timestamp']
)
```

If you have columns in your ModelSpec which denote **prediction IDs or timestamps**, then Fiddler can use these to power its analytics accordingly.

Let's call them out here and use them when configuring the Model.


```python
# id_column = '' # Optional: Specify the name of the ID column if you have one
timestamp_column = 'timestamp'
```

Define the weighted and unweighted versions of the model task parameters


```python
model_task = fdl.ModelTask"
".BINARY_CLASSIFICATION

# Weighted Model Task Params
task_params_weighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
    class_weights=sklearn.utils.class_weight.compute_class_weight(
        class_weight=""balanced"",
        classes=np.unique(sample_data_df[""Class""]),
        y=sample_data_df[""Class""],
    ).tolist(),
)

# Unweighted Model Task Params aka default Model Task Params
task_params_unweighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
)
```

Now, we onboard (create) the two models to Fiddler -- the first without any class weights and the second with defined class weights.


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
"
"    task_params=task_params_unweighted,
    event_ts_col=timestamp_column
)

model.create()
print(f'New unweighted model created with id = {model.id} and name = {model.name}')

weighted_model = fdl.Model.from_data(
    name=MODEL_NAME_WEIGHTED,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params_weighted,
    event_ts_col=timestamp_column
)

weighted_model.create()
print(f'New weighted model created with id = {weighted_model.id} and name = {weighted_model.name}')

```

# 4. Publish Static Baselines

Since Fiddler already knows how to process data for your models, we can now add a **baseline dataset**.

You can think of this as a static dataset which represents **""golden data,""** or the kind of data your model expects to receive.

Then, once we start sending production data"
" to Fiddler, you'll be able to see **drift scores** telling you whenever it starts to diverge from this static baseline.

***

Let's publish our **original data sample** as a pre-production dataset. This will automatically add it as a baseline for each model.


*For more information on how to design your baseline dataset, [click here](https://docs.fiddler.ai/client-guide/creating-a-baseline-dataset).*


```python
baseline_publish_job = model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {baseline_publish_job.id}'
)

baseline_publish_job_weighted = weighted_model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre"
"-production environment data upload with Job ID = {baseline_publish_job_weighted.id}'
)

# Uncomment the lines below to wait for the jobs to finish, otherwise they will run in the background.
# You can check the statuses on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# baseline_publish_job.wait()
# baseline_publish_job_weighted.wait()
```

# 5. Publish Production Events 

Publish the same events to both models with synthetic drift in the minority class


```python
production_data_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be as recent as today
production_data_df['timestamp'] = production_data_df['timestamp'] + (
    int(time.time() * 1000) - production_data_df['timestamp'].max()
)
production_data_df
```


```python
print(
    ""Percentage of minority class: {}%"".format(
        round(
            production"
"_data_df[""Class""].value_counts()[1] * 100 / production_data_df.shape[0], 4
        )
    )
)
```

We see that the percentage of minority class in production data is > 3 times than that of baseline data. This should create a big drift in the predictions.

We will now publish the same production/event data for both of the models -- the one with class weights and the one without class weights.


```python
production_publish_job = model.publish(production_data_df)

print(f'For Model: {model.name} - initiated production environment data upload with Job ID = {production_publish_job.id}')

production_publish_job_weighted = weighted_model.publish(production_data_df)

print(f'For Model: {weighted_model.name} - initiated production environment data upload with Job ID = {production_publish_job_weighted.id}')

# Uncomment the lines below to wait for the jobs to finish, otherwise they will run in the background.
# You can check the statuses"
" on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# production_publish_job.wait()
# production_publish_job_weighted.wait()
```

# 5. Compare the Two Models

**You're all done!**


In the Fiddler UI, we can see the model without the class weights defined the output/prediction drift in the minority class is very hard to detect (`<=0.05`) because it is obsured by the overwhelming volume of events in the majority class.  If we declare class weights, then we see a higher drift which is a more accurate respresentation of the production data where the ratio of minority is class is 3x.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/imabalance_data_1.png"" />
        </td>
    </tr>
</"
"table>

**What's Next?**

Try the [LLM Monitoring - Quick Start Notebook](https://docs.fiddler.ai/quickstart-notebooks/simple-llm-monitoring)

---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
title: ML Monitoring - Class Imbalance
slug: class-imbalance-monitoring-example
metadata:
  title: 'Quickstart: Class Imbalance Monitoring | Fiddler Docs'
  description: >-
    This document discusses the class imbalance problem in machine learning and
    how Fiddler uses a class weighting parameter to address it, showcasing the
    difference in detecting drift signals in the minority class.
  robots: index
icon: notebook
---

# ML Monitoring - Class Imbalance

Many ML use cases, like fraud detection and facial recognition, suffer from what is known as the _class imbalance problem_. This problem exists where a vast majority of the inferences seen by the model belong to only one class, known as the majority class. This makes detecting drift in the minority class very difficult as the ""signal"" is completely outweighed by the sheer number of inferences seen in the majority class.

This guide showcases how Fiddler uses a class weighting parameter to deal"
" with this problem. This notebook will onboard two identical models -- one without class imbalance weighting and one with class imbalance weighting -- to illustrate how drift signals in the minority class are easier to detect once properly amplified by Fiddler's unique class weighting approach..

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Imbalanced_Data.ipynb)

<div align=""left"">

<figure><img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" alt=""Google Colab"" width=""188""><figcaption></figcaption></figure>

</div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Imbalanced_Data.ipynb).

{% include ""../.gitbook"
"/includes/main-doc-footer.md"" %}

# Fiddler Quick Start Class Imbalance Guide

Many ML use cases, like fraud detection and facial recognition, suffer from what is known as the class imbalance problem.  This problem exists where a vast majority of the inferences seen by the model belong to only one class, known as the majority class.  This makes detecting drift in the minority class very difficult as the ""signal"" is completely outweighed by the large number of inferences seen in the majority class.  The following notebook showcases how Fiddler uses a class weighting paramater to deal with this problem. This notebook will onboard two identical models -- one without class imbalance weighting and one with class imbalance weighting -- to illustrate how drift signals in the minority class are easier to detect once properly amplified by Fiddler's unique class weighting approach.

1. Connect to Fiddler
2. Load a Data Sample
3. Create Both Model Versions
4. Publish Static Baselines
5"
". Publish Production Events
6. Compare the Two Models

## 0. Imports


```python
%pip install -q fiddler-client;

import time

import sklearn
import numpy as np
import pandas as pd
import fiddler as fdl

print(f""Running Fiddler Python client version {fdl.__version__}"")
```

# 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name"
".fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'
MODEL_NAME = 'imbalance_cc_fraud'
MODEL_NAME_WEIGHTED = 'imbalance_cc_fraud_weighted'
STATIC_BASELINE_NAME = 'baseline_dataset'

PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_production_data.csv'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the"
" fld.Project constructor and call the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

# 2. Load a Data Sample

In this example, we'll be looking at a fraud detection use case.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python

sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
sample_data_df
```


```python
sample_data_df"
"['Class'].value_counts()

print(
    'Percentage of minority class: {}%'.format(
        round(
            sample_data_df['Class'].value_counts()[1] * 100 / sample_data_df.shape[0], 4
        )
    )
)
```

# 3. Create Both Model Versions

Now, we will create two models:
1. One model with class weight parameters
2. One model without class weight parameters

Below, we first create a `ModelSpec` object which is common between the two. 


```python
model_spec = fdl.ModelSpec(
    inputs=set(sample_data_df.columns) - set(['Class', 'prediction_score', 'timestamp']),
    outputs=['prediction_score'],
    targets=['Class'],
    metadata=['timestamp']
)
```

If you have columns in your ModelSpec which denote **prediction IDs or timestamps**, then Fiddler can use these to power its analytics accordingly.

Let's call them out here and use them when configuring"
" the Model.


```python
# id_column = '' # Optional: Specify the name of the ID column if you have one
timestamp_column = 'timestamp'
```

Define the weighted and unweighted versions of the model task parameters


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

# Weighted Model Task Params
task_params_weighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
    class_weights=sklearn.utils.class_weight.compute_class_weight(
        class_weight=""balanced"",
        classes=np.unique(sample_data_df[""Class""]),
        y=sample_data_df[""Class""],
    ).tolist(),
)

# Unweighted Model Task Params aka default Model Task Params
task_params_unweighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
)
```

Now, we onboard (create) the two models to Fidd"
"ler -- the first without any class weights and the second with defined class weights.


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params_unweighted,
    event_ts_col=timestamp_column
)

model.create()
print(f'New unweighted model created with id = {model.id} and name = {model.name}')

weighted_model = fdl.Model.from_data(
    name=MODEL_NAME_WEIGHTED,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params_weighted,
    event_ts_col=timestamp_column
)

weighted_model.create()
print(f'New weighted model created with id = {weighted_model.id} and name = {weighted_model.name}')

```

# 4. Publish Static Baselines

Since Fidd"
"ler already knows how to process data for your models, we can now add a **baseline dataset**.

You can think of this as a static dataset which represents **""golden data,""** or the kind of data your model expects to receive.

Then, once we start sending production data to Fiddler, you'll be able to see **drift scores** telling you whenever it starts to diverge from this static baseline.

***

Let's publish our **original data sample** as a pre-production dataset. This will automatically add it as a baseline for each model.


*For more information on how to design your baseline dataset, [click here](https://docs.fiddler.ai/client-guide/creating-a-baseline-dataset).*


```python
baseline_publish_job = model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with"
" Job ID = {baseline_publish_job.id}'
)

baseline_publish_job_weighted = weighted_model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {baseline_publish_job_weighted.id}'
)

# Uncomment the lines below to wait for the jobs to finish, otherwise they will run in the background.
# You can check the statuses on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# baseline_publish_job.wait()
# baseline_publish_job_weighted.wait()
```

# 5. Publish Production Events 

Publish the same events to both models with synthetic drift in the minority class


```python
production_data_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be as recent as today
production_data_df['timestamp']"
" = production_data_df['timestamp'] + (
    int(time.time() * 1000) - production_data_df['timestamp'].max()
)
production_data_df
```


```python
print(
    ""Percentage of minority class: {}%"".format(
        round(
            production_data_df[""Class""].value_counts()[1] * 100 / production_data_df.shape[0], 4
        )
    )
)
```

We see that the percentage of minority class in production data is > 3 times than that of baseline data. This should create a big drift in the predictions.

We will now publish the same production/event data for both of the models -- the one with class weights and the one without class weights.


```python
production_publish_job = model.publish(production_data_df)

print(f'For Model: {model.name} - initiated production environment data upload with Job ID = {production_publish_job.id}')

production_publish_job_weighted = weighted_model.publish(production_data_df"
")

print(f'For Model: {weighted_model.name} - initiated production environment data upload with Job ID = {production_publish_job_weighted.id}')

# Uncomment the lines below to wait for the jobs to finish, otherwise they will run in the background.
# You can check the statuses on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# production_publish_job.wait()
# production_publish_job_weighted.wait()
```

# 5. Compare the Two Models

**You're all done!**


In the Fiddler UI, we can see the model without the class weights defined the output/prediction drift in the minority class is very hard to detect (`<=0.05`) because it is obsured by the overwhelming volume of events in the majority class.  If we declare class weights, then we see a higher drift which is a more accurate respresentation of the production data where the ratio of minority is class is"
" 3x.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/imabalance_data_1.png"" />
        </td>
    </tr>
</table>

**What's Next?**

Try the [LLM Monitoring - Quick Start Notebook](https://docs.fiddler.ai/quickstart-notebooks/simple-llm-monitoring)

---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
# Fiddler Quick Start Class Imbalance Guide

"
"Many ML use cases, like fraud detection and facial recognition, suffer from what is known as the class imbalance problem.  This problem exists where a vast majority of the inferences seen by the model belong to only one class, known as the majority class.  This makes detecting drift in the minority class very difficult as the ""signal"" is completely outweighed by the large number of inferences seen in the majority class.  The following notebook showcases how Fiddler uses a class weighting paramater to deal with this problem. This notebook will onboard two identical models -- one without class imbalance weighting and one with class imbalance weighting -- to illustrate how drift signals in the minority class are easier to detect once properly amplified by Fiddler's unique class weighting approach.

1. Connect to Fiddler
2. Load a Data Sample
3. Create Both Model Versions
4. Publish Static Baselines
5. Publish Production Events
6. Compare the Two Models

## 0. Imports


```"
"python
%pip install -q fiddler-client;

import time

import sklearn
import numpy as np
import pandas as pd
import fiddler as fdl

print(f""Running Fiddler Python client version {fdl.__version__}"")
```

# 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed"
" to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'
MODEL_NAME = 'imbalance_cc_fraud'
MODEL_NAME_WEIGHTED = 'imbalance_cc_fraud_weighted'
STATIC_BASELINE_NAME = 'baseline_dataset'

PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_production_data.csv'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and call the `create()` method. If the project already exists, it will"
" load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

# 2. Load a Data Sample

In this example, we'll be looking at a fraud detection use case.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python

sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
sample_data_df
```


```python
sample_data_df['Class'].value_counts()

print(
    'Percentage of minority class: {}%'.format"
"(
        round(
            sample_data_df['Class'].value_counts()[1] * 100 / sample_data_df.shape[0], 4
        )
    )
)
```

# 3. Create Both Model Versions

Now, we will create two models:
1. One model with class weight parameters
2. One model without class weight parameters

Below, we first create a `ModelSpec` object which is common between the two. 


```python
model_spec = fdl.ModelSpec(
    inputs=set(sample_data_df.columns) - set(['Class', 'prediction_score', 'timestamp']),
    outputs=['prediction_score'],
    targets=['Class'],
    metadata=['timestamp']
)
```

If you have columns in your ModelSpec which denote **prediction IDs or timestamps**, then Fiddler can use these to power its analytics accordingly.

Let's call them out here and use them when configuring the Model.


```python
# id_column = '' # Optional: Specify the name of the"
" ID column if you have one
timestamp_column = 'timestamp'
```

Define the weighted and unweighted versions of the model task parameters


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

# Weighted Model Task Params
task_params_weighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
    class_weights=sklearn.utils.class_weight.compute_class_weight(
        class_weight=""balanced"",
        classes=np.unique(sample_data_df[""Class""]),
        y=sample_data_df[""Class""],
    ).tolist(),
)

# Unweighted Model Task Params aka default Model Task Params
task_params_unweighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
)
```

Now, we onboard (create) the two models to Fiddler -- the first without any class weights and the second with defined class weights.


```python
"
"model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params_unweighted,
    event_ts_col=timestamp_column
)

model.create()
print(f'New unweighted model created with id = {model.id} and name = {model.name}')

weighted_model = fdl.Model.from_data(
    name=MODEL_NAME_WEIGHTED,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params_weighted,
    event_ts_col=timestamp_column
)

weighted_model.create()
print(f'New weighted model created with id = {weighted_model.id} and name = {weighted_model.name}')

```

# 4. Publish Static Baselines

Since Fiddler already knows how to process data for your models, we can now add a **baseline dataset"
"**.

You can think of this as a static dataset which represents **""golden data,""** or the kind of data your model expects to receive.

Then, once we start sending production data to Fiddler, you'll be able to see **drift scores** telling you whenever it starts to diverge from this static baseline.

***

Let's publish our **original data sample** as a pre-production dataset. This will automatically add it as a baseline for each model.


*For more information on how to design your baseline dataset, [click here](https://docs.fiddler.ai/client-guide/creating-a-baseline-dataset).*


```python
baseline_publish_job = model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {baseline_publish_job.id}'
)

baseline_publish_job_weighted = weighted_model.publish"
"(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {baseline_publish_job_weighted.id}'
)

# Uncomment the lines below to wait for the jobs to finish, otherwise they will run in the background.
# You can check the statuses on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# baseline_publish_job.wait()
# baseline_publish_job_weighted.wait()
```

# 5. Publish Production Events 

Publish the same events to both models with synthetic drift in the minority class


```python
production_data_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be as recent as today
production_data_df['timestamp'] = production_data_df['timestamp'] + (
    int(time.time() * 1000)"
" - production_data_df['timestamp'].max()
)
production_data_df
```


```python
print(
    ""Percentage of minority class: {}%"".format(
        round(
            production_data_df[""Class""].value_counts()[1] * 100 / production_data_df.shape[0], 4
        )
    )
)
```

We see that the percentage of minority class in production data is > 3 times than that of baseline data. This should create a big drift in the predictions.

We will now publish the same production/event data for both of the models -- the one with class weights and the one without class weights.


```python
production_publish_job = model.publish(production_data_df)

print(f'For Model: {model.name} - initiated production environment data upload with Job ID = {production_publish_job.id}')

production_publish_job_weighted = weighted_model.publish(production_data_df)

print(f'For Model: {weighted_model.name} - initiated production environment data upload with"
" Job ID = {production_publish_job_weighted.id}')

# Uncomment the lines below to wait for the jobs to finish, otherwise they will run in the background.
# You can check the statuses on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# production_publish_job.wait()
# production_publish_job_weighted.wait()
```

# 5. Compare the Two Models

**You're all done!**


In the Fiddler UI, we can see the model without the class weights defined the output/prediction drift in the minority class is very hard to detect (`<=0.05`) because it is obsured by the overwhelming volume of events in the majority class.  If we declare class weights, then we see a higher drift which is a more accurate respresentation of the production data where the ratio of minority is class is 3x.

<table>
    <tr>
        <td>
            <img src="""
"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/imabalance_data_1.png"" />
        </td>
    </tr>
</table>

**What's Next?**

Try the [LLM Monitoring - Quick Start Notebook](https://docs.fiddler.ai/quickstart-notebooks/simple-llm-monitoring)

---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
# Fiddler Quick Start Class Imbalance Guide

Many ML use cases, like fraud detection and facial recognition, suffer from what is known as the"
" class imbalance problem.  This problem exists where a vast majority of the inferences seen by the model belong to only one class, known as the majority class.  This makes detecting drift in the minority class very difficult as the ""signal"" is completely outweighed by the large number of inferences seen in the majority class.  The following notebook showcases how Fiddler uses a class weighting paramater to deal with this problem. This notebook will onboard two identical models -- one without class imbalance weighting and one with class imbalance weighting -- to illustrate how drift signals in the minority class are easier to detect once properly amplified by Fiddler's unique class weighting approach.

1. Connect to Fiddler
2. Load a Data Sample
3. Create Both Model Versions
4. Publish Static Baselines
5. Publish Production Events
6. Compare the Two Models

## 0. Imports


```python
%pip install -q fiddler-client;

import time

import sklearn
import"
" numpy as np
import pandas as pd
import fiddler as fdl

print(f""Running Fiddler Python client version {fdl.__version__}"")
```

# 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'
MODEL_NAME"
" = 'imbalance_cc_fraud'
MODEL_NAME_WEIGHTED = 'imbalance_cc_fraud_weighted'
STATIC_BASELINE_NAME = 'baseline_dataset'

PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_production_data.csv'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and call the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = f"
"dl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

# 2. Load a Data Sample

In this example, we'll be looking at a fraud detection use case.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python

sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
sample_data_df
```


```python
sample_data_df['Class'].value_counts()

print(
    'Percentage of minority class: {}%'.format(
        round(
            sample_data_df['Class'].value_counts()[1] * 100"
" / sample_data_df.shape[0], 4
        )
    )
)
```

# 3. Create Both Model Versions

Now, we will create two models:
1. One model with class weight parameters
2. One model without class weight parameters

Below, we first create a `ModelSpec` object which is common between the two. 


```python
model_spec = fdl.ModelSpec(
    inputs=set(sample_data_df.columns) - set(['Class', 'prediction_score', 'timestamp']),
    outputs=['prediction_score'],
    targets=['Class'],
    metadata=['timestamp']
)
```

If you have columns in your ModelSpec which denote **prediction IDs or timestamps**, then Fiddler can use these to power its analytics accordingly.

Let's call them out here and use them when configuring the Model.


```python
# id_column = '' # Optional: Specify the name of the ID column if you have one
timestamp_column = 'timestamp'
```

Define the weighted and"
" unweighted versions of the model task parameters


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

# Weighted Model Task Params
task_params_weighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
    class_weights=sklearn.utils.class_weight.compute_class_weight(
        class_weight=""balanced"",
        classes=np.unique(sample_data_df[""Class""]),
        y=sample_data_df[""Class""],
    ).tolist(),
)

# Unweighted Model Task Params aka default Model Task Params
task_params_unweighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
)
```

Now, we onboard (create) the two models to Fiddler -- the first without any class weights and the second with defined class weights.


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project"
".id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params_unweighted,
    event_ts_col=timestamp_column
)

model.create()
print(f'New unweighted model created with id = {model.id} and name = {model.name}')

weighted_model = fdl.Model.from_data(
    name=MODEL_NAME_WEIGHTED,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params_weighted,
    event_ts_col=timestamp_column
)

weighted_model.create()
print(f'New weighted model created with id = {weighted_model.id} and name = {weighted_model.name}')

```

# 4. Publish Static Baselines

Since Fiddler already knows how to process data for your models, we can now add a **baseline dataset**.

You can think of this as a static dataset which represents **""golden data,""**"
" or the kind of data your model expects to receive.

Then, once we start sending production data to Fiddler, you'll be able to see **drift scores** telling you whenever it starts to diverge from this static baseline.

***

Let's publish our **original data sample** as a pre-production dataset. This will automatically add it as a baseline for each model.


*For more information on how to design your baseline dataset, [click here](https://docs.fiddler.ai/client-guide/creating-a-baseline-dataset).*


```python
baseline_publish_job = model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {baseline_publish_job.id}'
)

baseline_publish_job_weighted = weighted_model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION"
",
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {baseline_publish_job_weighted.id}'
)

# Uncomment the lines below to wait for the jobs to finish, otherwise they will run in the background.
# You can check the statuses on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# baseline_publish_job.wait()
# baseline_publish_job_weighted.wait()
```

# 5. Publish Production Events 

Publish the same events to both models with synthetic drift in the minority class


```python
production_data_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be as recent as today
production_data_df['timestamp'] = production_data_df['timestamp'] + (
    int(time.time() * 1000) - production_data_df['timestamp'].max()
)
production_data_df
```


```python
"
"print(
    ""Percentage of minority class: {}%"".format(
        round(
            production_data_df[""Class""].value_counts()[1] * 100 / production_data_df.shape[0], 4
        )
    )
)
```

We see that the percentage of minority class in production data is > 3 times than that of baseline data. This should create a big drift in the predictions.

We will now publish the same production/event data for both of the models -- the one with class weights and the one without class weights.


```python
production_publish_job = model.publish(production_data_df)

print(f'For Model: {model.name} - initiated production environment data upload with Job ID = {production_publish_job.id}')

production_publish_job_weighted = weighted_model.publish(production_data_df)

print(f'For Model: {weighted_model.name} - initiated production environment data upload with Job ID = {production_publish_job_weighted.id}')

# Uncomment the lines below to wait for"
" the jobs to finish, otherwise they will run in the background.
# You can check the statuses on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# production_publish_job.wait()
# production_publish_job_weighted.wait()
```

# 5. Compare the Two Models

**You're all done!**


In the Fiddler UI, we can see the model without the class weights defined the output/prediction drift in the minority class is very hard to detect (`<=0.05`) because it is obsured by the overwhelming volume of events in the majority class.  If we declare class weights, then we see a higher drift which is a more accurate respresentation of the production data where the ratio of minority is class is 3x.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart"
"/images/imabalance_data_1.png"" />
        </td>
    </tr>
</table>

**What's Next?**

Try the [LLM Monitoring - Quick Start Notebook](https://docs.fiddler.ai/quickstart-notebooks/simple-llm-monitoring)

---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
# Fiddler Quick Start Class Imbalance Guide

Many ML use cases, like fraud detection and facial recognition, suffer from what is known as the class imbalance problem.  This problem exists where a vast majority of the inferences seen by the"
" model belong to only one class, known as the majority class.  This makes detecting drift in the minority class very difficult as the ""signal"" is completely outweighed by the large number of inferences seen in the majority class.  The following notebook showcases how Fiddler uses a class weighting paramater to deal with this problem. This notebook will onboard two identical models -- one without class imbalance weighting and one with class imbalance weighting -- to illustrate how drift signals in the minority class are easier to detect once properly amplified by Fiddler's unique class weighting approach.

1. Connect to Fiddler
2. Load a Data Sample
3. Create Both Model Versions
4. Publish Static Baselines
5. Publish Production Events
6. Compare the Two Models

## 0. Imports


```python
%pip install -q fiddler-client;

import time

import sklearn
import numpy as np
import pandas as pd
import fiddler as fdl

print(f"
"""Running Fiddler Python client version {fdl.__version__}"")
```

# 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'
MODEL_NAME = 'imbalance_cc_fraud'
MODEL_NAME_WEIGHTED = 'imbalance_cc_fraud"
"_weighted'
STATIC_BASELINE_NAME = 'baseline_dataset'

PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/imbalance_production_data.csv'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and call the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id ="
" {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

# 2. Load a Data Sample

In this example, we'll be looking at a fraud detection use case.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python

sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
sample_data_df
```


```python
sample_data_df['Class'].value_counts()

print(
    'Percentage of minority class: {}%'.format(
        round(
            sample_data_df['Class'].value_counts()[1] * 100 / sample_data_df.shape[0], 4
        )
    )
)
```

#"
" 3. Create Both Model Versions

Now, we will create two models:
1. One model with class weight parameters
2. One model without class weight parameters

Below, we first create a `ModelSpec` object which is common between the two. 


```python
model_spec = fdl.ModelSpec(
    inputs=set(sample_data_df.columns) - set(['Class', 'prediction_score', 'timestamp']),
    outputs=['prediction_score'],
    targets=['Class'],
    metadata=['timestamp']
)
```

If you have columns in your ModelSpec which denote **prediction IDs or timestamps**, then Fiddler can use these to power its analytics accordingly.

Let's call them out here and use them when configuring the Model.


```python
# id_column = '' # Optional: Specify the name of the ID column if you have one
timestamp_column = 'timestamp'
```

Define the weighted and unweighted versions of the model task parameters


```python
model_task = fdl.ModelTask"
".BINARY_CLASSIFICATION

# Weighted Model Task Params
task_params_weighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
    class_weights=sklearn.utils.class_weight.compute_class_weight(
        class_weight=""balanced"",
        classes=np.unique(sample_data_df[""Class""]),
        y=sample_data_df[""Class""],
    ).tolist(),
)

# Unweighted Model Task Params aka default Model Task Params
task_params_unweighted = fdl.ModelTaskParams(
    target_class_order=[0, 1],
    binary_classification_threshold=0.4,
)
```

Now, we onboard (create) the two models to Fiddler -- the first without any class weights and the second with defined class weights.


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
"
"    task_params=task_params_unweighted,
    event_ts_col=timestamp_column
)

model.create()
print(f'New unweighted model created with id = {model.id} and name = {model.name}')

weighted_model = fdl.Model.from_data(
    name=MODEL_NAME_WEIGHTED,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params_weighted,
    event_ts_col=timestamp_column
)

weighted_model.create()
print(f'New weighted model created with id = {weighted_model.id} and name = {weighted_model.name}')

```

# 4. Publish Static Baselines

Since Fiddler already knows how to process data for your models, we can now add a **baseline dataset**.

You can think of this as a static dataset which represents **""golden data,""** or the kind of data your model expects to receive.

Then, once we start sending production data"
" to Fiddler, you'll be able to see **drift scores** telling you whenever it starts to diverge from this static baseline.

***

Let's publish our **original data sample** as a pre-production dataset. This will automatically add it as a baseline for each model.


*For more information on how to design your baseline dataset, [click here](https://docs.fiddler.ai/client-guide/creating-a-baseline-dataset).*


```python
baseline_publish_job = model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {baseline_publish_job.id}'
)

baseline_publish_job_weighted = weighted_model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre"
"-production environment data upload with Job ID = {baseline_publish_job_weighted.id}'
)

# Uncomment the lines below to wait for the jobs to finish, otherwise they will run in the background.
# You can check the statuses on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# baseline_publish_job.wait()
# baseline_publish_job_weighted.wait()
```

# 5. Publish Production Events 

Publish the same events to both models with synthetic drift in the minority class


```python
production_data_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be as recent as today
production_data_df['timestamp'] = production_data_df['timestamp'] + (
    int(time.time() * 1000) - production_data_df['timestamp'].max()
)
production_data_df
```


```python
print(
    ""Percentage of minority class: {}%"".format(
        round(
            production"
"_data_df[""Class""].value_counts()[1] * 100 / production_data_df.shape[0], 4
        )
    )
)
```

We see that the percentage of minority class in production data is > 3 times than that of baseline data. This should create a big drift in the predictions.

We will now publish the same production/event data for both of the models -- the one with class weights and the one without class weights.


```python
production_publish_job = model.publish(production_data_df)

print(f'For Model: {model.name} - initiated production environment data upload with Job ID = {production_publish_job.id}')

production_publish_job_weighted = weighted_model.publish(production_data_df)

print(f'For Model: {weighted_model.name} - initiated production environment data upload with Job ID = {production_publish_job_weighted.id}')

# Uncomment the lines below to wait for the jobs to finish, otherwise they will run in the background.
# You can check the statuses"
" on the Jobs page in the Fiddler UI or use the job IDs to query the job statuses via the API.
# production_publish_job.wait()
# production_publish_job_weighted.wait()
```

# 5. Compare the Two Models

**You're all done!**


In the Fiddler UI, we can see the model without the class weights defined the output/prediction drift in the minority class is very hard to detect (`<=0.05`) because it is obsured by the overwhelming volume of events in the majority class.  If we declare class weights, then we see a higher drift which is a more accurate respresentation of the production data where the ratio of minority is class is 3x.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/imabalance_data_1.png"" />
        </td>
    </tr>
</"
"table>

**What's Next?**

Try the [LLM Monitoring - Quick Start Notebook](https://docs.fiddler.ai/quickstart-notebooks/simple-llm-monitoring)

---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
icon: notebook
---

# ML Monitoring - Model Versions

This guide will walk you through how you can use Model Versions feature in setting up multiple versions of the same model, **using sample data provided by Fiddler**.

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Model_Versions.ipynb)

<div align=""left"">

<figure><img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" alt=""Google Colab"" width=""188""><figcaption></figcaption></figure>

</div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Model_Versions.ipynb).

{% include ""../.gitbook/includes/main-doc-footer.md"
""" %}

# Model Versions

In this notebook, we present the steps for creating addtional versions of a model.  When a model is onboarded to Fiddler it is considered version 1 by default. To make signifacant changes to an existing model, such as altering the model schema, a new version of the model must be created. A model can have as many versions as desired and each can be live simultaneously or retained for historical viewing. 

This notebook is an example of how changes can be made in a `ModelSchema` and how Fiddler maintains them using versioning.

---

Model versioning docs can be referenced [here](https://docs.fiddler.ai/product-guide/monitoring-platform/model-versions) 

Model Versions are supported on Fiddler Python client version 3.1.0 and above using Python version 3.10+.

You can experience Fiddler's Model Versioning in minutes by following these quick steps:

1"
". Connect to Fiddler
2. Load a Data Sample
3. Create a Model: first version with no ModelTask
4. Second Version: target class and binary classification task & defined threshold
5. Third Version: change the datatype of a column and delete a column 
6. Fourth Version: change the column names
7. Fifth version: update column value range
8. Update Version Name
9. Delete a Model Version

# 0. Imports


```python
%pip install -q fiddler-client

import time as time

import pandas as pd
import fiddler as fdl

print(f""Running Fiddler Python client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1."
" The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'
MODEL_NAME = 'bank_churn_model_versions'
DATASET_NAME = 'baseline_dataset'

PATH_TO_SAMPLE_CSV = ""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/churn_data_sample.csv""
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or"
" Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and calling the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

## 2. Load a Data Sample

Load the sample dataset, store the list of columns, and create a subset of input columns (model features) for later use.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
column_list = sample_data_df.columns
input_columns"
" = list(
    column_list.drop(['predicted_churn', 'churn', 'customer_id', 'timestamp'])
)

sample_data_df
```

## 3. Create a Model

Create the first version of model in the project with NOT_SET task

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/model_versions_1.png"" />
        </td>
    </tr>
</table>


```python
# Note the model version label is semantic and can be set to any desired alphanumeric string
# **** rules? ****
version_v1 = 'v1'

# Define the model specification, the role each column plays in the Fiddler model
model_spec = fdl.ModelSpec(
    inputs=input_columns,
    outputs=['predicted_churn'],
    targets=['churn'],
    metadata=['customer_id', 'timestamp'],
    decisions=[],
    custom_features"
"=[],
)

try:
    model_v1 = fdl.Model.from_name(
        name=MODEL_NAME, project_id=project.id, version=version_v1
    )

    print(
        f'Loaded existing model with id = {model_v1.id}, name = {model_v1.name} and version = {model_v1.version}'
    )
except fdl.NotFound:
    model_v1 = fdl.Model.from_data(
        source=sample_data_df,
        name=MODEL_NAME,
        version=version_v1,
        project_id=project.id,
        spec=model_spec,
        task=fdl.ModelTask.NOT_SET,  # this sets the modeltask as NOT SET
    )

    model_v1.create()  # this creates the model
    print(
        f'New model created with id = {model_v1.id}, name = {model_v1.name} and version = {model_v1.version}'
    )
```

## 4."
" Second Version

Add a second Model version with binary classification task.

Update the version and provide target class and binary classification task & threshold.



```python
version_v2 = 'v2'

task_params = fdl.ModelTaskParams(
    binary_classification_threshold=0.5,
    target_class_order=['no', 'yes'],
)

try:
    model_v2 = fdl.Model.from_name(
        name=MODEL_NAME, project_id=project.id, version=version_v2
    )

    print(
        f'Loaded existing model with id = {model_v2.id}, name = {model_v2.name} and version = {model_v2.version}'
    )
except fdl.NotFound:
    model_v2 = model_v1.duplicate(version=version_v2)
    model_v2.task_params = task_params
    model_v2.task = fdl.ModelTask.BINARY_CLASSIFICATION

    model_v2.create()
    print(
        f'New model created with id"
" = {model_v2.id}, name = {model_v2.name} and version = {model_v2.version}'
    )
```

## 5. Third Version

For this third version of the Model we are:
1. Removing the input parameter ""tenure""
2. Changing the datatype of column ""geography"" from Category to String


```python
version_v3 = 'v3'

try:
    model_v3 = fdl.Model.from_name(
        name=MODEL_NAME, project_id=project.id, version=version_v3
    )

    print(
        f'Loaded existing model with id = {model_v3.id}, name = {model_v3.name} and version = {model_v3.version}'
    )
except fdl.NotFound:
    model_v3 = model_v2.duplicate(version=version_v3)

    # Remove the ""tenure"" column from the Model
    del model_v3.schema[
        'tenure'
    ]"
"  # this deletes the tenure column from the Model schema and subsequently the inputs
    input_columns.remove('tenure')
    model_v3.spec.inputs = input_columns

    # Categorical column ""hascrcard"" is currently numerical, changing it to categorical
    model_v3.schema['hascrcard'].min = (
        None  # Removing min and mix of a numerical column before changing datatype
    )
    model_v3.schema['hascrcard'].max = None
    model_v3.schema['hascrcard'].data_type = fdl.DataType.BOOLEAN
    model_v3.schema['hascrcard'].categories = [True, False]

    model_v3.create()
    print(
        f'New model created with id = {model_v3.id}, name = {model_v3.name} and version = {model_v3.version}'
    )
```

## 6. Fourth Version

Add a fourth version with a change in schema by changing the name of"
" the columns


```python
version_v4 = 'v4'

try:
    model_v4 = fdl.Model.from_name(
        name=MODEL_NAME, project_id=project.id, version=version_v4
    )

    print(
        f'Loaded existing model with id = {model_v4.id}, name = {model_v4.name} and version = {model_v4.version}'
    )
except fdl.NotFound:
    model_v4 = model_v3.duplicate(version=version_v4)
    model_v4.schema['age'].name = 'Age'  # we are renaming the column names
    model_v4.schema['creditscore'].name = 'CreditScore'
    model_v4.schema['geography'].name = 'Geography'
    model_v4.schema['balance'].name = 'BalanceNew'
    model_v4.schema['numofproducts'].name = 'NumOfProducts'
    model_v4.schema['hascrcard'].name ="
" 'HasCrCard'
    model_v4.schema['isactivemember'].name = 'IsActiveMember'
    model_v4.schema['estimatedsalary'].name = 'EstimatedSalary'
    model_v4.spec.inputs = [
        'CreditScore',
        'Geography',
        'Age',
        'BalanceNew',
        'NumOfProducts',
        'HasCrCard',
        'IsActiveMember',
        'EstimatedSalary',
    ]

    model_v4.create()
    print(
        f'New model created with id = {model_v4.id}, name = {model_v4.name} and version = {model_v4.version}'
    )
```

## 7. Fifth Version

Add a fifth version with where the schema is changing by increasing the max limit of the balance field.


```python
version_v5 = 'v5'

try:
    model_v5 = fdl.Model.from_name(
        name=MODEL_NAME, project_id=project.id, version=version_v"
"5
    )
    print(
        f'Loaded existing model with id = {model_v5.id}, name = {model_v5.name} and version = {model_v5.version}'
    )
except fdl.NotFound as e:
    model_v5 = model_v4.duplicate(version=version_v5)
    model_v5.schema['Age'].min = (
        18  # This sets the min and max of the age column, overriding what was inferred from the sample data
    )
    model_v5.schema['Age'].max = 85

    model_v5.schema['BalanceNew'].max = (
        1250000  # This sets the max value for the balance column, overriding what was inferred from the sample data
    )

    model_v5.create()
    print(
        f'New model created with id = {model_v5.id}, name = {model_v5.name} and version = {model_v5.version}'
    )
```

"
"## 8. Update version name


```python
model_v4.version = 'v4-old'  # Rename the existing version name to 'v4-old'
model_v4.update()

print(f'Model version updated to: {model_v4.version}')
```

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/model_versions_3.png"" />
        </td>
    </tr>
</table>

## 9. Delete Model Version

Delete version v5 of the Model


```python
model_delete_job = model_v5.delete()  # this deletes a specified version of the model

# Uncomment the line below to wait for the job to finish, otherwise it will run in the background.
# You can check the status on the Jobs page in the Fiddler UI or use the job ID to query the job status via the API.
# model"
"_delete_job.wait()
```



---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
title: ML Monitoring - Simple
slug: quick-start
metadata:
  title: 'Quickstart: Simple Monitoring | Fiddler Docs'
  description: >-
    This document provides a guide for using Fiddler for model monitoring using
    sample data provided by Fiddler.
  robots: index
icon: notebook
---

# ML Monitoring - Simple

This guide will walk you through the basic onboarding steps required to use Fiddler for model monitoring, **using sample data provided by Fiddler**.

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Simple_Monitoring.ipynb)

<div align=""left"">

<figure><img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" alt=""Google Colab"" width=""188"
"""><figcaption></figcaption></figure>

</div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Simple_Monitoring.ipynb).

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

# Fiddler Simple Monitoring Quick Start Guide

Fiddler is the pioneer in enterprise Model Performance Management (MPM), offering a unified platform that enables Data Science, MLOps, Risk, Compliance, Analytics, and other LOB teams to **monitor, analyze, and improve ML deployments at enterprise scale**.
Obtain contextual insights at any stage of the ML lifecycle, improve predictions, increase transparency and fairness, and optimize business revenue.

---

You can start using Fiddler ***in minutes*** by following these quick steps:

1. Connect to Fiddler
2. Load a Data Sample
3. Define the Model Specifications
4. Set"
" the Model Task
5. Create a Model
6. Set Up Alerts **(Optional)**
7. Create a Custom Metric **(Optional)**
8. Create a Segment **(Optional)**
9. Publish a Pre-production Baseline **(Optional)**
10. Configure a Rolling Baseline **(Optional)**
11. Publish Production Events

# 0. Imports


```python
%pip install -q fiddler-client

import time as time

import pandas as pd
import fiddler as fdl

print(f'Running Fiddler Python client version {fdl.__version__}')
```

## 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your"
" authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'  # If the project already exists, the notebook will create the model under the existing project.
MODEL_NAME = 'bank_churn_simple_monitoring'

STATIC_BASELINE_NAME = 'baseline_dataset'
ROLLING_BASELINE_NAME = 'rolling_baseline_1week'

# Sample data hosted on GitHub
PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/churn_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/f"
"iddler-examples/main/quickstart/data/v3/churn_production_data.csv'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and calling the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

You should now be"
" able to see the newly created project in the Fiddler UI.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/simple_monitoring_1.png"" />
        </td>
    </tr>
</table>

## 2. Load a Data Sample

In this example, we'll be considering the case where we're a bank and we have **a model that predicts churn for our customers**.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
column_list  = sample_data_df.columns
sample_data_df
```

## 3. Define the Model Specifications

In order to create a model in Fiddler, create a ModelSpec object with information about what each column of your"
" data sample should used for.

Fiddler supports four column types:
1. **Inputs**
2. **Outputs** (Model predictions)
3. **Target** (Ground truth values)
4. **Metadata**


```python
input_columns = list(
    column_list.drop(['predicted_churn', 'churn', 'customer_id', 'timestamp'])
)
```


```python
model_spec = fdl.ModelSpec(
    inputs=input_columns,
    outputs=['predicted_churn'],
    targets=['churn'],  # Note: only a single Target column is allowed, use metadata columns and custom metrics for additional targets
    metadata=['customer_id', 'timestamp'],
)
```

If you have columns in your ModelSpec which denote **prediction IDs or timestamps**, then Fiddler can use these to power its analytics accordingly.

Let's call them out here and use them when configuring the Model in step 5.


```python
id_column = 'customer_id'
timestamp_column = 'timestamp"
"'
```

## 4. Set the Model Task

Fiddler supports a variety of model tasks. In this case, we're adding a binary classification model.

For this, we'll create a ModelTask object and an additional ModelTaskParams object to specify the ordering of our positive and negative labels.

*For a detailed breakdown of all supported model tasks, click [here](https://docs.fiddler.ai/product-guide/task-types).*


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

task_params = fdl.ModelTaskParams(target_class_order=['no', 'yes'])
```

## 5. Create a Model

Create a Model object and publish it to Fiddler, passing in
1. Your data sample
2. The ModelSpec object
3. The ModelTask and ModelTaskParams objects
4. The ID and timestamp columns


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id="
"project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params,
    event_id_col=id_column,
    event_ts_col=timestamp_column,
)

model.create()
print(f'New model created with id = {model.id} and name = {model.name}')
```

On the project page, you should now be able to see the newly onboarded model with its model schema.

<table>
    <tr>
        <td>
            <img src=""https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/images/simple_monitoring_3.png?raw=true"" />
        </td>
    </tr>
</table>

<table>
    <tr>
        <td>
            <img src=""https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/images/simple_monitoring_4.png?raw=true"" />
        </td>
    </tr"
">
</table>

## 6. Set Up Alerts (Optional)

Fiddler allows creating alerting rules when your data or model predictions deviate from expected behavior.

The alert rules can compare metrics to **absolute** or **relative** values.

Please refer to [our documentation](https://docs.fiddler.ai/client-guide/alerts-with-fiddler-client) for more information on Alert Rules.

---
  
Let's set up some alert rules.

The following API call sets up a Data Integrity type rule which triggers an email notification when published events have 2 or more range violations in any 1 day bin for the `numofproducts` column.


```python
alert_rule_1 = fdl.AlertRule(
    name='Bank Churn Range Violation Alert',
    model_id=model.id,
    metric_id='range_violation_count',
    bin_size=fdl.BinSize.DAY,
    compare_to=fdl.CompareTo.RAW_VALUE,
    priority=fdl.Priority"
".HIGH,
    warning_threshold=2,
    critical_threshold=3,
    condition=fdl.AlertCondition.GREATER,
    columns=['numofproducts'],
)

alert_rule_1.create()
print(
    f'New alert rule created with id = {alert_rule_1.id} and name = {alert_rule_1.name}'
)

# Set notification configuration for the alert rule, a single email address for this simple example
alert_rule_1.set_notification_config(emails=['name@google.com'])
```

Let's add a second alert rule.

This one sets up a Performance type rule which triggers an email notification when precision metric is 5% higher than that from 1 hr bin one day ago.


```python
alert_rule_2 = fdl.AlertRule(
    name='Bank Churn Performance Alert',
    model_id=model.id,
    metric_id='precision',
    bin_size=fdl.BinSize.HOUR,
    compare_to=fdl.CompareTo.TIME_PERIOD,
"
"    compare_bin_delta=24,  # Multiple of the bin size
    condition=fdl.AlertCondition.GREATER,
    warning_threshold=0.05,
    critical_threshold=0.1,
    priority=fdl.Priority.HIGH,
)

alert_rule_2.create()
print(
    f'New alert rule created with id = {alert_rule_2.id} and name = {alert_rule_2.name}'
)

# Set notification configuration for the alert rule, a single email address for this simple example
alert_rule_2.set_notification_config(emails=['name@google.com'])
```

## 7. Create a Custom Metric (Optional)

Fiddler's [Custom Metric API](https://docs.fiddler.ai/python-client-3-x/api-methods-30#custommetric) enables user-defined formulas for custom metrics.  Custom metrics will be tracked over time and can be used in Charts and Alerts just like the many out of the box metrics provided by F"
"iddler.  Custom metrics can also be managed in the Fiddler UI.

Please refer to [our documentation](https://docs.fiddler.ai/product-guide/monitoring-platform/custom-metrics) for more information on Custom Metrics.

---
  
Let's create an example custom metric.


```python
custom_metric = fdl.CustomMetric(
    name='Lost Revenue',
    model_id=model.id,
    description='A metric to track revenue lost for each false positive prediction.',
    definition=""""""sum(if(fp(),1,0) * -100)"""""",  # This is an excel like formula which adds -$100 for each false positive predicted by the model
)

custom_metric.create()
print(
    f'New custom metric created with id = {custom_metric.id} and name = {custom_metric.name}'
)
```

## 8. Create a Segment (Optional)
Fiddler's [Segment API](https://docs.fiddler.ai/python-client-3-x/api-methods"
"-30#segments) enables defining named cohorts/sub-segments in your production data. These segments can be tracked over time, added to charts, and alerted upon. Segments can also be managed in the Fiddler UI.

Please refer to our [documentation](https://docs.fiddler.ai/product-guide/monitoring-platform/segments) for more information on the creation and management of segments.

Let's create a segment to track customers from Hawaii for a specific age range.


```python
segment = fdl.Segment(
    name='Hawaii Customers between 30 and 60',
    model_id=model.id,
    description='Hawaii Customers between 30 and 60',
    definition=""(age<60 or age>30) and geography=='Hawaii'"",
)

segment.create()
print(f'New segment created with id = {segment.id} and name = {segment.name}')
```

## 9. Publish a Static Baseline (Optional)

Since Fiddler already knows how"
" to process data for your model, we can now add a **baseline dataset**.

You can think of this as a static dataset which represents **""golden data,""** or the kind of data your model expects to receive.

Then, once we start sending production data to Fiddler, you'll be able to see **drift scores** telling you whenever it starts to diverge from this static baseline.

***

Let's publish our **original data sample** as a pre-production dataset. This will automatically add it as a baseline for the model.


*For more information on how to design your baseline dataset, [click here](https://docs.fiddler.ai/client-guide/creating-a-baseline-dataset).*


```python
baseline_publish_job = model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {"
"baseline_publish_job.id}'
)

# Uncomment the line below to wait for the job to finish, otherwise it will run in the background.
# You can check the status on the Jobs page in the Fiddler UI or use the job ID to query the job status via the API.
# baseline_publish_job.wait()
```

## 10. Configure a Rolling Baseline (Optional)

Fiddler also allows you to configure a baseline based on **past production data.**

This means instead of looking at a static slice of data, it will look into past production events and use what it finds for drift calculation.

Please refer to [our documentation](https://docs.fiddler.ai/client-guide/creating-a-baseline-dataset) for more information on Baselines.

---
  
Let's set up a rolling baseline that will allow us to calculate drift relative to production data from 1 week back.


```python
rolling_baseline = fdl.Baseline(
    model_id=model.id,
    name="
"ROLLING_BASELINE_NAME,
    type_=fdl.BaselineType.ROLLING,
    environment=fdl.EnvType.PRODUCTION,
    window_bin_size=fdl.WindowBinSize.DAY,  # Size of the sliding window
    offset_delta=7,  # How far back to set our window (multiple of window_bin_size)
)

rolling_baseline.create()
print(
    f'New rolling baseline created with id = {rolling_baseline.id} and name = {rolling_baseline.name}'
)
```

## 11. Publish Production Events

Finally, let's send in some production data!


Fiddler will **monitor this data and compare it to your baseline to generate powerful insights into how your model is behaving**.


---


Each record sent to Fiddler is called **an event**.
  
Let's load some sample events from a CSV file.


```python
production_data_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be"
" as recent as today
production_data_df['timestamp'] = production_data_df['timestamp'] + (
    int(time.time() * 1000) - production_data_df['timestamp'].max()
)
production_data_df
```


```python
production_publish_job = model.publish(production_data_df)

print(
    f'Initiated production environment data upload with Job ID = {production_publish_job.id}'
)

# Uncomment the line below to wait for the job to finish, otherwise it will run in the background.
# You can check the status on the Jobs page in the Fiddler UI or use the job ID to query the job status via the API.
# production_publish_job.wait()
```

# Get Insights
  
Return to your Fiddler environment to get enhanced observability into your model's performance.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/simple_monitor"
"ing_5.png"" />
        </td>
    </tr>
</table>

**What's Next?**

Try the [LLM Monitoring - Quick Start Notebook](https://docs.fiddler.ai/quickstart-notebooks/simple-llm-monitoring)

---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
title: ML Monitoring - Ranking
slug: ranking-model
excerpt: ''
metadata:
  title: 'Quickstart: Ranking Monitoring | Fiddler Docs'
  description: >-
    This document explains how Fiddler enables monitoring for a Ranking model
    using a dataset from Expedia that includes shopping and purchase data with
    information on price competitiveness.
  robots: index
icon: notebook
---

# ML Monitoring - Ranking

This notebook will show you how Fiddler enables monitoring for a Ranking model. This notebook uses a public dataset from Expedia that includes shopping and purchase data with information on price competitiveness. The data are organized around a set of ‚Äúsearch result impressions‚Äù, or the ordered list of hotels that the user sees after they search for a hotel on the Expedia website.

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest"
"/Fiddler_Quickstart_Ranking_Model.ipynb)

<div align=""left"">

<figure><img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" alt=""Google Colab"" width=""188""><figcaption></figcaption></figure>

</div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Ranking_Model.ipynb).

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

# Fiddler Ranking Model Quick Start Guide

Fiddler enables your teams to monitor and evaluate model rankings, providing insights into model performance and detecting issues like data drift before they impact your applications.

### Example: Expedia Search Ranking
The following dataset contains Expedia shopping and purchase data as well as information on price competitiveness. The data are organized around a set of ‚Äúsearch result impressions‚Äù, or the ordered"
" list of hotels that the user sees after they search for a hotel on the Expedia website. In addition to impressions from the existing algorithm, the data contain impressions where the hotels were randomly sorted, to avoid the position bias of the existing algorithm. The user response is provided as a click on a hotel. From: https://www.kaggle.com/c/expedia-personalized-sort/overview

1. Connect to Fiddler
2. Load a Data Sample
3. Define the Model Specifications and Model Task
4. Create a Model
5. Publish a Pre-production Baseline
6. Publish Production Events

## 0. Imports


```python
%pip install -q fiddler-client

import time as time

import pandas as pd
import fiddler as fdl

print(f""Running Fiddler Python client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your model"
" with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
# Constants for the default example, change as needed to create your own versions
PROJECT_NAME = 'quickstart_examples'
MODEL_NAME = 'search_ranking_example'
DATASET_NAME = 'expedia_dataset'

PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main"
"/quickstart/data/v3/expedia_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/expedia_logs.csv'
```

Next we use these credentials to connect to the Fiddler API.


```python
fdl.init(url=URL, token=TOKEN)
```

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and calling the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id ="
" {project.id} and name = {project.name}')
```

## 2. Load a Data Sample

Now we retrieve the Expedia Dataset as a data sample for this model.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
sample_data_df.head()
```

Fiddler uses this data sample to keep track of important information about your data.
  
This includes **data types**, **data ranges**, and **unique values** for categorical variables.

## 3. Define the Model Specifications and Model Task

To add a Ranking model you must specify the ModelTask as `RANKING` in the model info object.  

Additionally, you must provide the `group_by` argument that corresponds to the query search id. This `group_by` column should be present either in:
- `features` : if it is used to build and run the model
- `metadata_cols` : if not used by the model

Optionally, you can give a `"
"ranking_top_k` number (default is 50). This will be the number of results within each query to take into account while computing the performance metrics in monitoring.  

Unless the prediction column was part of your baseline dataset, you must provide the minimum and maximum values predictions can take in a dictionary format (see below).  

If your target is categorical (string), you need to provide the `target_class_order` argument. If your target is numerical and you don't specify this argument, Fiddler will infer it.   

This will be the list of possible values for the target **ordered**. The first element should be the least relevant target level, the last element should be the most relevant target level.


```python
model_spec = fdl.ModelSpec(
    inputs=list(
        sample_data_df.drop(
            columns=[
                'binary_relevance',
                'score',
                'graded_relevance',
                'position',
                'timestamp',
            ]
        ).columns
    ),
   "
" outputs=['score'],
    targets=['binary_relevance'],
    metadata=['timestamp', 'graded_relevance', 'position'],
)
```

If you have columns in your ModelSpec which denote **prediction IDs or timestamps**, then Fiddler can use these to power its analytics accordingly.

Let's call them out here and use them when configuring the Model in step 5.


```python
# id_column = '' # Optional: Specify the name of the ID column if you have one
timestamp_column = 'timestamp'
```


```python
model_task = fdl.ModelTask.RANKING

task_params = fdl.ModelTaskParams(
    group_by='srch_id', top_k=20, target_class_order=[0, 1]
)
```

## 4. Create a Model


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model"
"_task,
    task_params=task_params,
    event_ts_col=timestamp_column,
)

model.create()
print(f'New model created with id = {model.id} and name = {model.name}')
```

## 5. Publish a Pre-production Baseline **(Optional)**

We can publish the data sample from earlier to add it as a baseline.

For ranking, we need to ingest all events from a given query or search ID together. To do that, we need to transform the data to a grouped format.  
You can use the `group_by` utility function to perform this transformation.


```python
sample_df_grouped = fdl.utils.helpers.group_by(
    df=sample_data_df, group_by_col='srch_id'
)

baseline_publish_job = model.publish(
    source=sample_df_grouped,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=DATASET_NAME,
)
print(
    f'Initiated pre-production environment data"
" upload with Job ID = {baseline_publish_job.id}'
)

# Uncomment the line below to wait for the job to finish, otherwise it will run in the background.
# You can check the status on the Jobs page in the Fiddler UI or use the job ID to query the job status via the API.
# baseline_publish_job.wait()
```

# 6. Publish Events For Monitoring

Fiddler will **monitor this data and compare it to your baseline to generate powerful insights into how your model is behaving**.


---


Each record sent to Fiddler is called **an event**.
  
Let's load some sample events from a CSV file.


```python
production_data_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be as recent as today
production_data_df['timestamp'] = production_data_df['timestamp'] + (
    int(time.time() * 1000) - production_data_df['timestamp'].max()
)
production"
"_data_df
```

Again, let's group the data before sending it to Fiddler.


```python
df_logs_grouped = fdl.utils.helpers.group_by(
    df=production_data_df, group_by_col='srch_id'
)
df_logs_grouped
```

Now publish the events


```python
production_publish_job = model.publish(df_logs_grouped)

print(
    f'Initiated production environment data upload with Job ID = {production_publish_job.id}'
)

# Uncomment the line below to wait for the job to finish, otherwise it will run in the background.
# You can check the status on the Jobs page in the Fiddler UI or use the job ID to query the job status via the API.
# production_publish_job.wait()
```

# Get insights


**You're all done!**
  
You can now head to your Fiddler environment and start getting enhanced observability into your model's performance.

<table>
    <tr>
        <"
"td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/ranking_model_1.png"" />
        </td>
    </tr>
</table>

--------
**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
title: ML Monitoring - User-defined Feature Impact
slug: user-defined-feature-impact-quick-start
excerpt: Quickstart Notebook
metadata:
  title: 'Quickstart: User-defined Feature Impact | Fiddler Docs'
  description: >-
    This document provides a guide on using Fiddler's feature impact upload API
    to supply your own feature impact values for your Fiddler model.
  image: []
  robots: index
createdAt: Tue Nov 19 2024 14:00:57 GMT+0000 (Coordinated Universal Time)
updatedAt: Tue Nov 19 2024 14:00:57 GMT+0000 (Coordinated Universal Time)
icon: notebook
---

# ML Monitoring - Feature Impact

### User-defined Feature Impact Upload

This guide will walk you through the steps needed to upload your model's existing feature impact values to your Fiddler model. This notebook uses the same example model leveraged in the [ML"
" Monitoring - Simple](quick-start.md) quick start. If you have already run that notebook and the model exists in your Fiddler instance, then you may skip the setup steps in this guide as noted in the instructions.

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_User_Defined_Feature_Impact.ipynb)

<div align=""left""><figure><img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" alt=""Google Colab"" width=""188""><figcaption></figcaption></figure></div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_User_Defined_Feature_Impact.ipynb).

{% include ""../.git"
"book/includes/main-doc-footer.md"" %}

# Fiddler User-Defined Feature Impact Quick Start Guide

In this notebook we demonstrate how to upload your own precomputed feature impact values to a Fiddler model. Previous versions of Fiddler required you create either a surrogate or user model artifact with which to calculate the feature impact values within Fiddler. Both surrogate and user model artifact require extra steps when onboarding a model and may be unnecessary if the feature impact values already exist. 


---

The documentation for the user-defined feature impact upload API can be found online [here](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact).

User-Defined Feature Impact is supported on Fiddler version 24.12+ using Fiddler Python client API versions 3.3 and higher.

**Please note that you may skip Steps #2 - #5 and resume at [Step #6](#section_06)** if"
" you have already run Fiddler's [Simple Monitoring Quick Start Guide](https://docs.fiddler.ai/quickstart-notebooks/quick-start) and used the default values and sample data.

1. Connect to Fiddler - Initialization, create a project
2. Load a Data Sample
3. Define Your Model Specifications
4. Set a Model Task
5. Add Your Model
6. Upload Your Feature Impact Values

# 0. Imports


```python
%pip install -q fiddler-client

import pandas as pd
import fiddler as fdl

print(f""Running client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using our Python client API.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2"
". Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'  # If the project already exists, the notebook will create the model under the existing project.
MODEL_NAME = 'bank_churn_simple_monitoring'

# Sample data hosted on GitHub
PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/churn_data_sample.csv'
PATH_TO_FI_VALUES = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact_scores.json"
"'
PATH_TO_FI_VALUES_UPDATED = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact_scores_alt.json'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and call the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project"
" with id = {project.id} and name = {project.name}')
```

## 2. Load a Data Sample

In this example, we'll be considering the case where we're a bank and we have **a model that predicts churn for our customers**.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
column_list = sample_data_df.columns
sample_data_df
```

## 3. Define Your Model Specifications

In order to add your model to Fiddler, simply create a ModelSpec object with information about what each column of your data sample should used for.

Fiddler supports four column types:
1. **Inputs**
2. **Outputs** (Model predictions)
3. **Target** (Ground truth values)
4. **Metadata**


```python
input_columns = list(
"
"    column_list.drop(['predicted_churn', 'churn', 'customer_id', 'timestamp'])
)
model_spec = fdl.ModelSpec(
    inputs=input_columns,
    outputs=['predicted_churn'],
    targets=[
        'churn'
    ],  # Note: only a single Target column is allowed, use metadata columns and custom metrics for additional targets
    metadata=['customer_id', 'timestamp'],
)
id_column = (
    'customer_id'  # Indicates which column is your unique identifier for each event
)
timestamp_column = (
    'timestamp'  # Indicates which column is your timestamp for each event
)
```

## 4. Set a Model Task

Fiddler supports a variety of model tasks. In this case, we're adding a binary classification model.

For this, we'll create a ModelTask object and an additional ModelTaskParams object to specify the ordering of our positive and negative labels.

*For a detailed breakdown of all supported model tasks, click"
" here.*


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

task_params = fdl.ModelTaskParams(target_class_order=['no', 'yes'])
```

## 5. Add Your Model

Create a Model object and publish it to Fiddler, passing in
1. Your data sample
2. Your ModelSpec object
3. Your ModelTask and ModelTaskParams objects
4. Your ID and timestamp columns


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params,
    event_id_col=id_column,
    event_ts_col=timestamp_column,
)

model.create()
print(f'New model created with id = {model.id} and name = {model.name}')
```

## 6. Upload your feature impact values

**Note:** If skipping Steps #2 -"
" #5 because the Simple Monitoring Quick Start model already exists, you will still need to instantiate the fdl.Model object. Uncomment the next cell and run it.



```python
# model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)  # Load the model
# model 
```

Uploading your own feature impact values requires:

1. A Python dict containing each input column defined in your Model's schema and its numeric value
2. A local reference to the fdl.Model

In this example, the feature impact scores are stored as JSON so first they are converted to a dict after reading from the JSON file.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi_values_dict, update=False
)
feature_impacts
```

Feature impact values can be updated at any"
" time simply by setting the `update` parameter to True when calling [upload_feature_impact()](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact). The change takes effect immediately.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES_UPDATED, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi_values_dict, update=True
)
feature_impacts
```
# Fiddler User-Defined Feature Impact Quick Start Guide

In this notebook we demonstrate how to upload your own precomputed feature impact values to a Fiddler model. Previous versions of Fiddler required you create either a surrogate or user model artifact with which to calculate the feature impact values within Fiddler. Both surrogate and user model artifact require extra steps when onboarding a model and may be unnecessary if the feature impact values already exist."
" 


---

The documentation for the user-defined feature impact upload API can be found online [here](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact).

User-Defined Feature Impact is supported on Fiddler version 24.12+ using Fiddler Python client API versions 3.3 and higher.

**Please note that you may skip Steps #2 - #5 and resume at [Step #6](#section_06)** if you have already run Fiddler's [Simple Monitoring Quick Start Guide](https://docs.fiddler.ai/quickstart-notebooks/quick-start) and used the default values and sample data.

1. Connect to Fiddler - Initialization, create a project
2. Load a Data Sample
3. Define Your Model Specifications
4. Set a Model Task
5. Add Your Model
6. Upload Your Feature Impact Values

# 0. Imports


```python
%"
"pip install -q fiddler-client

import pandas as pd
import fiddler as fdl

print(f""Running client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using our Python client API.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'  # If"
" the project already exists, the notebook will create the model under the existing project.
MODEL_NAME = 'bank_churn_simple_monitoring'

# Sample data hosted on GitHub
PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/churn_data_sample.csv'
PATH_TO_FI_VALUES = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact_scores.json'
PATH_TO_FI_VALUES_UPDATED = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact_scores_alt.json'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld"
".Project constructor and call the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

## 2. Load a Data Sample

In this example, we'll be considering the case where we're a bank and we have **a model that predicts churn for our customers**.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV"
")
column_list = sample_data_df.columns
sample_data_df
```

## 3. Define Your Model Specifications

In order to add your model to Fiddler, simply create a ModelSpec object with information about what each column of your data sample should used for.

Fiddler supports four column types:
1. **Inputs**
2. **Outputs** (Model predictions)
3. **Target** (Ground truth values)
4. **Metadata**


```python
input_columns = list(
    column_list.drop(['predicted_churn', 'churn', 'customer_id', 'timestamp'])
)
model_spec = fdl.ModelSpec(
    inputs=input_columns,
    outputs=['predicted_churn'],
    targets=[
        'churn'
    ],  # Note: only a single Target column is allowed, use metadata columns and custom metrics for additional targets
    metadata=['customer_id', 'timestamp'],
)
id_column = (
    'customer_id'  # Indicates which column is your"
" unique identifier for each event
)
timestamp_column = (
    'timestamp'  # Indicates which column is your timestamp for each event
)
```

## 4. Set a Model Task

Fiddler supports a variety of model tasks. In this case, we're adding a binary classification model.

For this, we'll create a ModelTask object and an additional ModelTaskParams object to specify the ordering of our positive and negative labels.

*For a detailed breakdown of all supported model tasks, click here.*


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

task_params = fdl.ModelTaskParams(target_class_order=['no', 'yes'])
```

## 5. Add Your Model

Create a Model object and publish it to Fiddler, passing in
1. Your data sample
2. Your ModelSpec object
3. Your ModelTask and ModelTaskParams objects
4. Your ID and timestamp columns


```python
model = fdl"
".Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params,
    event_id_col=id_column,
    event_ts_col=timestamp_column,
)

model.create()
print(f'New model created with id = {model.id} and name = {model.name}')
```

## 6. Upload your feature impact values

**Note:** If skipping Steps #2 - #5 because the Simple Monitoring Quick Start model already exists, you will still need to instantiate the fdl.Model object. Uncomment the next cell and run it.



```python
# model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)  # Load the model
# model 
```

Uploading your own feature impact values requires:

1. A Python dict containing each input column defined in your Model's schema and its numeric value
2. A local reference"
" to the fdl.Model

In this example, the feature impact scores are stored as JSON so first they are converted to a dict after reading from the JSON file.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi_values_dict, update=False
)
feature_impacts
```

Feature impact values can be updated at any time simply by setting the `update` parameter to True when calling [upload_feature_impact()](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact). The change takes effect immediately.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES_UPDATED, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi_values_dict"
", update=True
)
feature_impacts
```
# Fiddler User-Defined Feature Impact Quick Start Guide

In this notebook we demonstrate how to upload your own precomputed feature impact values to a Fiddler model. Previous versions of Fiddler required you create either a surrogate or user model artifact with which to calculate the feature impact values within Fiddler. Both surrogate and user model artifact require extra steps when onboarding a model and may be unnecessary if the feature impact values already exist. 


---

The documentation for the user-defined feature impact upload API can be found online [here](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact).

User-Defined Feature Impact is supported on Fiddler version 24.12+ using Fiddler Python client API versions 3.3 and higher.

**Please note that you may skip Steps #2 - #5 and resume at [Step #6](#section_06"
")** if you have already run Fiddler's [Simple Monitoring Quick Start Guide](https://docs.fiddler.ai/quickstart-notebooks/quick-start) and used the default values and sample data.

1. Connect to Fiddler - Initialization, create a project
2. Load a Data Sample
3. Define Your Model Specifications
4. Set a Model Task
5. Add Your Model
6. Upload Your Feature Impact Values

# 0. Imports


```python
%pip install -q fiddler-client

import pandas as pd
import fiddler as fdl

print(f""Running client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using our Python client API.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler"
"
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'  # If the project already exists, the notebook will create the model under the existing project.
MODEL_NAME = 'bank_churn_simple_monitoring'

# Sample data hosted on GitHub
PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/churn_data_sample.csv'
PATH_TO_FI_VALUES = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact"
"_scores.json'
PATH_TO_FI_VALUES_UPDATED = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact_scores_alt.json'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in the fld.Project constructor and call the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded"
" existing project with id = {project.id} and name = {project.name}')
```

## 2. Load a Data Sample

In this example, we'll be considering the case where we're a bank and we have **a model that predicts churn for our customers**.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
column_list = sample_data_df.columns
sample_data_df
```

## 3. Define Your Model Specifications

In order to add your model to Fiddler, simply create a ModelSpec object with information about what each column of your data sample should used for.

Fiddler supports four column types:
1. **Inputs**
2. **Outputs** (Model predictions)
3. **Target** (Ground truth values)
4. **Metadata**


```python
input_columns ="
" list(
    column_list.drop(['predicted_churn', 'churn', 'customer_id', 'timestamp'])
)
model_spec = fdl.ModelSpec(
    inputs=input_columns,
    outputs=['predicted_churn'],
    targets=[
        'churn'
    ],  # Note: only a single Target column is allowed, use metadata columns and custom metrics for additional targets
    metadata=['customer_id', 'timestamp'],
)
id_column = (
    'customer_id'  # Indicates which column is your unique identifier for each event
)
timestamp_column = (
    'timestamp'  # Indicates which column is your timestamp for each event
)
```

## 4. Set a Model Task

Fiddler supports a variety of model tasks. In this case, we're adding a binary classification model.

For this, we'll create a ModelTask object and an additional ModelTaskParams object to specify the ordering of our positive and negative labels.

*For a detailed breakdown of all supported model tasks"
", click here.*


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

task_params = fdl.ModelTaskParams(target_class_order=['no', 'yes'])
```

## 5. Add Your Model

Create a Model object and publish it to Fiddler, passing in
1. Your data sample
2. Your ModelSpec object
3. Your ModelTask and ModelTaskParams objects
4. Your ID and timestamp columns


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params,
    event_id_col=id_column,
    event_ts_col=timestamp_column,
)

model.create()
print(f'New model created with id = {model.id} and name = {model.name}')
```

## 6. Upload your feature impact values

**Note:** If skipping Steps #"
"2 - #5 because the Simple Monitoring Quick Start model already exists, you will still need to instantiate the fdl.Model object. Uncomment the next cell and run it.



```python
# model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)  # Load the model
# model 
```

Uploading your own feature impact values requires:

1. A Python dict containing each input column defined in your Model's schema and its numeric value
2. A local reference to the fdl.Model

In this example, the feature impact scores are stored as JSON so first they are converted to a dict after reading from the JSON file.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi_values_dict, update=False
)
feature_impacts
```

Feature impact values can be updated"
" at any time simply by setting the `update` parameter to True when calling [upload_feature_impact()](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact). The change takes effect immediately.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES_UPDATED, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi_values_dict, update=True
)
feature_impacts
```
# Fiddler User-Defined Feature Impact Quick Start Guide

In this notebook we demonstrate how to upload your own precomputed feature impact values to a Fiddler model. Previous versions of Fiddler required you create either a surrogate or user model artifact with which to calculate the feature impact values within Fiddler. Both surrogate and user model artifact require extra steps when onboarding a model and may be unnecessary if the feature impact values already"
" exist. 


---

The documentation for the user-defined feature impact upload API can be found online [here](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact).

User-Defined Feature Impact is supported on Fiddler version 24.12+ using Fiddler Python client API versions 3.3 and higher.

**Please note that you may skip Steps #2 - #5 and resume at [Step #6](#section_06)** if you have already run Fiddler's [Simple Monitoring Quick Start Guide](https://docs.fiddler.ai/quickstart-notebooks/quick-start) and used the default values and sample data.

1. Connect to Fiddler - Initialization, create a project
2. Load a Data Sample
3. Define Your Model Specifications
4. Set a Model Task
5. Add Your Model
6. Upload Your Feature Impact Values

# 0. Imports


```python"
"
%pip install -q fiddler-client

import pandas as pd
import fiddler as fdl

print(f""Running client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using our Python client API.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples' "
" # If the project already exists, the notebook will create the model under the existing project.
MODEL_NAME = 'bank_churn_simple_monitoring'

# Sample data hosted on GitHub
PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/churn_data_sample.csv'
PATH_TO_FI_VALUES = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact_scores.json'
PATH_TO_FI_VALUES_UPDATED = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/custom_feature_impact_scores_alt.json'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new project by specifying a unique project name in"
" the fld.Project constructor and call the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

## 2. Load a Data Sample

In this example, we'll be considering the case where we're a bank and we have **a model that predicts churn for our customers**.
  
In order to get insights into the model's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.


```python
sample_data_df = pd.read_csv(PATH_TO"
"_SAMPLE_CSV)
column_list = sample_data_df.columns
sample_data_df
```

## 3. Define Your Model Specifications

In order to add your model to Fiddler, simply create a ModelSpec object with information about what each column of your data sample should used for.

Fiddler supports four column types:
1. **Inputs**
2. **Outputs** (Model predictions)
3. **Target** (Ground truth values)
4. **Metadata**


```python
input_columns = list(
    column_list.drop(['predicted_churn', 'churn', 'customer_id', 'timestamp'])
)
model_spec = fdl.ModelSpec(
    inputs=input_columns,
    outputs=['predicted_churn'],
    targets=[
        'churn'
    ],  # Note: only a single Target column is allowed, use metadata columns and custom metrics for additional targets
    metadata=['customer_id', 'timestamp'],
)
id_column = (
    'customer_id'  # Indicates which column"
" is your unique identifier for each event
)
timestamp_column = (
    'timestamp'  # Indicates which column is your timestamp for each event
)
```

## 4. Set a Model Task

Fiddler supports a variety of model tasks. In this case, we're adding a binary classification model.

For this, we'll create a ModelTask object and an additional ModelTaskParams object to specify the ordering of our positive and negative labels.

*For a detailed breakdown of all supported model tasks, click here.*


```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION

task_params = fdl.ModelTaskParams(target_class_order=['no', 'yes'])
```

## 5. Add Your Model

Create a Model object and publish it to Fiddler, passing in
1. Your data sample
2. Your ModelSpec object
3. Your ModelTask and ModelTaskParams objects
4. Your ID and timestamp columns


```python
model ="
" fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params,
    event_id_col=id_column,
    event_ts_col=timestamp_column,
)

model.create()
print(f'New model created with id = {model.id} and name = {model.name}')
```

## 6. Upload your feature impact values

**Note:** If skipping Steps #2 - #5 because the Simple Monitoring Quick Start model already exists, you will still need to instantiate the fdl.Model object. Uncomment the next cell and run it.



```python
# model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)  # Load the model
# model 
```

Uploading your own feature impact values requires:

1. A Python dict containing each input column defined in your Model's schema and its numeric value
2. A"
" local reference to the fdl.Model

In this example, the feature impact scores are stored as JSON so first they are converted to a dict after reading from the JSON file.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi_values_dict, update=False
)
feature_impacts
```

Feature impact values can be updated at any time simply by setting the `update` parameter to True when calling [upload_feature_impact()](https://docs.fiddler.ai/python-client-3-x/api-methods-30#upload_feature_impact). The change takes effect immediately.


```python
fi_values_series = pd.read_json(PATH_TO_FI_VALUES_UPDATED, typ='series')
fi_values_dict = fi_values_series.to_dict()

feature_impacts = model.upload_feature_impact(
    feature_impact_map=fi"
"_values_dict, update=True
)
feature_impacts
```
"
"---
title: ML Monitoring - CV Inputs
slug: cv-monitoring
metadata:
  title: 'Quickstart: CV Monitoring | Fiddler Docs'
  description: >-
    This document is a guide on using Fiddler for monitoring computer vision
    models and detecting drift in image data using Fiddler's Vector Monitoring
    approach.
  robots: index
icon: notebook
---

# ML Monitoring - CV Inputs

This guide will walk you through the basic steps required to use Fiddler for monitoring computer vision (CV) models. In this notebook we demonstrate how to detect drift in image data using model embeddings using Fiddler's unique Vector Monitoring approach.

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Image_Monitoring.ipynb)

<div align=""left"">

<figure><img src"
"=""https://colab.research.google.com/img/colab_favicon_256px.png"" alt=""Google Colab"" width=""188""><figcaption></figcaption></figure>

</div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_Image_Monitoring.ipynb).

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

# Monitoring Image data using Fiddler Vector Monitoring

In this notebook we present the steps for monitoring images. Fiddler employs a vector-based monitoring approach that can be used to monitor data drift in high-dimensional data such as NLP embeddings, images, video etc. In this notebook we demonstrate how to detect drift in image data using model embeddings and determine the cause of that drift.

Fiddler is the pioneer in enterprise Model Performance Management (MPM), offering a unified platform that enables Data Science, MLOps"
", Risk, Compliance, Analytics, and LOB teams to **monitor, explain, analyze, and improve ML deployments at enterprise scale**.
Obtain contextual insights at any stage of the ML lifecycle, improve predictions, increase transparency and fairness, and optimize business revenue.

---

You can experience Fiddler's Image monitoring ***in minutes*** by following these quick steps:

1. Connect to Fiddler
2. Load and generate embeddings for CIFAR-10 dataset
3. Upload the vectorized baseline dataset
4. Add metadata about your model
5. Inject data drift and publish production events
6. Get insights

## Imports


```python
!pip install torch==2.0.0
!pip install torchvision==0.15.1
!pip install -q fiddler-client
```


```python
import io
import numpy as np
import pandas as pd
import random
import time
import torch
import torch.nn as nn
import torchvision.transforms"
" as transforms
from torchvision.models import resnet18, ResNet18_Weights
import requests

import fiddler as fdl
print(f""Running Fiddler client version {fdl.__version__}"")
```

## 2. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using our API client.


---


**We need a few pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

These can be found by navigating to the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https://).
TOKEN = ''
```

Now just run the following code block to connect to the Fiddler API!


```python
fdl.init(
    url=URL,
    token=TOKEN
)
```

Once you connect, you can"
" create a new project by calling a Project's `create` method.


```python
PROJECT_NAME = 'image_monitoring'

project = fdl.Project(
    name=PROJECT_NAME
)

project.create()
```

## 2. Generate Embeddings for CIFAR-10 data

In this example, we'll use the popular CIFAR-10 classification dataset and a model based on Resnet-18 architecture. For the purpose of this example we have pre-trained the model.
  
In order to compute data and prediction drift, **Fiddler needs a sample of data that can serve as a baseline** for making comparisons with data in production. When it comes to computing distributional shift for images, Fiddler relies on the model's intermediate representations also known as activations or embeddings. You can read more about our approach [here](https://www.fiddler.ai/blog/monitoring-natural-language-processing-and-computer-vision-models-part-1).

In the the following cells we"
"'ll extract these embeddings.


```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Device to be used: {device}')
```

Let us load the pre-trained model


```python
MODEL_URL='https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/models/resnet18_cifar10_epoch5.pth'
MODEL_PATH='resnet18_cifar10_epoch5.pth'

def load_model(device):
    """"""Loads the pre-trained CIFAR-10 model""""""
    model = resnet18()
    model.fc = nn.Sequential(
        nn.Linear(512, 128),
        nn.ReLU(),
        nn.Linear(128, 10),
    )

    r = requests.get(MODEL_URL)
    with open(MODEL_PATH,'wb') as f:
        f.write(r.content)

    model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(device)))
    model.to(device)
"
"    return model

resnet_model = load_model(device)

```

We'll load four tranches of [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) data for this example.  ""reference"" ‚Äì corresponding to train-time reference data, and three ""production"" sets with diffrent transformations applied to simulate drift from the model's training data. Note that running the cell below will download the CIFAR-10 data and load them using torch's dataloaders.


```python
BATCH_SIZE = 32

transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),
    ])

DATA_BASE_URL = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/cv_monitoring/'

# download file from URL
DATA_URLS ="
" {
 'reference' : DATA_BASE_URL + 'reference/image_data.npz',
'production_1': DATA_BASE_URL + 'production_1/image_data.npz', # Undrifted
'production_2': DATA_BASE_URL + 'production_2/image_data.npz', # Blurred
'production_3': DATA_BASE_URL + 'production_3/image_data.npz'} # Darkened


def get_dataloader(dataset):
  response = requests.get(DATA_URLS[dataset])
  data = np.load(io.BytesIO(response.content))

  images = [transform(x) for x in data['arr_0']]
  labels = data['arr_1']

  tuple_list = list(zip(images, labels))

  return torch.utils.data.DataLoader(
      tuple_list,
      batch_size=BATCH_SIZE,
      shuffle=False,
      num_workers=2
  )

# let's test it
get_dataloader('reference')
```

***In the cell below we define functions that"
" will extract the 128-dimensional embedding from the FC1 layer of the model and package them in a dataframe along with predictions, ground-truth labels, and image URLs***


```python
from copy import deepcopy
import matplotlib.pyplot as plt
import numpy as np

import torch
import torch.nn.functional as F
import torchvision.transforms as transforms

torch.manual_seed(0)

CIFAR_CLASSES = (
    'plane', 'car', 'bird', 'cat',
    'deer', 'dog', 'frog',
    'horse', 'ship', 'truck',
)

global view_fc1_output_embeds

def fc1_hook_func(model, input, output):
    global view_fc1_output_embeds
    view_fc1_output_embeds = output

def idx_to_classes(target_arr):
    return [CIFAR_CLASSES[int(i)] for i in target_arr]

def generate_embeddings(model, device, dataset_name):
    """"""Generate embeddings for the inout images""""""

    dataloader"
" = get_dataloader(dataset_name)

    fc1_embeds = []
    output_scores = []
    target = []

    with torch.no_grad():
        model = model.eval()
        fc1_module = model.fc[0]
        fc1_hook = fc1_module.register_forward_hook(fc1_hook_func)
        correct_preds = 0
        total_preds = 0

        try:
            for inputs, labels in dataloader:
                inputs = inputs.to(device)
                labels = labels.to(device)
                outputs = model(inputs)
                outputs_smax = F.softmax(outputs, dim=1)
                _, preds = torch.max(outputs, 1)
                correct_preds += torch.sum(preds == labels.data).cpu().numpy()
                total_preds += len(inputs)

                fc1_embeds.append(view_fc1_output_embeds.cpu().detach().numpy())
                output_scores.append(outputs_smax.cpu().detach().numpy())
                target.append(labels.cpu().detach().numpy())

            fc1_embed"
"s = np.concatenate(fc1_embeds)
            output_scores = np.concatenate(output_scores)
            target = np.concatenate(target)

        except Exception as e:
            fc1_hook.remove()
            raise

        print(f'{correct_preds}/{total_preds}: {100*correct_preds/total_preds:5.1f}% correct predictions.')

    embs = deepcopy(fc1_embeds)
    labels = idx_to_classes(target)
    embedding_cols = ['emb_'+str(i) for i in range(128)]
    baseline_embeddings = pd.DataFrame(embs, columns=embedding_cols)

    columns_to_combine = baseline_embeddings.columns
    baseline_embeddings = baseline_embeddings.apply(lambda row: row[columns_to_combine].tolist(), axis=1).to_frame()
    baseline_embeddings = baseline_embeddings.rename(columns={baseline_embeddings.columns[0]: 'embeddings'})

    baseline_predictions = pd.DataFrame(output_scores, columns=CIFAR_CLASSES)
    baseline_labels = pd.DataFrame(labels, columns=['target'])
    embeddings_df ="
" pd.concat(
        [baseline_embeddings, baseline_predictions, baseline_labels],
        axis='columns',
        ignore_index=False
    )

    embeddings_df['image_url'] = embeddings_df.apply(lambda row:DATA_BASE_URL + dataset_name + '/' + str(row.name) + '.png', axis=1)


    return embeddings_df

```

We'll now extract the embeddings for training data which will serve as baseline for monitoring.


```python
sample_df = generate_embeddings(resnet_model, device, 'reference')
sample_df.head()
```

# 4. Add metadata about the model

Next we must tell Fiddler a bit more about our model.  This is done by by creating defining some information about our model's task, inputs, output, target and which features form the image embedding and then creating a `Model` object.

Let's first define our Image vector using the API below.


```python
image_embedding_feature = fdl.ImageEmbedding(
    name='image_feature',
"
"    source_column='image_url',
    column='embeddings',
)
```

Now let's define a `ModelSpec` object with information about the columns in our data sample.


```python
model_spec = fdl.ModelSpec(
    inputs=['embeddings'],
    outputs=CIFAR_CLASSES,
    targets=['target'],
    metadata=['image_url'],
    custom_features=[image_embedding_feature],
)

timestamp_column = 'timestamp'
```

Then let's specify some information about the model task.


```python
model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION

task_params = fdl.ModelTaskParams(target_class_order=list(CIFAR_CLASSES))
```

Then we create a `Model` schema using the example data.


```python
MODEL_NAME = 'resnet18'

model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=fdl.Project.from_name(PROJECT_NAME).id,
    source=sample_df,
    spec=model_spec,
   "
" task=model_task,
    task_params=task_params,
    event_ts_col=timestamp_column
)

model.create()
```

Additionally, let's publish the baseline data so we can use it as a reference for measuring drift and to compare production data with in Embedding Visualization for root-cause analysis.


```python
model.publish(
    source=sample_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name='train_time_reference',
)
```

# 5. Publish events to Fiddler
We'll publish events over past 3 weeks.

- Week 1: We publish CIFAR-10 test set, which would signify no distributional shift
- Week 2: We publish **blurred** CIFAR-10 test set
- Week 3: We publish **brightness reduced** CIFAR-10 test set


```python
for i, dataset_name in enumerate(['production_1', 'production_2', 'production_3']):
   "
" week_days = 6
    prod_df = generate_embeddings(resnet_model, device, dataset_name)
    week_offset = (2-i)*7*24*60*60*1e3
    day_offset = 24*60*60*1e3
    print(f'Publishing events from {dataset_name} transformation for week {i+1}.')
    for day in range(week_days):
        now = time.time() * 1000
        timestamp = int(now - week_offset - day*day_offset)
        events_df = prod_df.sample(1000)
        events_df['timestamp'] = timestamp
        model.publish(events_df)
```

## 6. Get insights

**You're all done!**
  
You can now head to your Fiddler URL and start getting enhanced observability into your model's performance.

Fiddler can now track your image drift over time based on the embedding vectors of the images published into the platform.

"
"While we saw performace degrade for the various drifted data sets in the notebook when publishing (which can also be plotted in the Monitoring chart), embedding drift doesn't require ground-truth labels and often serves as a useful proxy that can indicate a problem or degraded performance.  Data drift is indicated by nonzero Jensen-Shannon Distance measured with respect to a reference sample.

Please visit your Fiddler environment upon completion to check this out for yourself.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/image_monitoring_2024_12_1.png"" />
        </td>
    </tr>
</table>

In order to identify the root cause of data drift, Fiddler allows you to ""drill-down"" into time windows where embedding drift is measured.  As indicated in blue in the image above, by selecting a time bin and clicking the ""Embeddings"""
" button, you'll be take to an Embedding Visualization chart where data from that time window is compared against reference data (e.g. `train_time_reference` that we published above) in an interactive 3D UMAP plot.

In the image below, the embedded images in the drifted time period are semantically distinct to your model from those in the train-time sample.  By investigating these differenes, it's easy to determine that the third ""production"" sample is systematically darkened.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/image_monitoring_2024_12_2.png"" />
        </td>
    </tr>
</table>



---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http"
"://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
title: ML Monitoring - NLP Inputs
slug: simple-nlp-monitoring-quick-start
excerpt: Quickstart Notebook
metadata:
  title: 'Quickstart: NLP Monitoring | Fiddler Docs'
  description: >-
    This document provides a guide on using Fiddler to monitor NLP models by
    applying a multi-class classifier to the 20newsgroup dataset and monitoring
    text embeddings using Fiddler's Vector Monitoring approach.
  image: []
  robots: index
createdAt: Mon Aug 15 2022 23:29:02 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 14:00:57 GMT+0000 (Coordinated Universal Time)
icon: notebook
---

# ML Monitoring - NLP Inputs

This guide will walk you through the basic steps required to use Fiddler for monitoring NLP models. A multi-class classifier is applied to the 20"
"newsgroup dataset and the text embeddings are monitored using Fiddler's unique Vector Monitoring approach.

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_NLP_Multiclass_Monitoring.ipynb)

<div align=""left"">

<figure><img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" alt=""Google Colab"" width=""188""><figcaption></figcaption></figure>

</div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_NLP_Multiclass_Monitoring.ipynb).

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

# Monitoring a Multiclass Classifier Model with Text Inputs
Unstructured data"
" such as text are usually represented as high-dimensional vectors when processed by ML models. In this example notebook we present how [Fiddler Vector Monitoring](https://www.fiddler.ai/blog/monitoring-natural-language-processing-and-computer-vision-models-part-1) can be used to monitor NLP models using a text classification use case.

Following the steps in this notebook you can see how to onboard models that deal with unstructured text inputs. In this example, we use the 20Newsgroups dataset and train a multi-class classifier that is applied to vector embeddings of text documents.

We monitor this model at production time and assess the performance of Fiddler's vector monitoring by manufacturing synthetic drift via sampling from specific text categories at different deployment time intervals.

---

Now we perform the following steps to demonstrate how Fiddler NLP monitoring works:

1. Connect to Fiddler
2. Load a Data Sample
3. Define the Model Specifications
4. Create a Model"
"
5. Publish a Static Baseline (Optional)
6. Publish Production Events

Get insights!

## 0. Imports


```python

%pip install -q fiddler-client

import time as time

import numpy as np
import pandas as pd
import fiddler as fdl

print(f""Running Fiddler Python client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g"
". 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'
MODEL_NAME = 'nlp_newsgroups_multiclass'

STATIC_BASELINE_NAME = 'baseline_dataset'

# Sample data hosted on GitHub
PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/nlp_text_multiclassifier_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://media.githubusercontent.com/media/fiddler-labs/fiddler-examples/main/quickstart/data/v3/nlp_text_multiclassifier_production_data.csv'
```

Now just run the following to connect to your Fiddler environment.


```python
fdl.init(url=URL, token=TOKEN)
```

#### 1.a Create New or Load Existing Project

Once you connect, you can create a new"
" project by specifying a unique project name in the fld.Project constructor and calling the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

## 2. Load a Data Sample

Now we retrieve the 20Newsgroup dataset. This dataset is fetched from the [scikit-learn real-world datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#) and pre-processed using [this notebook](https://colab.re"
"search.google.com/github/fiddler-labs/fiddler-examples/blob/main/pre-proccessing/20newsgroups_prep_vectorization.ipynb).  For simplicity sake, we have stored a copy in our GitHub repo.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
sample_data_df
```

## 3. Define the Model Specifications

Next we should tell Fiddler a bit more about the model by creating a `ModelSpec` object that specifies the model's inputs, outputs, targets, and metadata, along with other information such as the enrichments we want performed on our model's data.

### Instruct Fiddler to generate embeddings for our unstructured model input

Fiddler offers a powerful set of enrichment services that we can use to enhance how we monitor our model's performance.  In this example, we instruct Fiddler to generate embeddings for our unstructured text.  These generated embeddings are a numerical vector that represent the"
" content and context of our unstructured input field, _original_text_.  These embeddings then power Fiddler's vector monitoring capability for detecting drift.

Before creating a `ModelSpec` object, we define a custom feature using the [fdl.Enrichment()](https://docs.fiddler.ai/python-client-3-x/api-methods-30#fdl.enrichment-private-preview) API. When creating an enrichment, a name must be assigned to the custom feature using the `name` argument. Each enrichment appears in the monitoring tab in the Fiddler UI with this assigned name. Finally, the default clustering setup can be modified by passing the number of cluster centroids to the `n_clusters` argument.

Here we define an [embedding fdl.Enrichment](https://docs.fiddler.ai/python-client-3-x/api-methods-30#embedding-private-preview) and then use that embedding enrichment to create a [fdl.TextEmbedding](https://docs.fidd"
"ler.ai/python-client-3-x/api-methods-30#customfeature) input that can be used to track drift and to be plotted in Fiddler's UMAP visualizations.


```python
fiddler_backend_enrichments = [
    fdl.Enrichment(
        name='Enrichment Text Embedding',
        enrichment='embedding',
        columns=['original_text'],
    ),
    fdl.TextEmbedding(
        name='Original TextEmbedding',
        source_column='original_text',
        column='Enrichment Text Embedding',
        n_clusters=6,
    ),
]
```


```python
model_spec = fdl.ModelSpec(
    inputs=['original_text'],
    outputs=[col for col in sample_data_df.columns if col.startswith('prob_')],
    targets=['target'],
    metadata=['n_tokens', 'string_size', 'timestamp'],
    custom_features=fiddler_backend_enrichments,
)
```

If you have columns in your ModelSpec which denote"
" **prediction IDs or timestamps**, then Fiddler can use these to power its analytics accordingly. Here we are just noting the column with the event datetime.


```python
timestamp_column = 'timestamp'
```

Fiddler supports a variety of model tasks. In this case, we're setting the task type to multi-class classification to inform Fiddler that is the type of model we are monitoring.


```python
model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION

task_params = fdl.ModelTaskParams(
    target_class_order=[col[5:] for col in model_spec.outputs]
)
```

## 4. Create a Model

Once we've specified information about our model, we can onboard (add) it to Fiddler by calling `Model.create`.


```python
model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=project.id,
    source=sample_data_df,
    spec=model_spec,
    task=model"
"_task,
    task_params=task_params,
    event_ts_col=timestamp_column,
)

model.create()
print(f'New model created with id = {model.id} and name = {model.name}')
```

## 5. Publish a Static Baseline (Optional)

Since Fiddler already knows how to process data for your model, we can now add a **baseline dataset**.

You can think of this as a static dataset which represents **""golden data,""** or the kind of data your model expects to receive. 

Note : The data given during the model creation is purely for schema inference and not a default baseline.

Then, once we start sending production data to Fiddler, you'll be able to see **drift scores** telling you whenever it starts to diverge from this static baseline.

***

Let's publish our **original data sample** as a pre-production dataset. This will automatically add it as a baseline for the model.


*For more information on how to"
" design your baseline dataset, [click here](https://docs.fiddler.ai/client-guide/creating-a-baseline-dataset).*


```python
baseline_publish_job = model.publish(
    source=sample_data_df,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=STATIC_BASELINE_NAME,
)
print(
    f'Initiated pre-production environment data upload with Job ID = {baseline_publish_job.id}'
)

# Uncomment the line below to wait for the job to finish, otherwise it will run in the background.
# You can check the status on the Jobs page in the Fiddler UI or use the job ID to query the job status via the API.
# baseline_publish_job.wait()
```

## 6. Publish Production Events

Now let's publish some sample production events into Fiddler. For the timestamps in the middle of the event dataset, we've over-sampled from certain topics which introduces synthetic drift. This oversampling to inject drift allows"
" us to better illustrate how Fiddler's vector monitoring approach detects drift in our unstructured inputs.


```python
production_data_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be as recent as today
production_data_df['timestamp'] = production_data_df['timestamp'] + (
    int(time.time() * 1000) - production_data_df['timestamp'].max()
)
production_data_df
```

Next, let's publish events our events with the synthetic drift


```python
production_publish_job = model.publish(production_data_df)

print(
    f'Initiated production environment data upload with Job ID = {production_publish_job.id}'
)

# Uncomment the line below to wait for the job to finish, otherwise it will run in the background.
# You can check the status on the Jobs page in the Fiddler UI or use the job ID to query the job status via the API.
# production_publish_job.wait()
```

#"
" Get Insights


**You're all done!**
  
Head to the Fiddler UI to start getting enhanced observability into your model's performance.



In particular, you can go to your model's default dashboard in Fiddler and check out the resulting drift chart . The first image below is a screenshot of the automatically generated drift chart which shows the drift of the original text. The second image is a custom drift chart with traffic details at an hourly time bin. (Annotation bubbles are not generated by the Fiddler UI.)

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/nlp_multiiclass_drift_2.png""  />
        </td>
    </tr>
</table>

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/n"
"lp_multiiclass_drift.png""  />
        </td>
    </tr>
</table>
"
"---
title: LLM Monitoring - Simple
slug: simple-llm-monitoring
createdAt: Mon Feb 26 2024 20:06:46 GMT+0000 (Coordinated Universal Time)
updatedAt: Tue Apr 16 2024 14:59:37 GMT+0000 (Coordinated Universal Time)
icon: notebook
---

# LLM Monitoring - Simple

This guide will walk you through the basic onboarding steps required to use Fiddler for monitoring of LLM applications, **using sample data provided by Fiddler**.

Click [this link to get started using Google Colab ‚Üí](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_LLM_Chatbot.ipynb)&#x20;

<div align=""left"">

<figure><img src=""https://colab.research.google.com/img/colab_favicon_256px"
".png"" alt=""Google Colab"" width=""188""><figcaption></figcaption></figure>

</div>

Or download the notebook directly from [GitHub](https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/latest/Fiddler_Quickstart_LLM_Chatbot.ipynb).

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

# Fiddler LLM Application Quick Start Guide

Fiddler is the pioneer in enterprise AI Observability, offering a unified platform that enables all model stakeholders to monitor model performance and to investigate the true source of model degredation.  Fiddler's AI Observability platform supports both traditional ML models as well as Generative AI applications.  This guide walks you through how to onboard an LLM chatbot application that is built using a RAG architecture.

---

You can start using Fiddler ***in minutes*** by following these 8 quick steps:

1. Connect to Fiddler"
"
2. Create a Fiddler Project
3. Load a Data Sample
4. Opt-in to Specific Fiddler LLM Enrichments
5. Add Information About the LLM Application
6. Publish Production Events

Get insights!

## 0. Imports


```python
%pip install -q fiddler-client

import time as time

import numpy as np
import pandas as pd
import fiddler as fdl

print(f""Running Fiddler Python client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can add information about your LLM Application with Fiddler, you'll need to connect using the Fiddler Python client.


---


**We need a couple pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your authorization token

Your authorization token can be found by navigating to the **Credentials** tab on"
" the **Settings** page of your Fiddler environment.


```python
URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').
TOKEN = ''
```

Constants for this example notebook, change as needed to create your own versions


```python
PROJECT_NAME = 'quickstart_examples'  # If the project already exists, the notebook will create the model under the existing project.
MODEL_NAME = 'fiddler_rag_llm_chatbot'

# Sample data hosted on GitHub
PATH_TO_SAMPLE_CSV = 'https://media.githubusercontent.com/media/fiddler-labs/fiddler-examples/main/quickstart/data/v3/chatbot_data_sample.csv'
PATH_TO_EVENTS_CSV = 'https://media.githubusercontent.com/media/fiddler-labs/fiddler-examples/main/quickstart/data/v3/chatbot_production_data.csv'
```

Now just run the following code block to connect to the F"
"iddler API!


```python
fdl.init(url=URL, token=TOKEN)
```

## 2. Create a Fiddler Project

Once you connect, you can create a new project by specifying a unique project name in the `Project` constructor and calling the `create()` method. If the project already exists, it will load it for use.


```python
try:
    # Create project
    project = fdl.Project(name=PROJECT_NAME).create()
    print(f'New project created with id = {project.id} and name = {project.name}')
except fdl.Conflict:
    # Get project by name
    project = fdl.Project.from_name(name=PROJECT_NAME)
    print(f'Loaded existing project with id = {project.id} and name = {project.name}')
```

## 3. Load a Data Sample

In this example, we'll be onboarding data in order to observe our **Fiddler chatbot application**"
".
  
In order to get insights into the LLM Applications's performance, **Fiddler needs a small sample of data** to learn the schema of incoming data.
Let's use a file with some historical prompts, source docs, and responses from our chatbot for the sample.


```python
sample_data_df = pd.read_csv(PATH_TO_SAMPLE_CSV)
sample_data_df
```

Fiddler will uses this data sample to keep track of important information about your data.  This includes **data types**, **data ranges**, and **unique values** for categorical variables.

## 4. Opt-in to Specific Fiddler LLM Enrichments

After picking a sample of our chatbot's prompts and responses, we can request that Fiddler execute a series of enrichment services that can ""score"" our prompts and responses for a variety of insights.  These enrichment services can detect AI safety issues like PII leakage, hallucinations, toxicity, and more.  We can also"
" opt-in for enrichment services like embedding generation which will allow us to track prompt and response outliers and drift. A full description of these enrichments can be found [here](https://docs.fiddler.ai/platform-guide/llm-monitoring/enrichments-private-preview).

---

Let's define the enrichment services we'd like to use.  Here we will opt in for embedding generation for our prompts, responses and source docs.  Additionally, let's opt in for PII detection, outlier detection through centroid distance metrics, and some other text-based evaluation scores.


```python
fiddler_backend_enrichments = [
    # prompt enrichment
    fdl.TextEmbedding(
        name='Prompt TextEmbedding',
        source_column='question',
        column='Enrichment Prompt Embedding',
        n_tags=10,
    ),
    # response enrichment
    fdl.TextEmbedding(
        name='Response TextEmbedding',
        source_column='response',
        column='Enrichment"
" Response Embedding',
        n_tags=10,
    ),
    # rag document enrichments
    fdl.TextEmbedding(
        name='Source Docs TextEmbedding',
        source_column='source_docs',
        column='Enrichment Source Docs Embedding',
        n_tags=10,
    ),
    # safety
    fdl.Enrichment(
        name='FTL Safety',
        enrichment='ftl_prompt_safety',
        columns=['question', 'response'],
    ),
    # hallucination
    fdl.Enrichment(
        name='Faithfulness',
        enrichment='ftl_response_faithfulness',
        columns=['source_docs', 'response'],
        config={'context_field': 'source_docs', 'response_field': 'response'},
    ),
    # text quality
    fdl.Enrichment(
        name='Enrichment QA TextStat',
        enrichment='textstat',
        columns=['question', 'response'],
        config={
            'statistics': [
               "
" 'char_count',
                'flesch_reading_ease',
                'flesch_kincaid_grade',
            ]
        },
    ),
    fdl.Enrichment(
        name='Enrichment QA Sentiment',
        enrichment='sentiment',
        columns=['question', 'response'],
    ),
    # PII detection
    fdl.Enrichment(
        name='Rag PII', enrichment='pii', columns=['question'], allow_list=['fiddler']
    ),
]
```

## 5.  Add Information About the LLM application

Now it's time to onboard information about our LLM application to Fiddler.  We do this by defining a `ModelSpec` object.


---


The `ModelSpec` object will contain some **information about how your LLM application operates**.
  
*Just include:*
1. The **input/output** columns.  For a LLM application, these are just the raw inputs and outputs of"
" our LLM application.
2. Any **metadata** columns.
3. The **custom features** which contain the configuration of the enrichments we opted for.

We'll also want a few other pieces of information:
1. The **task** your model or LLM application is performing (LLM, regression, binary classification, not set, etc.)
2. Which column to use to read timestamps from.


```python
model_spec = fdl.ModelSpec(
    inputs=['question', 'response', 'source_docs'],
    metadata=['session_id', 'comment', 'timestamp', 'feedback'],
    custom_features=fiddler_backend_enrichments,
)

model_task = fdl.ModelTask.LLM

timestamp_column = 'timestamp'
```

Then just publish all of this to Fiddler by configuring a `Model` object to represent your LLM application in Fiddler.


```python
llm_application = fdl.Model.from_data(
    source=sample_data_df,
    name"
"=MODEL_NAME,
    project_id=project.id,
    spec=model_spec,
    task=model_task,
    event_ts_col=timestamp_column,
    max_cardinality=5,
)
```

Now call the `create` method to publish it to Fiddler!


```python
llm_application.create()
print(
    f'New model created with id = {llm_application.id} and name = {llm_application.name}'
)
```

## 6. Publish Production Events

Information about your LLM application is now onboarded to Fiddler. It's time to start publishing some production data!  


---


Each record sent to Fiddler is called **an event**.  Events simply contain the inputs and outputs of a predictive model or LLM application.
  
Let's load in some sample events (prompts and responses) from a CSV file.


```python
llm_events_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Timeshifting the timestamp column"
" in the events file so the events are as recent as today
llm_events_df['timestamp'] = pd.to_datetime(llm_events_df['timestamp'])
time_diff = pd.Timestamp.now().normalize() - llm_events_df['timestamp'].max()
llm_events_df['timestamp'] += time_diff

llm_events_df
```

You can use the `Model.publish` function to start pumping data into Fiddler!
  
Just pass in the DataFrame containing your events.


```python
# Define the number of rows per chunk
chunk_size = 10

# Splitting the DataFrame into smaller chunks
for start in range(0, llm_events_df.shape[0], chunk_size):
    df_chunk = llm_events_df.iloc[start : start + chunk_size]
    llm_application.publish(df_chunk)
```

# Get insights

**You're all done!**
  
You can now head to your Fiddler environment and start getting enhanced observability into your LLM application"
"'s performance.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/LLM_chatbot_UMAP.png"" />
        </td>
    </tr>
</table>

**What's Next?**

Try the [ML Monitoring - Quick Start Guide](https://docs.fiddler.ai/quickstart-notebooks/quick-start)

---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.


```python

```
"
"---
title: Customer Churn Prediction
slug: customer-churn-prediction
excerpt: >-
  An Illustrative Walktrough - How Fiddler helps detect and diagnose model
  issues
createdAt: Tue May 17 2022 19:12:12 GMT+0000 (Coordinated Universal Time)
updatedAt: Wed Apr 03 2024 01:12:35 GMT+0000 (Coordinated Universal Time)
---

# Customer Churn Prediction

Churn prediction is a common task for machine learning models. Companies use churn models to determine the likelihood of customer's ‚Äúleaving‚Äù. Identifying if and when customers are may churn is extremely valuable. Having a robust and accurate churn prediction model helps businesses to take action to prevent customers from leaving the company. However, like any ML model, if left unattended, these models can degrade over time leading to customer loss.

The Fiddler AI Observability platform provides a variety of tools that can be used"
" to monitor, explain, analyze, and improve the performance of your ML model.

In this article, we will illustrate how Fiddler allows us to detect and diagnose problems using an example churn model.

> üëç Refer to this [quickstart guide](../QuickStart\_Notebooks/quick-start.md) to learn how to
>
> 1. Onboard this model to the Fiddler platform
> 2. Publish events on the Fiddler platform

### Example - Model Performance Degradation due to Data Integrity Issues

In this example, we will illustrate how Fiddler's model monitoring features will help us identify the root cause of model performance issues.

#### Step 1 - Monitor Drift

When we check out our model's default dashboard, we notice a drop in the predicted churn value and a rise in the predicted churn drift value. Our next step is to check if this has resulted in a drop in performance.

![Churn model's default monitoring dashboard]("
"../.gitbook/assets/558d6d6-Screenshot\_2024-04-02\_at\_7.37.47\_PM.png)

![Prediction drift chart - last 30 days](../.gitbook/assets/jsd\_bankchurn\_chart.png)

#### Step 2 - Monitor Performance Metrics

To further our analysis, we can simply add performance metrics to this chart -- like **precision**, **recall** and **F1** -- to determine if the drift has an impact on model performance. We‚Äôre choosing these metrics as they are suited for classification problems and help us in identifying the number of false positives and false negatives.

![Adding performance metrics to our chart](../.gitbook/assets/424ad56-Screenshot\_2024-04-02\_at\_7.45.40\_PM.png)

We notice that although the precision has remained constant, there is a drop in the F1-score and recall, which means that there are a few customers"
" who are likely to churn but the model is not able to predict their outcome correctly.

> üìò There could be a number of reasons for drop in performance:
>
> 1. Cases of extreme events (Outliers)
> 2. Data distribution changes / Drift
> 3. Model/Concept drift
> 4. Pipeline health issues

#### Step 3 - Check the cause of drift

Our next step would be to go back to the **Data Drift** chart to conduct some root cause analysis. The drift is calculated using **Jensen Shannon Divergence**, which compares the distributions of the two data sets being compared.

We can select the bin, March 22nd, where we see an increase in output drift. In the table below, we can see that the `numofproducts` feature is drifting too and since it is an impactful feature, it is contributing the most to the resulting output drift we already identified.

![Using Prediction Drift Impact to"
" correlate output drift to input drift](../.gitbook/assets/rca\_drift\_illustrative.png)

We can also see there is a difference in the distribution of the baseline and production data which leads to a drift.

![Seeing the feature drift in histogram form](../.gitbook/assets/e7db5e1-Screenshot\_2024-04-02\_at\_8.01.47\_PM.png)

The logical next step is to determin if the change in distribution is occurring for all inferences or for a particular model segment/cohort. It could also be due to other factors like time (seasonality etc.), fault in data reporting (sensor data), change in the unit in which the metric is reported etc.

Seasonality could be observed by plotting the data across time (provided we have enough data), a fault in data reporting would mean missing values, and change in unit of data would mean change in values for all subsections of data.

In order to investigate if the"
" change was only for a segment of data, we will go to the **Analyze** tab. We can do this by clicking the \[Analyze] button seen in the image above.

#### Step 4 - Root Cause Analysis

Within the Analyze tab of Root Cause Analysis (Public Preview), performance analytics charts can be examined across various segments. Here, an array of analytics charts is displayed. Modifying the source segment aids in investigation and root cause analysis.

![Leveraging Analyze (Preview) for performance analytics](../.gitbook/assets/rca-analyze-tab.png)

By selecting Query Analytics, the analyze page will have an auto-generated SQL query based on our date selection in our drift chart. From here, we can further customize the SQL queries to investigate the data as we see fit.

We check the distribution of the field `numofproducts` for our selection. We can do this by selecting **Chart Type - Feature Distribution** on the right-hand side of the tab"
".

![Root Cause Analysis - 1](../.gitbook/assets/9b1f7d1-Churn-image5-analyze-rca-1.png)

Root Cause Analysis - 1

We further check the performance of the model for our selection by selecting the **Chart Type - Slice Evaluation**.

![Root Cause Analysis - 2](../.gitbook/assets/350aef8-Churn-image6-analyze-rca-2.png)

Root Cause Analysis - 2

In order to check if the change in the range violation has occurred for a subsection of data, we can plot it against the categorical variable. In our case, we can check distribution of `numofproducts` against `age` and `geography`. For this we can plot a feature correlation plot for two features by querying data and selecting **Chart type - Feature Correlation**.

On plotting the feature correlation plot of `gender` vs `numofprodcuts`, we observe the distribution to"
" be similar.

![Root Cause Analysis - 3](../.gitbook/assets/4f73274-churn-image6-analyze-rca-2-1.png)

Root Cause Analysis - 3

![Root Cause Analysis - 4](../.gitbook/assets/aff3dbf-churn-image6-analyze-rca-2-2.png)

Root Cause Analysis - 4

For the sake of this example, let‚Äôs say that state of Hawaii (which is a value in the `geography` field in the data) announced that it has eased restrictions on number of loans, since loans is one of products, our hypothesis is the `numofproducts` would be higher for the state. To test this we will check the feature correlation between `geography` and `numofproducts`.

![Root Cause Analysis - 5](../.gitbook/assets/e1c31a1-churn-image6-analyze-rca-2-3.png)

Root Cause"
" Analysis - 5

We do see higher values for the state of Hawaii as compared to other states. We can further check distribution for the field `numofproducts` just for the state of Hawaii.

![Root Cause Analysis - 6](../.gitbook/assets/6664850-churn-image7--analyze-rca-3.png)

Root Cause Analysis - 6

On checking performance for the subset of Hawaii, we see a huge performance drop.

![Root Cause Analysis - 7](../.gitbook/assets/974b118-churn-image8--analyze-rca-4.png)

Root Cause Analysis - 7

On the contrary, we see a good performance for the subset of data without the ‚ÄòHawaii‚Äô.

![Root Cause Analysis - 8](../.gitbook/assets/ee29b35-churn-image6-analyze-rca-4-1-1.png)

Root Cause Analysis - 8

![Root Cause Analysis - 9](../."
"gitbook/assets/fc54636-churn-image9--analyze-rca-5.png)

Root Cause Analysis - 9

#### Step 5 - Measuring the impact of the ‚Äònumofproducts‚Äô feature

We notice the ""Hawaii"" cohort exhibits poor decisions of the model in the form of false negatives. After further refining our SQL query, we can hone in directly on the false negatives within our underperforming ""Hawaii"" cohort. Fiddler will now allow us to ""explain"" these false negatives by clicking the \[lightbulb] action.

![Explaining our False Negatives](../.gitbook/assets/026d5cb-churn-image11-impact2.png)

Explaining our False Negatives

We see that the feature `numofproducts` attributes significantly towards the data point being predicted not to churn.

![A local explanation for a false negative inference](../.gitbook/assets/65fd05d-churn-image12-impact3"
".png)

A local explanation for a false negative inference

We have seen that the performance of the churn model drops due to data drift in a particular cohort for one particular feature. We can improve the performance by retraining the model with new data but before that we must perform mitigation actions which would help us in preemptively detecting the model performance degradation and inform our retraining frequency.

#### Step 6 - Mitigation Actions

1. **Add alerts**\
   We can alert users to make sure we are notified the next time there is a performance degradation. For instance, in this example, there was a performance degradation due to feature drift within a segment. To mitigate this, we can define a segment and set up an alert which would notify us in the case of input drift on our problematic feature. Check out this [link](../UI\_Guide/monitoring-ui/alerts-with-fiddler-ui.md) to learn how to set up alerts on Fiddler platform.

{% include ""../."
"gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Fraud Detection
slug: fraud-detection
excerpt: >-
  How to monitor and improve your Fraud Detection ML Models using Fiddler's AI
  Observability platform
metadata:
  title: Fraud Detection | Fiddler Docs
  image: []
  keywords: fraud detection
  robots: index
createdAt: Tue Apr 19 2022 20:06:54 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 25 2024 21:15:49 GMT+0000 (Coordinated Universal Time)
---

# Fraud Detection

Fraud detection models have proven to be more effective than humans when it comes to detecting fraud. However, if left unattended, the performance of fraud detection models can degrade over time leading to big losses for the company and dissatisfied customers.

The **Fiddler AI Observability** platform provides a variety of tools that can be used to monitor, explain, analyze, and improve the performance"
" of your fraud detection model.

### Monitoring

There are several metrics that organizations should measure on an ongoing basis to ensure their fraud detection models are performing as expected. The metrics below highlight some of the key metrics that every organization should monitor to ensure model health.

#### Drift Detection

* **Drift with class-imbalance** - Fraud detection models suffer from highly imbalanced data between the minority class (fraud) and the majority class (not fraud). With Fiddler, users can specify class weights on a global or event level to improve drift detection within the minority class. Please see more information in [Class-Imbalanced Data](../product-guide/monitoring-platform/class-imbalanced-data.md).
* **Feature Impact** - Tells us the contribution of features to the model's prediction, averaged over the baseline dataset. The contribution is calculated using [random ablation feature impact](https://arxiv.org/pdf/1910.00174.pdf).
* **Feature/Input Drift** -"
" Tells us how much a feature is drifting away from the baseline dataset for the time period of interest. For more information on how drift metrics are calculated, see [Data Drift](../product-guide/monitoring-platform/data-drift-platform.md).
* **Prediction/Output Drift Impact** - A heuristic calculated by taking the product of Feature Impact and Feature Drift. The higher the score the more this feature contributed to the prediction value drift.

#### Performance Metrics

Accuracy might not be a good measure of model performance in the case of fraud detection as most of the cases are non-fraud. Therefore, we use monitor metrics like:

1. **Recall** - How many of the non-fraudulent cases were actually detected as fraud? A low recall value might lead to an increased number of cases for review even though all the fraud cases were predicted correctly.
2. **False Positive Rate** - Non-Fraud cases labeled as fraud, high FPR rate leads to dissatisfied customers.

####"
" Data Integrity

* **Range Violations** - This metric shows the percentage of data in the selected production data that has violated the range specified in the baseline data.
* **Missing Value Violations** - This metric shows the percentage of missing data for a feature in the selected production data.
* **Type Violations** - This metric shows the percentage of data in the selected production data that has violated the type specified in the baseline data.

### Explanability

When fraud detection models infer false positives or false negatives, there is a real cost to the organization. Local explanations are a very powerful way to identify what inputs and values influenced the model's incorrect decisions.

#### Point Overview

![Point Overview](../.gitbook/assets/c7249cf-XAI21.gif)

Point Overview

Explanations in the Fiddler AI Observability platform gives an inference-level overview for the inference selected. These explanations unveil which inputs had the strongest positive and negative feature attributions which led to the prediction output"
". We can choose from the explanation types. In the case of fraud detection, we can choose from SHAP, Fiddler SHAP, Mean-reset feature impact, Permutation Feature Impact.

In this example, ‚Äòcategory‚Äô has the highest positive attribution (35.1%), pushing the prediction value towards fraud, and ‚Äòamt‚Äô has the highest negative attribution(-45.8%), pushing the prediction value towards non-fraud.

![Explanation Type](../.gitbook/assets/b4704f6-XAI11.png)

Explanation Type

#### Feature Attribution

![Feature Attribution](../.gitbook/assets/9b91f72-XAI22.gif)

Feature Attribution

The Feature Attribution tab gives us information about how much each feature can be attributed to the prediction value based on the Explanation Type chosen. We can also change the value of a particular feature to measure how much the prediction value changes.\
In the example below we can see that on changing the value of feature ‚Äòamt‚Äô from "
"110 to 10k the prediction value changes from 0.001 to 0.577 (not fraud to fraud).

### Maximize the performance of your Fraud Detection Models with Fiddler!

Please refer to our [ML Monitoring - Class Imbalance Quick Start Guide](../QuickStart\_Notebooks/class-imbalance-monitoring-example.md) for a walkthrough on how to get started with using Fiddler for your fraud detection use case and an interactive demo on usability.

#### Overview

It is inevitable that a model‚Äôs performance will degrade over time. We can use the Fiddler AI Observability platform to monitor the model‚Äôs performance in production, look at various metrics, and also provide explanations to predictions on various data points.

In this walkthrough, we will look at a few scenarios common to a fraud model when monitoring for performance. We will show how you can:

1. Get baseline and production data onto the Fiddler Platform
2. Monitor drift for various features
3."
" Monitor performance metrics associated with fraud detection like recall, false-positive rate
4. Monitor data integrity Issues like range violations
5. Provide point explanations to the mislabelled points
6. Get to the root cause of the issues

#### Example - Model Performance Degradation due to Data Integrity Issues

**Step 1 - Setting up baseline and publishing production events**

Please refer to our [ML - Simple Monitoring Quickstart Guide](../QuickStart\_Notebooks/quick-start.md) for a step-by-step walkthrough of how to upload baseline and production data to the Fiddler platform.

![](../.gitbook/assets/0cc61d5-image.png)

**Step 2 - Access Model Insights**

Once the production events are published, we can monitor a variety of model metrics by leveraging the model's default dashboard. To access the default dashboard, click on the _**Insights**_ button for a model on the homepage or model schema page. This will display a series of"
" data drift, performance, traffic and data integrity time series charts for your model.

![The default dashboard for our fraud detection model](../.gitbook/assets/187e626-image.png)

The default dashboard for our fraud detection model

**Step 3 - Monitor Drift**

Once the production events are published, we can monitor drift for the model output using the ""Prediction Drift"" chart i.e. - _**pred\_is\_fraud**_, which is the probability value of a case being a fraud. Here we can see that the prediction value of _**pred\_is\_fraud**_ increased in mid-November.

![Clicking into the Prediction Drift chart](../.gitbook/assets/6123224-Screenshot\_2024-04-02\_at\_4.02.29\_PM.png)

Clicking into the Prediction Drift chart

**Step 4 - Monitor Performance Metrics**

Next, to check if the performance has degraded, we can"
" check the performance metrics by creating a new chart (or clicking into the ""Accuracy"" chart on the default dashboard). Here we can look at ‚ÄòAccuracy‚Äô and ‚ÄòFPR‚Äô of the model over time. We can see that the accuracy has gone down and FPR has gone up in the same period.

![Accuracy and FPR over time](../.gitbook/assets/75bd1d8-Screenshot\_2024-04-02\_at\_4.06.21\_PM.png)

Accuracy and FPR over time

**Step 5 - Monitor Data Integrity**

The performance drop could be due to a change in the quality of the data. To check that we can create a new chart (or click on the ‚ÄòData Integrity‚Äô chart on our default dashboard) to look for Missing Value Violations, Type Violations, Range Violations, etc. We can see the columns ‚Äòcategory‚Äô suffers range violations. Since this is a ‚Äòcategorical‚Äô column, there is"
" likely a new value that the model did not encounter during training.

![Range violations for](../.gitbook/assets/71c681b-Screenshot\_2024-04-02\_at\_4.09.50\_PM.png)

Range violations for ""Category"" identified

**Step 6 - Check the impact of the drift**

We can go back to the ‚ÄòData Drift‚Äô chart to measure how much the data integrity issue has impacted the prediction. We can select the bin in which the drift increased. The table below shows the Feature Impact, Feature Drift, and Prediction Drift Impact values for the selected bin. We can see that even though the Feature Impact for ‚ÄòCategory‚Äô value is less than the ‚ÄòAmt‚Äô (Amount) value, because of the drift, its Prediction Drift Impact is more.

![Determining which input is driving the output drift](../.gitbook/assets/f386772-Screenshot\_2024-04-02\_at\_4."
"14.18\_PM.png)

Determining which input is driving the output drift

We will now move on to check the difference between the production and baseline data for this bin. For this, we can click on \[Analyze] button. This click will land us on the Analyze tab ready to retrieve inferences on the day selected.

**Step 7 - Root Cause Analysis in the ‚ÄòAnalyze‚Äô tab**

The analyze tab pre-populated the left side of the tab with the query based on our selection. We can also write custom queries to slice the data for analysis.

![Analyze Tab](../.gitbook/assets/31a6110-RCA2.jpg)

Analyze Tab

![Analyze Query](../.gitbook/assets/a3e4b27-RCA3.png)

Analyze Query

On the right-hand side of the tab we can build charts on the tabular data based on the results of our custom query. For this RCA we will build"
" a ‚ÄòFeature Distribution‚Äô chart on the ‚ÄòCategory‚Äô column to check the distinct values and also measure the percentage of each value. We can see there are 15 distinct values along with their percentages.

![Feature Distribution - Production](../.gitbook/assets/4996cad-RCA4.png)

Feature Distribution - Production

Next, we will compare the Feature Distribution chart in production data vs the baseline data to find out about the data integrity violation. We can modify the query to obtain data for baseline data and produce a ‚ÄòFeature Distribution‚Äô chart for the same.

![Feature Distribution - Baseline](../.gitbook/assets/303c243-RCA5.png)

Feature Distribution - Baseline

We can see that the baseline data has just 14 unique values and ‚Äòinsurance‚Äô is not present in baseline data. This ‚ÄòCategory‚Äô value wasn‚Äôt present in the training data and crept in production data likely causing performance degradation.\
Next, we can perform a ‚Äòpoint explanation‚Äô for one"
" such case where the ‚ÄòCategory‚Äô value was ‚ÄòInsurance‚Äô and the prediction was incorrect to measure how much the ‚ÄòCategory‚Äô column contributed to the prediction by looking at its SHAP value.

![Mislabelled Data Point](../.gitbook/assets/c1c1c81-RCA6.png)

Mislabelled Data Point

We can click on the bulb sign beside the row to produce a point explanation. If we look at example 11, we can see that the output probability value was 0 (predicted as fraud according to the threshold of 0.5) but the actual value was ‚Äònot fraud‚Äô.

The bulb icon will take us to the ‚ÄòExplain‚Äô tab. Here we can see that the ‚Äòcategory‚Äô value contributed to the model predicting the case as ‚Äòfraud‚Äô.

![Point Explanation](../.gitbook/assets/16d1150-RCA7.png)

Point Explanation

**Step 7 - Actions**

We discovered that the prediction drift and performance drop were"
" due to the introduction of a new value in the ‚ÄòCategory‚Äô column. We can take steps so that we could identify this kind of issue in the future before it can result in business impact.

**Setting up Alerts**

In the ‚ÄòAlerts‚Äô section of Fiddler, we can set up alerts to notify us of as soon as a certain data issue happens. For example, for the case we discussed, we can set up alerts as shown below to alert us when the range violation increases beyond a certain threshold (e.g.-5%).

![An alert rule for detecting additional range violations on our 'category' input](../.gitbook/assets/5e4a0a7-Screenshot\_2024-04-02\_at\_4.17.40\_PM.png)

An alert rule for detecting additional range violations on our 'category' input

These alerts can further influence the retraining of the ML model, we can retrain the model including the new data so the newly"
" trained model contains the ‚Äòinsurance‚Äô category value. This should result in improved performance.

**Data Insights**

Below we can see the confusion matrix for November (before drift starts). We can observe a good performance with Recall at 100% and 0.1% FP

![Slice Evaluation - Feb 17](../.gitbook/assets/09c82f2-FraudInsights2.png)

Slice Evaluation - Feb 17

Below we can see the confusion matrix for late November (after drift starts). We can observe a performance drop with Recall at 50% and 9% FP

![Slice Evaluation - Feb 16](../.gitbook/assets/c1c6e39-FraudInsights1.png)

Slice Evaluation - Feb 16

#### Conclusion

Undetected fraud cases can lead to losses for the company and customers, not to mention damage reputation and relationship with customers. The Fiddler AI Observability platform can be used to identify the pitfalls in your ML"
" model and mitigate them before they have an impact on your business.

In this walkthrough, we investigated one such issue with a fraud detection model where a data integrity issue caused the performance of the ML model to drop.

Fiddler can be used to keep the health of your fraud detection model up by:

1. Monitoring the drift of the performance metric
2. Monitoring various performance metrics associated with the model
3. Monitoring data integrity issues that could harm the model performance
4. Investigating the features which have drifted/ compromised and analyzing them to mitigate the issue
5. Performing a root cause analysis to identify the exact cause and fix it
6. Diving into point explanations to identify how much the issue has an impact on a particular data point
7. Setting up alerts to make sure the issue does not happen again

We discovered there was an issue with the ‚ÄòCategory‚Äô column, wherein a new value was discovered in the production data. This led to the performance drop in the data likely"
" due to the range violation. We suggest two steps to mitigate this issue:

1. Setting up ‚Äòalerts‚Äô to identify similar issues in data integrity
2. Retraining the ML model after including the new data (with the ground truth labels) to teach the model of the new values

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

"
"# Events

{% swagger src=""../../.gitbook/assets/api_v3.yaml"" path=""/v3/events"" method=""post"" %}
[api_v3.yaml](../../.gitbook/assets/api_v3.yaml)
{% endswagger %}

{% swagger src=""../../.gitbook/assets/api_v3.yaml"" path=""/v3/events"" method=""delete"" %}
[api_v3.yaml](../../.gitbook/assets/api_v3.yaml)
{% endswagger %}

{% swagger src=""../../.gitbook/assets/api_v3.yaml"" path=""/v3/events"" method=""patch"" %}
[api_v3.yaml](../../.gitbook/assets/api_v3.yaml)
{% endswagger %}
"
"---
title: Fiddler Docs Chat
slug: fiddler-chat
excerpt: Ask the chatbot questions about documentation!
createdAt: Fri Jun 23 2023 20:45:17 GMT+0000 (Coordinated Universal Time)
updatedAt: Tue Dec 26 2023 17:29:09 GMT+0000 (Coordinated Universal Time)
icon: comment-lines
---

# Fiddler Chat

{% embed url=""https://fiddler-chatbot.streamlit.app/"" %}
"
"---
icon: memo-circle-check
---

# Release Notes

## Release 24.18 Notes
*December 4, 2024*

### What's New and Improved
* **Native Integration with AWS SageMaker AI**
  * Fiddler is now natively supported within the newly launched Amazon SageMaker partner AI ecosystem. 
  This integration enables enterprises to validate, monitor, analyze, and improve their ML models in production, all within their existing private and secure Amazon SageMaker AI environment. 
  Read the official announcement [here](https://www.fiddler.ai/blog/fiddler-delivers-native-enterprise-grade-ai-observability-to-amazon-sagemaker-ai-customers).
  Note Fiddler Python client version [3.7+](../python-client-history.md#370) is required for this feature.

* **Download Dataset Code in UI**
  * Now you can download your baseline and non-production datasets faster than ever with just a click! 
"
"  Building on the popular feature we introduced for production data in the Root Cause Analysis Events table, we've added ready-to-use Python code snippets right in the interface. 
  Simply copy and paste these snippets to jumpstart your data analysis in your notebooks.
  ![Example of the icon to click to generate the Python script for downloading dataset data within the Datasets tab of the Model UI.](../.gitbook/assets/model-dataset-tab-download-dataset-link.png)

* **Python Client Highlights**
  * The latest release of Fiddler's Python client brings two powerful new convenience features to streamline your workflow:
    * Our new [Project.get_or_create()](../Python_Client_3-x/api-methods-30.md#get_or_create) class function simplifies project creation in notebooks. This feature prevents name conflict errors during project creation when your notebook runs multiple times, saving you time and reducing the need for additional exception handling.
    * We've also added [model.remove_column()](../"
"Python_Client_3-x/api-methods-30.md#remove_column), a simpler way to remove columns during model onboarding. This function replaces the multi-step process previously required, making model configuration faster and more intuitive.
  * To enhance reliability, we've implemented a configurable HTTP retry mechanism that you can fine-tune to match your network environment.
  * For more details, please refer to the [Python Client Release notes](../python-client-history.md#370).

### Discontinued
* **The SQL Analyze Page Discontinued**
  * The legacy SQL Analyze page has been removed as of 24.18. The new Analyze experience within monitoring charts Root Cause Analysis now enables data table generation using Fiddler Query Language (FQL) and supports the creation of analytical charts such as confusion matrices, feature distribution charts, and more.

### Client Version
Client version [3.7+](../python-client-history.md#370) is required for the updates and features mentioned in"
" this release.


## Release 24.17 Notes
_By Sabina Cartacio ‚Ä¢ Nov 12, 2024_

### What's New and Improved
* **Feature Analytics in Root Cause Analysis (Public Preview)**
  * The root cause analysis experience within monitoring charts now allows users to view feature distribution, feature correlation, and correlation matrix.

### Discontinued

* **SQL Methods in the Python Client Discontinued**
  * From Client 3.6 and onwards, `get_slice` and `download_slice` are discontinued. 
  In their stead, use the new [`download_data`](../Python\_Client\_3-x/api-methods-30.md#download\_data) method to download production and non-production data from your Fiddler models. 
  If you have any questions or need any assistance migrating scripts using the deprecated methods, please contact your Fiddler customer success manager.
* Use of or support for **Python 3.8 is discontinued** by Fidd"
"ler. 
Note Python 3.8 has been designated [End of Life](https://devguide.python.org/versions/) as of October 7, 2024.

## Release 24.16 Notes
_By Sabina Cartacio ‚Ä¢ Oct 28, 2024_

### What's New and Improved

* **New Chart Type: Correlation Matrix (Public Preview)**
  * The Correlation Matrix chart enables users to visualize relationships between up to eight columns in a heatmap, making it easy to spot significant patterns. By clicking on any cell representing the relationship between two features, users can open a Feature Correlation chart for that pair, offering more detailed insights into the correlation score.

* **Events Table in Root Cause Analysis (Public Preview)**
  * The root cause analysis experience within monitoring charts now allows users to perform deeper investigations by viewing and downloading up to 1,000 raw events, providing valuable insights for understanding and addressing potential issues.

## Release 24.15"
" Notes

_By Dustin Basil ‚Ä¢ Oct 10, 2024_

### What's New and Improved

* **New Chart Type: Metric Card (Public Preview)**
  * We‚Äôre excited to introduce the Metric Card chart type, which allows users to display up to four key numerical values in a clear and concise card format. This new visualization enhances data presentation by enabling quick insights into critical metrics, making it easier for decision-makers to spot trends or performance indicators at a glance.
* **New Chart Type: Feature Correlation (Public Preview)**
  * The Feature Correlation chart, part of Feature Analytics charts, enables users to analyze and visualize the relationships between different features within their models. By offering a clear view of correlations, this tool supports more informed model diagnostics and refinement.

## Release 24.14 Notes

_By Dustin Basil ‚Ä¢ Sep 25, 2024_

### What's New and Improved

* This release focused on system performance, stability, and security enhancements. These improvements"
" ensure a smoother user experience and provide a more robust platform for future developments.

## Release 24.13 Notes

_By Sabina Cartacio ‚Ä¢ Sep 10, 2024_

### What's New and Improved

* NEW Standalone Feature Distribution Chart (Public Preview)
  * Create feature distribution charts for numerical and categorical data types that can be added to dashboards.
* Embedding Visualization UX Improvements
  * User interface and usability improvements to the UMAP embedding visualization chart.
* Additional database performance improvements.

### Deprecated and Decommissioned

* Fairness was decommissioned in v24.8, and the documentation has now been removed.

## Release 24.12 Notes

_By Rohan Sharma ‚Ä¢ Aug 29, 2024_

* Surfacing Error Messages for Failed Jobs
  * Error messages for failed jobs are now visible directly on the UI job status page, simplifying the process of diagnosing and resolving issues.
* User Selected Default Dashboards
  *"
" Any dashboard within a project can now be assigned as the default dashboard for a model, with all insights leading directly to the assigned default dashboard.
* Custom Feature Impact Feature Release Notes
  * Introducing Custom Feature Impact: Upload custom feature impact scores for your models, leveraging domain-specific knowledge or external data without requiring the corresponding model artifact.
  * Easy data upload via API endpoint with required parameters: Model UUID, Feature Names, and Impact Scores.
  * View updated feature impact scores in:
    * Model details page
    * Charts page
    * Explain page
  * Flexible update options: Update existing feature impact data by uploading new data for the same model and Seamless integration with existing model artifacts.
* Flexible Model Deployment
  * The `python-38` base image is no longer supported.

## Release 24.11 Notes

_By Sabina Cartacio ‚Ä¢ Aug 6, 2024_

### Client Version

Client version 3.3+ is required for the updates and"
" features mentioned in this release.

### What's New and Improved:

* Performance Analytics (Preview) Embedded in Monitoring Charts
  * Visualize performance analytics charts as part of the root cause analysis flow for Binary Classification, Multiclass Classification, and Regression models, spanning from confusion matrices, precision recall charts, prediction scatterplots and more.

## Release 24.10 Notes

_By Sabina Cartacio ‚Ä¢ July 23, 2024_

### Client Version

Client version 3.3+ is required for the updates and features mentioned in this release.

### What's New and Improved:

* Support for applied segments in monitoring charts
  * Create and apply segments dynamically in monitoring charts for exploratory analysis without requiring them to be saved to the model.
* User-Defined Feature Impact
  * The User-Defined Feature Impact enables you to upload custom feature impact for models. This feature addresses several issues reported by our customers, including model artifact size, onboarding complexity, and the need for custom"
" feature impact.
  * Key highlights
    * New method: UploadFeatureImpact
    * Improved Fiddler UI to display uploaded feature impact

## Release 24.9 Note

_By Rohan Sharma ‚Ä¢ July 12, 2024_

### What's New and Improved

* Enhanced access controls
  * Control access with precision: Manage user access to resources with Role-Based Access Control (RBAC), ensuring the right users have the right permissions.
  * Simplify user management: Assign roles to users and teams to streamline access control and enhance collaboration. \*‚â† Protect sensitive resources: Restrict access to sensitive resources, such as models and project settings, with granular permissions.
  * Work efficiently: Focus on your work without worrying about unauthorized access or data breaches.

## Release 24.8 Notes

_By Sabina Cartacio ‚Ä¢ June 18, 2024_

### Release of Fiddler Platform Version 24.8:

* **Performance Analytics Charts ("
"Public Preview)**
  * Visualize charts to aid in analyzing model performance for Binary Classification, Multiclass Classification, and Regression models.
  * Leverage applied segments in Performance Analytics charts to explore problematic cohorts of data.

## Release 24.7 Notes

_By Parth Domadia ‚Ä¢ June 4, 2024_

### What's New and Improved

TBD

## Release 24.6 Notes

_By Parth Domadia ‚Ä¢ May 22, 2024_

### Release of Fiddler Platform Version 24.6:

* Performance improvements
  * Improved the performance of various modules / APIs.
  * Improved observability which can help monitor health and performance of the operations.

### Client Version

* Client version 3.1.2+ is required for the updates and features mentioned in this release.

## Release 24.5 Notes

_By Sabina Cartacio ‚Ä¢ May 2, 2024_

### Release of Fiddler Platform Version 24"
".5:

* Support for model versions for streamlined model management

### What's New and Improved:

* **Model Versions**
  * Efficiently manage related models by creating structured versions, facilitating tasks like retraining and comparison analyses.
  * Users can maintain model lineage, efficiently manage updates, flexibly modify schemas, and adjust parameters.
  * **See**: [Model Versions](../product-guide/monitoring-platform/model-versions.md) for more information.
* **Airgapped Enrichments (alpha)**
  * For privacy sensitive use cases, all data getting enriched stays within customer premises.
* **New Deployment Base Images**
  * We have added new deployment base images to support model versioning.
  * See [here](../product-guide/explainability/flexible-model-deployment/).

#### Client Version

Client version 3.1.0+ is required for the updates and features mentioned in this release.

## Release 24.4 Notes

_By Parth Domadia"
" ‚Ä¢ April 26, 2024_

### Release of Fiddler Platform Version 24.4:

* UMAP UI changes
* SSO integration changes
* New concept: **Environments**
* Fundamental changes to product concepts

### What's New and Improved:

* **UMAP UI**
  * Vertical scrolling instead of horizontal scrolling for data cards
  * ""View More"" option to open data cards in maximized modal
  * Ability to toggle between data cards in the maximized modal
* **SSO integration changes**
  * Fiddler now integrates with Azure AD SSO, allowing you to leverage existing user roles for access control within Fiddler. This eliminates the need for manual user creation and simplifies user management within your organization. See [Azure AD SSO Support](../Deployment_Guide/single-sign-on-with-azure-ad.md) for details
* **Environments**
  * Each Model now has two environments (Pre-Production and Production)"
" used to house data in different ways.
  * A Model's Pre-Production environment is used to house non-time series data (Datasets).
  * A Model's Production environment is used to house time series data.
  * See [Data and Environments](doc:data-and-environments) for more
* **Product concept changes**
  * Datasets are no longer stored at the Project level. Instead, they're stored at the Model level under the Pre-Production Environment.
  * The Model Details page has been updated with a new design. See more [here](../Overview/product-tour.md#documented-ui-tour).

#### Client Version

Client version 3.0+ is required for the updates and features mentioned in this release.

### Client 3.x Release:

We are launching Client 3.x, this is revamped client 2.x as we move to more object oriented based methods. This means, any pipeline setup in client 2.x would eventually be required to"
" upgrade to the new methods. **Client 2.x will sunset approximately 6 months post this release.** Please take a look at the below resources to help you understand client 3.x and also how you can upgrade your pipelines:

* [API Documentation](../Python\_Client\_3-x/about-client-3x.md)
* [How to upgrade from client 2.x to 3.x](../Python\_Client\_3-x/upgrade-from-2x-to-3x.md)

### Deprecations and Removals:

* All IDs will be UUIDs instead of strings.
* Dataset deletion is not allowed anymore.

For API level changes and updates please check [client history](../python-client-history.md#3x-client-version)
"
"---
title: Compatibility Matrix
slug: compatibility-matrix
excerpt: ''
hidden: false
createdAt: Fri Apr 05 2024 19:54:39 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Oct 24 2024 20:18:26 GMT+0000 (Coordinated Universal Time)
---

# Compatibility Matrix

This table summarizes the compatibility between the Fiddler Python client and the Fiddler application. You can find:

* **Corresponding Fiddler Platform(s)**: Exactly the same features / API objects in both python client and the Fiddler Platform version
* **Lower Fiddler Platform**: Python client has features or API objects that may not be present in the Fiddler Platform, either due to that python client has additional new methods, or that the server has removed old APIs. However, everything they have in common (i.e., most APIs) will work.
* **Higher Fiddler Platform**: The Fiddler Platform has features the python client can't use, either due to the server has additional new APIs, or that python client has removed old API. However, everything they share in common (i.e., most APIs) will work.

| Client version   | Corresponding Fiddler Platform(s) ‚úÖ | Lower Fiddler Platform | Higher Fiddler Platform |
|------------------|-------------------------------------|------------------------|--------------------------|
| 3.7              | 24.18                               | Between 24.17 and 24.4 |                           
| 3.6              | 24.17                               | Between 24.16 and 24.4 | 28.18 or above           |
| 3.5              | 24.16                               | Between 24.15 and 24.4 | 24.17 or above           |
| 3.4              | 24.13, 24.14, 24.15                 | Between 24.12 and 24.4 | 24.16 or above           |
| 3.3              | 24.10, 24.11, 24.12                 | Between 24.9 and 24.4  | 24.13 or above           |
| 3.2              | 24.8, 24.9                          | Between 24.7 and 24.4  | 24.10 or above           |
| 3.1              | 24.5, 24.6, 24.7                    | 24.4                   | 24.8 or above            |
| 3.0              | 24.4                                | Not compatible         | 24.5 or above            |
"
"---
title: ""PagerDuty Integration""
slug: ""pagerduty""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:19:10 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Fiddler offers powerful alerting tools for monitoring models. By integrating with  
PagerDuty services, you gain the ability to trigger PagerDuty events within your monitoring  
workflow.

> üìò If your organization has already integrated with PagerDuty, then you may skip to the [Setup: In Fiddler](#setup-in-fiddler) section to learn more about setting up PagerDuty within Fiddler.

## Setup: In PagerDuty

1. Within your PagerDuty Team, navigate to **Services** ‚Üí **Service Directory**.

![](../../.gitbook/assets/0"
"slug: ""pagerduty"" ae47bb-pagerduty_1.png ""pagerduty_1.png"")

2. Within the Service Directory:
   - If you are creating a new service for integration, select **+New Service** and follow the prompts to create your service.
   - Click the **name of the service** you want to integrate with.

![](../../.gitbook/assets/956dbdf-pagerduty_2.png ""pagerduty_2.png"")

3. Navigate to **Integrations** within your service, and select **Add a new integration to this service**.

![](../../.gitbook/assets/ca2e4c2-pagerduty_3.png ""pagerduty_3.png"")

4. Enter an **Integration Name**, and under **Integration Type** select the option **Use our API directly**. Then, select the **Add Integration** button to save your new integration. You will be redirected to the Integrations page for your service.

!"
"slug: ""pagerduty"" [](../../.gitbook/assets/0f5d5ae-pagerduty_4.png ""pagerduty_4.png"")

5. Copy the **Integration Key** for your new integration.

![](../../.gitbook/assets/e144e08-pagerduty_5.png ""pagerduty_5.png"")

## Setup: In Fiddler

1. Within **Fiddler**, navigate to the **Settings** page, and then to the **PagerDuty Integration** menu. If your organization **already has a PagerDuty service integrated with Fiddler**, you will be able to find it in the list of services.

![](../../.gitbook/assets/8de1a6b-pagerduty_setup_f_1.png ""pagerduty_setup_f_1.png"")

2. If you are looking to integrate with a new service, select the **`+`** box on the top right. Then, enter the name of your service"
"slug: ""pagerduty"" , as well as the Integration Key copied from the end of the [Setup: In PagerDuty](#setup-in-pagerduty) section above. After creation, confirm that your new entry is now in the list of available services.

![](../../.gitbook/assets/9febb10-pagerduty_setup_f_2.png ""pagerduty_setup_f_2.png"")

> üöß Creating, editing, and deleting these services is an **ADMINSTRATOR**-only privilege. Please contact an **ADMINSTRATOR** within your organization to setup any new PagerDuty services

## PagerDuty Alerts in Fiddler

1. Within the **Projects** page, select the model you wish to use with PagerDuty.

![](../../.gitbook/assets/d9ad82e-pagerduty_fiddler_1.png ""pagerduty_fiddler_1.png"")

2. Select **Monitor** ‚Üí **Alerts** ‚Üí **"
"slug: ""pagerduty"" Add Alert**.

![](../../.gitbook/assets/b7118f0-pagerduty_fiddler_2.png ""pagerduty_fiddler_2.png"")

3. Enter the condition you would like to alert on, and under **PagerDuty Services**, select all services you would like the alert to trigger for. Additionally, select the **Severity** of this alert, and hit **Save**.

![](../../.gitbook/assets/8fbffde-pagerduty_fiddler_3.png ""pagerduty_fiddler_3.png"")

4. After creation, the alert will now trigger for the specified PagerDuty services.

> üìò Info
> 
> Check out the [alerts documentation](../../product-guide/monitoring-platform/alerts-platform.md) for more information on setting up alerts.

## FAQ

**Can Fiddler integrate with multiple PagerDuty services?**

- Yes. So long as the service is present within"
"slug: ""pagerduty""  **Settings** ‚Üí **PagerDuty Services**, anyone within your organization can select that service to be a recipient for an alert.
"
"---
title: ""Alerting Integrations""
slug: ""alerting-integrations""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:18:54 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
"
"---
title: ""BigQuery Integration""
slug: ""bigquery-integration""
excerpt: """"
hidden: false
createdAt: ""Fri May 20 2022 18:53:49 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Using Fiddler on your ML data stored in BigQuery

In this article, we will be looking at loading data from BigQuery tables and using the data for the following tasks-

1. Uploading baseline data to Fiddler
2. Onboarding a model to Fiddler and creating a surrogate
3. Publishing production data to Fiddler

## Step 1 - Enable BigQuery API

Before looking at how to import data from BigQuery to Fiddler, we will first see how to enable BigQuery API. This can be done as follows - 

1. In the GCP"
"slug: ""bigquery-integration""  platform, Go to the navigation menu -> click APIs & Services. Once you are there, click + Enable APIs and Services (Highlighted below). In the search bar, enter BigQuery API and click Enable.

![](../../.gitbook/assets/75ca647-Screen_Shot_2022-05-19_at_1.26.33_PM.png ""Screen Shot 2022-05-19 at 1.26.33 PM.png"")

![](../../.gitbook/assets/3dd5deb-Screen_Shot_2022-05-19_at_3.33.43_PM.png ""Screen Shot 2022-05-19 at 3.33.43 PM.png"")

2. In order to make a request to the API enabled in Step#1, you need to create a service account and get an authentication file for your Jupyter Notebook. To do so, navigate to the Credentials tab under APIs and Services console and click Create Credentials tab, and"
"slug: ""bigquery-integration""  then Service account under dropdown.

![](../../.gitbook/assets/ea63eca-Screen_Shot_2022-05-19_at_3.34.24_PM.png ""Screen Shot 2022-05-19 at 3.34.24 PM.png"")

3. Enter the Service account name and description. You can use the BigQuery Admin role under Grant this service account access to the project. Click Done. You can now see the new service account under the Credentials screen. Click the pencil icon beside the new service account you have created and click Add Key to add auth key. Please choose JSON and click CREATE. It will download the JSON file with auth key info. (Download path will be used to authenticate)

![](../../.gitbook/assets/662315e-Screen_Shot_2022-05-19_at_3.39.24_PM.png ""Screen Shot 2022-05-19 at 3.39.24 PM.png"")

"
"slug: ""bigquery-integration"" ## Step 2 - Import data from BigQuery

We will now use the generated key to connect to BigQuery tables from Jupyter Notebook. 

1. Install the following libraries in the python environment and load them to jupyter-

- Google-cloud
- Google-cloud-bigquery[pandas]
- Google-cloud-storage

2. Set the environment variable using the key that was generated in Step 1

```python
#Set environment variables for your notebook
import os
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '<path to json file>'
```

3. Import Google cloud client and initiate BigQuery service

```python
#Imports google cloud client library and initiates BQ service
from google.cloud import bigquery
bigquery_client = bigquery.Client()
```

4. Specify the query which will be used to import the data from BigQuery

```python
#Write Query on BQ
QUERY = """"""
SELECT * FROM `fiddler-bq.fidd"
"slug: ""bigquery-integration"" ler_test.churn_prediction_baseline` 
  """"""
```

5. Read the data using the query and write the data to a pandas dataframe

```python
#Run the query and write result to a pandas data frame
Query_Results = bigquery_client.query(QUERY)
baseline_df = Query_Results.to_dataframe()
```

Now that we have data imported from BigQuery to a dataframe, we can refer to the following pages to

1. [Upload baseline data and onboard a model ](../../Client_Guide/creating-a-baseline-dataset.md)
2. [Publish production events ](../../Client_Guide/publishing-production-data/publishing-batches-of-events.md)
"
"---
title: ""S3 Integration""
slug: ""integration-with-s3""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 17:40:36 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Pulling a dataset from S3

You may want to **pull a dataset directly from S3**. This may be used either to upload a baseline dataset, or to publish production traffic to Fiddler.

You can use the following code snippet to do so. Just fill out each of the string variables (`S3_BUCKET`, `S3_FILENAME`, etc.) with the correct information.

```python
import boto3
import pandas as pd

S3_BUCKET = 'my_bucket'
S3_FILENAME = 'my_baseline.csv'

AWS_ACCESS_KEY_ID = 'my_access_key'
AWS_SECRET_ACCESS_KEY ="
"slug: ""integration-with-s3""  'my_secret_access_key'
AWS_REGION = 'my_region'

session = boto3.session.Session(
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
    region_name=AWS_REGION
)

s3 = session.client('s3')

s3_data = s3.get_object(
    Bucket=S3_BUCKET,
    Key=S3_FILENAME
)['Body']

df = pd.read_csv(s3_data)
```

## Uploading the data to Fiddler

If your goal is to **use this data as a baseline dataset** within Fiddler, you can then proceed to upload your dataset (see [Uploading a Baseline Dataset](../../Client_Guide/creating-a-baseline-dataset.md)).

If your goal is to **use this data as a batch of production traffic**, you can then proceed to publish the batch to Fiddler (see [Publishing Batches of Events](../../Client_Guide/publishing"
"slug: ""integration-with-s3"" -production-data/publishing-batches-of-events.md) ). 

## What if I don‚Äôt want to hardcode my AWS credentials?

If you don‚Äôt want to hardcode your credentials, you can **use an AWS profile** instead. For more information on how to create an AWS profile, click [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html).

You can use the following code snippet to point your `boto3` session to the profile of your choosing.

```python
import boto3
import pandas as pd

S3_BUCKET = 'my_bucket'
S3_FILENAME = 'my_baseline.csv'

AWS_PROFILE = 'my_profile'

session = boto3.session.Session(
    profile_name=AWS_PROFILE
)

s3 = session.client('s3')

s3_data = s3.get_object(
    Bucket=S3_BUCKET,
    Key=S3_FILENAME
)['Body']

df = pd.read_csv(s3_data)
```

## What"
"slug: ""integration-with-s3""  if I don't want to load the data into memory?

If you would rather **save the data to a disk** instead of loading it in as a pandas DataFrame, you can use the following code snippet instead.

```python
import boto3
import pandas as pd
import fiddler as fdl

S3_BUCKET = 'my_bucket'
S3_FILENAME = 'my_baseline.csv'

AWS_ACCESS_KEY_ID = 'my_access_key'
AWS_SECRET_ACCESS_KEY = 'my_secret_access_key'
AWS_REGION = 'my_region'

OUTPUT_FILENAME = 's3_data.csv'

session = boto3.session.Session(
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
    region_name=AWS_REGION
)

s3 = session.client('s3')

s3.download_file(
    Bucket=S3_BUCKET,
    Key=S3_FILENAME,
    Filename=OUTPUT_FILENAME
)
```
"
"---
title: ""Kafka Integration""
slug: ""kafka-integration""
excerpt: """"
hidden: false
createdAt: ""Tue May 23 2023 16:48:07 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Apr 25 2024 21:16:14 GMT+0000 (Coordinated Universal Time)""
---
Fiddler Kafka connector is a service that connects to a [Kafka topic](https://kafka.apache.org/documentation/#intro_concepts_and_terms) containing production events for a model, and publishes the events to Fiddler.

## Pre-requisites

We assume that the user has an account with Fiddler, has already created a project, uploaded a dataset and onboarded a model. We will need the [url_id, org_id,](../../Client_Guide/installation-and-setup.md) project_id and model_id to configure the Kafka connector.

## Installation

The Kafka connector runs on Kubernetes within the customer‚Äôs environment. It is packaged as a Helm chart. To install:

```shell
helm repo add fiddler https://helm.fiddler.ai/stable/

helm repo update

kubectl -n kafka create secret generic fiddler-credentials --from-literal=auth=<API-KEY>

helm install fiddler-kafka fiddler/fiddler-kafka \
    --devel \
    --namespace kafka \
    --set fiddler.url=https://<FIDDLER-URL> \
    --set fiddler.org=<ORG> \
    --set fiddler.project_id=<PROJECT-ID> \
    --set fiddler.model_id=<MODEL-ID> \
    --set fiddler.ts_field=timestamp \
    --set fiddler.ts_format=INFER \
    --set kafka.host=kafka \
    --set kafka.port=9092 \
    --set kafka.topic=<KAFKA-TOPIC> \
    --set kafka.security_protocol=SSL \
    --set kafka.ssl_cafile=cafile \
    --set kafka.ssl_certfile=certfile \
    --set kafka.ssl_keyfile=keyfile \
    --set-string kafka.ssl_check_hostname=False

```

This creates a deployment that reads events from the Kafka topic and publishes it to the configured model. The deployment can be scaled as needed. However, if the Kafka topic is not partitioned, scaling will not result in any gains.

## Limitations

1. The connector assumes that there is a single dedicated topic containing production events for a given model. Multiple deployments can be created, one for each model, and scaled independently.
2. The connector assumes that events are published as JSON serialized dictionaries of key-value pairs. Support for other formats can be added on request. As an example, a Kafka message should look like the following:

```json
{
    ‚Äúfeature_1‚Äù: 20.7,
    ‚Äúfeature_2‚Äù: 45000,
    ‚Äúfeature_3‚Äù: true,
    ‚Äúoutput_column‚Äù: 0.79,
    ‚Äútarget_column‚Äù: 1,
    ‚Äúts‚Äù: 1637344470000,
}

```
"
"---
title: ""Data Pipeline Integrations""
slug: ""data-pipeline-integrations""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:18:12 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
"
"---
title: ""Snowflake Integration""
slug: ""snowflake-integration""
excerpt: """"
hidden: false
createdAt: ""Wed Jun 22 2022 14:51:45 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Using Fiddler on your ML data stored in Snowflake

In this article, we will be looking at loading data from Snowflake tables and using the data for the following tasks-

1. Uploading baseline data to Fiddler
2. Onboarding a model to Fiddler and creating a surrogate
3. Publishing production data to Fiddler

### Import data from Snowflake

In order to import data from Snowflake to Jupyter notebook, we will use the snowflake library, this can be installed using the following command in your Python environment.

```python
pip install snowflake-connector-python
```

Once the library is installed, we would require the following to establish a connection to Snowflake

- Snowflake Warehouse
- Snowflake Role
- Snowflake Account
- Snowflake User
- Snowflake Password

These can be obtained from your Snowflake account under the ‚ÄòAdmin‚Äô option in the Menu as shown below or by running the queries -

- Warehouse - select CURRENT_WAREHOUSE()
- Role - select CURRENT_ROLE()
- Account - select CURRENT_ACCOUNT()

'User' and 'Password' are the same as one used for logging into your Snowflake account.

![](../../.gitbook/assets/c2f4cf4-Screen_Shot_2022-06-14_at_4.17.36_PM.png ""Screen Shot 2022-06-14 at 4.17.36 PM.png"")

Once you have this information, you can set up a Snowflake connector using the following code -

```python
# establish Snowflake connection
connection = connector.connect(
  user=snowflake_username,
  password=snowflake_password,
  account=snowflake_account,
  role=snowflake_role,
  warehouse=snowflake_warehouse
)
```

You can then write a custom SQL query and import the data to a pandas dataframe.

```python
# sample SQL query
sql_query = 'select * from FIDDLER.FIDDLER_SCHEMA.CHURN_BASELINE LIMIT 100'

# create cursor object
cursor = connection.cursor()

# execute SQL query inside Snowflake
cursor.execute(sql_query)

baseline_df = cursor.fetch_pandas_all()
```

### Publish Production Events

In order to publish production events from Snowflake, we can load the data to a pandas dataframe and publish it to fiddler using _fdl.Model.publish_ api.

Now that we have data imported from Snowflake to a jupyter notebook, we can refer to the following notebooks to

- [Upload baseline data and onboard a model](../../Client_Guide/creating-a-baseline-dataset.md)
- [Publish production events](../../Client_Guide/publishing-production-data/publishing-batches-of-events.md)
"
"---
title: ""Airflow Integration""
slug: ""airflow-integration""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:18:03 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Tue Dec 19 2023 20:40:11 GMT+0000 (Coordinated Universal Time)""
---
Apache Airflow is an open source platform ETL platform to manage company‚Äôs complex  
workflows. Companies are increasingly integrating their ML models pipeline into Airflow DAGs to manage and monitor all the components of their ML model system.

By integrating Fiddler into an existing Airflow DAG, you will be able to train, manage, and onboard your models while  actively monitoring performance, data quality, and troubleshooting degradations across your models.

Fiddler can be easily integrated into your existing airflow DAG for ML model pipeline. A notebook which is used for publishing events can be orchestrated to run as a part of your"
"slug: ""airflow-integration""  airflow DAG using a ‚ÄòPapermill Operator‚Äô.

## Steps for the walkthrough

1. Setup airflow on your local or docker, these steps can be followed. [Link](https://airflow.apache.org/docs/apache-airflow/stable/start/index.html)

2. Add your jupyter notebook containing the code for publishing to your airflow home directory. In this example we will use the 2 different notebooks - 

   a. [Notebook to onboard ML model to Fiddler platform](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/master/integration-examples/airflow/notebooks/Fiddler_Churn_Add_Model.ipynb)

   b. [Notebook to push production events to Fiddler platform](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/master/integration-examples/airflow/notebooks/Fiddler_Churn_Event_Publishing.ipynb)

3"
"slug: ""airflow-integration"" . Add an orchestration code to your airflow directory, airflow will pick up the orchestration code and construct a DAG as defined. The orchestration code contains the ‚Äòpapermill operator‚Äô to orchestrate the jupyter notebooks which will be used to onboard models and publish events to Fiddler. Please refer to our [orchestration code](https://github.com/fiddler-labs/fiddler-examples/tree/master/integration-examples/airflow/DAGs).

4. The run interval can be set up in orchestration code as ‚Äòschedule_interval‚Äô in the DAG class. This interval can be based on the frequency of training and inference of your ML model.

5. Once the DAGs are set up it can be monitored on the UI. Below we can see dummy DAGs have been set up with placeholder nodes for ‚Äòdata preparation ETL‚Äô and ‚Äòmodel training/inference‚Äô. We have two DAGs - 

   a. To set up Fiddler model registration after"
"slug: ""airflow-integration""  preparing baseline data (training pipeline)

   b. To publish events to Fiddler after data preparation and ML model inference (inference pipeline)

## Label Update

An important business use case is integrating Fiddler‚Äôs ‚ÄòLabel Update‚Äô as a part of your ML workflow using Airflow. Label update can be used to update the ground truth feature in your data. This can be done using the ‚Äò‚Äã‚Äãpublish_event‚Äô api, passing the event, event_id parameters, and making the update_event parameter as ‚ÄòTrue‚Äô.  
The code to update label can be found in the [notebook](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/master/integration-examples/airflow/notebooks/Fiddler_Churn_Label_Update.ipynb)  
This notebook can be integrated to run as a part of your airflow DAG using the [sample code](https://github.com/fiddler-labs/fiddler-examples/blob/master/int"
"slug: ""airflow-integration"" egration-examples/airflow/DAGs/fiddler_event_update.py)

## Papermill Operator

```
operator_var = PapermillOperator(
        task_id=""task_name"",
        input_nb=""input_jupyter_notebook"",
        output_nb=""output_jupyter_notebook"",
        parameters={""variable_1"": ""{{ value }}""},
    )
```

## Airflow DAG

Below is an example of Model Registration Airflow DAG run history

![](../../.gitbook/assets/3fb8a21-model_registration_1.png ""model_registration_1.png"")

Model Registration Airflow DAG flow

![](../../.gitbook/assets/2891852-model_registration_2.png ""model_registration_2.png"")
"
"---
title: SageMaker Integration
slug: sagemaker-integration
excerpt: ''
createdAt: Fri May 13 2022 14:21:38 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 05 2024 00:32:06 GMT+0000 (Coordinated Universal Time)
---


## Introduction

Integrating AWS SageMaker with Fiddler allows you to monitor your deployed models easily.
This guide shows you how to use an AWS Lambda function leveraging the Fiddler Python client to read SageMaker inference logs from an Amazon S3 bucket and send those inferences to your Fiddler instance.
This setup provides real-time visibility, simplifies how you monitor your models, and gives essential insights into how your SageMaker models perform and behave.

{% hint style=""info"" %}

The Fiddler AI Observability Platform is now available within Amazon SageMaker AI, a part of SageMaker Unified Studio. 
This native integration"
" enables SageMaker customers to use Fiddler to monitor ML models privately and securely, all without leaving Amazon SageMaker AI.

Learn more about the new Amazon SageMaker AI with Fiddler native integration offering [here](https://www.fiddler.ai/blog/fiddler-delivers-native-enterprise-grade-ai-observability-to-amazon-sagemaker-ai-customers).

{% endhint %}

## Prerequisites

* An actively served SageMaker model with
  * Data capture enabled
  * Inference logs persisted to S3 in JSONL format
* Access to a Fiddler environment
  * Your SageMaker model onboarded to Fiddler.
  Check out our ML Monitoring - Simple Quick Start Guide for onboarding your models.

## Setup Overview

1. Configure SageMaker for data capture
2. Onboard your SageMaker model to Fiddler 
3. Create an AWS Lambda function for data integration between SageMaker and Fiddler
4."
" Monitor and analyze your model in Fiddler

### Detailed Steps

This guide assumes that your SageMaker model is set up and onboarded to Fiddler, as noted in the prerequisites.

* Create a new AWS Lambda function

  * Begin by creating a new AWS Lambda function.

* Set Up Environment Variables

  * In your Lambda function, create the following environment variables
    * FIDDLER_URL - The URL of your Fiddler environment (including https:// e.g. 'https://your_company_name.fiddler.ai').
    * FIDDLER_TOKEN - Your Fiddler authorization token (see [here] for more details on connecting to Fiddler with our Python client).
    * FIDDLER_MODEL_UUID - The unique identifier of your Fiddler model which can be found in the UI on the model card page or `model.id` if you have a reference to this model in your notebook using the Fiddler Python client.
    *"
" FIDDLER_MODEL_COLUMNS - Your Fiddler model's input columns. These should align with the values to expect from the SageMaker event's JSONL ""inputs"" data. These need to be in the same order as sent in the event.
    * FIDDLER_MODEL_OUTPUT_COLUMN - The name of the model output column in Fiddler. The value is from the SageMaker event's JSONL ""outputs"" data.
    * FIDDLER_TIMESTAMP_COLUMN - Optionally, the name of the timestamp column in Fiddler. This is optionally pre-defined when you onboard your model to Fiddler and it tells Fiddler to look for this column in your inferences for the datetime the event occurred. The alternative is to not included a timestamp and let Fiddler insert the current datetime as soon as your inference is uploaded: this works well for streaming real-time and near-time inferences.

{% hint style=""info"" %}

If you have provisioned Fidd"
"ler via the [SageMaker AI marketplace](https://docs.aws.amazon.com/sagemaker/latest/dg/partner-apps.html), you will also need to set these 3 environment variables for the Fiddler Python client to properly authenticate.

* AWS_PARTNER_APP_AUTH - Set to **True** to allow authentication via SageMaker AI.
* AWS_PARTNER_APP_ARN - The **ARN** of your SakeMaker AI provisioned Fiddler app instance.
* AWS_PARTNER_APP_URL - The **URL** of your SakeMaker AI provisioned Fiddler app instance.

{% endhint %}

![AWS Lambda function console on the environment variables tab showing the example variables used in this guide.](../../.gitbook/assets/aws-lambda-sagemaker-s3-data-pipeline.png)

* Set Up Trigger for Lambda Function

  * Ensure that you configure a trigger for your Lambda function so that it is invoked upon ‚ÄúObject creation‚Äù events in the S3 bucket"
" associated with your model.
  
![AWS Lambda function console on the triggers tab showing the details of the trigger used in this guide.](../../.gitbook/assets/aws-lambda-sagemaker-s3-trigger-info.png)

* Add Code to Your Lambda Function

  * Paste the example script into your new Lambda function function.

![AWS Lambda function console on the main view showing the code tab and some of the code used in this guide.](../../.gitbook/assets/aws-lambda-sagemaker-s3-python-code.png)

* Customize

  * The output of your SageMaker model's endpoint may be different than this example which would at the least require adjusting the dictionary keys used to extract the inference values in the `process_jsonl_content` function.
  * This example is not a complete production solution. You may wish to consider validation, logging, and other requirements expected by your organization's standards.


### Python Script for Lambda Function

```py
import os 
import json
import uuid"
"
import boto3
import logging
from typing import Dict, List, Any
import fiddler as fdl

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Load environment variables, customize to model and use case
url = os.getenv('FIDDLER_URL')
token = os.getenv('FIDDLER_TOKEN')
model_uuid = os.getenv('FIDDLER_MODEL_UUID')
model_columns = os.getenv('FIDDLER_MODEL_COLUMNS')
model_output_column = os.getenv('FIDDLER_MODEL_OUTPUT_COLUMN')
timestamp_column = os.getenv('FIDDLER_TIMESTAMP_COLUMN')

# Initialize AWS clients
s3_client = boto3.client('s3')

# Initialize Fiddler connection and the Fiddler Model receiving events
fdl.init(url=url, token=token)
fiddler_model = fdl.Model.get(id_=model_uuid)

def get_all_columns() -> List[str]:
    # The types of columns needed when publishing"
" depend on use case. Typically,
    # you would expect to pass at least your model inputs and output(s) and often
    # metadata such as IDs, dates, data segments, etc.
    return model_columns.split(',') + [timestamp_column] + [model_output_column]

def process_jsonl_content(event_data: str, column_names: str) -> Dict[str, Any]:
    input_data = event_data['captureData']['endpointInput']['data']
    input_values = input_data.split(',')  # Split the CSV string into a list
    
    # Extract the model prediction from 'captureData/endpointOutput/data'
    model_prediction = event_data['captureData']['endpointOutput']['data']

    # Optionally, you can set your own timestamp value on the inference occurrence time,
    # or let Fiddler default it to the time of publish.
    timestamp_value = event_data['eventMetadata']['inferenceTime']
  
    # Combine inputs and any metadata values with the output into"
" a single row
    all_values = input_values + [timestamp_value] + [model_prediction]

    # Create dictionary using zip to pair column names with their values
    return dict(zip(get_all_columns(), all_values))


def parse_sagemaker_log(log_file_path: str) -> List[Dict[str, Any]]:
    try:
        # Collect all events in a List, 1 event per JSON-line in the file
        event_rows = []
        with open(log_file_path, 'r') as file:
            for line in file:
                event = json.loads(line.strip())
                row = process_jsonl_content(event)
                event_rows.append(row)
        
        return event_rows
        
    except json.JSONDecodeError as e:
        logger.error(f'Error parsing JSONL content: {str(e)}')
        raise

def publish_to_fiddler(inferences: List[Dict[str, Any]], model: fdl.Model):
    # There are multiple options for publishing data to"
" Fiddler, check
    # the online documentation for batch, streaming, and REST API options.
    event_ids = model.publish(
        source=inferences,
        environment=fdl.EnvType.PRODUCTION
    )

    return event_ids 


def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    
    # Process each record in the event, streaming to Fiddler in batches
    for record in event['Records']:
        # Extract bucket and key information
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']

        logger.info(f'Processing new file: {key} from bucket: {bucket}')

        # Persist log file to a temporary location 
        tmp_key = key.replace('/', '')
        download_path = f'/tmp/{uuid.uuid4()}{tmp_key}'
        s3_client.download_file(bucket, key, download_path)

        # Retrieve the inference event(s"
") from the log file
        results = parse_sagemaker_log(download_path)

        # Push the inference events to Fiddler
        event_ids = publish_to_fiddler(results, fiddler_model)
        logger.info(f'Published events to Fiddler with ID(s): {event_ids}')

    return {
        'statusCode': 200,
        'body': {'message': 'Successfully processed events', 'results': results},
    }
```
"
"---
title: ML Flow Integration
slug: ml-flow-integration
excerpt: ''
createdAt: Fri Sep 15 2023 18:36:23 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# ML Flow Integration

Fiddler allows your team to onboard, monitor, explain, and analyze your models developed with [MLFlow](https://mlflow.org/).

This guide shows you how to ingest the model metadata and artifacts stored in your MLFlow model registry and use them to set up model observability in the Fiddler Platform:

1. Exporting Model Metadata from MLFlow to Fiddler
2. Uploading Model Artifacts to Fiddler for XAI

### Adding Model Information

Using the \*\*[MLFlow API](https://mlflow.org/docs/latest/python\_api/mlflow.html) \*\* you can"
" query the model registry and get the **model signature** which describes the inputs and outputs as a dictionary. You can use this dictionary to build out the [ModelInfo](broken-reference) object required to the model to Fiddler:

```python
import mlflow 
from mlflow.tracking import MlflowClient

client = MlflowClient() #initiate MLFlow Client 

#Get the model URI
model_version_info = client.get_model_version(model_name, model_version)
model_uri = client.get_model_version_download_uri(model_name, model_version_info) 

#Get the Model Signature
mlflow_model_info = mlflow.models.get_model_info(model_uri)
model_inputs_schema = model_info.signature.inputs.to_dict()
model_inputs = [ sub['name'] for sub in model_inputs_schema ]
```

Now you can use the model signature to build the Fiddler ModelInfo object:

```python
features = model_inputs

model_task = fdl.ModelTask.BINARY_CLASSIFICATION"
"

model_info = fdl.ModelInfo.from_dataset_info(
	dataset_info = client.get_dataset_info(YOUR_PROJECT,YOUR_DATASET),
	target =  ""TARGET COLUMN"", 
  dataset_id=DATASET_ID,
  model_task=model_task, 
  features=features,
  outputs=['output_column'])
```

### Uploading Model Files

Sharing your [model artifacts](../../product-guide/explainability/artifacts-and-surrogates.md#model-artifacts-and-model-package) helps Fiddler explain your models. By leveraging the MLFlow API you can download these model files:

```python
import os  
import mlflow  
from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository

model_name = ""example-model-name""  
model_stage = ""Staging""  # Should be either 'Staging' or 'Production'

mlflow.set_tracking_uri(""databricks"")  
os.makedirs(""model"", exist_ok=True)  
local_path = ModelsArtifactRepository(
"
"  f'models:/{model_name}/{model_stage}').download_artifacts("""", dst_path=""model"")  

print(f'{model_stage} Model {model_name} is downloaded at {local_path}')  
```

Once you have the model file, you can create a [package.py](../../Client\_Guide/model-task-examples/binary-classification-1.md) file in this model directory that describes how to access this model.

Finally, you can upload all the model artifacts to Fiddler:

```python
client.add_model_artifact(  
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    model_dir='model/',
)
```

Alternatively, you can skip uploading your model and use Fiddler to generate a [surrogate model](../../product-guide/explainability/artifacts-and-surrogates.md#surrogate-model) to get low-fidelity explanations for your model.
"
"---
title: Databricks Integration
slug: databricks-integration
excerpt: ''
createdAt: Thu Feb 02 2023 20:38:54 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 25 2024 21:19:00 GMT+0000 (Coordinated Universal Time)
---

# Databricks Integration

Fiddler allows your team to monitor, explain and analyze your models developed and deployed in [Databricks Workspace](https://docs.databricks.com/introduction/index.html) by integrating with [MLFlow](https://docs.databricks.com/mlflow/index.html) for model asset management and utilizing Databricks Spark environment for data management.

To validate and monitor models built on Databricks using Fiddler, you can follow these steps:

1. [Creating a Fiddler Project](databricks-integration.md#creating-a-fiddler-project)
2. [Uploading a Baseline Dataset](datab"
"ricks-integration.md#uploading-a-baseline-dataset)
3. [Adding Model Information](databricks-integration.md#adding-model-information)
4. [Uploading Model Files (for Explainability)](databricks-integration.md#uploading-model-files)
5. [Publishing Events](databricks-integration.md#publishing-events)
   1. Batch Models
   2. Live Models

### Creating a Fiddler Project

Launch a [Databricks notebook](https://docs.databricks.com/notebooks/index.html) from your workspace and run the following code:

```python
!pip install -q fiddler-client
import fiddler as fdl
```

Now that you have the Fiddler library installed, you can connect to your Fiddler environment. Please use the [UI administration guide](../../UI\_Guide/administration-ui/) to help you find your Fiddler credentials.

```python
URL ="
" """"
ORG_ID = """"
AUTH_TOKEN = """"
client = fdl.FiddlerApi(url=URL, org_id=ORG_ID, auth_token=AUTH_TOKEN)
```

Finally, you can set up a new project using:

```python
client.create_project(""YOUR_PROJECT_NAME"")
```

### Uploading a Baseline Dataset

You can grab your baseline dataset from a[ delta table](https://docs.databricks.com/getting-started/dataframes-python.html) and share it with Fiddler as a baseline dataset:

```python
baseline_dataset = spark.read.table(""YOUR_DATASET"").select(""*"").toPandas()

dataset_info = fdl.DatasetInfo.from_dataframe(baseline_upload, max_inferred_cardinality=100)
  
client.upload_dataset(
  project_id=PROJECT_ID,
  dataset_id=DATASET_ID,
  dataset={'baseline': baseline_upload},
  info=dataset_info)
```

### Adding Model Information

Using the \*\*[MLFlow API](https://"
"docs.databricks.com/reference/mlflow-api.html) \*\* you can query the model registry and get the **model signature** which describes the inputs and outputs as a dictionary. You can use this dictionary to build out the [ModelInfo](broken-reference) object required to the model to Fiddler:

```python
import mlflow 
from mlflow.tracking import MlflowClient

client = MlflowClient() #initiate MLFlow Client 

#Get the model URI
model_version_info = client.get_model_version(model_name, model_version)
model_uri = client.get_model_version_download_uri(model_name, model_version_info) 

#Get the Model Signature
mlflow_model_info = mlflow.models.get_model_info(model_uri)
model_inputs_schema = model_info.signature.inputs.to_dict()
model_inputs = [ sub['name'] for sub in model_inputs_schema ]
```

Now you can use the model signature to build the Fiddler ModelInfo object :

```python
features"
" = model_inputs

model_task = fdl.ModelTask.BINARY_CLASSIFICATION

model_info = fdl.ModelInfo.from_dataset_info(
	dataset_info = client.get_dataset_info(YOUR_PROJECT,YOUR_DATASET),
	target =  ""TARGET COLUMN"", 
  dataset_id=DATASET_ID,
  model_task=model_task, 
  features=features,
  outputs=['output_column'])
```

### Uploading Model Files

Sharing your [model artifacts](../../Client\_Guide/uploading-model-artifacts.md) helps Fiddler explain your models. By leveraging the MLFlow API you can download these model files:

```python
import os  
import mlflow  
from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository

model_name = ""example-model-name""  
model_stage = ""Staging""  # Should be either 'Staging' or 'Production'

mlflow.set_tracking_uri(""databricks"")  
os.makedirs(""model"", exist_ok=True)  
local_path"
" = ModelsArtifactRepository(
  f'models:/{model_name}/{model_stage}').download_artifacts("""", dst_path=""model"")  

print(f'{model_stage} Model {model_name} is downloaded at {local_path}')  
```

Once you have the model file, you can create a [package.py](../../Client\_Guide/model-task-examples/binary-classification-1.md) file in this model directory that describes how to access this model.

Finally, you can upload all the model artifacts to Fiddler:

```python
client.add_model_artifact(  
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    model_dir='model/',
)
```

Alternatively, you can skip uploading your model and use Fiddler to generate a [surrogate model](../../Client\_Guide/surrogate-models-client-guide.md) to get low-fidelity explanations for your model.

### Publishing Events

Now you can publish all the events from your models."
" You can do this in two ways:

#### Batch Models

If your models run batch processes with your models or your aggregate model outputs over a timeframe, then you can use the table change feed from Databricks to select only the new events and send them to Fiddler:

```python
changes_df = spark.read.format(""delta"") \
.option(""readChangeFeed"", ""true"") \
.option(""startingVersion"",last_version) \
.option(""endingVersion"", new_version) \
.table(""inferences"").toPandas()


client.publish_events_batch(
   project_id=PROJECT_ID,
   model_id=MODEL_ID,
   batch_source=changes_df,
   timestamp_field='timestamp')

```

#### Live Models

For models with live predictions or real-time applications, you can add the following code snippet to your prediction pipeline and send every event to Fiddler in real-time:

```python
example_event = model_output.toPandas() #turn your model's ouput in a pandas data"
"fram 

client.publish_event(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    event=example_event,
    event_id='event_001',
    event_timestamp=1637344470000)
```

_Support for Inference tables and hosted endpoints is coming soon!_
"
"---
title: ""ML Platform Integrations""
slug: ""ml-platform-integrations""
excerpt: """"
hidden: false
createdAt: ""Wed Jun 22 2022 14:27:52 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
"
"---
title: ""Datadog Integration""
slug: ""datadog-integration""
excerpt: """"
hidden: false
createdAt: ""Wed Jun 21 2023 15:21:52 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Fiddler offers an integration with Datadog which allows Fiddler and Datadog customers to bring their AI Observability metrics from Fiddler into their centralized Datadog dashboards.  Additionally, a Fiddler license can now be procured through the [Datadog Marketplace](https://www.datadoghq.com/blog/tag/datadog-marketplace/). This [integration](https://www.datadoghq.com/blog/monitor-machine-learning-models-fiddler/) enables you to centralize your monitoring of ML models and the applications that utilize them within one unified platform.

## Integrating Fiddler with Datadog

Instructions for integrating Fiddler with Datadog can be found on the ""Integrations"" section of your Datadog console.  Simply search for ""Fiddler"" and follow the installation instructions provided on the ""Configure"" tab.  Please reach out to [support@fiddler.ai](mailto:support@fiddler.ai) with any issues or questions.

![](../../.gitbook/assets/3f9dcd9-Screenshot_2023-06-21_at_10.28.17_AM.png)

![](../../.gitbook/assets/9fa9503-Screenshot_2023-06-21_at_10.31.54_AM.png)

![](../../.gitbook/assets/218dfc2-Screenshot_2023-06-21_at_10.45.14_AM.png)
"
"---
title: Updating model artifacts
slug: updating-model-artifacts
excerpt: >-
  This document explains how to update a model artifact or surrogate in Fiddler
  using the `Model.update_artifact` or `Model.update_surrogate` functions,
  allowing you to replace a surrogate model or your own uploaded model.
metadata:
  description: >-
    This document explains how to update a model artifact or surrogate in
    Fiddler using the `Model.update_artifact` or `Model.update_surrogate`
    functions, allowing you to replace a surrogate model or your own uploaded
    model.
  image: []
  robots: index
createdAt: Fri Apr 05 2024 12:34:08 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Apr 19 2024 13:34:58 GMT+0000 (Coordinated Universal Time)
---

# Updating Model Artifacts

If you need to update an existing model artifact or model surrogate in Fiddler, you can use the [update\_artifact()](../Python\_Client\_3-x/api-methods-30.md#update\_artifact) or [update\_surrogate()](../Python\_Client\_3-x/api-methods-30.md#update\_surrogate) functions. These functions allow you to replace existing model surrogates or your own uploaded model artifacts as needed.

#### Update an existing model artifact

Once you have prepared the [model artifacts directory](../product-guide/explainability/artifacts-and-surrogates.md), you can update your model.

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'

project_id = fdl.Project.from_name(PROJECT_NAME).id
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project_id)

job = model.update_artifact(
    model_dir=MODEL_DIR,
)
job.wait()
```

#### Update an existing model surrogate

One reason to update an existing surrogate is if you wish to use an updated baseline dataset from the one used originally. Fiddler will use this updated dataset when creating the model surrogate

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'
DATASET_NAME = 'YOUR_UPDATED_DATASET_NAME'

project_id = fdl.Project.from_name(PROJECT_NAME).id
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project_id)
dataset = fdl.Dataset.from_name(name=DATASET_NAME, model_id=model.id)

model.update_surrogate(
  dataset_id=dataset.id
)
```

{% include ""../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Create a Project and Onboard a Model for Observation
slug: create-a-project-and-model
excerpt: >-
  This document explains the importance of creating a project in Fiddler to
  organize models and control access. It provides instructions on creating a
  project, listing all projects, defining a ModelSpec, and onboarding a model to
  Fiddler.
metadata:
  description: >-
    This document explains the importance of creating a project in Fiddler to
    organize models and control access. It provides instructions on creating a
    project, listing all projects, defining a ModelSpec, and onboarding a model
    to Fiddler.
  image: []
  robots: index
createdAt: Wed Mar 27 2024 06:16:41 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Apr 19 2024 11:41:04 GMT+0000 (Coordinated Universal Time)
"
"---

# Create a Project and Onboard a Model for Observation

### What is a Project?

A project helps organize models under observation and serves as the authorization unit to manage access to your models. To onboard a model to Fiddler, you need to have a project to associate it with. Once Fiddler's Python client is connected to your environment, you can either create a new project or use an existing one to onboard your model.

### Create a Project

Using the Python client, you can create a project by calling the Project object's create function after setting the desired project name.

```python
# Please be mindful that the Project name must be unique and should not have spaces or special characters.

PROJECT_NAME = 'quickstart_example'

## Create the Project
project = fdl.Project(name=PROJECT_NAME)
project.create()
print(f'New project created with id = {project.id}')

```

You should now see the newly created project on the Projects page in the Fiddler"
" UI.

![](../.gitbook/assets/b58d3b2-image.png)

### List All Projects

Using an existing project, you may list all the projects that you are authorized to view.

```python
for project in fdl.Project.list():
    print(f'Project: {project.id} - {project.name}')

## This will print ALL project IDs and Names that you have access to.
```

### Onboarding a Model

To onboard a model you need to define a **ModelSpec** and optionally a **Model Task**. If you do not specify a model task during Model creation it can be set later or left unset.

#### Define the ModelSpec

A **ModelSpec object** defines what role each column of your inference data serves in your model.

Fiddler supports five column roles:

1. Inputs (features),
2. Outputs (predictions),
3. Targets (ground truth labels),
4. Metadata (additional information passed along with the inference)
5. Custom"
" features (additional information that Fiddler should generate like embeddings or enrichments)

```python
model_spec = fdl.ModelSpec(
    inputs=['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary'],
    outputs=['probability_churned'],
    targets=['Churned'],
    decisions=[],
    metadata=[],
    custom_features=[],
)
```

#### Define the Model Task

Fiddler supports a variety of model tasks. Create a `ModelTask` object and an additional `ModelTaskParams` object to specify the ordering of labels. For a detailed breakdown of all supported model tasks, [click here](../Python\_Client\_3-x/api-methods-30.md#modeltask).

```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION
task_params = fdl.ModelTaskParams(target_class_order=['no', 'yes"
"'])
```

#### Infer the Model Schema

Onboard the model schema to Fiddler by passing in:

1. the data sample dataframe, called `sample_df` below
2. the `ModelSpec` object
3. the `ModelTask` and `ModelTaskParams` objects
4. the event/inference ID column and event/inference timestamp columns

```python
MODEL_NAME = 'my_model'

model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=fdl.Project.from_name(PROJECT_NAME).id,
    source=sample_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params,
    event_id_col=id_column,
    event_ts_col=timestamp_column
)
```

Depending on the input size this step might take a moment to complete. It is not a local operation, but requires uploading the sample dataframe to the Fiddler HTTP API.

#### Review and Edit the Schema

Schema inference is"
" just a helping hand. The resulting schema needs human review and potentially some edits, as documented in the section titled [Customizing your Model Schema](customizing-your-model-schema.md).

#### Onboard the Model

After making sure the schema looks good, the model can be onboarded with the following API call:

```python
model.create()
```

{% include ""../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Specifying Custom Missing Value Representations
slug: specifying-custom-missing-value-representations
excerpt: ''
createdAt: Tue Aug 30 2022 18:19:30 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 22 2024 18:41:20 GMT+0000 (Coordinated Universal Time)
---

# Specifying Custom Missing Value Representations

There may be cases where your data contains missing values, but they are represented in a nonstandard way (other than `null` or `NaN`). As an example, suppose an upstream system uses ""-1.0"" or ""-999"" in place of `null` for a particular Float column. Fiddler offers a method to specify **your own missing value representations for each column** when defining your model schema. See below for an example.

***

You can modify your `fdl.ModelSchema` object just before onboarding your model to include details about which values should be replaced with nulls when publishing data to Fiddler.

```python
model.schema['my_column'].replace_with_nulls = [
  '-1.0',
  '-999'
]
```

{% include ""../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Publishing Inferences
slug: publishing-inferences
excerpt: >-
  This document explains how to publish pre-production and production data to a
  Fiddler model for inference data analysis. It provides code examples for batch
  publishing pre-production data and streaming production data.
metadata:
  description: >-
    This document explains how to publish pre-production and production data to
    a Fiddler model for inference data analysis. It provides code examples for
    batch publishing pre-production data and streaming production data.
  image: []
  robots: index
createdAt: Thu Mar 28 2024 11:36:51 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Apr 19 2024 11:54:04 GMT+0000 (Coordinated Universal Time)
---

# Publishing Inferences

Once a model's schema has been onboarded, you can publish events, also known as inferences, so that Fiddler can analyze that data to ensure expected performance. Event data can be sent as either pre-production or production data. Pre-production data refers to static datasets, such as model training or testing data, while production data consists of the model's time series data that is actively being monitored.

### Publish Pre-production Data to a Model

Fiddler allows only batch publication of pre-production data as it is typically available in its entirety. You can publish multiple pre-production datasets to a model. You may use a dataframe, parquet file, or CSV file for uploading a dataset.

```python
job = model.publish(
    source=DATASET_FILE_PATH,
    environment=fdl.EnvType.PRE_PRODUCTION,
    dataset_name=DATASET_NAME,
)
# The publish() method is asynchronous. Use the publish job's wait() method 
# if sychronous behavior is desired.
# job.wait() 
```

### Publish Production Data to a Model

#### Publish Events as a Stream

Fiddler supports event streams to be published to a model.

```python
model.event_ts_col = 'timestamp_col'
model.event_id_col = 'event_id_col'
DATASET_FILE_PATH = ""dataset.csv""

df = pd.read_csv(DATASET_FILE_PATH)

# Generate event_id which is later needed for label updates
df[model.event_id_col] = [str(uuid4()) for _ in range(len(df))]
_add_timestamp(df=df, event_ts_col=model.event_ts_col)

event_ids = model.publish(source=df)

print(f'{len(event_ids)} events published')
```

#### Publish Production Events - Batch

The Fiddler client supports publishing micro batch streams (up to 1K events, configurable)

```python
events = df.sample(10).to_dict(orient='records') # this will give list of event dictionaries

events_ids = model.publish(source=events)

print(f'{len(events_ids)} events published')
```

#### Publish Production Label Updates - Stream

Fiddler supports updates of existing events for provided event ids.

```python
updated_events = [
        {
            model.event_id_col: event_id,
            model.spec.targets[0]: model.task_params.target_class_order[0],
        }
        for event_id in df.sample(100)[model.event_id_col]
]

events_ids = model.publish(source=updated_events, update=True)

print(f'{len(events_ids)} events updated')
```

{% include ""../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Customizing Your Model Schema
slug: customizing-your-model-schema
excerpt: ''
createdAt: Mon May 23 2022 16:36:05 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 25 2024 21:05:55 GMT+0000 (Coordinated Universal Time)
---

# Customizing your Model Schema

There can be occasions when the `fdl.ModelSchema` object generated by `fdl.Model.from_data` infers a column's data type **differently than the type intended** by the model developer. In these cases you can modify the ModelSchema columns as needed prior to creating the model in Fiddler.

Let's walk through an example of how to do this.

***

Suppose you've loaded in a dataset as a pandas DataFrame.

```python
import pandas as pd

df = pd.read_csv('example_dataset.csv')
```

Below is an example of what is displayed upon inspection.

"
"![](../.gitbook/assets/3ffd956-example\_df\_1.png)

***

You then create a `fdl.Model` object by inferring the column schema details from this DataFrame.

```python
model = fdl.Model.from_data(
  name='my_model',
  project_id=PROJECT_ID,
  source=df
)
```

Below is an example of what is displayed upon inspection of `model.schema`.

![](../.gitbook/assets/8be7229-Screen\_Shot\_2024-04-22\_at\_2.26.13\_PM.png)

```json
```

Upon inspection you may notice that **a few things are off**:

1. The [value range](customizing-your-model-schema.md#modifying-a-columns-value-range) of `output_column` is set to `[0.01, 0.99]`, when it should really be `[0.0, 1.0]`.
2. There are"
" no [possible values](customizing-your-model-schema.md#modifying-a-columns-possible-values) set for `feature_3`.
3. The [data type](customizing-your-model-schema.md#modifying-a-columns-data-type) of `feature_3` is set to [`fdl.DataType.STRING`](../Python\_Client\_3-x/api-methods-30.md#datatype), when it should really be [`fdl.DataType.CATEGORY`](../Python\_Client\_3-x/api-methods-30.md#datatype).

What's the downside of not making sure that ranges and categories are reviewed and properly set? A production traffic event that encodes a number outside of the specified value range or a category value that is not in the set of valid category values will further down the line be flagged as a so-called Data Integrity violation. Depending on the alerting config, this may result in an alert. It's also worth noting however that an event which has a violation in its columns is"
" still processed, and metrics that can be generated are still generated.

The below examples demonstrate how to address each of the issues noted:

### Modifying a Column‚Äôs Value Range

Let's say we want to modify the range of `output_column` in the above `fdl.Model` object to be `[0.0, 1.0]`.

You can do this by setting the `min` and `max` of the `output_column` column.

```python
model.schema['output_column'].min = 0.0
model.schema['output_column'].max = 1.0
```

### Modifying a Column‚Äôs Possible Values

Let's say we want to modify the possible values of `feature_3` to be `['Yes', 'No']`.

You can do this by setting the `categories` of the `feature_3` column.

```python
model.schema['feature_3'].categories = ['Yes', 'No']
```

### Mod"
"ifying a Column‚Äôs Data Type

Let's say we want to modify the data type of `feature_3` to be [`fdl.DataType.CATEGORY`](../Python\_Client\_3-x/api-methods-30.md#datatype).

You can do this by setting the `data_type` of the `feature_3` column.

```python
model.schema['feature_3'].data_type = fdl.DataType.CATEGORY
```

**Note**: when converting a column to a CATEGORY, you must also set the the list of unique possible values:

```python
model.schema['feature_3'].categories = ['Yes', 'No']
```

**Note**: if converting a column from numeric (integer or float) to a category, you must also remove the min/max numeric range values that were automatically calculated from the sample data.

```python
model.schema['output_column'].min = None
model.schema['output_column'].max = None
```

A complete example might look like"
" this:

```python
sample_data_df = pd.read_csv('some.csv')

# configure your ModelSpec here
# model_spec = ...

# configure your ModelTask here, or use NOT_SET
# model_task = ...

# Infer your model's schema from a sample of data
ml_model = fdl.Model.from_data(
  source=sample_data_df,
  name=MODEL_NAME,
  version=VERSION_NAME,
  project_id=project.id,
  spec=model_spec,
  task=model_task
)

# Make any adjustmens to the inferred ModelSchema BEFORE creating the model
ml_model.schema['feature_3'].data_type = fdl.DataType.CATEGORY
ml_model.schema['feature_3'].categories = ['0', '1']

# The original datatype was inferred as an integer, but we preferred a category.
# Clear out the min and max values derived from the sample data as it does not apply to categories
ml_model.schema['output_column'].min = None
ml"
"_model.schema['output_column'].max = None

# Now create the model with the inferred schema and your overrides
ml_model.create()
```

{% include ""../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Alerts with Fiddler Client
slug: alerts-with-fiddler-client
excerpt: >-
  Users can set up alert rules in Fiddler using the API client to add, and
  delete, get a list of all alert rules, and get a list of triggered alerts. The
  API allows for creating different types of alert rules such as Data Integrity
  and Performance alerts.
metadata:
  description: >-
    Users can set up alert rules in Fiddler using the API client to add, and
    delete, get a list of all alert rules, and get a list of triggered alerts.
    The API allows for creating different types of alert rules such as Data
    Integrity and Performance alerts.
  image: []
  robots: index
createdAt: Fri Apr 05 2024 05:40:12 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 25 2024 21:09:45"
" GMT+0000 (Coordinated Universal Time)
---

# Alerts with Fiddler Client

In addition to using the Fiddler UI, users have the flexibility to set up alert rules using the Fiddler API client to enable the following workflows:

* Add alert rule(s)
* Delete alert rule(s)
* Get the list of all alert rule(s)
* Get the list of triggered alert(s)

> üìò The user guide for setting up alert rules in the Fiddler UI is [here](../UI\_Guide/monitoring-ui/alerts-with-fiddler-ui.md).

### Add an Alert Rule

The Fiddler client can be used to create a variety of alert rules types including **Data Drift**, **Performance**, **Data Integrity**, and **Service Metrics**.

#### Example 1: Data Integrity Alert Rule to Compare Against a Raw Percentage Value

Using the [fdl.AlertRule.create()](../Python\_Client\_3-x/api-methods-30"
".md#create) API method, let's set up a Data Integrity alert for the **age** column that triggers a warning email notification when published events exceed 5% null values in a 1 day bin and a critical email notification when exceeding 10% null values in a 1 day bin. Notice the comparison type `compare_to=fdl.CompareTo.RAW_VALUE` which considers only the calculated metric value for the given `bin_size` and threshold values.

```python
import fiddler as fdl

MODEL_ID = '299c7b40-b87c-4dad-bb94-251dbcd3cbdf'

alert_rule = fdl.AlertRule(
    name='Banke Churn Missing Values Percent',
    model_id=MODEL_ID,
    metric_id='null_violation_percentage',
    priority=fdl.Priority.HIGH,
    compare_to=fdl.CompareTo.RAW_VALUE,
    condition=fdl.AlertCondition.GREATER,
    bin_size"
"=fdl.BinSize.DAY,
    critical_threshold=0.1,
    warning_threshold=0.05,
    columns=['age'],
).create()

notifications = alert_rule.set_notification_config(
    emails=['abc@xyz.com', 'admin@xyz.com'],
)
```

#### Example 2: Performance Alert Rule to Compare Against a Previous Time Window

This example of a Performance alert rule uses comparison type `compare_to=fdl.CompareTo.TIME_PERIOD`, also known as a relative alert, which considers the calculated metric value for the given `bin_size` and threshold values with respect to the same bin in a previous time period. In this case the `compare_bin_delta=1` parameter indicates the alert rule should compare the _precision_ metric for the previous day with the _precision_ metric for the current day. To compare the day's metric to the day's metric from 7 days ago, set `compare_bin_delta=7`.

```python
import fiddler as f"
"dl

MODEL_ID='4531bfd9-2ca2-4a7b-bb5a-136c8da09ca2'
alert_rule = fdl.AlertRule(
  name='Bank Churn Precision Relative',
  model_id=MODEL_ID,
  metric_id='precision',
  priority=fdl.Priority.HIGH,
  compare_to=fdl.CompareTo.TIME_PERIOD,
  compare_bin_delta=1,
  condition=fdl.AlertCondition.GREATER,
  bin_size=fdl.BinSize.DAY,
  critical_threshold=0.1,
  warning_threshold=0.05,
).create()

notifications = alert_rule.set_notification_config(
  emails=['abc@xyz.com', 'admin@xyz.com'],
)
```

> üöß Please note, the possible values for compare\_bin\_delta vs bin\_size are:

| Bin Size      | Allowed Compare bin delta             |
| ------------- | ------------------------------------- |
| BinSize.Hour  | \["
"1, 24, 24 \* 7, 24 \* 30, 24 \* 90] |
| BinSize.Day   | \[1, 7, 30, 90]                       |
| BinSize.Week  | \[1]                                  |
| BinSize.Month | \[1]                                  |

### Retrieving Alert Rules

The [fdl.AlertRule.list()](../Python\_Client\_3-x/api-methods-30.md#list) API method is used to get a list of all alert rules with respect to the filtering parameters, returning a Python Iterator of matching alert rules. Note that all parameters are optional.

```python
import fiddler as fdl

MODEL_ID='4531bfd9-2ca2-4a7b-bb5a-136c8da09ca2'
alert_rules = fdl.AlertRule.list(
  model_id=MODEL_ID,# Optional parameter
  metric_id"
"='jsd', # Optional parameter
  columns=['age'], # Optional parameter
  ordering=['critical_threshold'], # Add **-** prefix for descending sort ['-critical_threshold']
)
```

### Delete an Alert Rule

Alert rules can be deleted by calling the [delete()](../Python\_Client\_3-x/api-methods-30.md#delete) function on an instantiated fdl.AlertRule object. An alert rule can be retrieved by either the [fdl.AlertRule.list()](../Python\_Client\_3-x/api-methods-30.md#list) API method using filters or [fdl.AlertRule.get()](../Python\_Client\_3-x/api-methods-30.md#get) API method using the unique identifier which is found in the Alert Rule tab of the Fiddler Alerts page.

![Alert Rule list with Copy alert rule ID highlighted in an alert rule's context menu](../.gitbook/assets/client-guide-alerts-uuid-example.png)

```python"
"
# Delete from a list of alerts
MODEL_ID='4531bfd9-2ca2-4a7b-bb5a-136c8da09ca2'
alert_rules = fdl.AlertRule.list(
  model_id=MODEL_ID
)

for alert_rule in alert_rules: 
    if alert_rule.name == 'Bank Churn Precision Relative': 
        alert_rule.delete()
        break

# Delete using the alert rule's unique identifier
ALERT_RULE_ID = '6da9c3c0-a9fa-4ab6-8b64-8d07b0736e77'
rule = fdl.AlertRule.get(id_=ALERT_RULE_ID)
rule.delete()
```

### Get Triggered Alerts

Use the [fdl.AlertRecord.list()](../Python\_Client\_3-x/api-methods-30.md#list-1) API method to get the triggered alerts for a given alert rule. Note the alert rule unique"
" identifier is required. The time range and sort order parameters are optional.

```python
from datetime import datetime

triggered_alerts = fdl.AlertRecord.list(
  alert_rule_id=ALERT_RULE_ID,
  start_time=datetime(2024, 9, 1), # optional
  end_time=datetime(2024, 9, 24), # optional 
  ordering = ['alert_time_bucket'], # ['-alert_time_bucket'] for descending, optional.
)
```

#### Notifications

After creating a new alert rule, assign the notification method for Fiddler to use when the alert rule is triggered. Currently Fiddler supports email, PagerDuty, Slack webhooks, and custom webhooks for notification types. You can specify 1 or more notification types and each type supports 1 or more recipients. Note that PagerDuty, Slack webhooks, and custom webhooks must be defined in advance by your Fiddler administrator.

The following example creates a notification"
" configuration using email addresses, PagerDuty services, and a Slack webhook:

```python
ALERT_RULE_ID = '72e8835b-cde2-4dd2-a435-a35d4b51196b'
rule = fdl.AlertRule.get(id_=ALERT_RULE_ID)

rule.set_notification_config(
  emails=['abc@xyz.com', 'admin@xyz.com'],
  webhooks=['8b403d99-530a-4c5a-a519-89688d65ddc1'], # Webhook UUID 
  pagerduty_services = ['pagerduty_service_1','pagerduty_service_2'], # PagerDuty service names
  pagerduty_severity = 'critical' # Only applies to PagerDuty, ignored otherwise 
)
```

{% include ""../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Adding a Surrogate Model
slug: surrogate-models-client-guide
excerpt: ''
createdAt: Tue Dec 13 2022 22:22:39 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Apr 19 2024 13:23:06 GMT+0000 (Coordinated Universal Time)
---

# Adding a Surrogate Model

Fiddler‚Äôs explainability features require a model on the backend that can generate explanations for you.

> üìò If you don't want to or cannot upload your actual model file, Surrogate Models serve as a way for Fiddler to generate approximate explanations.

A surrogate model **is built automatically** when you call [`add_surrogate`](../Python\_Client\_3-x/api-methods-30.md#add\_surrogate) on an existing model that has a [baseline dataset](creating-a-baseline-dataset.md) defined. You just need to provide a few key details on how your model operates during onboarding.

### Surrogate Model prerequisites:

* An onboarded model with:
  * A defined model task (regression, binary classification, etc.)
  * A target column (ground truth labels)
  * An output column (model predictions)
  * Model feature columns
  * A baseline dataset

#### Update the artifact

```python
DATASET_NAME = 'YOUR_DATASET_NAME'

dataset = fdl.Dataset.from_name(name=DATASET_NAME, model_id=model.id)

job = model.add_surrogate(
    dataset_id=dataset.id
)
job.wait()
```

{% include ""../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Installation and Setup
slug: installation-and-setup
excerpt: >-
  The document provides instructions on how to install, import, authorize, and
  connect the Fiddler Python SDK client to your Fiddler environment for use in
  Jupyter Notebooks or automated pipelines
metadata:
  description: >-
    The document provides instructions on how to install, import, authorize, and
    connect the Fiddler Python SDK client to your Fiddler environment for use in
    Jupyter Notebooks or automated pipelines.
  image: []
  robots: index
createdAt: Tue May 10 2022 17:14:02 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 25 2024 21:03:35 GMT+0000 (Coordinated Universal Time)
---

# Installation and Setup

Fiddler offers a **Python SDK client** that lets you connect directly to your Fiddler environment from a"
" Jupyter Notebook or automated pipeline.

***

### Install the Fiddler Client

The client is available for download from PyPI via pip:

```python
pip install -q fiddler-client
```

### Import the Fiddler Client

Once you've installed the client, you can import the `fiddler` package into any Python script:

```python
import fiddler as fdl
```

***

### Authorize the Client

To use the Fiddler client, you will need **authorization details** that contain

* The [URL](installation-and-setup.md#finding-your-url) you are connecting to
* An [authorization token](installation-and-setup.md#finding-your-authorization-token) for your user

#### Find your URL

The URL should point to **where Fiddler has been deployed** for your organization.

On-premise customers will use the URL specified by their IT operations team. If using Fiddler‚Äôs managed cloud service you will have"
" been provided a unique URL is, and it will be in one of the forms shown below.

```html
# Managed SaaS
https://<YOUR UNIQUE APP NAME>.fiddler.ai

# Managed SaaS Peering
https://<YOUR UNIQUE APP NAME>.cloud.fiddler.ai
```

#### Find your Authorization Token

To find your authorization token, navigate to the **Settings** page, click the **Credentials** tab, and then use the **Create Key** button (if there is not already a authorization token for your user).

![](../.gitbook/assets/890613d-Screenshot\_2024-04-01\_at\_6.30.42\_AM.png)

### Connect the Client to Fiddler

Once you've located the URL of your Fiddler environment and your authorization token, you can connect the Fiddler client to your environment.

```python
URL = 'https://app.fiddler.ai'
AUTH_TOKEN = ''"
" #Specify your authorization token available in the Fiddler Settings page - Credentials tab

# Connect to the Fiddler client
# This call will also validate the client vs server version compatibility.

fdl.init(url=URL, token=AUTH_TOKEN)

print(f'Client version: {fdl.__version__}')
print(f'Server version: {fdl.conn.server_version}')
print(f'Organization id: {fdl.conn.organization_id}')
print(f'Organization name: {fdl.conn.organization_name}')
```

### Set Log Level

Set the log level for the desired verbosity.

```
# Default log level is INFO
fdl.set_logging()

# Set DEBUG log level
fdl.set_logging(level=logging.DEBUG)
```

> üìò Info
>
> For detailed documentation on the Fiddler Python client‚Äôs many features, check out the [API reference](../Python\_Client\_3-x/api-methods-30.md) section.

{% include ""../.gitbook/includes"
"/main-doc-dev-footer.md"" %}
"
"---
title: Uploading model artifacts
slug: uploading-model-artifacts
excerpt: >-
  This document provides instructions on how to upload a model artifact into
  Fiddler by creating the model and updating the artifact.
metadata:
  description: >-
    This document provides instructions on how to upload a model artifact into
    Fiddler by creating the model and updating the artifact.
  image: []
  robots: index
createdAt: Fri Apr 05 2024 12:04:04 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Apr 19 2024 13:25:53 GMT+0000 (Coordinated Universal Time)
---

# Uploading Model Artifacts

Before uploading your model artifact into Fiddler, you need to add the model using [model.create](create-a-project-and-model.md) function as well as [create a baseline for it](creating-a-baseline-dataset.md).

Once you have prepared the [model artifact directory](../product-guide/explainability/artifacts-and-surrogates.md), you can upload your model.

#### Upload the artifact

```python

job = model.add_artifact(
    model_dir=MODEL_ARTIFACTS_DIR,
)
job.wait()
```

{% include ""../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: ""Specifying Custom Features""
slug: ""vector-monitoring""
excerpt: ""\""Patented Fiddler Technology\""""
hidden: false
createdAt: ""Thu Oct 19 2023 19:24:35 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Apr 19 2024 13:33:04 GMT+0000 (Coordinated Universal Time)""
---
# Vector Monitoring for Unstructured Data

```python
CF1 = fdl.CustomFeature.from_columns(['f1','f2','f3'], custom_name = 'vector1')
CF2 = fdl.CustomFeature.from_columns(['f1','f2','f3'], n_clusters=5, custom_name = 'vector2')
CF3 = fdl.TextEmbedding(name='text_embedding',column='embedding',source_column='text')
CF4 = fdl.ImageEmbedding(name='image_embedding',column='embedding',source_column='image_url')
```

### Passing Custom Features List to Model Spec

```python
model_spec = fdl.ModelSpec(
    inputs=['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance'],
    outputs=['probability_churned'],
    targets=['Churned'],
    decisions=[],
    metadata=[],
    custom_features=[CF1,CF2,CF3,CF4],
)
```

> üìò Quick Start for NLP Monitoring
> 
> Check out our [Quick Start guide for NLP monitoring](../QuickStart_Notebooks/simple-nlp-monitoring-quick-start.md) for a fully functional notebook example.
"
"---
title: Creating a Baseline Dataset
slug: creating-a-baseline-dataset
excerpt: >-
  This document explains the importance of setting up a baseline dataset for
  monitoring data integrity in production. It provides examples of creating
  different types of baselines such as static pre-production, static production,
  and rolling production.
metadata:
  description: >-
    This document explains the importance of setting up a baseline dataset for
    monitoring data integrity in production. It provides examples of creating
    different types of baselines such as static pre-production, static
    production, and rolling production.
  image: []
  robots: index
createdAt: Thu Apr 04 2024 09:18:41 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Apr 19 2024 12:22:36 GMT+0000 (Coordinated Universal Time)
---

# Creating a Baseline Dataset

To monitor drift or data integrity issues in production data, baseline data is needed for comparison. A baseline dataset is a **representative sample** of the data you expect to see in production. It represents the ideal data that your model works best on. For this reason, **a baseline dataset should be sampled from your model‚Äôs training set.**

**A few things to keep in mind when designing a baseline dataset:**

* It‚Äôs important to include **enough data** to ensure you have a representative sample of the training set.
* You may want to consider **including extreme values (min/max)** of each column in your training set so you can properly monitor range violations in production data. However, if you choose not to, you can manually specify these ranges before uploading, see [customizing your dataset schema](customizing-your-model-schema.md).

## Baseline Type: Static Pre-production

```python
dataset = next(fdl.Dataset.list(model_id=model.id))

static_pre_prod_baseline = fdl.Baseline(
    name='static_preprod_1',
    model_id=model.id,
    environment=fdl.EnvType.PRE_PRODUCTION,
    type_=fdl.BaselineType.STATIC,
    dataset_id=dataset.id,
)
static_pre_prod_baseline.create()

print(f'Static pre-production baseline created with id - {static_pre_prod_baseline.id}')
```

## Baseline Type: Static Production

```python
static_prod_baseline = fdl.Baseline(
    name='static_prod_1',
    model_id=model.id,
    environment=fdl.EnvType.PRODUCTION,
    type_=fdl.BaselineType.STATIC,
    start_time=(datetime.now() - timedelta(days=0.5)).timestamp(),
    end_time=(datetime.now() - timedelta(days=0.25)).timestamp(),
)
static_prod_baseline.create()

print(f'Static production baseline created with id - {static_prod_baseline.id}')
```

## Baseline Type: Rolling Production

```python
rolling_prod_baseline = fdl.Baseline(
    name='rolling_prod_1',
    model_id=model.id,
    environment=fdl.EnvType.PRODUCTION,
    type_=fdl.BaselineType.ROLLING,
    window_bin_size=fdl.WindowBinSize.WEEK,
    offset_delta=4,
)
rolling_prod_baseline.create()

print(f'Rolling production baseline created with id - {rolling_prod_baseline.id}')
```

## List Baselines

```python
for baseline in fdl.Baseline.list(model_id=model.id):
    print(f'Dataset: {baseline.id} - {baseline.name}')
```

{% include ""../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Multi-class Classification Model Package.py
slug: multiclass-classification-1
excerpt: ''
createdAt: Tue Apr 19 2022 20:12:40 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# Multiclass Classification 1

> üöß Note
>
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](../uploading-model-artifacts.md).

Suppose you would like to upload a model artifact for a **multiclass classification model**.

Following is an example of what the `package.py` script might look like.

```python
import pickle
from pathlib import Path
import pandas as pd

PACKAGE_PATH = Path(__file__).parent

OUTPUT_COLUMNS = ['probability_0', 'probability_1', 'probability_2']

class MyModel:

    def __init__(self):
        
        # Load the model
        with open('{PACKAGE_PATH}/model.pkl', 'rb') as pkl_file:
            self.model = pickle.load(pkl_file)

    def predict(self, input_df):
        
        # Store predictions in a DataFrame
        return pd.DataFrame(self.model.predict_proba(input_df), columns=OUTPUT_COLUMNS)

def get_model():
    return MyModel()
```

Here, we are assuming that the model prediction columns that have been specified in the [`fdl.ModelSpec`](../../Python\_Client\_3-x/api-methods-30.md#modelspec) object are called `probability_0`, `probability_1`, and `probability_2`.

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Ranking Model Package.py
slug: uploading-a-ranking-model-artifact
excerpt: ''
createdAt: Mon Oct 31 2022 21:23:47 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# Uploading A Ranking Model Artifact

> üöß Note
>
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](../uploading-model-artifacts.md).

Suppose you would like to upload a model artifact for a **ranking model**.

Following is an example of what the `package.py` script may look like.

```python
import pickle
from pathlib import Path
import pandas as pd

PACKAGE_PATH = Path(__file__).parent

class ModelPackage:

    def __init__(self):
        self.output_columns = ['score']
        with open('{PACKAGE_PATH}/model.pkl', 'rb') as infile:
            self.model = pickle.load(infile)
    
    def predict(self, input_df):
        pred = self.model.predict(input_df)
        return pd.DataFrame(pred, columns=self.output_columns)
    
def get_model():
    return ModelPackage()
```

Here, we are assuming that the model prediction column that has been specified in the [`fdl.ModelSpec`](../../Python\_Client\_3-x/api-methods-30.md#modelspec) object is called `score`.

Please checkout this [quickstart notebook](../../QuickStart\_Notebooks/ranking-model.md) to work through an example of onboarding a ranking model on to Fiddler.

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Package.py Examples
slug: model-task-examples
excerpt: ''
createdAt: Tue Apr 19 2022 20:14:39 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# Model Task Examples



{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}
"
"---
title: Regression Model Package.py
slug: regression
excerpt: ''
createdAt: Tue Apr 19 2022 20:12:29 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# Regression

> üöß Note
>
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](../uploading-model-artifacts.md).

Suppose you would like to upload a model artifact for a **regression model**.

Following is an example of what the `package.py` script may look like.

```python
import pickle
from pathlib import Path
import pandas as pd

PACKAGE_PATH = Path(__file__).parent

OUTPUT_COLUMN = ['predicted_quality']

class MyModel:

    def __init__(self):
        
        # Load the model
        with open('{PACKAGE_PATH}/model.pkl', 'rb') as pkl_file:
            self.model = pickle.load(pkl_file)

    def predict(self, input_df):
        
        # Store predictions in a DataFrame
        return pd.DataFrame(self.model.predict(input_df), columns=OUTPUT_COLUMN)

def get_model():
    return MyModel()
```

Here, we are assuming that the model prediction column that has been specified in the [`fdl.ModelSpec`](../../Python\_Client\_3-x/api-methods-30.md#modelspec) object is called `predicted_quality`.

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Binary Classification Model Package.py
slug: binary-classification-1
excerpt: ''
createdAt: Tue Apr 19 2022 20:12:34 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 25 2024 21:08:14 GMT+0000 (Coordinated Universal Time)
---

# Binary Classification 1

> üöß Note
>
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](../uploading-model-artifacts.md).

Suppose you would like to upload a model artifact for a **binary classification model**.

Following is an example of what the `package.py` script may look like.

```python
import pickle
from pathlib import Path
import pandas as pd

PACKAGE_PATH = Path(__file__).parent

OUTPUT_COLUMN = ['probability_over_50k']

class MyModel:

    def __init__(self):
        
        # Load the model
        with open(f'{PACKAGE_PATH}/model.pkl', 'rb') as pkl_file:
            self.model = pickle.load(pkl_file)

    def predict(self, input_df):
        
        # Store predictions in a DataFrame
        return pd.DataFrame(self.model.predict_proba(input_df)[:, 1], columns=OUTPUT_COLUMN)

def get_model():
    return MyModel()
```

Here, we are assuming that the model prediction column that has been specified in the [ModelSpec](../../Python\_Client\_3-x/api-methods-30.md#modelspec) object is called `probability_over_50k`.

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Publishing Batches of Events
slug: publishing-batches-of-events
excerpt: ''
createdAt: Tue Apr 19 2022 20:15:35 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 22 2024 18:48:57 GMT+0000 (Coordinated Universal Time)
---

# Publishing Batches Of Events

Fiddler has a flexible ETL framework for retrieving and publishing batches of production data, either from local storage or from the cloud. This provides maximum flexibility in how you are required to store your data when publishing events to Fiddler.

***

**The following data formats are currently supported:**

* pandas DataFrame objects (`pd.DataFrame`)
* CSV files (`.csv`),
* Parquet files (`.pq`)

***

**The following data locations are supported:**

* In memory (for DataFrames)
* Local disk
* AWS S3

***

Once you have a batch of events stored somewhere, all you need to do to publish the batch to Fiddler is call the Fiddler client's `fdl.Model.publish` function.

```python
model.publish(""my_batch.csv"")
```

_After calling the function, please allow 3-5 minutes for events to populate the_ _**Monitor**_ _page._

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Publishing Ranking Events
slug: ranking-events
excerpt: ''
createdAt: Thu Jun 30 2022 22:05:47 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 25 2024 15:03:06 GMT+0000 (Coordinated Universal Time)
---

# Ranking Events

### Publish ranking events

#### The grouped format

Before publishing ranking model events into Fiddler, we need to make sure they are in **grouped format** (i.e. the listing returned within the same **query id**‚Äîwhich is usually the `group_by` argument passed as a part of [TaskParams](../../Python\_Client\_3-x/api-methods-30.md#modeltaskparams) to the [Model](../../Python\_Client\_3-x/api-methods-30.md#model)object‚Äîis in the same row with other cells as lists). The first row in the example below indicates there are 3 items returned"
" by **query id**(`srch_id'` in the table) 1.

Below is an example of what this might look like.

| srch\_id | price\_usd                 | review\_score      | ...    | prediction               | target     |
| -------- | -------------------------- | ------------------ | ------ | ------------------------ | ---------- |
| 101      | \[134.77,180.74,159.80]    | \[5.0,2.5,4.5]     | \[...] | \[1.97, 0.84,-0.69]      | \[1,0,0]   |
| ...      | ...                        |                    | ...    | ...                      | ...        |
| 112      | \[26.00,51.00,205.11,73.2] | \[3.0,4.5,2.0,1.0] |"
" \[...] | \[10.75,8.41,-0.23,-3.2] | \[0,1,0,0] |

In the above example, `srch_id` is the name of our `group_by` column, and the other columns all contain lists corresponding to the given group.

#### How can I convert a flat CSV file into this format?

If you're storing your data in a flat CSV file (i.e. each row contains a single item), Fiddler provides a utility function that can be used to convert the flat CSV file into the grouped format specified above.

```python
from fiddler.utils.helpers import group_by
import pandas as pd

df = pd.read_csv('path/to/ranking_events.csv')
df_grouped = group_by(df=df, group_by_col='srch_id')
```

#### Call `model.publish()`

```python

model.publish(df_grouped)

```

In the above example,"
" the `group_by_col` argument should refer to the same column that was specified in the `group_by` argument passed to the [Model Object](../../Python\_Client\_3-x/api-methods-30.md#model).

### Update ranking events

#### Prepare the updating dataframe

We also support updating events for ranking model. You can use `model.publish` method call with `update` flag to `True` and keep the grouped format unchanged.

For example, you might want to alter the exisiting `target` after events are published. You can create a dataframe in the format below where you add the required updates to the desired column. You can then use `model.publish` to send us the updated dataframe. Fiddler will recognise the updates to be made and make the changes in the updated columns, while keeping the rest the same.

#### Call `model.publish()` with `update` flag set to True

```python
model.publish(source=modified_df_grouped, update"
"=True)
```

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"# Deleting Events

Fiddler supports deleting previously published production events. 
Starting from the 24.19 release, this capability is only available via the REST API on [events DELETE](../../api-integration/api_guidelines/events.md) endpoint.
**Note:** This action is irreversible.

Set `update_metrics=True` to indicate existing metrics should be re-computed. Otherwise, only the raw events will be deleted, and the existing monitoring metrics will continue to reflect the deleted events.

There are two methods to identify the events for deletion:
- **Delete by time range**: Passing in the range start time and range end time(`event timestamp` i.e. [model](../../Python_Client_3-x/api-methods-30.md#model).`event_ts_col`)
  - All events that fall into the specified time range are deleted. `update_metrics` can be configured either to `True` or `False`. The most common use case is recall events published by mistake; in this case"
", set `update_metrics=True` to recompute the monitoring metrics for accuracy.
- **Delete specific events**: passing in the list of event identifiers(`event_ids`, i.e. [model](../../Python_Client_3-x/api-methods-30.md#model).`event_id_col`))
  - All events matching the passed event_ids are deleted. `update_metrics=False` is enforced. The monitoring metrics won't be recomputed. The most common use case is for compliance concern.
You can specify either one of the modes by passing the corresponding parameters in the request, but not both at the same time. 


**Usage params**

| Parameter      | Type            | Default | Description                                                                                                                                             |
|----------------|-----------------|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------|
| model_id       | UUID            | -       | Unique identifier for the model from which production events are deleted.                                                                               |
| time_range     | Optional\[dict] | -       | A dictionary with `start_time`(inclusive"
") and `end_time`(exclusive), indicating the range of events to be deleted. (i.e. `start_time` ‚â§ t < `end_time`) |
| event_ids      | Optional\[list] | -       | List of event_ids to be deleted.                                                                                                                        |
| update_metrics | Optional\[bool] | False   | Determines if the monitoring metrics are updated following the deletion.                                                                                                   |


***

### Example: Deleting Events by Time Range


```python
import requests
headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {token}'}
data = {'model_id': model.id,
        'time_range':{
            'start_time':'2024-09-27 17:00:00',
            'end_time':'2024-09-27 17:30:00',
            },
        'update_metrics':True,
        }
response = requests.delete(
    url=f'{url}/v3/events',
    headers=headers"
",
    json=data,
)

```


### Example: Deleting Events by Event IDs

```python
import requests
headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {token}'}
data = {
         'model_id': model.id,
         'event_ids': ['event_id1', 'event_id2'],
         'update_metrics':False,
      }
response = requests.delete(
  url=f'{url}/v3/events',
  headers=headers,
  json=data,
)

```

Refer to our [REST API documentation](../../api-integration/api_guidelines/events.md) for more details.

> üìò Please delete events with caution when `update_metrics=True`. We recommend not deleting events while there is an ongoing publish or update operation within the same data range.

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}
"
"---
title: Publishing Production Data
slug: publishing-production-data
excerpt: ''
createdAt: Fri Nov 18 2022 23:28:25 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 25 2024 21:08:49 GMT+0000 (Coordinated Universal Time)
---

# Publishing Production Data

This Section guides you on the various ways you can provide event data to Fiddler and update and retrieve them.



{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}
"
"---
title: Streaming Live Events
slug: streaming-live-events
excerpt: ''
createdAt: Tue Apr 19 2022 20:07:23 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 22 2024 18:47:45 GMT+0000 (Coordinated Universal Time)
---

# Streaming Live Events

This process is very simple, but it requires that each event is structured as a Python dictionary that maps field names (as they are defined in your Model's schema) to values.

***

### Example 1: A simple three-input fraud model.

```python
my_event = {
    ""age"": 30,
    ""gender"": ""Male"",
    ""salary"": 80000.0,
    ""predicted_fraud"": 0.89,
    ""is_fraud"": 1
}
```

> üöß Note
>
> If you have a pandas DataFrame, you can easily **convert it into a list of event dictionaries** in the above form by using its `to_dict` function.

```python
my_events = my_df.to_dict(orient=""records"")
```

***

Then to upload the event to Fiddler, all you have to do is call the Fiddler client's `fdl.Model.publish` method.

```python
# For a single event. Note it must be passed as an array.
model.publish([my_event])

# For multiple events where `my_events` is an array Python dictionaries
model.publish(my_events)
```

_After calling the function, please allow 3-5 minutes for events to populate the_ _**Monitor**_ _page._

> üìò Info
>
> The `event_timestamp` field should contain the **Unix timestamp in milliseconds** for the time the event occurred.
>
> If you do not specify an event timestamp column in the event data published to Fiddler, the current time will be used for each published event. This timestamp will be used to plot the event on time series charts for monitoring. You can specify a custom event timestamp by setting its column name on your `fdl.Model`'s event\_ts\_col property and ensure it is present on each inference published to Fiddler.

### Example 2: Bank churn event

Here's an example using a bank churn model.

```python
# Publish an event
model.publish([{
  ""CreditScore"": 650,      # data type: int
  ""Geography"": ""France"",   # data type: category
  ""Gender"": ""Female"",
  ""Age"": 45,
  ""Tenure"": 2,
  ""Balance"": 10000.0,      # data type: float
  ""NumOfProducts"": 1,
  ""HasCrCard"": ""Yes"",
  ""isActiveMember"": ""Yes"",
  ""EstimatedSalary"": 120000,
  ""probability_churned"": 0.105,
  ""churn"": 1
}]
)
```

The `fdl.Model.publish` API can be called in real-time right after your model inference.

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Updating Events
slug: updating-events
excerpt: ''
createdAt: Tue Apr 19 2022 20:16:43 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 22 2024 18:57:15 GMT+0000 (Coordinated Universal Time)
---

# Updating Events

Fiddler supports _partial_ updates of events. Specifically, for your [target](../../Python\_Client\_3-x/api-methods-30.md#modelspec) column and [metadata](../../Python\_Client\_3-x/api-methods-30.md#modelspec) columns.

The most common use case for this functionality is updating ground truth labels. Existing events that lack ground truth labels can be updated once the actual values are discovered. Or you might find that the initially uploaded labels are wrong and wish to correct them.

We also support updating **metadata** columns with new values. You can observe the changes in [Analytics](../../UI\_"
"Guide/analytics-ui/), but the [Performance](../../UI\_Guide/monitoring-ui/performance.md) metrics of the columns won't be reflected.

Other columns(Decision, Input and Output) can only be sent at insertion time (with `update_event=False`). They are dropped if you send them and set `update_event=True`.

Set `update_event=True` to indicate that you are updating an existing event. You only need to provide the fields that you want to change‚Äîany fields you leave out will remain as they were before the update.

***

### Example in client 3.x

Below is the code you can use to update the target and metadata columns values for selected events (for instance when you get ground truth data or some other information relating to the event at a later stage)

```python
model_spec = fdl.ModelSpec(
    inputs=['input1'],
    targets=['target']
    outputs='output'
    metadata=['uri', 'record_tag'],
)

fdl"
"_model = fdl.Model.from_data(
    name=model_name,
    project_id=project.id,
    source=dataset_df,
    spec=model_spec,
    event_id_col='event_id'
)

model.publish(source='my_events.csv', update=True)
```

Where `my_events.csv` contains the corresponding columns to be updated, as well as the `event_id_col` of the model: `event_id`. Here is an example of `my_events.csv`:

| event\_id | target     | uri                      | record\_tag  |
| --------- | ---------- | ------------------------ | ------------ |
| `A1`      | `Selected` | `s3://dataset/image.jpg` | `category_1` |

Refer to [publish](../../Python\_Client\_3-x/api-methods-30.md#publish) doc for more details on different sources and parameters.

> üìò **There are a few points to be aware of:**
>
> * [Performance](../../"
"UI\_Guide/monitoring-ui/performance.md) metrics (available from the **Performance** tab of the **Monitor** page) will be computed as events are updated.
>   * For example, if the ground truth values are originally missing from events in a given time range, there will be **no performance metrics available** for that time range. Once the events are updated, performance metrics will be computed and will populate the monitoring charts.
>   * Events that do not originally have ground truth labels should be **uploaded with empty values**‚Äînot dummy values. If dummy values are used, you will have improper performance metrics, and once the new values come in, the old, incorrect values will still be present.
>   * Update on **Metadata** columns won't be reflected.
> * In order to update existing events, you will need access to the event IDs used at the time of upload. If you do not have access to those event IDs, you can find them by using"
" the `fdl.Model.get_slice` API and checking the `__event_id` column from the resulting DataFrame.
> * If you pass an updated timestamp for an existing event, **this timestamp will be used** for plotting decisions and computed performance metrics on the Dashboards and Charts. That is, the bin for which data will appear will depend on the new timestamp, not the old one.

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Uploading a TensorFlow HDF5 Model Artifact
slug: tensorflow-hdf5
excerpt: ''
createdAt: Tue Apr 19 2022 20:14:00 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# Tensorflow Hdf5

> üöß Note
>
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](../uploading-model-artifacts.md).

Suppose you would like to upload a model artifact for a **TensorFlow (HDF5) model**.

Following is an example of what the `package.py` script may look like.

```python
import pickle
from pathlib import Path
import pandas as pd
import tensorflow as tf

PACKAGE_PATH = Path(__file__).parent

OUTPUT_COLUMN = ['probability_over_50k']

class MyModel:

    def __init__(self):
        
        # Load the model
        self.model = tf.keras.models.load_model(PACKAGE_PATH / 'model.h5')

    def predict(self, input_df):
        
        # Store predictions in a DataFrame
        return pd.DataFrame(self.model.predict(input_df), columns=OUTPUT_COLUMN)

def get_model():
    return MyModel()
```

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Uploading an XGBoost Model Artifact
slug: xgboost
excerpt: ''
createdAt: Tue Apr 19 2022 20:13:35 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# Xgboost

> üöß Note
>
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](../uploading-model-artifacts.md).

Suppose you would like to upload a model artifact for a **XGBoost model**.

Following is an example of what the `package.py` script may look like.

```python
import pickle
from pathlib import Path
import pandas as pd
import xgboost as xgb

PACKAGE_PATH = Path(__file__).parent

OUTPUT_COLUMN = ['probability_over_50k']

class MyModel:

    def __init__(self):
        
        # Load the model
        with open(PACKAGE_PATH / 'model.pkl', 'rb') as pkl_file:
            self.model = pickle.load(pkl_file)

    def transform_input(self, input_df):
        
        # Convert DataFrame to XGBoost DMatrix
        return xgb.DMatrix(input_df)

    def predict(self, input_df):
        
        # Apply data transformation
        transformed_input = self.transform_input(input_df)
        
        # Store predictions in a DataFrame
        return pd.DataFrame(self.model.predict(transformed_input), columns=OUTPUT_COLUMN)

def get_model():
    return MyModel()
```

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Uploading a TensorFlow SavedModel Model Artifact
slug: tensorflow-savedmodel
excerpt: ''
createdAt: Tue Apr 19 2022 20:13:41 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# Tensorflow Savedmodel

> üöß Note
>
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](../uploading-model-artifacts.md).

Suppose you would like to upload a model artifact for a **TensorFlow (SavedModel) model**.

Following is an example of what the `package.py` script may look like.

```python
import pickle
from pathlib import Path
import pandas as pd
import tensorflow as tf

PACKAGE_PATH = Path(__file__).parent

OUTPUT_COLUMN = ['probability_over_50k']

class MyModel:

    def __init__(self):
        
        # Load the model
        self.model = tf.keras.models.load_model(PACKAGE_PATH / 'saved_model')

    def predict(self, input_df):
        
        # Store predictions in a DataFrame
        return pd.DataFrame(self.model.predict(input_df), columns=OUTPUT_COLUMN)

def get_model():
    return MyModel()
```

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: ML Framework Examples
slug: ml-framework-examples
excerpt: ''
createdAt: Tue Apr 19 2022 20:13:20 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# ML Framework Examples



{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}
"
"---
title: Uploading a scikit-learn Model Artifact
slug: scikit-learn
excerpt: ''
createdAt: Tue Apr 19 2022 20:13:31 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# Scikit Learn

> üöß Note
>
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](../uploading-model-artifacts.md).

Suppose you would like to upload a model artifact for a **scikit-learn model**.

Following is an example of what the `package.py` script might look like.

```python
import pickle
from pathlib import Path
import pandas as pd
from sklearn.linear_model import LogisticRegression

PACKAGE_PATH = Path(__file__).parent

OUTPUT_COLUMN = ['probability_over_50k']

class MyModel:

    def __init__(self):
        
        # Load the model
        with open(PACKAGE_PATH / 'model.pkl', 'rb') as pkl_file:
            self.model = pickle.load(pkl_file)

    def predict(self, input_df):
        
        # Store predictions in a DataFrame
        return pd.DataFrame(self.model.predict_proba(input_df)[:, 1], columns=OUTPUT_COLUMN)

def get_model():
    return MyModel()
```

{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}

"
"---
title: Upgrade from 2.x to 3.x
slug: upgrade-from-2x-to-3x
excerpt: ''
createdAt: Tue Apr 23 2024 19:33:10 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri May 03 2024 14:26:20 GMT+0000 (Coordinated Universal Time)
---

# Upgrade from 2.x To 3.x

With Platform Version `24.4` bringing changes in the model onboarding flow, the Python client has been upgraded to reflect these changes using an object-oriented design. This article is a guide on how your existing 2.x API method calls change when upgrading to Fiddler Python client 3.x. Please note that the below 3.x methods are more fully documented in [API documentation](api-methods-30.md). You can also take a look at this [notebook](https://drive.google.com/file/d/1g1ld"
"fxaDnY7zokewFTnqnYbQcTgF72fT/view?usp=sharing) which walks you through usage changes with the new client

### Notable Changes

* **Dataset management**: In client 3.x and platform 24.4+, the dataset is a model asset rather than a project asset. Multiple datasets can be associated with a model for different purposes like feature impact computation or surrogate model generation.
* **Explicit triggering**: In client 3.x, actions such as feature importance and impact computation must be triggered explicitly unlike in client 2.x where they were part of the add artifact/surrogate model methods.
* **Onboarding prerequisites**: A dataset is no longer a perquisite for onboarding a model into Fiddler and the DatasetInfo object has been deprecated.
*   **Naming conventions**: In client 2.x API method parameters for unique identifiers used semantic names such as project\_id='my\_project' and model"
"\_id='my\_model'. With client 3.x we will expose automatically generated unique identifiers for objects like projects and models. This unique identifier is what is to be used for any ""id"" or ""\_id"" parameter. Your semantic names will be associated with an object's ""name"" parameter and will allow retrieval by your semantic name in addition to get by ""id"".

    **Flow Changes**

    **Import**

    * **2.x**: `import fiddler as fdl`
    * **3.x**: `import fiddler as fdl`

    **Initialization**

    *   **2.x**:

        ```python
        client = fdl.FiddlerApi(
            url=URL,
            org_id=ORG_ID,
            auth_token=AUTH_TOKEN
        )
        ```
    *   **3.x**:

        ```python
        fdl.init(
            url=URL,
            token=AUTH_TOKEN
        )
        ```

        `"
"org_id` is no longer required and will be inferred from the token.

    **Projects**

    **Get Projects**

    * **2.x**: `projects = client.get_projects()`
    * **3.x**: `projects = fdl.Project.list()`

    **Add Project**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'

        project = client.create_project(project_id=PROJECT_ID)
        ```
    *   **3.x**:

        ```python
        PROJECT_NAME = 'YOUR_PROJECT_NAME'

        project = fdl.Project(name=PROJECT_NAME)
        project.create()
        ```

    **Delete Project**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'

        client.delete_project(project_id=PROJECT_ID)
        ```
    *   **3.x**:

        ```python
        PROJECT_NAME = 'YOUR_PROJECT_NAME'

        project = fdl.Project.from_name(name"
"=PROJECT_NAME)
        project.delete()
        ```

    **Models**

    **Get Model**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'
        MODEL_ID = 'YOUR_MODEL_NAME'

        model = client.get_model(
            project_id=PROJECT_ID,
            model_id=MODEL_ID
        )
        ```
    *   **3.x**:

        ```python
        PROJECT_NAME = 'YOUR_PROJECT_NAME'
        MODEL_NAME = 'YOUR_MODEL_NAME'

        project = fdl.Project.from_name(name=PROJECT_NAME)
        model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)
        ```

    **List Models**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'

        models = client.list_models(project_id=PROJECT_ID))
        ```
    *   **3.x**:

        ```python
        PROJECT_ID = '6dbf"
"8656-1b6b-4f80-ba2b-b75739526dc2'

        models = fdl.Model.list(project_id=PROJECT_ID)
        ```

    **Add Model**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'
        MODEL_ID = 'YOUR_MODEL_NAME'
        DATASET_ID = 'YOUR_DATASET_NAME'

        client.add_model(
            project_id=PROJECT_ID,
            dataset_id=DATASET_ID,
            model_id=MODEL_ID,
            model_info=model_info
        )
        ```
    *   **3.x**:

        ```python
        DATASET_FILE_PATH = <path_to_file>
        MODEL_NAME = 'YOUR_MODEL_NAME'
        PROJECT_ID = '6dbf8656-1b6b-4f80-ba2b-b75739526dc2'

        MODEL_SPEC = fdl.ModelSpec(
            inputs=['CreditScore', 'Ge"
"ography', 'Gender', 'Age', 'Tenure', 'Balance'],
            outputs=['probability_churned'],
            targets=['Churned'],
            decisions=[],
            metadata=[],
            custom_features=[],
        )

        model = fdl.Model.from_data(
            source=DATASET_FILE_PATH,
            name=MODEL_NAME,
            project_id=PROJECT_ID,
            spec=MODEL_SPEC,
        )
        model.create()
        ```

    \\

    **Add Model Artifact**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'
        MODEL_ID = 'YOUR_MODEL_NAME'
        MODEL_DIR = <path_to_dir_containing_artifact>
        DEPLOYMENT_PARAMS = {'deployment_type': 'MANUAL', 'cpu': 1000}

        job_id = client.add_model_artifact(
            project_id=PROJECT_ID,
            model_id=MODEL_ID,
            model_dir=MODEL_DIR,
            deployment_params=fdl"
".DeploymentParams(**DEPLOYMENT_PARAMS))
        )
        ```
    *   **3.x**:

        ```python
        MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'
        DEPLOYMENT_PARAMS = {'deployment_type': 'MANUAL', 'cpu': 1000}
        MODEL_DIR = <path_to_dir_containing_artifact>

        model = fdl.Model.get(id_=MODEL_ID)
        job = model.add_artifact(
                model_dir=MODEL_DIR,
                deployment_params=fdl.DeploymentParams(**DEPLOYMENT_PARAMS)
              )
        job.wait()
        ```

    > üìò Computation of Feature Importance
    >
    > Pre-compute of feature importance and feature impact needs to be done manually in 3.x. Below blocks showcase how it can be done with client 3.x

    *   **3.x**: **New steps**

        ```"
"python
        DATASET_ID = '5e1e67d2-5170-45ce-a851-68bdde1ac1ad'

        importance_job = model.precompute_feature_importance(
                  dataset_id=DATASET_ID
              )
        importance_job.wait()

        impact_job = model.precompute_feature_impact(
                  dataset_id=DATASET_ID
              )
        impact_job.wait()
        ```

    **Update Model**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'
        MODEL_ID = 'YOUR_MODEL_NAME'

        models = client.update_model(
            project_id=PROJECT_ID,
            model_id=MODEL_ID,
        )
        ```
    *   **3.x**:

        ```python
        MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'

        model = fdl.Model.get(id_=MODEL_ID"
")
        model.xai_params.default_explain_method = 'SHAP'
        model.update()
        ```

    **Update Model Artifact**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'
        MODEL_ID = 'YOUR_MODEL_NAME'
        MODEL_DIR = <path_to_dir_containing_artifact>
        DEPLOYMENT_PARAMS = {'deployment_type': 'MANUAL', 'cpu': 1000}

        job_id = client.update_model_artifact(
            project_id=PROJECT_ID,
            model_id=MODEL_ID,
            model_dir=MODEL_DIR,
            deployment_params=fdl.DeploymentParams(**DEPLOYMENT_PARAMS))
        )
        ```
    *   **3.x**:

        ```python
        MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'
        DEPLOYMENT_PARAMS = {'deployment_type': 'MANUAL', 'cpu"
"': 1000}
        MODEL_DIR = <path_to_dir_containing_artifact>

        model = fdl.Model.get(id_=MODEL_ID)
        job = model.update_artifact(
                model_dir=MODEL_DIR,
                deployment_params=fdl.DeploymentParams(**DEPLOYMENT_PARAMS)
              )
        job.wait()
        ```

    \\

    > üìò Computation of Feature Importance
    >
    > Pre-compute of feature importance and feature impact needs to be done manually in 3.x. Below blocks showcase how it can be done with client 3.x

    *   **3.x**: **New steps**

        ```python
        DATASET_ID = '5e1e67d2-5170-45ce-a851-68bdde1ac1ad'

        importance_job = model.precompute_feature_importance(
                  dataset_id=DATASET_ID
              )
        importance_job.wait()

        impact_job = model.precompute_feature_impact"
"(
                  dataset_id=DATASET_ID
              )
        impact_job.wait()
        ```

    **Download Artifacts**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'
        MODEL_ID = 'YOUR_MODEL_NAME'
        OUTPUT_DIR = <path_to_dir_to_download_artifact>

        client.download_artifacts(
            project_id=PROJECT_ID,
            model_id=MODEL_ID,
            output_dir=OUTPUT_DIR
        )
        ```
    *   **3.x**:

        ```python
        MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'
        OUTPUT_DIR = <path_to_dir_to_download_artifact>

        model = fdl.Model.get(id_=MODEL_ID)
        model.download_artifact(
            output_dir=OUTPUT_DIR
        )
        ```

    **Get Model Deployment**

    *   **2.x**:

        ```"
"python
        PROJECT_ID = 'YOUR_PROJECT_NAME'
        MODEL_ID = 'YOUR_MODEL_NAME'

        model_deployment = client.get_model_deployment(
                              project_id=PROJECT_ID,
                              model_id=MODEL_ID
                          )
        ```
    *   **3.x**:

        ```python
        MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'

        # Using Model
        model = fdl.Model.get(id_=MODEL_ID)
        model_deployment = model.deployment

        # Using ModelDeployment
        model_deployment = fdl.ModelDeployment.of(model_id=MODEL_ID)
        ```

    **Add Model Surrogate**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'
        MODEL_ID = 'YOUR_MODEL_NAME'
        DEPLOYMENT_PARAMS = {'deployment_type': 'MANUAL', 'cpu': 1000}

"
"        job_id = client.add_model_surrogate(
            project_id=PROJECT_ID,
            model_id=MODEL_ID,
            deployment_params=fdl.DeploymentParams(**DEPLOYMENT_PARAMS))
        )
        ```
    *   **3.x**:

        ```python
        MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'
        DATASET_ID = '5e1e67d2-5170-45ce-a851-68bdde1ac1ad'
        DEPLOYMENT_PARAMS = {'deployment_type': 'MANUAL', 'cpu': 1000}

        model = fdl.Model.get(id_=MODEL_ID)
        job = model.add_surrogate(
                dataset_id=DATASET_ID,
                deployment_params=fdl.DeploymentParams(**DEPLOYMENT_PARAMS))
        job.wait()
        ```

    **Update Model Surrogate**

    *   **2.x**"
":

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'
        MODEL_ID = 'YOUR_MODEL_NAME'
        DEPLOYMENT_PARAMS = {'deployment_type': 'MANUAL', 'cpu': 1000}

        job_id = client.update_model_surrogate(
            project_id=PROJECT_ID,
            model_id=MODEL_ID,
            deployment_params=fdl.DeploymentParams(**DEPLOYMENT_PARAMS))
        )
        ```
    *   **3.x**:

        ```python
        MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'
        DATASET_ID = '5e1e67d2-5170-45ce-a851-68bdde1ac1ad'
        DEPLOYMENT_PARAMS = {'deployment_type': 'MANUAL', 'cpu': 1000}

        model = fdl.Model.get(id_=MODEL_ID)
        job = model.update_surrogate(
"
"                dataset_id=DATASET_ID,
                deployment_params=fdl.DeploymentParams(**DEPLOYMENT_PARAMS))
        job.wait()
        ```

    **Update Model Deployment**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'
        MODEL_ID = 'YOUR_MODEL_NAME'

        model_deployment = client.update_model_deployment(
                              project_id=PROJECT_ID,
                              model_id=MODEL_ID,
                              cpu=1000
                          )
        ```
    *   **3.x**:

        ```python
        MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'

        model_deployment = fdl.ModelDeployment.of(
                            model_id=MODEL_ID
                           )
        model_deployment.cpu = 1000
        model_deployment.update()
        ```

    **Delete Model**

    *   **2.x**:

        ```python
       "
" PROJECT_ID = 'YOUR_PROJECT_NAME'
        MODEL_ID = 'YOUR_MODEL_NAME'

        client.delete_model(
            project_id=PROJECT_ID,
            model_id=MODEL_ID,
        )
        ```
    *   **3.x**:

        ```python
        MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'

        model = fdl.Model.get(id_=MODEL_ID)
        job = model.delete()
        job.wait()
        ```

    **Datasets**

    **Get Dataset**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'
        DATASET_ID = 'YOUR_MODEL_NAME'

        dataset = client.get_dataset(
            project_id=PROJECT_ID,
            dataset_id=DATASET_ID
        )
        ```
    *   **3.x**:

        ```python
        # From id
        DATASET_ID = '5e1"
"e67d2-5170-45ce-a851-68bdde1ac1ad'
        dataset = fdl.Dataset.get(id_=DATASET_ID)

        # From name
        DATASET_NAME = 'test_dataset'
        MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'
        dataset = fdl.Dataset.from_name(
              name=DATASET_NAME,
              model_id=MODEL_ID
        )
        ```

    **List Datasets**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'
        datasets = client.list_datasets(project_id=PROJECT_ID)
        ```
    *   **3.x**:

        ```python
        # In 3.x, datasets are stored at a model level rather than project level.
        MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab"
"33cd8b9b93'
        datasets = fdl.Dataset.list(model_id=MODEL_ID)
        ```

    **Upload Dataset**

    *   **2.x**:

        ```python
        baseline_df = pd.read_csv(PATH_TO_BASELINE_CSV)
        dataset_info = fdl.DatasetInfo.from_dataframe(baseline_df)

        PROJECT_ID = 'YOUR_PROJECT_NAME'
        DATASET_ID = 'YOUR_MODEL_NAME'

        client.upload_dataset(
            project_id=PROJECT_ID,
            dataset_id=DATASET_ID,
            dataset={
                'baseline': baseline_df
            },
            info=dataset_info
        )
        ```
    *   **3.x**:

        ```python
        # Fiddler no longer requires a dataset to be added before model creation.
        # However, you can optionally add upload dataset to a model using the
        # publish method as shown below.
        MODEL_ID = 'e38ab1ad-1a50-40b8"
"-8bee-ab33cd8b9b93'
        DATASET_NAME = 'YOUR_DATASET_NAME'
        DATASET_FILE_PATH = <path_to_dataset>

        job = model.publish(
            source=DATASET_FILE_PATH,
            environment=fdl.EnvType.PRE_PRODUCTION,
            dataset_name=DATASET_NAME,
        )
        # The publish() method is asynchronous by default as in previous versions. 
        # Use the publish job's wait() method if synchronous behavior is desired.
        # job.wait()        
        ```

    **Baselines**

    **Get Baseline**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'
        DATASET_ID = 'YOUR_MODEL_NAME'
        BASELINE_ID = 'YOUR_BASELINE_NAME'

        baseline = client.get_baseline(
            project_id=PROJECT_ID,
            model_id=MODEL_ID,
            baseline_id=BASELINE_ID
        )
        ```
    *  "
" **3.x**:

        ```python
        # From UUID
        BASELINE_ID = '5e1e67d2-5170-45ce-a851-68bdde1ac1ad'
        baseline = fdl.Baseline.get(id_=BASELINE_ID)

        # From name
        MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'
        BASELINE_NAME = 'YOUR_BASELINE_NAME'

        baseline = fdl.Baseline.from_name(
              name=BASELINE_NAME,
              model_id=MODEL_ID
        )
        ```

    **Add Baseline**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'
        MODEL_ID = 'YOUR_MODEL_NAME'
        BASELINE_ID = 'YOUR_BASELINE_NAME'
        DATASET_ID = 'YOUR_DATASET_NAME'

        # Static baseline
        baseline = client"
".add_baseline(
              project_id=PROJECT_ID,
              model_id=MODEL_ID,
              baseline_id=BASELINE_ID,
              type=fdl.BaselineType.STATIC,
              dataset_id=DATASET_ID
        )

        # Rolling baseline
        baseline = client.add_baseline(
              project_id=PROJECT_ID,
              model_id=MODEL_ID,
              baseline_id=BASELINE_NAME,
              type=fdl.BaselineType.ROLLING_PRODUCTION,
              offset=fdl.WindowSize.ONE_MONTH, # How far back to set our window
              window_size=fdl.WindowSize.ONE_WEEK, # Size of the sliding window
        )
        ```
    *   **3.x**:

        ```python
        BASELINE_NAME = 'YOUR_BASELINE_NAME'
        MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'

        # Static baseline
        baseline = fdl.Baseline"
"(
            name=BASELINE_NAME,
            model_id=MODEL_ID,
            environment=fdl.EnvType.PRE_PRODUCTION,
            type_=fdl.BaselineType.STATIC,
            dataset_id=DATASET_ID,
        )
        baseline.create()

        # Rolling baseline
        baseline = fdl.Baseline(
            name=BASELINE_NAME,
            model_id=MODEL_ID,
            environment=fdl.EnvType.PRODUCTION,
            type_=fdl.BaselineType.ROLLING,
            window_bin_size=fdl.WindowBinSize.HOUR,
            offset_delta=fdl.WindowBinSize.HOUR,
        )
        baseline.create()
        ```

    **List Baselines**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'

        baselines = client.list_baselines(project_id=PROJECT_ID)
        ```
    *   **3.x**:

        ```python
        MODEL_ID = 'e38ab1ad-"
"1a50-40b8-8bee-ab33cd8b9b93'

        baselines = fdl.Baseline.list(model_id=MODEL_ID)
        ```

    **Delete Baselines**

    *   **2.x**:

        ```python
        PROJECT_ID = 'YOUR_PROJECT_NAME'
        MODEL_ID = 'YOUR_MODEL_NAME'
        BASELINE_ID = 'YOUR_BASELINE_NAME'

        client.delete_baseline(
            project_id=PROJECT_ID,
            model_id=MODEL_ID,
            baseline_id=BASELINE_ID
        )
        ```
    *   **3.x**:

        ```python
        BASELINE_ID = '5e1e67d2-5170-45ce-a851-68bdde1ac1ad'

        baseline = fdl.Baseline.get(id_=BASELINE_ID)
        baseline.delete()
        ```

    **Event Publishing**

    > üìò Source
    >
    > In 3.x, an event"
" data source for batch publishing can be one of CSV filepath, parquet filepath, or Pandas dataframe and a Python list\[dict] for streaming publishing.

    **Publish batch production events**

    *   **Pre-requisite**:

        ```python
          PROJECT_ID = 'YOUR_PROJECT_NAME'
          MODEL_ID = 'YOUR_MODEL_NAME'
          production_df = pd.read_csv(PATH_TO_EVENTS_CSV)
        ```
    *   **2.x**:

        ```python
        job = client.publish_events_batch(
            project_id=PROJECT_ID,
            model_id=MODEL_ID,
            id_field='event_id',
            batch_source=production_df,
            timestamp_field='timestamp',
            update_event=False,
        )
        ```
    *   **3.x**:

        ```python
        # Instantiate project and model object in 3.x
        project = fdl.Project.from_name(name=PROJECT_ID)
        model = fdl.Model.from_name(project_id=project.id, name=MODEL"
"_ID)

        # One-time only on migration from v1/v2: Before publishing events, update your model
        # with the event unique identifier and event timestamp as these are now Model properties
        # instead of publish() method parameters.
        model.event_ts_col = 'timestamp'
        model.event_id_col = 'event_id'
        model.update()

        job = model.publish(source=production_df, update=False)
        # The publish() method is asynchronous by default as in previous versions. 
        # Use the publish job's wait() method if synchronous behavior is desired.
        # job.wait() 
        ```

        For examples of updating labels in 3.x, refer to [publish](api-methods-30.md#publish) API documentation.

    **Publish Production Events Streaming**

    * **Pre-requisite**:

    ```python
      PROJECT_ID = 'YOUR_PROJECT_NAME'
      MODEL_ID = 'YOUR_MODEL_NAME'
      event_dict = [{
        'A':"
" [0], 
        'B': [0], 
      }]
      event_id = 7342881 
      event_time = '2024-05-01 00:00:00' 
    ```

    * **2.x**:

    ```python
    client.publish_event(
        project_id=PROJECT_ID,
        model_id=MODEL_ID,
        event=event_dict,
        event_timestamp=event_time,
        event_id=event_id,
        update_event= False
    )
    ```

    * **3.x**:

    ```python
    # setup project and model object in 3.x
    project = fdl.Project.from_name(name=PROJECT_ID)
    model = fdl.Model.from_name(project_id=project.id, name=MODEL_ID)

    # One-time only on migration from v1/v2: Before publishing events, update your model
    # with the event unique identifier and event timestamp as these are now Model properties
    # instead of"
" publish() method parameters.
    model.event_ts_col = 'timestamp'
    model.event_id_col = 'event_id'
    model.update()

    # Add the event_id and timestamp fields to every event to be updated. Alternatively, alter 
    # your data pipeline to include these fields on the event dictionary sent to Fiddler 
    # to avoid the following step.
    event_dict['event_id'] = event_id
    event_dict['timestamp'] = event_time

    # The source accepts a list of 1 or more dictionaries
    model.publish(source=[event_dict])
    ```

    **Custom Metrics**

    **Get Custom Metric**

    * **2.x**:

    ```python
    CUSTOM_METRIC_ID = 'YOUR_METRIC_NAME'

    client.get_custom_metric(metric_id=METRIC_ID)
    ```

    * **3.x**:

    ```python
    # From UUID
    CUSTOM_METRIC_ID = '7057867c-6dd"
"8-4915-89f2-a5f253dd4a3a'
    custom_metric = fdl.CustomMetric.get(id_=CUSTOM_METRIC_ID)

    # From name
    MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'
    CUSTOM_METRIC_NAME = 'YOUR_METRIC_NAME'

    custom_metric = fdl.CustomMetric.from_name(
            name=CUSTOM_METRIC_NAME,
            model_id=MODEL_ID,
        )
    ```

    **List Custom Metrics**

    * **2.x**:

    ```python
    PROJECT_ID = 'YOUR_PROJECT_NAME'
    MODEL_ID = 'YOUR_MODEL_NAME'

    client.get_custom_metrics(
        project_id=PROJECT_ID,
        model_id=MODEL_ID,
    )
    ```

    * **3.x**:

    ```python
    MODEL_ID = 'e38ab1ad-1a50-40b"
"8-8bee-ab33cd8b9b93'

    custom_metrics = fdl.CustomMetric.list(model_id=MODEL_ID)
    ```

    **Add Custom Metric**

    * **2.x**:

    ```python
    PROJECT_ID = 'YOUR_PROJECT_NAME'
    MODEL_ID = 'YOUR_MODEL_NAME'
    CUSTOM_METRIC_NAME = 'YOUR_METRIC_NAME'
    DEFINITION = 'average(""Age"")'
    DESCRIPTION = 'Testing custom metrics'

    client.get_custom_metrics(
        name=CUSTOM_METRIC_NAME,
        project_id=PROJECT_ID,
        model_id=MODEL_ID,
        definition=DEFINITION,
        description=DESCRIPTION,
    )
    ```

    * **3.x**:

    ```python
    CUSTOM_METRIC_NAME = 'YOUR_METRIC_NAME'
    MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'
    DEFINITION = 'average"
"(""Age"")'
    DESCRIPTION = 'Testing custom metrics'

    custom_metric = fdl.CustomMetric(
        name=CUSTOM_METRIC_NAME,
        model_id=MODEL_ID,
        definition=DEFINITION,
        description=DESCRIPTION,
    )
    custom_metric.create()
    ```

    **Delete Custom Metric**

    * **2.x**:

    ```python
    CUSTOM_METRIC_ID = 'YOUR_METRIC_NAME'

    client.delete_custom_metric(metric_id=METRIC_ID)
    ```

    * **3.x**:

    ```python
    CUSTOM_METRIC_ID = '7057867c-6dd8-4915-89f2-a5f253dd4a3a'

    custom_metric = fdl.CustomMetric.get(id_=CUSTOM_METRIC_ID)
    custom_metric.delete()
    ```

    **Segments**

    **Get Segment**

    * **2.x**:

    ```python
    SEGMENT_ID = 'YOUR_SEGMENT_NAME'

   "
" client.get_segment(segment_id=SEGMENT_ID)
    ```

    * **3.x**:

    ```python
    # From UUID
    SEGMENT_ID = '2c22a28b-08b8-4dd6-9238-7d7f1b5b4cb7'
    segment = fdl.Segment.get(id_=SEGMENT_ID)

    # From name
    MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'
    SEGMENT_NAME = 'YOUR_SEGMENT_NAME'

    segment = fdl.Segment.from_name(
        name=SEGMENT_NAME,
        model_id=MODEL_ID,
    )
    ```

    **List Segments**

    * **2.x**:

    ```python
    PROJECT_ID = 'YOUR_PROJECT_NAME'
    MODEL_ID = 'YOUR_MODEL_NAME'

    client.get_segments(
        project_id=PROJECT_ID,
"
"        model_id=MODEL_ID,
    )
    ```

    * **3.x**:

    ```python
    MODEL_ID = '2c22a28b-08b8-4dd6-9238-7d7f1b5b4cb7'

    segment = fdl.Segment.list(model_id=MODEL_ID)
    ```

    **Add Segment**

    * **2.x**:

    ```python
    PROJECT_ID = 'YOUR_PROJECT_NAME'
    MODEL_ID = 'YOUR_MODEL_NAME'
    SEGMENT_NAME = 'YOUR_SEGMENT_NAME'
    DEFINITION = 'Age < 60'
    DESCRIPTION = 'Testing segment'

    client.add_segment(
        name=SEGMENT_NAME,
        project_id=PROJECT_ID,
        model_id=MODEL_ID,
        definition=DEFINITION,
        description=DESCRIPTION,
    )
    ```

    * **3.x**:

    ```python
    SEGMENT_NAME = 'YOUR_SEGMENT_NAME'
"
"    MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'
    DEFINITION = 'Age < 60'
    DESCRIPTION = 'Testing segment'

    segment = fdl.Segment(
        name=SEGMENT_NAME,
        model_id=MODEL_ID,
        definition=DEFINITION,
        description=DESCRIPTION
    )
    segment.create()
    ```

    **Delete Segment**

    * **2.x**:

    ```python
    SEGMENT_ID = 'YOUR_SEGMENT_NAME'
    client.delete_segment(
      segment_id=SEGMENT_ID
    )
    ```

    * **3.x**:

    ```python
    SEGMENT_ID = '2c22a28b-08b8-4dd6-9238-7d7f1b5b4cb7'

    segment = fdl.Segment.get(id_=SEGMENT_ID)
    segment.delete()
   "
" ```

    **Alerts**

    **List Alert Rules**

    * **2.x**:

    ```python
    MODEL_ID = 'YOUR_MODEL_NAME'

    rules = client.get_alert_rules(model_id=MODEL_ID)
    ```

    * **3.x**:

    ```python
    MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'

    rules = fdl.AlertRule.list(model_id=MODEL_ID)
    ```

    \\

    **Add Alert Rule**

    * **2.x**:

    ```python
    notifications_config = client.build_notifications_config(
        emails = ""name@google.com"",
    )
    PROJECT_ID = 'YOUR_PROJECT_NAME'
    MODEL_ID = 'YOUR_MODEL_NAME'

    rule = client.add_alert_rule(
        name = ""Bank Churn Range Violation Alert1"",
        project_id = PROJECT_ID,
        model_id = MODEL_ID,
"
"        alert_type = fdl.AlertType.DATA_INTEGRITY,
        metric = fdl.Metric.RANGE_VIOLATION,
        bin_size = fdl.BinSize.ONE_DAY,
        compare_to = fdl.CompareTo.RAW_VALUE,
        compare_period = None,
        priority = fdl.Priority.HIGH,
        warning_threshold = 2,
        critical_threshold = 3,
        condition = fdl.AlertCondition.GREATER,
        column = ""numofproducts"",
        notifications_config = notifications_config
    )
    ```

    * **3.x**:

    ```python
    ALERT_NAME = 'YOUR_ALERT_NAME'
    METRIC_NAME = 'null_violation_count'
    MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'

    rule = fdl.AlertRule(
      name=ALERT_NAME,
      model_id=MODEL_ID,
      metric_id=METRIC"
"_NAME,
      priority=fdl.Priority.MEDIUM,
      compare_to=fdl.CompareTo.RAW_VALUE,
      condition=fdl.AlertCondition.GREATER,
      bin_size=fdl.BinSize.HOUR,
      critical_threshold=1,
      warning_threshold=0.32,
    )
    rule.create()
    ```

    **Delete Alert Rule**

    * **2.x**:

    ```python
    ALERT_RULE_ID = '31109d19-b8aa-4db0-a4d5-aa0706987b1b'

    client.delete_alert_rule(alert_rule_uuid=ALERT_RULE_ID)
    ```

    * **3.x**:

    ```python
    ALERT_RULE_ID = '31109d19-b8aa-4db0-a4d5-aa0706987b1b'

    rule = fdl.AlertRule.get(id_=ALERT_RULE_ID)
    rule.delete()
    ```

    \\

    **Get"
" Triggered Alerts**

    * **2.x**:

    ```python
    ALERT_RULE_ID = '31109d19-b8aa-4db0-a4d5-aa0706987b1b'

    rules = client.get_triggered_alerts(alert_rule_uuid=ALERT_RULE_ID)
    ```

    * **3.x**:

    ```python
    ALERT_RULE_ID = '31109d19-b8aa-4db0-a4d5-aa0706987b1b'

    rules = fdl.AlertRecord.list(
        alert_rule_id=ALERT_RULE_ID,
        start_time=datetime(),
        end_time=datetime()
    )
    ```

    **Enable Notifications**

    * **2.x**:

    ```python
    ALERT_RULE_ID = '31109d19-b8aa-4db0-a4d5-aa0706987b1b'

    notifications = client.update_alert_rule(
        alert_config_uuid=AL"
"ERT_RULE_ID,
        enable_notification=True
    )
    ```

    * **3.x**:

    ```python
    ALERT_RULE_ID = '31109d19-b8aa-4db0-a4d5-aa0706987b1b'

    rule = fdl.AlertRule.get(id_=ALERT_RULE_ID)
    rule.enable_notifications()
    ```

    **Disable Notifications**

    * **2.x**:

    ```python
    ALERT_RULE_ID = '31109d19-b8aa-4db0-a4d5-aa0706987b1b'

    notifications = client.update_alert_rule(
        alert_config_uuid=ALERT_RULE_ID,
        enable_notification=False
    )
    ```

    * **3.x**:

    ```python
    ALERT_RULE_ID = '31109d19-b8aa-4db0-a4d5-aa0706987b1b'

    rule = fdl.AlertRule"
".get(id_=ALERT_RULE_ID)
    rule.disable_notifications()
    ```

    **Webhooks**

    **Get Webhook**

    * **2.x**:

    ```python
    WEBHOOK_UUID = '00cb3169-7983-497c-8f3c-d25df26543b0'

    webhook = client.get_webhook(uuid=WEBHOOK_UUID)
    ```

    * **3.x**:

    ```python
    WEBHOOK_ID = '00cb3169-7983-497c-8f3c-d25df26543b0'

    webhook = fdl.Webhook.get(id_=WEBHOOK_ID)
    ```

    **List Webhooks**

    * **2.x**:

    ```python
    webhooks = client.get_webhooks()
    ```

    * **3.x**:

    ```python
    webhooks = fdl.Webhook.list()
    ```

    **Add Webhook**

   "
" * **2.x**:

    ```python
    WEBHOOK_NAME = 'YOUR_WEBHOOK_NAME'
    WEBHOOK_URL = 'https://hooks.slack.com/services/T9EAVLUQ5/xxxxxxxxxx'
    WEBHOOK_PROVIDER = 'SLACK'

    webhook = client.add_webhook(
        name=WEBHOOK_NAME,
        url=WEBHOOK_URL,
        provider=WEBHOOK_PROVIDER
    )
    ```

    * **3.x**:

    ```python
    WEBHOOK_NAME = 'YOUR_WEBHOOK_NAME'
    WEBHOOK_URL = 'https://hooks.slack.com/services/xxxxxxxxxx'
    WEBHOOK_PROVIDER = 'SLACK'

    webhook = fdl.Webhook(
        name=WEBHOOK_NAME,
        url=WEBHOOK_URL,
        provider=WEBHOOK_PROVIDER
    )
    webhook.create()
    ```

    **Update Webhook**

    * **2.x**:

    ```python
    WEBHOOK_UUID = '00cb316"
"9-7983-497c-8f3c-d25df26543b0'
    WEBHOOK_NAME = 'YOUR_WEBHOOK_NAME'
    WEBHOOK_URL = 'https://hooks.slack.com/services/xxxxxxxxxx'
    WEBHOOK_PROVIDER = 'SLACK'

    webhook = client.update_webhook(
      uuid=WEBHOOK_UUID,
      name=WEBHOOK_NAME,
      url=WEBHOOK_URL,
      provider=WEBHOOK_PROVIDER
    )
    ```

    * **3.x**:

    ```python
    WEBHOOK_ID = '00cb3169-7983-497c-8f3c-d25df26543b0'

    webhook = fdl.Webhook.get(id_=WEBHOOK_ID)
    webhook.name = 'YOUR_WEBHOOK_NAME_CHANGE'
    webhook.update()
    ```

    **Delete Webhook**

    * **2.x**:

    ```python
    WEBHOOK_UUID = '00cb3169-7983"
"-497c-8f3c-d25df26543b0'

    webhook = client.delete_webhook(uuid=WEBHOOK_UUID)
    ```

    * **3.x**:

    ```python
    WEBHOOK_ID = '00cb3169-7983-497c-8f3c-d25df26543b0'

    webhook = fdl.Webhook.get(id_=WEBHOOK_ID)
    webhook.delete()
    ```

    **XAI**

    **Get Explanation**

    * **2.x**:

    ```python
    PROJECT_ID = 'YOUR_PROJECT_NAME'
    MODEL_ID = 'YOUR_MODEL_NAME'

    # RowDataSource
    explain = client.get_explanation(
      project_id=PROJECT_ID,
      model_id=MODEL_ID,
      input_data_source=fdl.RowDataSource(row={})
    )
    # EventIdDataSource
    explain = client.get_explanation(
      project_id=PROJECT_ID,
      model_id=MODEL_ID"
",
      input_data_source=fdl.EventIdDataSource()
    )
    ```

    * **3.x**:

    ```python
    MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'

    # RowDataSource
    model = fdl.Model.get(id_=MODEL_ID)
    explain = model.explain(
        input_data_source=fdl.RowDataSource(row={})
    )

    # EventIdDataSource
    model = fdl.Model.get(id_=MODEL_ID)
    explain = model.explain(
        input_data_source=fdl.EventIdDataSource()
    )
    ```

    **Get Fairness**

    * **2.x**:

    ```python
    PROJECT_ID = 'YOUR_PROJECT_NAME'
    MODEL_ID = 'YOUR_MODEL_NAME'

    # DatasetDataSource
    fairness = client.get_fairness(
        project_id=PROJECT_ID,
        model_id=MODEL_ID,
"
"        data_source=fdl.DatasetDataSource(),
        protected_features=[],
        positive_outcome='',
    )
    # SqlSliceQueryDataSource
    fairness = client.get_fairness(
        project_id=PROJECT_ID,
        model_id=MODEL_ID,
        data_source=fdl.SqlSliceQueryDataSource(),
        protected_features=[],
        positive_outcome='',
    )
    ```

    * **3.x**:

    ```python
    MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'

    # DatasetDataSource
    model = fdl.Model.get(id_=MODEL_ID)
    fairness = model.get_fairness(
        data_source=fdl.DatasetDataSource(),
        protected_features=[],
        positive_outcome='',
    )

    # SqlSliceQueryDataSource
    model = fdl.Model.get(id_=MODEL_ID)
    fairness = model.get_fairness(
        data_source=fdl.Sql"
"SliceQueryDataSource(),
        protected_features=[],
        positive_outcome='',
    )
    ```

    **Get Feature Impact**

    * **2.x**:

    ```python
    PROJECT_ID = 'YOUR_PROJECT_NAME'
    MODEL_ID = 'YOUR_MODEL_NAME'

    # DatasetDataSource
    impact =client.get_feature_impact(
      project_id=PROJECT_ID,
      model_id=MODEL_ID,
      data_source=fdl.DatasetDataSource()
    )

    # SqlSliceQueryDataSource
    impact = client.get_feature_impact(
      project_id=PROJECT_ID,
      model_id=MODEL_ID,
      data_source=fdl.SqlSliceQueryDataSource()
    )
    ```

    * **3.x**:

    ```python
    MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'

    # DatasetDataSource
    model = fdl.Model.get(id_=MODEL_ID)
    impact ="
" model.get_feature_impact(
      data_source=fdl.DatasetDataSource()
    )

    # SqlSliceQueryDataSource
    model = fdl.Model.get(id_=MODEL_ID)
    impact = model.get_feature_impact(
        data_source=fdl.SqlSliceQueryDataSource()
    )
    ```

    **Get Feature Importance**

    * **2.x**:

    ```python
    PROJECT_ID = 'YOUR_PROJECT_NAME'
    MODEL_ID = 'YOUR_MODEL_NAME'

    # DatasetDataSource
    importance = client.get_feature_importance(
      project_id=PROJECT_ID,
      model_id=MODEL_ID,
      data_source=fdl.DatasetDataSource()
    )

    # SqlSliceQueryDataSource
    importance = client.get_feature_importance(
      project_id=PROJECT_ID,
      model_id=MODEL_ID,
      data_source=fdl.SqlSliceQueryDataSource()
    )
    ```

    * **3.x**:

    ```python
    MODEL_ID = 'e38ab"
"1ad-1a50-40b8-8bee-ab33cd8b9b93'

    # DatasetDataSource
    model = fdl.Model.get(id_=MODEL_ID)
    importance = model.get_feature_importance(
            data_source=fdl.DatasetDataSource()
    )

    # SqlSliceQueryDataSource
    model = fdl.Model.get(id_=MODEL_ID)
    importance = model.get_feature_importance(
            data_source=fdl.SqlSliceQueryDataSource()
    )
    ```

    **Get Mutual Information**

    * **2.x**:

    ```python
    PROJECT_ID = 'YOUR_PROJECT_NAME'
    DATASET_ID = 'YOUR_DATASET_NAME'
    QUERY=f'select * from production.{MODEL_NAME} limit 10'

    mutual_info = client.get_mutual_information(
        project_id=PROJECT_ID,
        dataset_id=DATASET_ID,
        query=QUERY,
        COLUMN_NAME=''
    )
    ```

    * **3.x"
"**:

    ```python
    MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'
    QUERY=f'select * from production.{MODEL_NAME} limit 10'

    model = fdl.Model.get(id_=MODEL_ID)
    mutual_info = model.get_mutual_info(
        query=QUERY,
        COLUMN_NAME=''
    )
    ```

    **Get Predictions**

    * **2.x**:

    ```python
    PROJECT_ID = 'YOUR_PROJECT_NAME'
    MODEL_ID = 'YOUR_MODEL_NAME'
    DF = <any dataframe>

    predict = client.get_predictions(
        project_id=PROJECT_ID,
        model_id=MODEL_ID,
        input_df=DF
    )
    ```

    * **3.x**:

    ```python
    MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9"
"b93'
    DF = <any dataframe>

    model = fdl.Model.get(id_=MODEL_ID)
    predict = model.predict(df=DF)
    ```

    **Get Slice**

    * **2.x**:

    ```python
    PROJECT_ID = 'YOUR_PROJECT_NAME'
    QUERY=f'select * from production.{MODEL_NAME} limit 10'

    slice = client.get_slice(
        project_id=PROJECT_ID,
        sql_query=QUERY
    )
    ```

    * **3.x**:

    ```python
    MODEL_ID = 'e38ab1ad-1a50-40b8-8bee-ab33cd8b9b93'
    QUERY=f'select * from production.{MODEL_NAME} limit 10'

    model = fdl.Model.get(id_=MODEL_ID)
    slice = model.get_slice(query=QUERY)
    ```

    **Jobs**

    **Get Job**

    * **2.x**:

    ```"
"python
    JOB_ID = '904e33e0-c4a2-45ca-b8dc-43c9f3ac5519'

    job = client.get_job(uuid=JOB_ID)
    ```

    * **3.x**:

    ```python
    JOB_ID = '904e33e0-c4a2-45ca-b8dc-43c9f3ac5519'

    job = fdl.Job.get(id_=JOB_ID)
    ```

    **Wait for Job**

    * **2.x**:

    ```python
    JOB_ID = '904e33e0-c4a2-45ca-b8dc-43c9f3ac5519'

    job = client.wait_for_job(uuid=JOB_ID
    ```

    * **3.x**:

    ```python
    JOB_ID = '904e33e0-c4a2-45ca-b8dc-43c9f"
"3ac5519'

    job = fdl.Job.get(id_=JOB_ID)
    job.wait()
    ```

    **Watch Job**

    * **2.x**:

    ```python
    JOB_ID = '904e33e0-c4a2-45ca-b8dc-43c9f3ac5519'

    job = client.watch_job(uuid=JOB_ID)
    ```

    * **3.x**:

    ```python
    JOB_ID = '904e33e0-c4a2-45ca-b8dc-43c9f3ac5519'

    job = fdl.Job.get(id_=JOB_ID)
    job.watch()
    ```



{% include ""../.gitbook/includes/main-doc-dev-footer.md"" %}
"
"---
title: About Client 3.x
slug: about-client-3x
excerpt: ''
createdAt: Mon Mar 18 2024 14:18:09 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Oct 24 2024 20:29:08 GMT+0000 (Coordinated Universal Time)
---

# About Client 3.x

The Fiddler Client contains many useful methods for sending and receiving data to and from the Fiddler platform.

Fiddler provides a Python Client that allows you to connect to Fiddler directly from a Python notebook or automated pipeline.

Each client function is documented with a description, usage information, and code examples.

### Initialization

The Client object is used to communicate with Fiddler.

#### Import Fiddler

To import fiddler client, we can use the below method:

```python
import fiddler as fdl
from fiddler import Model, Project
```

#### Authenticate and Connect

In order to use the client, you'll need to provide authentication details as shown below.

**Usage params**

| Parameter | Type            | Default | Description                                                                                                             |
| --------- | --------------- | ------- | ----------------------------------------------------------------------------------------------------------------------- |
| url       | str             | -       | The URL of Fiddler Platform to make a connection to.                                                                    |
| token     | str             | -       | The authorization token used to authenticate with Fiddler. Can be found on the Credentials tab of the Settings page.    |
| proxies   | Optional\[dict] | -       | Dictionary mapping protocol to the URL of the proxy                                                                     |
| timeout   | Optional\[int]  | 60      | Seconds to wait for the server to send data before giving up.                                                           |
| verify    | Optional\[bool] | True    | Controls whether we verify the server‚Äôs TLS certificate.                                                                |
| validate  | Optional\[bool] | True    | Whether to validate the server/Client Version compatibility. Some functionalities might not work if this is turned off. |

**Return params**

None

**Usage**

```python
fdl.init(
  url: str,
  token: str,
  proxies: dict[str, Any],
  timeout: int,
  verify: bool,
  validate: bool
)
```

```python
fdl.init(
  url=""https://xyz.fiddler.ai"",
  token=""abc""
)
```

**Raises**

| Error code         | Issue                                                           |
| ------------------ | --------------------------------------------------------------- |
| ValueError         | URL or token missing while initializing the client.             |
| IncompatibleClient | Fiddler Platform version is not compatible with Client version. |

> üìò Info
>
> To maximize compatibility, **please ensure that your Client Version matches the server version for your Fiddler instance.**
>
> When you connect to Fiddler using the code on the right, you'll receive a notification if there is a version mismatch between the client and server.
>
> You can install a specific version of fiddler-client using pip:\
> `pip install fiddler-client==X.X.X`



{% include ""../.gitbook/includes/main-doc-dev-footer.md"" %}
"
"---
title: API Methods
slug: api-methods-30
excerpt: ''
createdAt: Mon Mar 18 2024 13:41:35 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu May 09 2024 15:48:54 GMT+0000 (Coordinated Universal Time)
---

## Alerts

## AlertRule

AlertRule object contains the below fields.

| Parameter           | Type                                                            | Default | Description                                                                                                                                   |
| ------------------- | --------------------------------------------------------------- | ------- | --------------------------------------------------------------------------------------------------------------------------------------------- |
| id                  | UUID                                                            | -       | Unique identifier of the AlertRule.                                                                                                         |
| name                | str                                                             | -       | Unique name of the AlertRule.                                                                                                                |
| model               | [Model](api-methods-30.md#model)                                | -       | The associated model details.                                                                                                                         |
| project             | [Project](api-methods-30.md#project)                            | -       | The"
" associated project details                                                                                         |
| baseline            | Optional\[[Baseline](api-methods-30.md#baseline)]               | None    | The associated baseline.                                                                                                        |
| segment             | Optional\[[Segment](api-methods-30.md#segment)]                | None    | Details of segment for the alert.                                                                                                             |
| priority            | Union\[str, [Priority](api-methods-30.md#priority)]             | -       | <p>To set the priority for the AlertRule. Select from:<br>1. Priority.LOW<br>2. Priority.MEDIUM<br>3. Priority.HIGH.</p>                     |
| compare\_to         | Union\[str, [CompareTo](api-methods-30.md#compareto)]                                          | -       | <p>Select from the two:<br>1. CompareTo.RAW_VALUE<br>2. CompareTo.TIME_PERIOD</p>                                                             |
| metric\_id          | Union\["
"str, UUID]                                               | -       | Type of alert metric UUID or string denoting [metric](#alert-metric-id) ID.                                                                                     |
| critical\_threshold | float                                                           | -       | Critical alert is triggered when this value satisfies the condition to the selected metric_id.                                                          |
| condition           | Union\[str, [AlertCondition](api-methods-30.md#alertcondition)] | -       | <p>Select from:<br>1. AlertCondition.LESSER<br>2. AlertCondition.GREATER</p>                                                                  |
| bin\_size           | Union\[str, [BinSize](api-methods-30.md#binsize)]               | -       | Bin size for example fdl.BinSize.HOUR.                                                                                                               |
| columns             | Optional\[List\[str]]                                           | None    | List of 1 or more column names for the rule to evaluate. Use ['\_\_ANY\_\_'] to evaluate all"
" columns.                                 |
| baseline\_id        | Optional\[UUID]                                                 | None    | UUID of the baseline for the alert.                                                                                                           |
| segment\_id         | Optional\[UUID]                                                 | None    | UUID of segment for the alert                                                                                                                 |
| compare\_bin\_delta | Optional\[int]                                                  | None    | Indicates previous period for comparison e.g. for fdl.BinSize.DAY, compare_bin_delta=1 will compare 1 day back, compare_bin_delta=7 will compare 7 days back.                                                                          |
| warning\_threshold  | Optional\[float]                                                | None    | Warning alert is triggered when this value satisfies the condition to the selected metric_id.                                                           |
| created\_at         | datetime                                                        | -       | The creation timestamp.                                                                                                         |
| updated\_at         | datetime                                                        | -       | The timestamp of most recent update.                                                                                                  |
| evaluation\_delay   | int                                                             | 0       | Specifies"
" a delay in hours before AlertRule is evaluated. The delay period must not exceed one year(8760 hours). |

### constructor()

Initialize a new AlertRule on Fiddler Platform.

**Parameters**

| Parameter           | Type                                                            | Default | Description                                                                                                                                   |
| ------------------- | --------------------------------------------------------------- | ------- | --------------------------------------------------------------------------------------------------------------------------------------------- |
| name                | str                                                             | -       | Unique name of the model                                                                                                                      |
| model\_id           | UUID                                                            | -       | Details of the model.                                                                                                                         |
| metric\_id          | Union\[str, UUID]                                               | -       | Type of alert metric UUID or enum.                                                                                                            |
| columns             | Optional\[List\[str]]                                           | None    | List of column names on which AlertRule is to be created. It can take \['\_\_ANY\_\_'] to check for all columns.                                 |
| baseline\_id        | Optional\[UUID]                                                 | None    | UUID of the"
" baseline for the alert.                                                                                                           |
| segment\_id         | Optional\[UUID]                                                 | None    | UUID of the segment for the alert.                                                                                                            |
| priority            | Union\[str, [Priority](api-methods-30.md#priority)]             | -       | <p>To set the priority for the AlertRule. Select from:<br>1. Priority.LOW<br>2. Priority.MEDIUM<br>3. Priority.HIGH.</p>                     |
| compare\_to         | Union\[str, [CompareTo](api-methods-30.md#compareto)]                                          | -       | <p>Select from the two:<br>1. CompareTo.RAW_VALUE (absolute alert)<br>2. CompareTo.TIME_PERIOD (relative alert)</p>                           |
| compare\_bin\_delta | Optional\[int]                                                  | None    | Compare the metric to a previous time period in units of bin\_size.                                                                           |
| warning\_"
"threshold  | Optional\[float]                                                | None    | Threshold value to crossing which a warning level severity alert will be triggered.                                                           |
| critical\_threshold | float                                                           | -       | Threshold value to crossing which a critical level severity alert will be triggered.                                                          |
| condition           | Union\[str, [AlertCondition](api-methods-30.md#alertcondition)] | -       | <p>Select from:<br>1. AlertCondition.LESSER<br>2. AlertCondition.GREATER</p>                                                                  |
| bin\_size           | Union\[str, [BinSize](api-methods-30.md#binsize)]               | -       | Size of the bin for AlertRule.                                                                                                               |
| evaluation\_delay   | int                                                             | 0       | To introduce a delay in the evaluation of the alert, specifying the duration in hours. The delay period must not exceed one year(8760 hours). |

**Usage**

```python
MODEL"
"_NAME = 'test_model'
PROJECT_NAME = 'test_project'
BASELINE_NAME = 'test_baseline'
SEGMENT_NAME = 'test_segment'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)
baseline = fdl.Baseline.from_name(name=BASELINE_NAME, model_id=model.id)
segment = fdl.Segment.from_name(name=SEGMENT_NAME, model_id=model.id)

alert_rule = fdl.AlertRule(
    name='Bank Churn Drift Hawaii Region',
    model_id=model.id,
    baseline_id=baseline.id,
    metric_id='jsd',
    priority=fdl.Priority.HIGH,
    compare_to=fdl.CompareTo.TIME_PERIOD,
    compare_bin_delta=1,
    condition=fdl.AlertCondition.GREATER,
    bin_size=fdl.BinSize.DAY,
    critical_threshold=0.5,
    warning_threshold=0.1,
"
"    columns=['gender', 'creditscore'],
    segment_id=segment.id,
    evaluation_delay=1
)
```

***

### create()

Create a new AlertRule.

**Parameters**

No

**Usage**

```python
MODEL_NAME = 'test_model'
PROJECT_NAME = 'test_project'
BASELINE_NAME = 'test_baseline'
SEGMENT_NAME = 'test_segment'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)
baseline = fdl.Baseline.from_name(name=BASELINE_NAME, model_id=model.id)
segment = fdl.Segment.from_name(name=SEGMENT_NAME, model_id=model.id)

alert_rule = fdl.AlertRule(
    name='Bank Churn Drift Hawaii Region',
    model_id=model.id,
    baseline_id=baseline.id,
    metric_id='jsd',
    priority=fdl.Priority.HIGH,
    compare_to=fdl.CompareTo.TIME"
"_PERIOD,
    compare_bin_delta=1,
    condition=fdl.AlertCondition.GREATER,
    bin_size=fdl.BinSize.DAY,
    critical_threshold=0.5,
    warning_threshold=0.1,
    columns=['gender', 'creditscore'],
    segment_id=segment.id,
    evaluation_delay=1
).create()
```

**Returns**

| Return Type                               | Description          |
| ----------------------------------------- | -------------------- |
| [AlertRule](api-methods-30.md#alert-rule) | AlertRule instance. |

### get()

Get a single AlertRule.

**Parameters**

| Parameter | Type | Default | Description                           |
| --------- | ---- | ------- | ------------------------------------- |
| id\_      | UUID | -       | Unique identifier for the AlertRule. |

**Usage**

```python
ALERT_RULE_ID='ed8f18e6-c319-4374-8884-71126a6bab85'

alert = fdl.Alert"
"Rule.get(id_=ALERT_RULE_ID)
```

**Returns**

| Return Type                               | Description          |
| ----------------------------------------- | -------------------- |
| [AlertRule](api-methods-30.md#alert-rule) | AlertRule instance. |

**Raises**

| Error code | Issue                                                               |
| ---------- | ------------------------------------------------------------------- |
| NotFound   | AlertRule with given identifier not found.                         |
| Forbidden  | Current user may not have permission to view details of AlertRule. |

***

### list()

Get a list of AlertRules .

**Parameters**

| Parameter    | Type                  | Default | Description                                                                                                             |
| ------------ | --------------------- | ------- | ----------------------------------------------------------------------------------------------------------------------- |
| model\_id    | Union\[str, UUID]     | None    | Unique identifier for the model to which AlertRule belongs.                                                            |
| project\_id  | Optional\[UUID]       | None    | Unique identifier for the project to which AlertRule belongs                                                           |
| metric\_id   | Optional\["
"UUID]       | None    | Type of alert metric UUID or enum.                                                                                      |
| columns      | Optional\[List\[str]] | None    | List of column names on which AlertRule is to be created. It can take \['**ANY**'] to check for all columns.           |
| baseline\_id | Optional\[UUID]       | None    | UUID of the baseline for the AlertRule.                                                                                     |
| ordering     | Optional\[List\[str]] | None    | List of AlertRule fields to order by. Eg. \[‚Äòalert\_time\_bucket‚Äô] or \[‚Äò- alert\_time\_bucket‚Äô] for descending order. |

**Usage**

```python
MODEL_ID = '299c7b40-b87c-4dad-bb94-251dbcd3cbdf'

alerts = fdl.AlertRule.list(model_id=MODEL_ID)
```

**Returns**

| Return Type                                                 | Description                       |
| ------------------------------------------------"
"----------- | --------------------------------- |
| Iterator\[[AlertRule](api-methods-30.md#alertrule)] | Iterator of AlertRule instances. |

***

### delete()

Delete an existing AlertRule.

**Parameters**

| Parameter | Type | Default | Description                     |
| --------- | ---- | ------- | ------------------------------- |
| id\_      | UUID | -       | Unique UUID of the AlertRule . |

**Usage**

```python
MODEL_ID = '299c7b40-b87c-4dad-bb94-251dbcd3cbdf'
ALERT_RULE_NAME = 'Testing Alert API'

alert_rules = fdl.AlertRule.list(model_id=MODEL_ID)

for alert_rule in alert_rules:
    if alert_rule.name == ALERT_RULE_NAME:
        alert_rule.delete()
        break
```

**Returns**

No

**Raises**

| Error code | Issue                                                               |
| ---------- | ------------------------------------------------------------------- |
| NotFound   | AlertRule with given identifier not found.                         |
"
"| Forbidden  | Current user may not have permission to view details of AlertRule. |

***

### enable\_notifications()

Enable an AlertRule's notification.

**Parameters**

| Parameter | Type | Default | Description                     |
| --------- | ---- | ------- | ------------------------------- |
| id\_      | UUID | -       | Unique UUID of the AlertRule . |

**Usage**

```python
ALERT_NAME = ""YOUR_ALERT_NAME""
MODEL_ID = '299c7b40-b87c-4dad-bb94-251dbcd3cbdf'

alerts_list = fdl.AlertRule.list(model_id=MODEL_ID)
for alert_rule in alerts_list:
    if ALERT_NAME == alert.name:
        alert_rule.enable_notifications()
        break
```

**Returns**

None

**Raises**

| Error code | Issue                                                               |
| ---------- | ------------------------------------------------------------------- |
| NotFound   | AlertRule with given identifier not found.                         |
| Forbidden  | Current user may not have permission to view"
" details of AlertRule. |

***

### disable\_notifications()

Disable notifications for an AlertRule.

**Parameters**

| Parameter | Type | Default | Description                     |
| --------- | ---- | ------- | ------------------------------- |
| id\_      | UUID | -       | Unique UUID of the AlertRule . |

**Usage**

```python
ALERT_NAME = ""YOUR_ALERT_NAME""
MODEL_ID = '299c7b40-b87c-4dad-bb94-251dbcd3cbdf'

alerts_list = fdl.AlertRule.list(model_id=MODEL_ID)
for alert_rule in alerts_list:
    if ALERT_NAME == alert.name:
        alert_rule.disable_notifications()
```

**Returns**

None

**Raises**

| Error code | Issue                                                               |
| ---------- | ------------------------------------------------------------------- |
| NotFound   | AlertRule with given identifier not found.                         |
| Forbidden  | Current user may not have permission to view details of AlertRule. |

***

## Alert Notifications

Alert notifications for an"
" AlertRule.

| Parameter           | Type                   | Default | Description                                                |
| ------------------- | ---------------------- | ------- | ---------------------------------------------------------- |
| emails              | Optional\[List\[str]]  | None    | List of emails to send notification to.                    |
| PagerDuty\_services | Optional\[List\[str]]  | None    | List of PagerDuty services to trigger the alert to.        |
| PagerDuty\_severity | Optional\[str]         | None    | Severity of PagerDuty.                                     |
| webhooks            | Optional\[List\[UUID]] | None    | List of [webhook](api-methods-30.md#webhook-object) UUIDs. |

### set\_notification\_config()

Set NotificationConfig for an AlertRule.

**Parameters**

| Parameter           | Type                   | Default | Description                                                |
| ------------------- | ---------------------- | ------- | ---------------------------------------------------------- |
| emails              | Optional\[List\[str]]  | None   "
" | List of emails to send notification to.                    |
| PagerDuty\_services | Optional\[List\[str]]  | None    | List of PagerDuty services to trigger the alert to.        |
| PagerDuty\_severity | Optional\[str]         | None    | Severity of PagerDuty.                                     |
| webhooks            | Optional\[List\[UUID]] | None    | List of [webhook](api-methods-30.md#webhook-object) UUIDs. |

**Usage**

```python
ALERT_RULE_ID = '72e8835b-cde2-4dd2-a435-a35d4b51196b'
rule = fdl.AlertRule.get(id_=ALERT_RULE_ID)

rule.set_notification_config(
  emails=['abc@xyz.com', 'admin@xyz.com'],
  webhooks=['8b403d99-530a-4c5a-a519-89688d65ddc1'], #"
" Webhook UUID
  PagerDuty_services = ['PagerDuty_service_1','PagerDuty_service_2'], # PagerDuty service names
  PagerDuty_severity = 'critical' # Only applies to PagerDuty, ignored otherwise
)

```

**Returns**

| Return Type                                                | Description                            |
| ---------------------------------------------------------- | -------------------------------------- |
| NotificationConfig                                         | Alert notification settings for an AlertRule. |

> If `PagerDuty_severity` is passed without specifying `PagerDuty_services` then the `PagerDuty_severity` is ignored.

**Raises**

| Error code | Issue                             |
| ---------- | --------------------------------- |
| BadRequest | All 4 input parameters are empty. |
| ValueError | Webhook ID is incorrect.          |

### get\_notification\_config()

Get notification configuration for an AlertRule.

**Parameters**

None

**Usage**

```python
ALERT_RULE_ID = '72e8835b-cde2-4"
"dd2-a435-a35d4b51196b'
rule = fdl.AlertRule.get(id_=ALERT_RULE_ID)

rule.get_notification_config()
```

**Returns**

| Return Type         | Description                            |
| ------------------- | -------------------------------------- |
| NotificationConfig  | Alert notification settings for an AlertRule. |

**Raises**

| Error code | Issue                             |
| ---------- | --------------------------------- |
| BadRequest | All 4 input parameters are empty. |
| ValueError | Webhook ID is incorrect.          |

***

## Triggered Alerts

## AlertRecord

An AlertRecord details an AlertRule's triggered alert.

| Parameter                    | Type             | Default | Description                                                                                                                                     |
| ---------------------------- | ---------------- | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| id                           | UUID             | -       | Unique identifier for the triggered AlertRule.                                                                                                 |
| alert\_rule\_id              | UUID             | -       | Unique identifier for the AlertRule which needs to be triggered.                                                                               |
"
"| alert\_run\_start\_time      | int              | -       | Timestamp of AlertRule evaluation in epoch.                                                                                                    |
| alert\_time\_bucket          | int              | -       | Timestamp pointing to the start of the time bucket in epoch.                                                                                    |
| alert\_value                 | float            | -       | Value of the metric for alert\_time\_bucket.                                                                                                    |
| baseline\_time\_bucket       | Optional\[int]   | None    | Timestamp pointing to the start of the baseline time bucket in epoch, only if AlertRule is of 'time period' based comparison.                  |
| baseline\_value              | Optional\[float] | None    | Value of the metric for baseline\_time\_bucket.                                                                                                 |
| is\_alert                    | bool             | -       | Boolean to indicate if alert was supposed to be triggered.                                                                                      |
| severity                     | str              | -       | Severity of alert represented by [Severity](api-method"
"s-30.md#severity), calculated based on value of metric and AlertRule thresholds. |
| failure\_reason              | str              | -       | String message if there was a failure sending notification.                                                                                     |
| message                      | str              | -       | String message sent as a part of email notification.                                                                                            |
| feature\_name                | Optional\[str]   | None    | Name of feature for which alert was triggered.                                                                                                  |
| alert\_record\_main\_version | int              | -       | Main version of triggered alert record in int, incremented when the value of severity changes.                                                  |
| alert\_record\_sub\_version  | int              | -       | Sub version of triggered alert record in int, incremented when another alert with same severity as before is triggered.                         |
| created\_at                  | datetime         | -       | Time at which trigger AlertRule was created.                                                                                                   |
| updated\_at                  | datetime         | -"
"       | Latest time at which trigger AlertRule was updated.                                                                                            |

### list()

List AlertRecords triggered for an AlertRule.

**Parameters**

| Parameter       | Type                  | Default | Description                                                                                                             |
| --------------- | --------------------- | ------- | ----------------------------------------------------------------------------------------------------------------------- |
| alert\_rule\_id | UUID                  | -       | Unique identifier for the AlertRule which needs to be triggered.                                                       |
| start\_time     | Optional\[datetime]   | None    | Start time to filter trigger alerts in yyyy-MM-dd format, inclusive.                                                    |
| end\_time       | Optional\[datetime]   | None    | End time to filter trigger alerts in yyyy-MM-dd format, inclusive.                                                      |
| ordering        | Optional\[List\[str]] | None    | List of AlertRule fields to order by. Eg. \[‚Äòalert\_time\_bucket‚Äô] or \[‚Äò- alert\_time\_bucket‚Äô] for descending order. |

**Usage**

```python"
"
ALERT_NAME = ""YOUR_ALERT_NAME""
MODEL_ID = '299c7b40-b87c-4dad-bb94-251dbcd3cbdf'
triggered_alerts = None

alerts_list = fdl.AlertRule.list(model_id=MODEL_ID)
for alert_rule in alerts_list:
    if ALERT_NAME == alert.name:
        triggered_alerts = fdl.AlertRecord.list(
            alert_rule_id=ALERT_RULE_ID,
            start_time=datetime(2024, 9, 1), # optional
            end_time=datetime(2024, 9, 24), # optional
            ordering = ['alert_time_bucket'], # ['-alert_time_bucket'] for descending sort, optional.
        )
```

**Returns**

| Return Type                                                | Description                                                   |
| ---------------------------------------------------------- | ------------------------------------------------------------- |
| Iterator\[[AlertRecord](api-methods-30.md#alertrecord)]    | Iterable of triggered AlertRule instances for an AlertRule. |

"
"***

## Baselines

Baseline datasets are used for making comparisons with production data.

A baseline dataset should be sampled from your model's training set, so it can serve as a representation of what the model expects to see in production.

## Baseline

Baseline object contains the below fields.

| Parameter    | Type                                 | Default | Description                                                                                                                  |
| ------------ | ------------------------------------ | ------- | ---------------------------------------------------------------------------------------------------------------------------- |
| id           | UUID                                 | -       | Unique identifier for the baseline.                                                                                          |
| name         | str                                  | -       | Baseline name.                                                                                                               |
| type\_       | [BaselineType](api-methods-30.md#baselinetype) | -       | Baseline type can be static (Pre-production or production) or rolling(production). |
| start\_time  | Optional\[int]                       | None    | Epoch to be used as start time for STATIC baseline.                                                                          |
| end\_time    | Optional\[int]                       | None"
"    | Epoch to be used as end time for STATIC baseline.                                                                            |
| offset       | Optional\[int]                       | None    | Offset in seconds relative to current time to be used for ROLLING baseline.                                                  |
| window\_size | Optional\[int]                       | None    | Span of window in seconds to be used for ROLLING baseline.                                                                   |
| row\_count   | Optional\[int]                       | None    | Number of rows in baseline.                                                                                                  |
| model        | [Model](api-methods-30.md#model)     | -       | Details of the model.                                                                                                        |
| project      | [Project](api-methods-30.md#project) | -       | Details of the project to which the baseline belongs.                                                                        |
| dataset      | [Dataset](api-methods-30.md#dataset) | -       | Details of the dataset from which baseline is derived.                                                                       |
| created\_at  | datetime"
"                             | -       | Time at which baseline was created.                                                                                          |
| updated\_at  | datetime                             | -       | Latest time at which baseline was updated.                                                                                   |

### constructor()

Initialize a new baseline instance.

**Parameters**

| Parameter         | Type                                 | Default | Description                                                                                                                                                       |
| ----------------- | ------------------------------------ | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| name              | str                                  | -       | Unique name of the baseline.                                                                                                                                      |
| model\_id         | UUID                                 | -       | Unique identifier for the model to add baseline to.                                                                                                               |
| environment       | [EnvType](api-methods-30.md#envtype) | -       | Type of environment. Can either be PRE\_PRODUCTION or PRODUCTION.                                                                                                 |
| type\_            | [BaselineType](api-methods-30.md#baselinetype) | -       | Baseline type can be static (pre-production or production) or rolling(production"
").                                      |
| dataset\_id       | Optional\[UUID]                      | None    | Unique identifier for the dataset on which the baseline is created.                                                                                               |
| start\_time       | Optional\[int]                       | None    | Epoch to be used as start time for STATIC baseline.                                                                                                               |
| end\_time         | Optional\[int]                       | None    | Epoch to be used as end time for STATIC baseline.                                                                                                                 |
| offset\_delta     | Optional\[int]                       | None    | <p>Number of times of <a href=""api-methods-30.md#windowbinsize"">WindowBinSize</a> to be used for ROLLING baseline.<br>offset = offset_delta * window_bin_size</p> |
| window\_bin\_size | Optional\[str]                       | None    | Span of window in seconds to be used for ROLLING baseline using [WindowBinSize](api-methods-30.md#window"
"binsize)                                                  |

**Usage**

```python
BASELINE_NAME = 'YOUR_BASELINE_NAME'
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'
DATASET_NAME = 'YOUR_DATASET_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)
dataset = fdl.Dataset.from_name(name=DATASET_NAME, model_id=model.id)

baseline = fdl.Baseline(
        name=BASELINE_NAME,
        model_id=model.id,
        environment=fdl.EnvType.PRE_PRODUCTION,
        dataset_id=dataset.id,
        type_=fdl.BaselineType.STATIC,
    )
```

### create()

Adds a baseline to Fiddler.

**Parameters**

No

**Usage**

```python
BASELINE_NAME = 'YOUR_BASELINE_NAME'
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'
DATASET_NAME"
" = 'YOUR_DATASET_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)
dataset = fdl.Dataset.from_name(name=DATASET_NAME, model_id=model.id)

baseline = fdl.Baseline(
        name=BASELINE_NAME,
        model_id=model.id,
        environment=fdl.EnvType.PRE_PRODUCTION,
        dataset_id=dataset.id,
        type_=fdl.BaselineType.STATIC,
    ).create()
```

**Returns**

| Return Type                            | Description        |
| -------------------------------------- | ------------------ |
| [Baseline](api-methods-30.md#baseline) | Baseline instance. |

**Raises**

| Error code | Issue                                                                  |
| ---------- | ---------------------------------------------------------------------- |
| Conflict   | Baseline with same name may exist in project .                         |
| NotFound   | Given dataset may not exist in for the input model.                    |
| ValueError |"
" Validation failures like wrong window size, start\_time, end\_time etc |

### get()

Get baseline from Fiddler Platform based on UUID.

**Parameters**

| Parameter | Type | Default | Description                         |
| --------- | ---- | ------- | ----------------------------------- |
| id\_      | UUID | -       | Unique identifier for the baseline. |

**Usage**

```python
BASELINE_ID = 'af05646f-0cef-4638-84c9-0d195df2575d'
baseline = fdl.Baseline.get(id_=BASELINE_ID)
```

**Returns**

| Return Type                            | Description        |
| -------------------------------------- | ------------------ |
| [Baseline](api-methods-30.md#baseline) | Baseline instance. |

**Raises**

| Error code | Issue                                                             |
| ---------- | ----------------------------------------------------------------- |
| NotFound   | Baseline with given identifier not found.                         |
| Forbidden  | Current user may not have permission to view details of"
" baseline. |

***

### from\_name()

Get baseline from Fiddler Platform based on name.

**Parameters**

| Parameter | Type        | Default | Description                      |
| --------- | ----------- | ------- | -------------------------------- |
| name      | str         | -       | Name of the baseline.            |
| model\_id | UUID \| str | -       | Unique identifier for the model. |

**Usage**

```python
BASELINE_NAME = 'YOUR_BASELINE_NAME'
MODEL_ID = '4531bfd9-2ca2-4a7b-bb5a-136c8da09ca2'

baseline = fdl.Baseline.from_name(
    name=BASELINE_NAME,
    model_id=MODEL_ID
)
```

**Returns**

| Return Type                            | Description        |
| -------------------------------------- | ------------------ |
| [Baseline](api-methods-30.md#baseline) | Baseline instance. |

**Raises**

| Error code | Issue                                                             |
| ----------"
" | ----------------------------------------------------------------- |
| NotFound   | Baseline with given identifier not found.                         |
| Forbidden  | Current user may not have permission to view details of baseline. |

***

### list()

List all baselines accessible to user.

**Parameters**

| Parameter | Type | Default | Description                                 |
| --------- | ---- | ------- | ------------------------------------------- |
| model\_id | UUID | -       | UUID of the model associated with baseline. |

**Usage**

```python
MODEL_ID = '4531bfd9-2ca2-4a7b-bb5a-136c8da09ca2'
baselines = fdl.Baseline.list(model_id=MODEL_ID)
```

**Returns**

| Return Type                                       | Description                       |
| ------------------------------------------------- | --------------------------------- |
| Iterable\[[Baseline](api-methods-30.md#baseline)] | Iterable of all baseline objects. |

**Raises**

| Error code | Issue                                                             |
| ---------- | ----------------------------------------------------------------- |
| Forbidden"
"  | Current user may not have permission to view details of baseline. |

***

### delete()

Deletes a baseline.

**Parameters**

| Parameter | Type | Default | Description                   |
| --------- | ---- | ------- | ----------------------------- |
| id\_      | UUID | -       | Unique UUID of the baseline . |

**Usage**

```python
BASELINE_NAME = 'YOUR_BASELINE_NAME'
MODEL_ID = '4531bfd9-2ca2-4a7b-bb5a-136c8da09ca2'

baseline = fdl.Baseline.from_name(name=BASELINE_NAME, model_id=MODEL_ID)
baseline.delete()
```

**Returns**

None

**Raises**

| Error code | Issue                                                    |
| ---------- | -------------------------------------------------------- |
| NotFound   | Baseline with given identifier not found.                |
| Forbidden  | Current user may not have permission to delete baseline. |

***

## Custom Metrics

User-defined metrics to extend Fiddler's built-in"
" metrics.

## CustomMetric

CustomMetric object contains the below parameters.

| Parameter   | Type           | Default | Description                                                  |
| ----------- | -------------- | ------- | ------------------------------------------------------------ |
| id          | UUID           | -       | Unique identifier for the custom metric.                     |
| name        | str            | -       | Custom metric name.                                          |
| model\_id   | UUID           | -       | UUID of the model in which the custom metric is being added. |
| definition  | str            | -       | Definition of the custom metric.                             |
| description | Optional\[str] | None    | Description of the custom metric.                            |
| created\_at | datetime       | -       | Time of creation of custom metric.                           |

### constructor()

Initialize a new custom metric.

**Parameters**

| Parameter   | Type           | Default | Description                                                  |
| ----------- | -------------- | ------- | ------------------------------------------------------------ |
| name        | str            | -       | Custom metric"
" name.                                          |
| model\_id   | UUID           | -       | UUID of the model in which the custom metric is being added. |
| definition  | str            | -       | Definition of the custom metric.                             |
| description | Optional\[str] | None    | Description of the custom metric.                            |

**Usage**

```python
METRIC_NAME = 'YOUR_CUSTOM_METRIC_NAME'
MODEL_ID = '4531bfd9-2ca2-4a7b-bb5a-136c8da09ca2'

metric = fdl.CustomMetric(
    name=METRIC_NAME,
    model_id=MODEL_ID,
    definition=""average(if(\""spend_amount\"">1000, \""spend_amount\"", 0))"", #Use Fiddler Query Language (FQL) to define your custom metrics
    description='Get average spend for users spending over $1000',
)
```

### get()

Get CustomMetric from Fidd"
"ler Platform based on model UUID.

**Parameters**

| Parameter | Type | Default | Description                                           |
| --------- | ---- | ------- | ----------------------------------------------------- |
| model\_id | UUID | -       | UUID of the model associated with the custom metrics. |

**Usage**

```python
MODEL_ID = '4531bfd9-2ca2-4a7b-bb5a-136c8da09ca2'

metrics = fdl.CustomMetric.list(model_id=MODEL_ID)
```

**Returns**

| Return Type                                               | Description                            |
| --------------------------------------------------------- | -------------------------------------- |
| Iterable\[[CustomMetric](api-methods-30.md#custommetric)] | Iterable of all custom metric objects. |

**Raises**

| Error code | Issue                                                                  |
| ---------- | ---------------------------------------------------------------------- |
| Forbidden  | Current user may not have permission to view details of custom metric. |

### from\_name()

Get CustomMetric from Fiddler Platform based on name and model UUID.

"
"**Parameters**

| Parameter | Type        | Default | Description                      |
| --------- | ----------- | ------- | -------------------------------- |
| name      | str         | -       | Name of the custom metric.       |
| model\_id | UUID \| str | -       | Unique identifier for the model. |

**Usage**

```python
METRIC_NAME = 'YOUR_CUSTOM_METRIC_NAME'
MODEL_ID = '4531bfd9-2ca2-4a7b-bb5a-136c8da09ca2'

metric = fdl.CustomMetric.from_name(
    name=METRIC_NAME,
    model_id=MODEL_ID
)
```

**Returns**

| Return Type                                    | Description             |
| ---------------------------------------------- | ----------------------- |
| [CustomMetric](api-methods-30.md#custommetric) | Custom Metric instance. |

**Raises**

| Error code | Issue                                                                  |
| ---------- | ---------------------------------------------------------------------- |
| NotFound   | Custom metric with given identifier not found."
"                         |
| Forbidden  | Current user may not have permission to view details of custom metric. |

### create()

Creates a custom metric for a model on Fiddler Platform.

**Parameters**

None

**Usage**

```python
METRIC_NAME = 'YOUR_CUSTOM_METRIC_NAME'
MODEL_ID = '4531bfd9-2ca2-4a7b-bb5a-136c8da09ca2'

metric = fdl.CustomMetric(
    name=METRIC_NAME,
    model_id=MODEL_ID,
    definition=""average(if(\""spend_amount\"">1000, \""spend_amount\"", 0))"", #Use Fiddler Query Language (FQL) to define your custom metrics
    description='Get average spend for users spending over $1000',
).create()
```

**Returns**

| Return Type                                    | Description             |
| ---------------------------------------------- | ----------------------- |
| [CustomMetric](api-methods-30.md#custommetric)"
" | Custom Metric instance. |

**Raises**

| Error code | Issue                                               |
| ---------- | --------------------------------------------------- |
| Conflict   | Custom metric with same name may exist in project . |
| BadRequest | Invalid definition.                                 |
| NotFound   | Given model may not exist.                          |

### delete()

Delete a custom metric.

**Parameters**

| Parameter | Type | Default | Description                       |
| --------- | ---- | ------- | --------------------------------- |
| id\_      | UUID | -       | Unique UUID of the custom metric. |

**Usage**

```python
METRIC_NAME = 'YOUR_CUSTOM_METRIC_NAME'
MODEL_ID = '4531bfd9-2ca2-4a7b-bb5a-136c8da09ca2'

metric = fdl.CustomMetric.from_name(name=METRIC_NAME, model_id=MODEL_ID)
metric.delete()
```

**Returns**

No

**Raises**

| Error code | Issue                                                         |
| ---------- | ------------------------------------------------------------- |
|"
" NotFound   | Custom metric with given identifier not found.                |
| Forbidden  | Current user may not have permission to delete custom metric. |

***

## Datasets

Datasets (or baseline datasets) are used for making comparisons with production data.

***

## Dataset

Dataset object contains the below parameters.

| Parameter    | Type                                           | Default | Description                                               |
| ------------ | ---------------------------------------------- | ------- | --------------------------------------------------------- |
| id           | UUID                                           | -       | Unique identifier for the dataset.                        |
| name         | str                                            | -       | Dataset name.                                             |
| row\_count   | int                                            | None    | Number of rows in dataset.                                |
| model_id     | [Model](api-methods-30.md#model)               | -       | Unique identifier of the associated model                 |
| project_id   | [Project](api-methods-30.md#project)           | -       | Unique identifier of the associated project               |

***

### get"
"()

Get dataset from Fiddler Platform based on UUID.

**Parameters**

| Parameter | Type | Default | Description                        |
| --------- | ---- | ------- | ---------------------------------- |
| id\_      | UUID | -       | Unique identifier for the dataset. |

**Usage**

```python
DATASET_ID = 'ba6ec4e4-7188-44c5-ba84-c2cb22b4bb00'
dataset = fdl.Dataset.get(id_=DATASET_ID)
```

**Returns**

| Return Type                          | Description       |
| ------------------------------------ | ----------------- |
| [Dataset](api-methods-30.md#dataset) | Dataset instance. |

**Raises**

| Error code | Issue                                                            |
| ---------- | ---------------------------------------------------------------- |
| NotFound   | Dataset with given identifier not found.                         |
| Forbidden  | Current user may not have permission to view details of dataset. |

***

### from\_name()

Get dataset from Fiddler Platform based on name and"
" model UUID.

**Usage params**

| Parameter | Type        | Default | Description                      |
| --------- | ----------- | ------- | -------------------------------- |
| name      | str         | -       | Name of the dataset.             |
| model\_id | UUID \| str | -       | Unique identifier for the model. |

**Usage**

```python
DATASET_NAME = 'YOUR_DATASET_NAME'
MODEL_ID = '4531bfd9-2ca2-4a7b-bb5a-136c8da09ca2'

dataset = fdl.Dataset.from_name(
    name=DATASET_NAME,
    model_id=MODEL_ID
)

```

**Returns**

| Return Type                          | Description       |
| ------------------------------------ | ----------------- |
| [Dataset](api-methods-30.md#dataset) | Dataset instance. |

**Raises**

| Error code | Issue                                                            |
| ---------- | ---------------------------------------------------------------- |
| NotFound   | Dataset not found in the given project name.                     |
"
"| Forbidden  | Current user may not have permission to view details of dataset. |

***

### list()

Get a list of all datasets associated to a model.

**Parameters**

| Parameter | Type | Default | Description                                 |
| --------- | ---- | ------- | ------------------------------------------- |
| model\_id | UUID | -       | UUID of the model associated with baseline. |

**Usage**

```python
MODEL_ID = '4531bfd9-2ca2-4a7b-bb5a-136c8da09ca2'

datasets = fdl.Dataset.list(model_id=MODEL_ID)
```

**Returns**

| Return Type                                     | Description                      |
| ----------------------------------------------- | -------------------------------- |
| Iterable\[[Dataset](api-methods-30.md#dataset)] | Iterable of all dataset objects. |

**Raises**

| Error code | Issue                                                            |
| ---------- | ---------------------------------------------------------------- |
| Forbidden  | Current user may not have permission to view details of dataset. |

***

## Jobs

"
"A Job is used to track asynchronous processes such as batch publishing of data.

## Job

Job object contains the below fields.

| Parameter      | Type            | Default | Description                                                          |
| -------------- | --------------- | ------- | -------------------------------------------------------------------- |
| id             | UUID            | -       | Unique identifier for the job.                                       |
| name           | str             | -       | Name of the job.                                                     |
| status         | str             | -       | Current status of job.                                               |
| progress       | float           | -       | Progress of job completion.                                          |
| info           | dict            | -       | Dictionary containing resource\_type, resource\_name, project\_name. |
| error\_message | Optional\[str]  | None    | Message for job failure, if any.                                     |
| error\_reason  | Optional\[str]  | None    | Reason for job failure, if any.                                      |
| extras         | Optional\[dict]"
" | None    | Metadata regarding the job.                                          |

#### get()

Get the job instance using job UUID.

**Parameters**

| Parameter | Type | Default | Description                                              |
| --------- | ---- | ------- | -------------------------------------------------------- |
| id\_      | UUID | -       | Unique UUID of the project to which model is associated. |
| verbose   | bool | False   | Flag to get `extras` metadata about the tasks executed.  |

**Usage**

```python
JOB_ID = '1531bfd9-2ca2-4a7b-bb5a-136c8da09ca1'
job = fdl.Job.get(id_=JOB_ID)
```

**Returns**

| Return Type                  | Description                             |
| ---------------------------- | --------------------------------------- |
| [Job](api-methods-30.md#job) | Single job object for the input params. |

**Raises**

| Error code | Issue                                                        |
| ---------- | ------------------------------------------------------------ |
| Forbidden  | Current"
" user may not have permission to view details of job. |

***

### wait()

Wait for job to complete either with success or failure status.

**Parameters**

| Parameter | Type           | Default | Description                                         |
| --------- | -------------- | ------- | --------------------------------------------------- |
| interval  | Optional\[int] | 3       | Interval in seconds between polling for job status. |
| timeout   | Optional\[int] | 1800    | Timeout in seconds for iterator to stop.            |

**Usage**

```python
JOB_ID = '1531bfd9-2ca2-4a7b-bb5a-136c8da09ca1'
job = fdl.Job.get(id_=JOB_ID)
job.wait()
```

**Returns**

| Return Type                  | Description                             |
| ---------------------------- | --------------------------------------- |
| [Job](api-methods-30.md#job) | Single job object for the input params. |

**Raises**

| Error code  "
" | Issue                                                        |
| ------------ | ------------------------------------------------------------ |
| Forbidden    | Current user may not have permission to view details of job. |
| TimeoutError | When the default time out of 1800 secs.                      |

### watch()

Watch job status at given interval and yield job object.

**Parameters**

| Parameter | Type           | Default | Description                                         |
| --------- | -------------- | ------- | --------------------------------------------------- |
| interval  | Optional\[int] | 3       | Interval in seconds between polling for job status. |
| timeout   | Optional\[int] | 1800    | Timeout in seconds for iterator to stop.            |

**Usage**

```python
JOB_ID = ""69f846db-5aac-44fe-9fa5-14f40048e4b2""
job = fdl.Job.get(id_=JOB_ID)

for ijob in job.watch(interval=30, timeout=1200):
    print(f'Status: {ijob.status}"
" - progress: {ijob.progress}')
```

**Returns**

| Return Type                             | Description              |
| --------------------------------------- | ------------------------ |
| Iterator\[[Job](api-methods-30.md#job)] | Iterator of job objects. |

**Raises**

| Error code   | Issue                                                        |
| ------------ | ------------------------------------------------------------ |
| Forbidden    | Current user may not have permission to view details of job. |
| TimeoutError | When the default time out of 1800 secs.                      |

***

## Models

A Model is a **representation of your machine learning model** which can be used for monitoring, explainability, and more.
You **do not need to upload your model artifact in order to onboard your model**, but doing so will significantly improve the quality of explanations generated by Fiddler.

## Model

Model object contains the below parameters.

| Parameter                  | Type                                                 | Default                | Description                                                                                      |
| -------------------------- | ---------------------------------------------------- | ---------------------- | ------------------------------------------------------------------------------------------------ |
| id"
"                         | UUID                                                 | -                      | Unique identifier for the model.                                                                 |
| name                       | str                                                  | -                      | Unique name of the model (only alphanumeric and underscores are allowed).                        |
| input\_type                | [ModelInputType](api-methods-30.md#modelinputtype)   | ModelInputType.TABULAR | Input data type used by the model.                                                               |
| task                       | [ModelTask](api-methods-30.md#modeltask)             | ModelTask.NOT\_SET     | Task the model is designed to address.                                                           |
| task\_params               | [ModelTaskParams](api-methods-30.md#modeltaskparams) | -                      | Task parameters given to a particular model.                                                     |
| schema                     | [ModelSchema](api-methods-30.md#modelschema)         | -                      | Model schema defines the details of each column.                                                 |
| version                    | Optional\[str]                                       | -"
"                      | Unique version name within a model                                                               |
| spec                       | [ModelSpec](api-methods-30.md#modelspec)             | -                      | Model spec defines how model columns are used along with model task.                             |
| description                | str                                                  | -                      | Description of the model.                                                                        |
| event\_id\_col             | str                                                  | -                      | Column containing event id.                                                                      |
| event\_ts\_col             | str                                                  | -                      | Column containing event timestamp.                                                               |
| xai\_params                | [XaiParams](api-methods-30.md#xaiparams)             | -                      | Explainability parameters of the model.                                                          |
| artifact\_status           | str                                                  | -                      | Artifact Status of the model.                                                                    |
| artifact\_files            | list\[dict]                                          | -                      | Dictionary containing file details of model artifact.                                            |
| is\_binary\_ranking\_model | bool"
"                                                 | -                      | True if model is [ModelTask.RANKING](api-methods-30.md#modeltask) and has only 2 target classes. |
| created\_at                | datetime                                             | -                      | Time at which model was created.                                                                 |
| updated\_at                | datetime                                             | -                      | Latest time at which model was updated.                                                          |
| created\_by                | [User](api-methods-30.md#user)                       | -                      | Details of the who created the model.                                                            |
| updated\_by                | [User](api-methods-30.md#user)                       | -                      | Details of the who last updated the model.                                                       |
| project                    | [Project](api-methods-30.md#project)                 | -                      | Details of the project to which the model belongs.                                               |
| organization               | [Organization](api-methods-30.md#organization)       | -                      | Details"
" of the organization to which the model belongs.                                          |

### constructor()

Initialize a new model instance.

**Usage**

```python
model = fdl.Model(
    name='model_name',
    project_id=project.id,
    task=fdl.ModelTask.BINARY_CLASSIFICATION,
    task_params=fdl.ModelTaskParams(target_class_order=['no', 'yes']),
    schema=model_schema,
    spec=model_spec,
    event_id_col='column_name_1',
    event_ts_col='column_name_2',
)
```

**Parameters**

| Parameter      | Type                                                 | Default                | Description                                                          |
| -------------- | ---------------------------------------------------- | ---------------------- | -------------------------------------------------------------------- |
| name           | str                                                  | -                      | Unique name of the model                                             |
| project\_id    | UUID                                                 | -                      | Unique identifier for the project to which model belongs.            |
| input\_type    | [ModelInputType](api-methods-30.md#modelinputtype)"
"   | ModelInputType.TABULAR | Input data type used by the model.                                   |
| task           | [ModelTask](api-methods-30.md#modeltask)             | ModelTask.NOT\_SET     | Task the model is designed to address.                               |
| schema         | [ModelSchema](api-methods-30.md#modelschema)         | -                      | Model schema defines the details of each column.                     |
| spec           | [ModelSpec](api-methods-30.md#modelspec)             | -                      | Model spec defines how model columns are used along with model task. |
| version        | Optional\[str]                                       | -                      | Unique version name within a model                                   |
| task\_params   | [ModelTaskParams](api-methods-30.md#modeltaskparams) | -                      | Task parameters given to a particular model.                         |
| description    | str                                                  | -                      | Description of the model.                                           "
" |
| event\_id\_col | str                                                  | -                      | Column containing event id.                                          |
| event\_ts\_col | str                                                  | -                      | Column containing event timestamp.                                   |
| xai\_params    | [XaiParams](api-methods-30.md#xaiparams)             | -                      | Explainability parameters of the model.                              |

### from\_data()

Build model instance from the given dataframe or file(csv/parquet).

**Parameters**

| Parameter        | Type                                                 | Default                | Description                                                          |
| ---------------- | ---------------------------------------------------- | ---------------------- | -------------------------------------------------------------------- |
| source           | pd.DataFrame \| Path \| str                          | -                      | Pandas dataframe or path to csv/parquet file                         |
| name             | str                                                  | -                      | Unique name of the model                                             |
| project\_id      | UUID \| str                                          | -                      | Unique identifier for the project to which model belongs.            |
| input\_"
"type      | [ModelInputType](api-methods-30.md#modelinputtype)   | ModelInputType.TABULAR | Input data type used by the model.                                   |
| task             | [ModelTask](api-methods-30.md#modeltask)             | ModelTask.NOT\_SET     | Task the model is designed to address.                               |
| spec             | [ModelSpec](api-methods-30.md#modelspec)             | -                      | Model spec defines how model columns are used along with model task. |
| version          | Optional\[str]                                       | -                      | Unique version name within a model                                   |
| task\_params     | [ModelTaskParams](api-methods-30.md#modeltaskparams) | -                      | Task parameters given to a particular model.                         |
| description      | Optional\[str]                                       | -                      | Description of the model.                                            |
| event\_id\_col   | Optional\[str"
"]                                       | -                      | Column containing event id.                                          |
| event\_ts\_col   | Optional\[str]                                       | -                      | Column containing event timestamp.                                   |
| xai\_params      | [XaiParams](api-methods-30.md#xaiparams)             | -                      | Explainability parameters of the model.                              |
| max\_cardinality | Optional\[int]                                       | None                   | Max cardinality to detect categorical columns.                       |
| sample\_size     | Optional\[int]                                       | -                      | No. of samples to use for generating schema.                         |

**Usage**

```python
MODEL_NAME = 'example_model'
PROJECT_ID = '1531bfd9-2ca2-4a7b-bb5a-136c8da09ca1'
MODEL_SPEC = {
  'custom_features': [],
  'decisions': ['Decisions'],
  'inputs': [
    'CreditScore',
   "
" 'Geography',
  ],
  'metadata': [],
  'outputs': ['probability_churned'],
  'schema_version': 1,
  'targets': ['Churned'],
}

# Without version
model = fdl.Model.from_data(
  source=<file_path>,
  name=MODEL_NAME,
  project_id=PROJECT_ID,
  spec=fdl.ModelSpec(**MODEL_SPEC),
)

# With version
model = fdl.Model.from_data(
  source=<file_path>,
  name=MODEL_NAME,
  version='v2',
  project_id=PROJECT_ID,
  spec=fdl.ModelSpec(**MODEL_SPEC),
)
```

**Returns**

| Return Type                      | Description     |
| -------------------------------- | --------------- |
| [Model](api-methods-30.md#model) | Model instance. |

**Notes**

* > `from_data` will not create a model entry on Fiddler Platform.\
  > Instead this method only returns a model instance"
" which can be edited, call `.create()` to onboard the model to\
  > Fiddler Platform.
* > `spec` is optional to `from_data` method. However, a `spec` with at least `inputs` is required for model onboarding.
* > Make sure `spec` is passed to `from_data` method if model requires custom features. This method generates centroids\
  > which are needed for custom feature drift computation
* > If `version` is not explicitly passed, Fiddler Platform will treat it as `v1` version of the model.

### create()

Onboard a new model to Fiddler Platform

**Parameters**

No

**Usage**

```python
model = fdl.Model.from_data(...)
model.create()
```

**Returns**

| Return Type                      | Description     |
| -------------------------------- | --------------- |
| [Model](api-methods-30.md#model) | Model instance. |

**Raises**

| Error code |"
" Issue                                       |
| ---------- | ------------------------------------------- |
| Conflict   | Model with same name may exist in project . |

### get()

Get model from Fiddler Platform based on UUID.

**Parameters**

| Parameter | Type        | Default | Description                      |
| --------- | ----------- | ------- | -------------------------------- |
| id\_      | UUID \| str | -       | Unique identifier for the model. |

**Returns**

| Return Type                      | Description     |
| -------------------------------- | --------------- |
| [Model](api-methods-30.md#model) | Model instance. |

**Raises**

| Error code | Issue                                                          |
| ---------- | -------------------------------------------------------------- |
| NotFound   | Model with given identifier not found.                         |
| Forbidden  | Current user may not have permission to view details of model. |

**Usage**

```python
MODEL_ID = '4531bfd9-2ca2-4a7b-bb5a-136c8da09ca2'
model = fdl"
".Model.get(id_=MODEL_ID)
```

### from\_name()

Get model from Fiddler Platform based on name and project UUID.

**Parameters**

| Parameter   | Type          | Default | Description                        |
| ----------- | ------------- | ------- | ---------------------------------- |
| name        | str           | -       | Name of the model.                 |
| project\_id | UUID \| str   | -       | Unique identifier for the project. |
| version     | Optional\[str] | -       | Unique version name within a model |

> `version` parameter is available from `fiddler-client==3.1` onwards

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'

PROJECT = fdl.Project.from_name(name=PROJECT_NAME)

# Without version
MODEL = fdl.Model.from_name(name=MODEL_NAME, project_id=PROJECT.id)

# With version
MODEL = fdl.Model.from_name(name=MODEL_NAME"
", project_id=PROJECT.id, version='v2')
```

**Returns**

| Return Type                      | Description     |
| -------------------------------- | --------------- |
| [Model](api-methods-30.md#model) | Model instance. |

**Notes**

* When the version is not passed, then the model created without any version will be fetched. Fiddler internally\
  assigns version=v1 when not passed.
* When the version is passed, method will fetch the model corresponding to that specific version.

**Raises**

| Error code | Issue                                                          |
| ---------- | -------------------------------------------------------------- |
| NotFound   | Model not found in the given project name.                     |
| Forbidden  | Current user may not have permission to view details of model. |


### list()

Gets all models of a project.

**Parameters**

| Parameter   | Type            | Default | Description                                              |
| ----------- | --------------- | ------- | -------------------------------------------------------- |
| project\_id | UUID \| str     | -      "
" | Unique UUID of the project to which model is associated. |
| name        | Optional\[str]   | -       | Model name. Pass this to fetch all versions of a model.  |

**Returns**

| Return Type                                                 | Description                        |
| ----------------------------------------------------------- | ---------------------------------- |
| Iterable\[[Model Compact](api-methods-30.md#model-compact)] | Iterable of model compact objects. |

**Errors**

| Error code | Issue                                                      |
| ---------- | ---------------------------------------------------------- |
| Forbidden  | Current user may not have permission to the given project. |

**Usage example**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)

models = fdl.Model.list(project_id=project.id)
```

**Notes**

> Since `Model` contains a lot of information, list operations does not return all the fields of a model.\
> Instead this method returns `Model"
"Compact` objects on which `.fetch()` can be called to get the complete `Model`\
> instance.\
> For most of the use-cases, `ModelCompact` objects are sufficient.

### update()

Update an existing model. Only following fields are allowed to be updated, backend will ignore if any other\
field is updated on the instance.

**Parameters**

| Parameter      | Type                                                | Default | Description                             |
| -------------- | --------------------------------------------------- | ------- | --------------------------------------- |
| version        | Optional\[str]                                      | None    | Model version name                      |
| xai\_params    | Optional\[[XaiParams](api-methods-30.md#xaiparams)] | None    | Explainability parameters of the model. |
| description    | Optional\[str]                                      | None    | Description of the model.               |
| event\_id\_col | Optional\[str]                                      | None    | Column containing event id.             |
| event\_"
"ts\_col | Optional\[str]                                      | None    | Column containing event timestamp.      |

> `version` parameter is available from `fiddler-client==3.1` onwards

**Usage**

```python
model.description = 'YOUR_MODEL_DESCRIPTION'
model.update()
```

**Returns**

No

**Raises**

| Error code | Issue                      |
| ---------- | -------------------------- |
| BadRequest | If field is not updatable. |


### duplicate()

Duplicate the model instance with the given version name.

This call will not save the model on Fiddler Platform. After making changes to the model instance, call `.create()` to add the model version to Fiddler Platform.

> Added in version 3.1.0

**Parameters**

| Parameter | Type           | Default | Description        |
| --------- | -------------- | ------- | ------------------ |
| version   | Optional\[str] | None    | Model version name |

**Usage**

```python
PROJECT"
"_ID = '1531bfd9-2ca2-4a7b-bb5a-136c8da09ca1'
MODEL_NAME = 'test_model'

model = Model.from_name(name=MODEL_NAME, project_id=PROJECT_ID, version='v3')
new_model = model.duplicate(version='v4')
new_model.schema['Age'].min = 18
new_model.schema['Age'].max = 60

new_model.create()
```

**Returns**

| Return Type                      | Description     |
| -------------------------------- | --------------- |
| [Model](api-methods-30.md#model) | Model instance. |

**Raises**

| Error code | Issue                      |
| ---------- | -------------------------- |
| BadRequest | If field is not updatable. |


### remove_column()

Remove column from model object(in-place)

Modifies the model object in-place by removing the column with name column_name.
Do this before uploading the model to the Fiddler Platform ("
"which can be done with the
create() method), otherwise the change does not take effect.

> Added in version 3.7.0

**Parameters**

| Parameter     | Type   | Default | Description                                      |
| ------------- | ------ | ------- | ------------------------------------------------ |
| column_name   | str    | -       | Name of the column to be removed                 |
| missing_ok    | bool   | -       | If False, raises an error if column is not found |

**Usage**

```python
MODEL_NAME = 'example_model'
PROJECT_ID = '1531bfd9-2ca2-4a7b-bb5a-136c8da09ca1'
MODEL_SPEC = {
  'custom_features': [],
  'decisions': ['Decisions'],
  'inputs': [
    'CreditScore',
    'Geography',
  ],
  'metadata': [],
  'outputs': ['probability_churned'],
  'schema_version': 1,
 "
" 'targets': ['Churned'],
}

# Build model instance from the given dataframe or file(csv/parquet).
model = fdl.Model.from_data(
  source=<file_path>,
  name=MODEL_NAME,
  project_id=PROJECT_ID,
  spec=fdl.ModelSpec(**MODEL_SPEC),
)

# Remove a column from model
model.remove_column(column_name='CreditScore', missing_ok=False)

```

**Returns**

| Return Type | Description     |
| ------------| --------------- |
| None        |                 |

**Raises**

| Error code | Issue                                             |
| ---------- | ------------------------------------------------- |
| KeyError   | If column is not present and missing_ok is False. |


### delete()

Delete a model.

**Parameters**

No

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id"
"=project.id)

job = model.delete()
job.wait()
```

**Returns**

| Return Type                         | Description                           |
| ----------------------------------- | ------------------------------------- |
| [Job](api-methods-30.md#job-object) | Async job details for the delete job. |

**Notes**

> Model deletion is an async process, hence a job object is returned on `delete()` call.\
> Call `job.wait()` to wait for the job to complete. If you are planning to create a model with the same\
> name, please wait for the job to complete, otherwise backend will not allow new model with same name.

### add\_surrogate()

Add surrogate existing model.

**Parameters**

| Parameter          | Type                                                              | Default | Description                  |
| ------------------ | ----------------------------------------------------------------- | ------- | ---------------------------- |
| dataset\_id        | UUID \| str                                                       | -       | Dataset identifier           |
| deployment\_params | Optional\[[DeploymentParams](api-methods-30"
".md#deploymentparams)] | -       | Model deployment parameters. |

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'
DATASET_NAME = 'YOUR_DATASET_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)
dataset = fdl.Dataset.from_name(name=DATASET_NAME, model_id=model.id)

DEPLOYMENT_PARAMS = {'memory': 1024, 'cpu': 1000}

model.add_surrogate(
  dataset_id=dataset.id,
  deployment_params=fdl.DeploymentParams(**DEPLOYMENT_PARAMS)
)
```

**Returns**

| Return Type                  | Description                                  |
| ---------------------------- | -------------------------------------------- |
| [Job](api-methods-30.md#job) | Async job details for the add surrogate job. |

**Raises**

| Error code | Issue                                  |
| ---------- | --------------------------------------"
" |
| BadRequest | Invalid deployment parameter is passed |

### update\_surrogate()

Update surrogate existing model.

**Parameters**

| Parameter          | Type                                                              | Default | Description                  |
| ------------------ | ----------------------------------------------------------------- | ------- | ---------------------------- |
| dataset\_id        | UUID \| str                                                       | -       | Dataset identifier           |
| deployment\_params | Optional\[[DeploymentParams](api-methods-30.md#deploymentparams)] | None    | Model deployment parameters. |

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'
DATASET_NAME = 'YOUR_DATASET_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)
dataset = fdl.Dataset.from_name(name=DATASET_NAME, model_id=model.id)

DEPLOYMENT_PARAMS = {'memory': 1024, 'cpu': 1000}

model.update_surrogate(
"
"  dataset_id=dataset.id,
  deployment_params=fdl.DeploymentParams(**DEPLOYMENT_PARAMS)
)
```

**Returns**

| Return Type                  | Description                                     |
| ---------------------------- | ----------------------------------------------- |
| [Job](api-methods-30.md#job) | Async job details for the update surrogate job. |

### add\_artifact()

Add artifact files to existing model.

**Parameters**

| Parameter          | Type                                                              | Default | Description                                        |
| ------------------ | ----------------------------------------------------------------- | ------- | -------------------------------------------------- |
| model\_dir         | str                                                               | -       | Path to directory containing artifacts for upload. |
| deployment\_params | Optional\[[DeploymentParams](api-methods-30.md#deploymentparams)] | None    | Model deployment parameters.                       |

**Usage**

```python
MODEL_DIR = 'PATH_TO_MODEL_DIRECTORY'
DEPLOYMENT_PARAMS = {'memory': 1024, 'cpu': 1000}

model.add_artifact(
  model_dir"
"=MODEL_DIR,
  deployment_params=fdl.DeploymentParams(**DEPLOYMENT_PARAMS)
)
```

**Returns**

| Return Type                  | Description                                 |
| ---------------------------- | ------------------------------------------- |
| [Job](api-methods-30.md#job) | Async job details for the add artifact job. |

### update\_artifact()

Update existing artifact files in a model.

**Parameters**

| Parameter          | Type                                                              | Default | Description                                        |
| ------------------ | ----------------------------------------------------------------- | ------- | -------------------------------------------------- |
| model\_dir         | str                                                               | -       | Path to directory containing artifacts for upload. |
| deployment\_params | Optional\[[DeploymentParams](api-methods-30.md#deploymentparams)] | None    | Model deployment parameters.                       |

**Usage**

```python
MODEL_DIR = 'PATH_TO_MODEL_DIRECTORY'
DEPLOYMENT_PARAMS = {'memory': 1024, 'cpu': 1000}

model.update_artifact(
  model_dir=MODEL"
"_DIR,
  deployment_params=fdl.DeploymentParams(**DEPLOYMENT_PARAMS)
)
```

**Returns**

| Return Type                  | Description                                 |
| ---------------------------- | ------------------------------------------- |
| [Job](api-methods-30.md#job) | Async job details for the add artifact job. |

### download\_artifact()

Download existing artifact files in a model.

**Parameters**

| Parameter   | Type | Default | Description                                  |
| ----------- | ---- | ------- | -------------------------------------------- |
| output\_dir | str  | -       | Path to directory to download the artifacts. |

**Usage**

```python
OUTPUT_DIR = 'PATH_TO_TARGET_DIRECTORY'
model.download_artifact(output_dir=OUTPUT_DIR)
```

**Returns**

No

### Properties

### datasets

List all datasets associated with a model.

**Parameters**

No

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'

project = fdl.Project.from_name(name"
"=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)

model.datasets
```

**Returns**

| Return Type                                     | Description                    |
| ----------------------------------------------- | ------------------------------ |
| Iterable\[[Dataset](api-methods-30.md#dataset)] | Iterable of dataset instances. |

**Raises**

| Error code | Issue                                                          |
| ---------- | -------------------------------------------------------------- |
| Forbidden  | Current user may not have permission to view details of model. |

### model\_deployment

Get the model deployment object associated with the model.

**Parameters**

No

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)

model.model_deployment
```

**Returns**

| Return Type                                            | Description                |
| ------------------------------------------------------ | -------------------------- |
| [Model"
" deployment](api-methods-30.md#model-deployment) | Model deployment instance. |

**Raises**

| Error code | Issue                                                          |
| ---------- | -------------------------------------------------------------- |
| NotFound   | Model with given identifier not found.                         |
| Forbidden  | Current user may not have permission to view details of model. |

### publish()

Publish Pre-production or production events.

**Parameters**

| Parameter     | Type                                                    | Default            | Description                                                                                                                                                                                                                                                                                                               |
| ------------- | ------------------------------------------------------- | ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| source        | Union\[list\[dict\[str, Any]], str, Path, pd.DataFrame] | -                  | <p>Source can be:<br>1. Path or str path: path for data file.<br>2. list[dict]: list of event dicts. EnvType.PRE_PRODUCTION not supported.<br>3. dataframe: events dataframe.</p>                                                                                                                                         |
| environment   | EnvType                                                 | Env"
"Type.PRODUCTION | Either EnvType.PRE\_PRODUCTION or EnvType.PRODUCTION                                                                                                                                                                                                                                                                      |
| dataset\_name | Optional\[str]                                          | None               | Name of the dataset. Not supported for EnvType.PRODUCTION                                                                                                                                                                                                                                                                 |
| update        | Optional\[bool]                                         | False              | If True, the events data passed in the publish call will be used to update previously published event records matched by their event\_ids (note that only updating target and metadata columns is supported). For more details refer to [Updating Events](../Client\_Guide/publishing-production-data/updating-events.md) |

**Usage**

**Pre-requisite**

```python
# Before publishing, make sure you set up the necessary fields of the model(if any).
# If you set the fields to non-empty value, We expect them passed in the source.
model.event_ts_col = 'timestamp'
model.event_id_col = 'event_id'
model.update()
```

**Publish dataset (pre"
"-production data) from file**

```python
# Publish File
FILE_PATH = 'PATH_TO_DATASET_FILE'
job = model.publish(
  source=FILE_PATH,
  environment=fdl.EnvType.PRE_PRODUCTION,
  dataset_name='training_dataset'
)
# The publish() method is asynchronous by default. Use the publish job's wait() method
# if synchronous behavior is desired.
# job.wait()
```

**Publish dataset (pre-production data) from dataframe**

```python
df = pd.DataFrame(np.random.randint(0, 100, size=(10, 4)), columns=list('ABCD'))
job = model.publish(
  source=df,
  environment=fdl.EnvType.PRE_PRODUCTION,
  dataset_name='training_dataset'
)
# The publish() method is asynchronous by default. Use the publish job's wait() method
# if synchronous behavior is desired.
# job.wait()
```

**Publish production events from list**

List is only supported for production"
" data but not for pre-production.

Events are published as a stream. This mode is recommended If you have a high volume of continuous real-time traffic of events, as it allows for more efficient processing on our backend.

It returns a list of `event_id` for each of the published events.

```python
# Publish list of dictionary objects
events = [
  {'A': 56, 'B': 68, 'C': 67, 'D': 27, 'event_id': 'A1', 'timestamp':'2024-05-01 00:00:00'},
  {'A': 43, 'B': 59, 'C': 64, 'D': 18, 'event_id': 'A2', 'timestamp':'2024-05-01 00:00:00'},
  ...
]
event_ids = model.publish(
  source=events,
  environment=fdl.EnvType.PRODUCTION
)
```

**Notes**

"
"> In this example where `model.event_id_col`=`event_id`, we expect `event_id` as the required key of the dictionary. Otherwise if you keep `model.event_id_col=None`, our backend will generate unique event ids and return these back to you. Same for `model.event_ts_col`, we assign current time as event timestamp in case of `None`.

**Publish production events from file**

Batch events is faster if you want to publish a large-scale set of historical data.

```python
# Publish File
FILE_PATH = 'PATH_TO_EVENTS_FILE'
job = model.publish(
  source=FILE_PATH,
  environment=fdl.EnvType.PRODUCTION,
)
# The publish() method is asynchronous by default. Use the publish job's wait() method
# if synchronous behavior is desired.
# job.wait()
```

**Publish production events from dataframe**

```python
df = pd.DataFrame(
    {
        'A': np.random.randint(0, 100, size"
"=(2)),
        'B': np.random.randint(0, 100, size=(2)),
        'C': np.random.randint(0, 100, size=(2)),
        'D': np.random.randint(0, 100, size=(2)),
        'timestamp': [time.time()]*2,  # optional model.event_ts_col
        'event_id': ['A1', 'A2'],      # optional model.event_id_col
    }
)
event_ids = model.publish(
  source=df,
  environment=fdl.EnvType.PRODUCTION,
)
```

**Update events**

if you need to update the target or metadata columns for a previously published production event, set `update`=True. For more details please refer to [Updating Events](../Client\_Guide/publishing-production-data/updating-events.md). Note only production events can be updated.

**Update production events from list**

```python
# suppose 'A' is the target, 'B"
"' is the metadata. The Model.event_id_col is required.
events_update = [
    {
        'A': [0],                   
        'B': [0], 
        'event_id': ['A1'], 
    },
    {
        'A': [1], 
        'B': [1], 
        'event_id': ['A2'], 
    },
]
event_ids = model.publish(
    source=events_update,
    environment=fdl.EnvType.PRODUCTION,
    update=True,
)
```

**Update production events from dataframe**

```python
df_update = pd.DataFrame(
    {
        'A': [0, 1],  # suppose 'A' is the target
        'B': [0, 1],  # suppose 'B' is the metadata
        'event_id': ['A1', 'A2'],  # required model.event_id_col
    }
)
event_ids = model.publish(
    source=df_update,
"
"    environment=fdl.EnvType.PRODUCTION,
    update=True,
)
```

**Returns**

In case of streaming publish

| Return Type      | Source      | Description              |
| ---------------- | ----------- | ------------------------ |
| list\[UUID\|str] | list\[dict] | List of event identifier |

In case of batch publish

| Return Type                  | Source                          | Description                             |
| ---------------------------- | ------------------------------- | --------------------------------------- |
| [Job](api-methods-30.md#job) | Union\[str, Path, pd.DataFrame] | Job object for file/dataframe published |

## Model Compact

Model object contains the below parameters.

| Parameter | Type           | Default | Description                        |
| --------- | -------------- | ------- | ---------------------------------- |
| id        | UUID           | -       | Unique identifier for the model.   |
| name      | str            | -       | Unique name of the model           |
| version   | Optional\[str] |"
" -       | Unique version name within a model |

#### fetch()

Fetch the model instance from Fiddler Platform.

**Parameters**

No

**Returns**

| Return Type                      | Description     |
| -------------------------------- | --------------- |
| [Model](api-methods-30.md#model) | Model instance. |

**Raises**

| Error code | Issue                                                          |
| ---------- | -------------------------------------------------------------- |
| NotFound   | Model not found for the given identifier                       |
| Forbidden  | Current user may not have permission to view details of model. |

***

## Model deployment

Get model deployment object of a particular model.

### Model deployment:

Model deployment object contains the below parameters.

| Parameter        | Type                                               | Default                         | Description                                                                                                                                                                         |
| ---------------- | -------------------------------------------------- | ------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| id               | UUID                                               | -                               | Unique identifier for the model.                                                                                                                                                    |
| model            | [Model](api-methods-30.md#model)                   |"
" -                               | Details of the model.                                                                                                                                                               |
| project          | [Project](api-methods-30.md#project)               | -                               | Details of the project to which the model belongs.                                                                                                                                  |
| organization     | [Organization](api-methods-30.md#organization)     | -                               | Details of the organization to which the model belongs.                                                                                                                             |
| artifact\_type   | [ArtifactType](api-methods-30.md#artifacttype)     | -                               | Task the model is designed to address.                                                                                                                                              |
| deployment\_type | [DeploymentType](api-methods-30.md#deploymenttype) | -                               | Type of deployment of the model.                                                                                                                                                    |
| image\_uri       | Optional\[str]                                     | md-base/python/python-311:1.0.0 | A Docker image reference. See available images [here](../product-guide/explainability/flexible-model-deployment/). |
| active"
"           | bool                                               | True                            | Status of the deployment.                                                                                                                                                           |
| replicas         | Optional\[str]                                     | 1                               | <p>The number of replicas running the model.<br>Minimum value: 1<br>Maximum value: 10<br>Default value: 1</p>                                                                       |
| cpu              | Optional\[str]                                     | 100                             | <p>The amount of CPU (milli cpus) reserved per replica.<br>Minimum value: 10<br>Maximum value: 4000 (4vCPUs)<br>Default value: 100</p>                                              |
| memory           | Optional\[str]                                     | 256                             | <p>The amount of memory (mebibytes) reserved per replica.<br>Minimum value: 150<br>Maximum value: 16384 (16GiB)<br>Default value: 256</p>                                           |
| created\_at      | datetime                                           |"
" -                               | Time at which model deployment was created.                                                                                                                                         |
| updated\_at      | datetime                                           | -                               | Latest time at which model deployment was updated.                                                                                                                                  |
| created\_by      | [User](api-methods-30.md#user)                     | -                               | Details of the user who created the model deployment.                                                                                                                               |
| updated\_by      | [User](api-methods-30.md#user)                     | -                               | Details of the user who last updated the model deployment.                                                                                                                          |

***

### Update model deployment

Update an existing model deployment.

**Parameters**

| Parameter | Type            | Default | Description                                                                                                                               |
| --------- | --------------- | ------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| active    | Optional\[bool] | True    | Status of the deployment.                                                                                                                 |
| replicas  | Optional\[str]  | 1       | <p>The number of replicas running the model.<br>Minimum value: "
"1<br>Maximum value: 10<br>Default value: 1</p>                             |
| cpu       | Optional\[str]  | 100     | <p>The amount of CPU (milli cpus) reserved per replica.<br>Minimum value: 10<br>Maximum value: 4000 (4vCPUs)<br>Default value: 100</p>    |
| memory    | Optional\[str]  | 256     | <p>The amount of memory (mebibytes) reserved per replica.<br>Minimum value: 150<br>Maximum value: 16384 (16GiB)<br>Default value: 256</p> |

**Usage**

```python
# Update CPU allocation and activate the model pod
model_id = 'a920ddb6-edb7-473b-a5f7-035f91e1d53a'
model = fdl.Model.get(model_id)
model_deployment"
" = model.deployment
model_deployment.cpu = 300
model_deployment.active = True
model_deployment.update()
```

**Returns**

No

**Raises**

| Error code | Issue                      |
| ---------- | -------------------------- |
| BadRequest | If field is not updatable. |

***

## Organizations

Organization in which all the projects, models are present.

***

### Organization:

Organization object contains the below parameters.

| Parameter   | Type     | Default | Description                                    |
| ----------- | -------- | ------- | ---------------------------------------------- |
| id          | UUID     | -       | Unique identifier for the organization.        |
| name        | str      | -       | Unique name of the organization.               |
| created\_at | datetime | -       | Time at which organization was created.        |
| updated\_at | datetime | -       | Latest time at which organization was updated. |

***

## Projects

Projects are **used to organize your models and datasets**. Each project can"
" represent a machine learning task (e.g. predicting house prices, assessing creditworthiness, or detecting fraud).

A project **can contain one or more models** (e.g. lin\_reg\_house\_predict, random\_forest\_house\_predict).

***

### Project

Project object contains the below parameters.

| Parameter    | Type                                           | Default | Description                                               |
| ------------ | ---------------------------------------------- | ------- | --------------------------------------------------------- |
| id           | UUID                                           | None    | Unique identifier for the project.                        |
| name         | str                                            | None    | Unique name of the project.                               |
| created\_at  | datetime                                       | None    | Time at which project was created.                        |
| updated\_at  | datetime                                       | None    | Latest time at which project was updated.                 |
| created\_by  | [User](api-methods-30.md#user)                 | None    | Details of the who created the project.                   |
| updated"
"\_by  | [User](api-methods-30.md#user)                 | None    | Details of the who last updated the project.              |
| organization | [Organization](api-methods-30.md#organization) | None    | Details of the organization to which the project belongs. |

### create()

Creates a project using the specified name.

**Parameters**

| Parameter | Type | Default | Description                 |
| --------- | ---- | ------- | --------------------------- |
| name      | str  | None    | Unique name of the project. |

**Usage**

```python
PROJECT_NAME = 'bank_churn'
project = fdl.Project(name=PROJECT_NAME)
project.create()
```

**Returns**

| Return Type                          | Description       |
| ------------------------------------ | ----------------- |
| [Project](api-methods-30.md#project) | Project instance. |

**Raises**

| Error code | Issue                             |
| ---------- | --------------------------------- |
| Conflict   | Project with"
" same name may exist. |

### get()

Get project from Fiddler Platform based on UUID.

**Parameters**

| Parameter | Type | Default | Description                        |
| --------- | ---- | ------- | ---------------------------------- |
| id\_      | UUID | None    | Unique identifier for the project. |

**Usage**

```python
PROJECT_ID = '1531bfd9-2ca2-4a7b-bb5a-136c8da09ca1'
project = fdl.Project.get(id_=PROJECT_ID)
```

**Returns**

| Return Type                          | Description       |
| ------------------------------------ | ----------------- |
| [Project](api-methods-30.md#project) | Project instance. |

**Raises**

| Error code | Issue                                                            |
| ---------- | ---------------------------------------------------------------- |
| NotFound   | Project with given identifier not found.                         |
| Forbidden  | Current user may not have permission to view details of project. |

### from\_name()

Get project from Fidd"
"ler Platform based on name.

**Parameters**

| Parameter     | Type | Default | Description          |
| ------------- | ---- | ------- | -------------------- |
| project\_name | str  | None    | Name of the project. |

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
project = fdl.Project.from_name(name=PROJECT_NAME)
```

**Returns**

| Return Type                          | Description       |
| ------------------------------------ | ----------------- |
| [Project](api-methods-30.md#project) | Project instance. |

**Raises**

| Error code | Issue                                                            |
| ---------- | ---------------------------------------------------------------- |
| NotFound   | Project not found in the given project name.                     |
| Forbidden  | Current user may not have permission to view details of project. |

### get\_or\_create()

> Added in version 3.7.0

Get the project instance if exists, otherwise create a new project.

**Parameters**

| Parameter | Type | Default | Description                "
" |
| --------- | ---- | ------- | --------------------------- |
| name      | str  | None    | Unique name of the project. |



**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
project = fdl.Project.get_or_create(name=PROJECT_NAME)
```

**Returns**

| Return Type                          | Description       |
| ------------------------------------ | ----------------- |
| [Project](api-methods-30.md#project) | Project instance. |

**Raises**

| Error code | Issue                                                          |
| ---------- |----------------------------------------------------------------|
| Forbidden  | Current user may not have permission to view/create a project. |


### list()

Gets all projects in an organization.

**Parameters**

No

**Returns**

| Return Type                                      | Description                  |
| ------------------------------------------------ | ---------------------------- |
| Iterable\[[Project](api-methods-30.md#project) ] | Iterable of project objects. |

**Errors**

| Error code | Issue                                                      |
| ---------- | ---------------------------------------------------------- |
| Forbidden"
"  | Current user may not have permission to the given project. |

**Usage example**

```python
projects = fdl.Project.list()
```

### delete()

Delete a project.

**Parameters**

| Parameter | Type | Default | Description                  |
| --------- | ---- | ------- | ---------------------------- |
| id\_      | UUID | None    | Unique UUID of the project . |

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
project = fdl.Project.from_name(name=PROJECT_NAME)

project.delete()
```

**Returns**

None

### Properties

### List models()

List all models associated with a project.

**Parameters**

| Parameter | Type | Default | Description                  |
| --------- | ---- | ------- | ---------------------------- |
| id\_      | UUID | None    | Unique UUID of the project . |

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
project = fdl.Project.from_name(name=PROJECT_NAME)

project.models
```

**"
"Returns**

| Return Type                                 | Description                |
| ------------------------------------------- | -------------------------- |
| Iterable\[[Model](api-methods-30.md#model)] | Iterable of model objects. |

**Raises**

| Error code | Issue                                                            |
| ---------- | ---------------------------------------------------------------- |
| NotFound   | Project with given identifier not found.                         |
| Forbidden  | Current user may not have permission to view details of project. |

## Segments

Fiddler offers the ability to segment your data based on a custom condition.

### Segment

Segment object contains the below parameters.

| Parameter   | Type           | Default | Description                                 |
| ----------- | -------------- | ------- | ------------------------------------------- |
| id          | UUID           | -       | Unique identifier for the segment.          |
| name        | str            | -       | Segment name.                               |
| model\_id   | UUID           | -       | UUID of the model to which segment belongs. |
| definition  | str            | -"
"       | Definition of the segment.                  |
| description | Optional\[str] | None    | Description of the segment.                 |
| created\_at | datetime       | -       | Time of creation of segment.                |

***

### constructor()

Initialize a new segment.

**Usage params**

| Parameter   | Type           | Default | Description                                 |
| ----------- | -------------- | ------- | ------------------------------------------- |
| name        | str            | -       | Segment name.                               |
| model\_id   | UUID           | -       | UUID of the model to which segment belongs. |
| definition  | str            | -       | Definition of the segment.                  |
| description | Optional\[str] | None    | Description of the segment.                 |

**Usage**

```python
SEGMENT_NAME = 'YOUR_SEGMENT_NAME'
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = f"
"dl.Model.from_name(name=MODEL_NAME, project_id=project.id)

segment = fdl.Segment(
        name=SEGMENT_NAME,
        model_id=model.id,
        definition=""Age < 60"", #Use Fiddler Query Language (FQL) to define your custom segments
        description='Users with Age under 60',
    )
```

***

### get()

Get segment from Fiddler Platform based on UUID.

**Parameters**

| Parameter | Type | Default | Description                        |
| --------- | ---- | ------- | ---------------------------------- |
| id\_      | UUID | -       | Unique identifier for the segment. |

**Usage**

```python
SEGMENT_ID = 'ba6ec4e4-7188-44c5-ba84-c2cb22b4bb00'
segment = fdl.Segment.get(id_= SEGMENT_ID)
```

**Returns**

| Return Type                          | Description       |
| ------------------------------------ | ----------------- |
| ["
"Segment](api-methods-30.md#segment) | Segment instance. |

**Raises**

| Error code | Issue                                                            |
| ---------- | ---------------------------------------------------------------- |
| NotFound   | Segment with given identifier not found.                         |
| Forbidden  | Current user may not have permission to view details of segment. |

***

### from\_name()

Get segment from Fiddler Platform based on name and model UUID.

**Parameters**

| Parameter | Type        | Default | Description                      |
| --------- | ----------- | ------- | -------------------------------- |
| name      | str         | -       | Name of the segment.             |
| model\_id | UUID \| str | -       | Unique identifier for the model. |

**Usage**

```python
SEGMENT_NAME = 'YOUR_SEGMENT_NAME'
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id"
")

segment = fdl.Segment.from_name(
    name=NAME,
    model_id=model.id
)
```

**Returns**

| Return Type                          | Description       |
| ------------------------------------ | ----------------- |
| [Segment](api-methods-30.md#segment) | Segment instance. |

**Raises**

| Error code | Issue                                                            |
| ---------- | ---------------------------------------------------------------- |
| NotFound   | Segment with given identifier not found.                         |
| Forbidden  | Current user may not have permission to view details of segment. |

### list()

List all segments in the given model.

**Parameters**

| Parameter | Type | Default | Description                                    |
| --------- | ---- | ------- | ---------------------------------------------- |
| model\_id | UUID | -       | UUID of the model associated with the segment. |

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name"
"(name=MODEL_NAME, project_id=project.id)

segment = fdl.Segment.list(model_id=model.id)
```

**Returns**

| Return Type                                     | Description                      |
| ----------------------------------------------- | -------------------------------- |
| Iterable\[[Segment](api-methods-30.md#segment)] | Iterable of all segment objects. |

**Raises**

| Error code | Issue                                                            |
| ---------- | ---------------------------------------------------------------- |
| Forbidden  | Current user may not have permission to view details of segment. |

### create()

Adds a segment to a model.

**Parameters**

No

**Usage**

```python
SEGMENT_NAME = 'YOUR_SEGMENT_NAME'
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)

segment = fdl.Segment(
        name=SEGMENT_NAME,
        model_id=model.id,
        definition=""Age < 60"
""", #Use Fiddler Query Language (FQL) to define your custom segments
        description='Users with Age under 60',
    ).create()
```

**Returns**

| Return Type                          | Description       |
| ------------------------------------ | ----------------- |
| [Segment](api-methods-30.md#segment) | Segment instance. |

**Raises**

| Error code | Issue                                           |
| ---------- | ----------------------------------------------- |
| Conflict   | Segment with same name may exist for the model. |
| BadRequest | Invalid definition.                             |
| NotFound   | Given model may not exist .                     |

### delete()

Delete a segment.

**Parameters**

No

**Usage**

```python
SEGMENT_NAME = 'YOUR_SEGMENT_NAME'
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)

segment = fdl.Segment"
".from_name(name=SEGMENT_NAME,model_id=model.id)


segment.delete()
```

**Returns**

No

**Raises**

| Error code | Issue                                                   |
| ---------- | ------------------------------------------------------- |
| NotFound   | Segment with given identifier not found.                |
| Forbidden  | Current user may not have permission to delete segment. |

## Webhooks

Webhooks integration for alerts to be posted on Slack or other apps.

### Webhook()

Webhook object contains the below parameters.

| Parameter   | Type                                                 | Default | Description                                                                |
| ----------- | ---------------------------------------------------- | ------- | -------------------------------------------------------------------------- |
| id          | UUID                                                 | -       | Unique identifier for the webhook.                                         |
| name        | str                                                  | -       | Unique name of the webhook.                                                |
| url         | str                                                  | -       | Webhook integration URL.                                                   |
| provider    | [WebhookProvider](api-methods-30.md#webhookprovider) | -       | App in which the"
" webhook needs to be integrated. Either 'SLACK' or 'OTHER' |
| created\_at | datetime                                             | -       | Time at which webhook was created.                                         |
| updated\_at | datetime                                             | -       | Latest time at which webhook was updated.                                  |

***

### constructor()

Initialize a new webhook.

**Parameters**

| Parameter | Type                                                 | Default | Description                                      |
| --------- | ---------------------------------------------------- | ------- | ------------------------------------------------ |
| name      | str                                                  | -       | Unique name of the webhook.                      |
| url       | str                                                  | -       | Webhook integration URL.                         |
| provider  | [WebhookProvider](api-methods-30.md#webhookprovider) | -       | App in which the webhook needs to be integrated. |

**Usage**

```python
WEBHOOK_NAME = 'test_webhook_config_name'
WEBHOOK_URL = 'https://www.slack.com'
WEBHOOK_PROVIDER = 'SLACK'
webhook ="
" fdl.Webhook(
        name=WEBHOOK_NAME, url=WEBHOOK_URL, provider=WEBHOOK_PROVIDER
    )
```

### get()

Gets all details of a particular webhook from UUID.

**Parameters**

| Parameter | Type | Default | Description                        |
| --------- | ---- | ------- | ---------------------------------- |
| id\_      | UUID | -       | Unique identifier for the webhook. |

**Usage**

```python
WEBHOOK_ID = 'a5b654eb-15c8-43c8-9d50-9ba6eea9a0ff'
webhook = fdl.Webhook.get(id_=WEBHOOK_ID)
```

**Returns**

| Return Type                          | Description       |
| ------------------------------------ | ----------------- |
| [Webhook](api-methods-30.md#webhook) | Webhook instance. |

**Raises**

| Error code | Issue                                                            |
| ---------- | ---------------------------------------------------------------- |
| NotFound   | Webhook with given identifier not found"
".                         |
| Forbidden  | Current user may not have permission to view details of webhook. |

### from\_name()

Get Webhook from Fiddler Platform based on name.

**Parameters**

| Parameter | Type | Default | Description          |
| --------- | ---- | ------- | -------------------- |
| name      | str  | -       | Name of the webhook. |

**Usage**

```python
WEBHOOK_NAME = 'YOUR_WEBHOOK_NAME'

webhook = fdl.Webhook.from_name(
    name=WEBHOOK_NAME
)
```

**Returns**

| Return Type                          | Description       |
| ------------------------------------ | ----------------- |
| [Webhook](api-methods-30.md#webhook) | Webhook instance. |

**Raises**

| Error code | Issue                                                            |
| ---------- | ---------------------------------------------------------------- |
| NotFound   | Webhook with given name not found.                               |
| Forbidden  | Current user may not have permission to view details of webhook. |

### list()

Gets"
" all webhooks accessible to a user.

**Parameters**

No

**Usage**

```python
WEBHOOKS = fdl.Webhook.list()
```

**Returns**

| Return Type                                     | Description                  |
| ----------------------------------------------- | ---------------------------- |
| Iterable\[[Webhook](api-methods-30.md#webhook)] | Iterable of webhook objects. |

**Raises**

| Error code | Issue                                                            |
| ---------- | ---------------------------------------------------------------- |
| Forbidden  | Current user may not have permission to view details of webhook. |

### create()

Create a new webhook.

**Parameters**

No

**Usage**

```python
WEBHOOK_NAME = 'YOUR_WEBHOOK_NAME'
WEBHOOK_URL = 'https://www.slack.com'
WEBHOOK_PROVIDER = 'SLACK'
webhook = fdl.Webhook(
        name=WEBHOOK_NAME, url=WEBHOOK_URL, provider=WEBHOOK_PROVIDER
    )
webhook.create()
```

**Returns**

| Return Type                          | Description     |
"
"| ------------------------------------ | --------------- |
| [Webhook](api-methods-30.md#webhook) | Webhook object. |

### update()

Update an existing webhook.

**Parameters**

| Parameter | Type                                                 | Default | Description                                      |
| --------- | ---------------------------------------------------- | ------- | ------------------------------------------------ |
| name      | str                                                  | -       | Unique name of the webhook.                      |
| url       | str                                                  | -       | Webhook integration URL.                         |
| provider  | [WebhookProvider](api-methods-30.md#webhookprovider) | -       | App in which the webhook needs to be integrated. |

**Usage**

```python
WEBHOOK_NAME = ""YOUR_WEBHOOK_NAME""
webhook_list = fdl.Webhook.list()
webhook_instance = None

for webhook in webhook_list:
    if WEBHOOK_NAME == webhook.name:
    webhook_instance = webhook

webhook_instance.name = 'NEW_WEBHOOK_NAME'
webhook_instance.update()
``"
"`

**Returns**

None

**Raises**

| Error code | Issue                      |
| ---------- | -------------------------- |
| BadRequest | If field is not updatable. |

### delete()

Delete a webhook.

**Parameters**

| Parameter | Type | Default | Description                 |
| --------- | ---- | ------- | --------------------------- |
| id\_      | UUID | -       | Unique UUID of the webhook. |

**Usage**

```python
WEBHOOK_NAME = ""YOUR_WEBHOOK_NAME""
webhook_instance = None

for webhook in webhook_list:
    if WEBHOOK_NAME == webhook.name:
    webhook_instance = webhook

webhook_instance.delete()
```

**Returns**

None

***

## Explainability

Explainability methods for models.

### precompute\_feature\_importance

Pre-compute feature importance for a model on a dataset. This is used in various places in the UI.\
A single feature importance can be precomputed (computed and cached) for a model.

**Parameters**

|"
" Parameter       | Type             | Default | Description                                                                                   |
| --------------- | ---------------- | ------- | --------------------------------------------------------------------------------------------- |
| dataset\_id     | UUID             | -       | The unique identifier of the dataset.                                                         |
| num\_samples    | Optional\[int]   | None    | The number of samples used.                                                                   |
| num\_iterations | Optional\[int]   | None    | The maximum number of ablated model inferences per feature.                                   |
| num\_refs       | Optional\[int]   | None    | The number of reference points used in the explanation.                                       |
| ci\_level       | Optional\[float] | None    | The confidence level (between 0 and 1).                                                       |
| update          | Optional\[bool]  | False   | Flag to indicate whether the precomputed feature importance should be recomputed and updated. |

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME"
"'
DATASET_NAME = 'YOUR_DATASET_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)
dataset = fdl.Dataset.from_name(name=DATASET_NAME, model_id=model.id)

job = model.precompute_feature_importance(dataset_id=dataset.id, update=False)
```

**Returns**

| Return Type                  | Description                                 |
| ---------------------------- | ------------------------------------------- |
| [Job](api-methods-30.md#job) | Async job details for the pre-compute job . |

### get\_precomputed\_feature\_importance

Get pre-computed global feature **importance** for a model over a dataset or a slice.

**Parameters**

No

**Usage**

```python
feature_importance = model.get_precomputed_feature_importance()
```

**Returns**

| Return Type | Description                                         |
| ----------- | --------------------------------------------------- |
| Tuple       | A named tuple"
" with the feature importance results . |

### get\_feature\_importance()

Get global feature **importance** for a model over a dataset or a slice.

**Usage params**

| Parameter       | Type                                                                                                                                   | Default | Description                                                                                                       |
| --------------- | -------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------------------------------------------------------------------------------------------------------- |
| data\_source    | [DatasetDataSource](api-methods-30.md#datasetdatasource) | -       | Dataset data Source for the input dataset to compute feature importance on |
| num\_iterations | Optional\[int]                                                                                                                         | None    | The maximum number of ablated model inferences per feature.                                                       |
| num\_refs       | Optional\[int]                                                                                                                         | None    | The number of reference points used in the explanation.                                                           |
| ci\_level       | Optional\[float]                                                                                                                       | None    | The confidence level (between 0 and 1).                                                                           |

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
"
"MODEL_NAME = 'YOUR_MODEL_NAME'
DATASET_NAME = 'YOUR_DATASET_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)
dataset = fdl.Dataset.from_name(name=DATASET_NAME, model_id=model.id)

model = fdl.Model.get(id_=model.id)

# Dataset data source
feature_importance = model.get_feature_importance(
        data_source=fdl.DatasetDataSource(
            env_type='PRE-PRODUCTION',
            env_id=dataset.id,
        ),
    )
```

**Returns**

| Return Type | Description                                         |
| ----------- | --------------------------------------------------- |
| Tuple       | A named tuple with the feature importance results . |

**Raises**

| Error code | Issue                           |
| ---------- | ------------------------------- |
| BadRequest | If dataset id is not specified. |

***

### precompute\_feature\_impact()

Pre-compute feature impact for a model on a dataset"
". This is used in various places in the UI.\
A single feature impact can be precomputed (computed and cached) for a model.

**Usage params**

| Parameter       | Type             | Default | Description                                                                                                                                                              |
| --------------- | ---------------- | ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| dataset\_id     | UUID             | -       | The unique identifier of the dataset.                                                                                                                                    |
| num\_samples    | Optional\[int]   | None    | The number of samples used.                                                                                                                                              |
| num\_iterations | Optional\[int]   | None    | The maximum number of ablated model inferences per feature.                                                                                                              |
| num\_refs       | Optional\[int]   | None    | The number of reference points used in the explanation.                                                                                                                  |
| ci\_level       | Optional\[float] | None    | The confidence level (between 0 and 1).                                                                                                                                  |
| min\_support    | Optional\[int]  "
" | 15      | Only used for NLP (TEXT inputs) models. Specify a minimum support (number of times a specific word was present in the sample data) to retrieve top words. Default to 15. |
| update          | Optional\[bool]  | False   | Flag to indicate whether the precomputed feature impact should be recomputed and updated.                                                                                |

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'
DATASET_NAME = 'YOUR_DATASET_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)
dataset = fdl.Dataset.from_name(name=DATASET_NAME, model_id=model.id)

job = model.precompute_feature_impact(dataset_id=dataset.id, update=False)
```

**Returns**

| Return Type                  | Description                                 |
| ---------------------------- | ------------------------------------------- |
| [Job](api-methods"
"-30.md#job) | Async job details for the pre-compute job . |

### upload\_feature\_impact()

Upload a custom feature impact for a model of input type `TABULAR`. All input features need to be passed for the method to run successfully. Partial upload of feature impacts are not supported.

**Usage params**

| Parameter            | Type            | Default | Description                                                                                                                                             |
| -------------------- | --------------- | ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| feature\_impact\_map | dict            | -       | <p>Feature impacts dictionary with feature name as key and impact as value.<br>Impact value is of type float and can be positive, negative or zero.</p> |
| update               | Optional\[bool] | False   | Flag to indicate whether the feature impact is being uploaded or updated.                                                                               |

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'
DATASET_NAME = 'YOUR_DATASET_NAME'
"
"FEATURE_IMPACT_MAP = {'feature_1': 0.1, 'feature_2': 0.4, 'feature_3': -0.05}

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)
dataset = fdl.Dataset.from_name(name=DATASET_NAME, model_id=model.id)

feature_impacts = model.upload_feature_impact(feature_impact_map=FEATURE_IMPACT_MAP, update=False)
```

**Returns**

| Return Type | Description                                                                                                               |
| ----------- | ------------------------------------------------------------------------------------------------------------------------- |
| Dict        | Dictionary with feature\_names, feature\_impact\_scores, system\_generated, model\_task, model\_input\_type, created\_at. |

### get\_precomputed\_feature\_impact()

Get pre-computed global feature **impact** for a model over a dataset or a slice.

**Parameters**

No

**Usage**

```python
feature"
"_impact = model.get_precomputed_feature_impact()
```

**Returns**

| Return Type | Description                                     |
| ----------- | ----------------------------------------------- |
| Tuple       | A named tuple with the feature impact results . |

### get\_feature\_impact()

Get global feature **impact** for a model over a dataset or a slice.

**Parameters**

| Parameter       | Type                                                                                                                                   | Default | Description                                                                                                                                                             |
| --------------- | -------------------------------------------------------------------------------------------------------------------------------------- | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| data\_source    | [DatasetDataSource](api-methods-30.md#datasetdatasource) | -       | Dataset data Source for the input dataset to compute feature importance   |
| num\_iterations | Optional\[int]                                                                                                                         | None    | The maximum number of ablated model inferences per feature.                                                                                                             |
| num\_refs       | Optional\[int]                                                                                                                         | None    | The number of reference points used in the explanation.                                                                                                                 |
| ci\_level       | Optional"
"\[float]                                                                                                                       | None    | The confidence level (between 0 and 1).                                                                                                                                 |
| min\_support    | Optional\[int]                                                                                                                         | 15      | Only used for NLP (TEXT inputs) models. Specify a minimum support (number of times a specific word was present in the sample data)to retrieve top words. Default to 15. |
| output\_columns | Optional\[list\[str]]                                                                                                                  | None    | Only used for NLP (TEXT inputs) models. Output column names to compute feature impact on.                                                                               |

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'
DATASET_NAME = 'YOUR_DATASET_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)
dataset = fdl.Dataset.from_name(name=DATASET_NAME, model_id=model.id)

model ="
" fdl.Model.get(id_=model.id)

# Dataset data source
feature_impact = model.get_feature_impact(
        data_source=fdl.DatasetDataSource(
            env_type='PRE-PRODUCTION',
            env_id=dataset.id,
        ),
    )
```

**Returns**

| Return Type | Description                                     |
| ----------- | ----------------------------------------------- |
| Tuple       | A named tuple with the feature impact results . |

**Raises**

| Error code | Issue                                                 |
| ---------- | ----------------------------------------------------- |
| BadRequest | If dataset id is not specified or query is not valid. |

### precompute\_predictions()

Pre-compute predictions for a model on a dataset.

**Parameters**

| Parameter   | Type            | Default | Description                                                                                               |
| ----------- | --------------- | ------- | --------------------------------------------------------------------------------------------------------- |
| dataset\_id | UUID            | -       | Unique identifier of the dataset used for prediction.                                                     |
| chunk\_size | Optional\[int]  | None    | Chunk size for"
" fetching predictions.                                                                      |
| update      | Optional\[bool] | False   | Flag to indicate whether the pre-computed predictions should be re-computed and updated for this dataset. |

**Usage**

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'
DATASET_NAME = 'YOUR_DATASET_NAME'

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)
dataset = fdl.Dataset.from_name(name=DATASET_NAME, model_id=model.id)

model.precompute_predictions(dataset_id=dataset.id, update=False)
```

**Returns**

| Return Type                  | Description                                |
| ---------------------------- | ------------------------------------------ |
| [Job](api-methods-30.md#job) | Async job details for the prediction job . |

### explain()

Get explanation for a single observation.

**Parameters**

| Parameter           | Type                                                                                                               | Default                     | Description                                                                                                                                "
"                                                                                                                                  |
| ------------------- | ------------------------------------------------------------------------------------------------------------------ | --------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| input\_data\_source | Union\[[RowDataSource](api-methods-30.md#rowdatasource), [EventIdDataSource](api-methods-30.md#eventiddatasource)] | -                           | DataSource for the input data to compute explanation on (RowDataSource, EventIdDataSource).                                                                                                                                                                                  |
| ref\_data\_source   | Optional\[[DatasetDataSource](api-methods-30.md#datasetdatasource)]                                                | None                        | <p>Dataset data Source for the reference data to compute explanation.<br>Only used for non-text models and the following methods:<br>'SHAP', 'FIDDLER_SHAP', 'PERMUTE', 'MEAN_RESET'.</p>                                                                                  |
| method              | Optional\[Union\[[ExplainMethod](api-methods-30.md#explainmethod), str]]                                           | ExplainMethod.FIDDL"
"ER\_SHAP | <p>Explanation method name. Could be your custom explanation method or one of the following method:<br>'SHAP', 'FIDDLER_SHAP', 'IG', 'PERMUTE', 'MEAN_RESET', 'ZERO_RESET'.</p>                                                                                              |
| num\_permutations   | Optional\[int]                                                                                                     | None                        | <p>For Fiddler SHAP, that corresponds to the number of coalitions to sample to estimate the Shapley values of each single-reference game.<br>For the permutation algorithms, this corresponds to the number of permutations from the dataset to use for the computation.</p> |
| ci\_level           | Optional\[float]                                                                                                   | None                        | The confidence level (between 0 and 1) to use for the confidence intervals in Fiddler SHAP. Not used for other methods.                                                                                                                                                      |
| top\_n\_class       | Optional\[int]                                                                                                    "
" | None                        | For multiclass classification models only, specifying if only the n top classes are computed or all classes (when parameter is None).                                                                                                                                        |

**Usage**

```python
# RowDataSource and
explain_result = model.explain(
        input_data_source=fdl.RowDataSource(
            row={
                'CreditScore': 619,
                'Geography': 'France',
                'Gender': 'Female',
                'Age': 42,
                'Tenure': 2,
                'Balance': 0.0,
                'NumOfProducts': 1,
                'HasCrCard': 'Yes',
                'IsActiveMember': 'Yes',
                'EstimatedSalary': 101348.88,
            },
        ),
        ref_data_source=fdl.DatasetDataSource(
            env_type='PRODUCTION',
        ),
    )

# EventIdDataSource
explain_result = model.explain(
        input_data_source=fdl.EventIdDataSource(
            event_id='553"
"1bfd9-2ca2-4a7b-bb5a-136c8da09ca0',
            env_type=fdl.EnvType.PRE_PRODUCTION
        ),
        ref_data_source=fdl.DatasetDataSource(
            env_type='PRODUCTION',
        ),
    )
```

**Return params**

| Return Type | Description                                 |
| ----------- | ------------------------------------------- |
| Tuple       | A named tuple with the explanation results. |

**Raises**

| Error code   | Issue                                      |
| ------------ | ------------------------------------------ |
| NotSupported | If specified source type is not supported. |



### download\_data()

Download data using environment and segments, to csv or parquet file. 10M rows is the max size that can be downloaded.

**Parameters**

| Parameter   | Type                  | Default | Description                                                                                             |
| ----------- | --------------------- | ------- | ------------------------------------------------------------------------------------------------------- |
| output\_dir | Union\[Path, str]     | -       | Path"
" to download the file.                                                                              |
| env\_type   | [EnvType](api-methods-30.md#envtype)              | -       | Type of environment to query (PRODUCTION or PRE_PRODUCTION)   |
| env\_id     | UUID                  | None    | If PRE_PRODUCTION env selected, provide the uuid of the dataset to query. |
| start\_time | Optional\[datetime] | None | Start time to retrieve data, only for PRODUCTION env. If no time zone is indicated, UTC is assumed.|
| end\_time   | Optional\[datetime] | None | End time to retrieve data, only for PRODUCTION env. If no time zone is indicated, UTC is assumed.|
| segment\_id | Optional\[UUID] | None | Optional segment UUID to query data using a saved segment associated with the model |
| segment\_definition | Optional\[str] | None | Optional segment FQL definition to query data using an applied segment"
". This segment will not be saved to the model. |
| columns | Optional\[List\[str]] | None |  Allows caller to explicitly specify list of columns to retrieve. Default to None which fetch all columns from the model. |
| max\_rows   | Optional\[int]        | None    | Number of maximum rows to fetch.                                                                        |
| chunk_size | Optional\[int] | 1000 |  Number of rows per chunk to download data. You can increase that number for faster download if you query less than 1000 columns and don't have vector columns.|
| fetch\_vectors | Optional\[bool] | None | Whether the vectors columns are fetched or not. Default to False. |
| output_format | [DownloadFormat](api-methods-30.md#downloadformat) | PARQUET | Indicating if the result should be a CSV file or a PARQUET file. |

**Usage**

```python
DATASET_ID = '"
"YOUR_DATASET_UUID'

model.download_data(
    output_dir='test_download',
    env_type=fdl.EnvType.PRODUCTION,
    env_id=None,  # Not needed for Environment PRODUCTION
    start_time=datetime(2024, 10, 4, 0, 0, 0),
    end_time=datetime(2024, 10, 5,  0, 0, 0),
    segment_definition=""Geography=='Germany'"",
    segment_id=None,
    columns=['Geography', 'EstimatedSalary'],
    output_format=fdl.DownloadFormat.PARQUET,
    max_rows=1000000,
    chunk_size=10000,
    fetch_vectors=False,
)
```

**Returns**

Parquet or CSV file with slice data contents downloaded to the Path mentioned in output\_dir.

**Raises**

| Error code | Issue                    |
| ---------- | ------------------------ |
| BadRequest | If given segment is not implemented correctly |

### predict()

"
"Run model on an input dataframe.

**Parameters**

| Parameter   | Type           | Default | Description                          |
| ----------- | -------------- | ------- | ------------------------------------ |
| df          | pd.DataFrame   | None    | Feature dataframe.                   |
| chunk\_size | Optional\[int] | None    | Chunk size for fetching predictions. |

**Usage**

```python
data = {
        'row_id': 1109,
        'fixed acidity': 10.8,
        'volatile acidity': 0.47,
        'citric acid': 0.43,
        'residual sugar': 2.1,
        'chlorides': 0.171,
        'free sulfur dioxide': 27.0,
        'total sulfur dioxide': 66.0,
        'density': 0.9982,
        'pH': 3.17,
        'sulphates': 0.76,
        'alcohol': 10"
".8,
    }
df = pd.DataFrame(data, index=data.keys())
predictions = model.predict(df=df)
```

**Returns**

| Return Type | Description                            |
| ----------- | -------------------------------------- |
| Dataframe   | A pandas DataFrame of the predictions. |


***

## Constants

### ModelInputType

Input data type used by the model.

| Enum Value             | Description                                            |
| ---------------------- | ------------------------------------------------------ |
| ModelInputType.TABULAR | For tabular models.                                    |
| ModelInputType.TEXT    | For text models.                                       |
| ModelInputType.MIXED   | For models which can be a mixture of text and tabular. |

### ModelTask

The model‚Äôs algorithm type.

| Enum Value                           | Description                                       |
| ------------------------------------ | ------------------------------------------------- |
| ModelTask.REGRESSION                 | For regression models.                            |
| ModelTask.BINARY\_CLASSIFICATION     | For binary classification models.                 |
| ModelTask.MULTICLASS\_"
"CLASSIFICATION | For multiclass classification models.             |
| ModelTask.RANKING                    | For ranking classification models.                |
| ModelTask.LLM                        | For LLM models.                                   |
| ModelTask.NOT\_SET                   | For other model tasks or no model task specified. |

### DataType

The available data types when defining a model [Column](api-methods-30.md#column).

| Enum Value         | Description                 |
| ------------------ |-----------------------------|
| DataType.FLOAT     | For floats.                 |
| DataType.INTEGER   | For integers.               |
| DataType.BOOLEAN   | For booleans.               |
| DataType.STRING    | For strings.                |
| DataType.CATEGORY  | For categorical types.      |
| DataType.TIMESTAMP | For 32-bit Unix timestamps. |
| DataType.VECTOR    | For vector types            |

### CustomFeatureType

This is an enumeration defining the types of custom features that can be created.

| Enum                                     | Value"
"                                                     |
| ---------------------------------------- | --------------------------------------------------------- |
| CustomFeatureType.FROM\_COLUMNS          | Represents custom features derived directly from columns. |
| CustomFeatureType.FROM\_VECTOR           | Represents custom features derived from a vector column.  |
| CustomFeatureType.FROM\_TEXT\_EMBEDDING  | Represents custom features derived from text embeddings.  |
| CustomFeatureType.FROM\_IMAGE\_EMBEDDING | Represents custom features derived from image embeddings. |
| CustomFeatureType.ENRICHMENT             | Represents custom features derived from an enrichment.    |

### ArtifactType

Indicator of type of a model artifact.

| Enum Value                   | Description         |
| ---------------------------- | ------------------- |
| ArtifactType.SURROGATE       | For surrogates.     |
| ArtifactType.PYTHON\_PACKAGE | For python package. |

### DeploymentType

Indicator of how the model was deployed.

| Enum Value                     | Description            |
| ------------------------------ |"
" ---------------------- |
| DeploymentType.BASE\_CONTAINER | For base containers.   |
| DeploymentType.MANUAL          | For manual deployment. |

### EnvType

Environment type of a dataset.

| Enum Value              | Description                |
| ----------------------- | -------------------------- |
| EnvType.PRODUCTION      | For production events.     |
| EnvType.PRE\_PRODUCTION | For pre production events. |

### BaselineType

Type of a baseline.

| Enum Value           | Description                      |
| -------------------- | -------------------------------- |
| BaselineType.STATIC  | For static production baseline.  |
| BaselineType.ROLLING | For rolling production baseline. |

### DownloadFormat

File format to download

| Enum Value              | Description                       |
| ----------------------- | --------------------------------- |
| DownloadFormat.PARQUET  | Download data into a Parquet file |
| DownloadFormat.CSV      | Download data into a CSV file     |

### WindowBinSize

Window for rolling bas"
"elines.

| Enum Value          | Description                       |
| ------------------- | --------------------------------- |
| WindowBinSize.HOUR  | For rolling window to be 1 hour.  |
| WindowBinSize.DAY   | For rolling window to be 1 day.   |
| WindowBinSize.WEEK  | For rolling window to be 1 week.  |
| WindowBinSize.MONTH | For rolling window to be 1 month. |

### WebhookProvider

Specifies the integration provider or OTHER for generic callback response.

| Enum Value            | Description        |
| --------------------- | ------------------ |
| WebhookProvider.SLACK | For slack.         |
| WebhookProvider.OTHER | For any other app. |

### AlertCondition

Specifies the comparison operator to use for an alert threshold value.

| Enum Value             | Description                |
| ---------------------- | -------------------------- |
| AlertCondition.GREATER | The greater than operator. |
| AlertCondition.LESSER  | the less than"
" operator.    |

### BinSize

Specifies the comparison operator to use for an alert threshold value.

| Enum Value    | Description     |
| ------------- | --------------- |
| BinSize.HOUR  | The 1 hour bin. |
| BinSize.DAY   | the 1 day bin.  |
| BinSize.WEEK  | The 7 day bin.  |
| BinSize.MONTH | The 30 day bin. |

### CompareTo

Specifies the type of evaluation to use for an alert.

| Enum Value            | Description                                                         |
| --------------------- | ------------------------------------------------------------------- |
| CompareTo.RAW_VALUE   | For an absolute comparison of a specified value to the alert metric |
| CompareTo.TIME_PERIOD | For a relative comparison of the alert metric to the same metric from a previous time period. |

### Priority

Priority level label for alerts.

| Enum Value       | Description                |
| ---------------- | -------------------------- |
| Priority.LOW     | The low priority label.    |
"
"| Priority.MEDIUM  | The medium priority label. |
| Priority.HIGH    | The high priority label.   |

### Severity

Severity level for alerts.

| Enum Value        | Description                                                                               |
| ----------------- | ----------------------------------------------------------------------------------------- |
| Severity.DEFAULT  | For AlertRule when none of the thresholds have passed.                                   |
| Severity.WARNING  | For AlertRule when alert crossed the warning\_threshold but not the critical\_threshold. |
| Severity.CRITICAL | For AlertRule when alert crossed the critical\_raw\_threshold.                           |

### Alert Metric ID

AlertRule metric_id parameter constants.

| Metric Type     | Metric Id Constant         | Metric Name                |
|-----------------|----------------------------|----------------------------|
| Drift           | jsd                        | Jensen-Shannon Distance    |
|                 | psi                        | Population Stability Index |
| Service Metrics | traffic                    | Traffic                    |
| Data Integrity  | null_violation_count       | Missing Value Violation    |
|                 | type"
"_violation_count       | Type Violation             |
|                 | range_violation_count      | Range Violation            |
|                 | any_violation_count        | Any Violation              |
|                 | null_violation_percentage  | % Missing Value Violation  |
|                 | type_violation_percentage  | % Type Violation           |
|                 | range_violation_percentage | % Range Violation          |
|                 | any_violation_percentage   | % Any Violation            |
| Statistics      | sum                        | Sum                        |
|                 | average                    | Average                    |
|                 | frequency                  | Frequency                  |
| Performance     | accuracy                   | Accuracy                   |
|                 | log_loss                   | Log Loss                   |
|                 | map                        | MAP                        |
|                 | ndcg_mean                  | NDhorCG                    |
|                 | query_count                | Query Count                |
|                 | precision                  | Precision                  |
|                 | recall"
"                     | Recall / TPR               |
|                 | f1_score                   | F1                         |
|                 | geometric_mean             | Geometric Mean             |
|                 | data_count                 | Total Count                |
|                 | expected_calibration_error | Expected Calibration Error |
|                 | auc                        | AUC                        |
|                 | auroc                      | AUROC                      |
|                 | calibrated_threshold       | Calibrated Threshold       |
|                 | fpr                        | False Positive Rate        |
| Custom Metrics  | UUID of custom metric      | Custom Metric Name         |

***

## Schemas

### Column

A model column representation.

| Parameter            | Type                                          | Default | Description                                                           |
| -------------------- | --------------------------------------------- | ------- | --------------------------------------------------------------------- |
| name                 | str                                           | None    | Column name provided by the customer.                                 |
| data\_type           | list\[[Datatype](api-methods-30.md#datatype)] | None    | List of columns."
"                                                      |
| min                  | Union\[int, float]                            | None    | Min value of integer/float column.                                    |
| max                  | Union\[int, float]                            | None    | Max value of integer/float column.                                    |
| categories           | list                                          | None    | List of unique values of a categorical column.                        |
| bins                 | list\[Union\[int, float]]                     | None    | Bins of integer/float column.                                         |
| replace\_with\_nulls | list                                          | None    | Replace the list of given values to NULL if found in the events data. |
| n\_dimensions        | int                                           | None    | Number of dimensions of a vector column.                              |

## fdl.Enrichment (Private Preview)

| Input Parameter | Type            | Default | Description                                                                                                 |
| --------------- | --------------- | ------- | ----------------------------------------------------------------------------------------------------------- |
| name            | str             |         | The name of the"
" custom feature to generate                                                                  |
| enrichment      | str             |         | The enrichment operation to be applied                                                                      |
| columns         | List\[str]      |         | The column names on which the enrichment depends                                                            |
| config          | Optional\[List] | {}      | (optional): Configuration specific to an enrichment operation which controls the behavior of the enrichment |

```python
fiddler_custom_features = [
        fdl.TextEmbedding(
            name='question_cf',
            source_column='question',
            column='question_embedding',
        ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['question'],
    custom_features=fiddler_custom_features,
)
```

_Note_

Enrichments are **disabled** by default. To enable them, contact your administrator. Failing to do so will result in an error during the `add_model` call.

***

### Embedding (Private Preview)

**Supported Models:**

| model\_name                            | size  | Type"
"                    | pooling\_method | Notes           |
| -------------------------------------- | ----- | ----------------------- | --------------- | --------------- |
| BAAI/bge-small-en-v1.5                 | small | Sentence Transformer    |                 |                 |
| sentence-transformers/all-MiniLM-L6-v2 | med   | Sentence Transformer    |                 |                 |
| thenlper/gte-base                      | med   | Sentence Transformer    |                 | _**(default)**_ |
| gpt2                                   | med   | Encoder NLP Transformer | last\_token     |                 |
| distilgpt2                             | small | Encoder NLP Transformer | last\_token     |                 |
| EleuteherAI/gpt-neo-125m               | med   | Encoder NLP Transformer | last\_token     |                 |
| google/bert\_uncased\_L-4\_H-256\_A-4  | small | Decoder NLP Transformer | first\_token   "
" | Smallest Bert   |
| bert-base-cased                        | med   | Decoder NLP Transformer | first\_token    |                 |
| distilroberta-base                     | med   | Decoder NLP Transformer | first\_token    |                 |
| xlm-roberta-large                      | large | Decoder NLP Transformer | first\_token    | Multilingual    |
| roberta-large                          | large | Decoder NLP Transformer | first\_token    |                 |

```python
fiddler_custom_features = [
      fdl.Enrichment(
          name='Question Embedding', # name of the enrichment, will be the vector col
          enrichment='embedding',
          columns=['question'], # only one allowed per embedding enrichment, must be a text column in dataframe
          config={ # optional
            'model_name': ... # default: 'thenlper/gte-base'
            'pooling_method': ... # choose from '{first/last/mean}_token"
"'. Only required if NOT using a sentence transformer
          }
      ),
      fdl.TextEmbedding(
        name='question_cf', # name of the text embedding custom feature
        source_column='question', # source - raw text
        column='Question Embedding', # the name of the vector - output of the embedding enrichment
      ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['question'],
    custom_features=fiddler_custom_features,
)
```

If embeddings have already been generated for any field and sent to Fiddler, they can be imported for visualization in UMAP by modifying the `column` field of TextEmbedding to be the column with the embeddings. The Embedding enrichment can also be removed for the corresponding input field, as there is no need for Fiddler to generate the embeddings in the case that embeddings are prepopulated and imported into Fiddler.


The above example will lead to generation of new column:

| Column                 | Type"
"   | Description                                           |
| ---------------------- | ------ | ----------------------------------------------------- |
| FDL Question Embedding | vector | Embeddings corresponding to string column `question`. |




_Note_

In the context of Hugging Face models, particularly transformer-based models used for generating embeddings, the pooling\_method determines how the model processes the output of its layers to produce a single vector representation for input sequences (like sentences or documents). This is crucial when using these models for tasks like sentence or document embedding, where you need a fixed-size vector representation regardless of the input length.

***

### Centroid Distance (Private Preview)

```
fiddler_custom_features = [
      fdl.Enrichment(
        name='question_embedding',
        enrichment='embedding',
        columns=['question'],
      ),
      fdl.TextEmbedding(
          name='question_cf',
          source_column='question',
          column='question_embedding',
      ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['question'],
    custom"
"_features=fiddler_custom_features,
)
```



The above example will lead to generation of new column:

| Column                                      | Type  | Description                                                                                      |
| ------------------------------------------- | ----- | ------------------------------------------------------------------------------------------------ |
| FDL Centroid Distance (question\_embedding) | float | <p>Distance from the nearest K-Means centroid present in<br><code>question_embedding</code>.</p> |

_Note_

Does not calculate membership for preproduction data, so you cannot calculate drift. Centroid Distance is automatically added if the `TextEmbedding` enrichment is created for any given model.

***

### Personally Identifiable Information (Private Preview)

**List of PII entities**

| Entity Type         | Description                                                                                                                                                                                                                                                    | Detection Method                                    | Example                                                                                                           |
| ------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| CREDIT\_CARD        | A credit card number is between 12 to 19 digits. [https://en.wikipedia.org/wiki/Payment\_"
"card\_number](https://en.wikipedia.org/wiki/Payment\_card\_number)                                                                                                    | Pattern match and checksum                          | <p><code>4111111111111111</code><br><code>378282246310005</code> (American Express)</p>                           |
| CRYPTO              | A Crypto wallet number. Currently only Bitcoin address is supported                                                                                                                                                                                            | Pattern match, context and checksum                 | `1BoatSLRHtKNngkdXEeobR76b53LETtpyT`                                                                              |
| DATE\_TIME          | Absolute or relative dates or periods or times smaller than a day.                                                                                                                                                                                             | Pattern match and context                           | ../2024                                                                                                           |
| EMAIL\_ADDRESS      | An email address identifies an email box to which email messages are delivered                                                                                                                                                                                 | Pattern match, context and RFC-822 validation       | `trust@fiddler.ai`                                                                                                |
| IBAN\_CODE          | The International Bank Account Number"
" (IBAN) is an internationally agreed system of identifying bank accounts across national borders to facilitate the communication and processing of cross border transactions with a reduced risk of transcription errors. | Pattern match, context and checksum                 | `DE89 3704 0044 0532 0130 00`                                                                                     |
| IP\_ADDRESS         | An Internet Protocol (IP) address (either IPv4 or IPv6).                                                                                                                                                                                                       | Pattern match, context and checksum                 | <p><code>1.2.3.4</code><br><code>127.0.0.12/16</code><br><code>1234:BEEF:3333:4444:5555:6666:7777:8888</code></p> |
| LOCATION            | Name of politically or geographically defined location (cities, provinces, countries, international regions, bodies of water, mountains                                                                                                                        | Custom logic and context                            | <p>PAL"
"O ALTO<br>Japan</p>                                                                                         |
| PERSON              | A full person name, which can include first names, middle names or initials, and last names.                                                                                                                                                                   | Custom logic and context                            | Joanna Doe                                                                                                        |
| PHONE\_NUMBER       | A telephone number                                                                                                                                                                                                                                             | Custom logic, pattern match and context             | `5556667890`                                                                                                      |
| URL                 | A URL (Uniform Resource Locator), unique identifier used to locate a resource on the Internet                                                                                                                                                                  | Pattern match, context and top level url validation | [www.fiddler.ai](http://www.fiddler.ai)                                                                           |
| US SSN              | A US Social Security Number (SSN) with 9 digits.                                                                                                                                                                                                               | Pattern match and context                           | `1234-00-5678`                                                                                                    |
| US\_DRIVER\_LICENSE | A US driver license according to [https://ntsi.com/drivers-license-format/](https://"
"ntsi.com/drivers-license-format/)                                                                                                                                          | Pattern match and context                           |                                                                                                                   |
| US\_ITIN            | US Individual Taxpayer Identification Number (ITIN). Nine digits that start with a ""9"" and contain a ""7"" or ""8"" as the 4 digit.                                                                                                                                | Pattern match and context                           | 912-34-1234                                                                                                       |
| US\_PASSPORT        | A US passport number begins with a letter, followed by eight numbers                                                                                                                                                                                           | Pattern match and context                           | L12345678                                                                                                         |

```python
fiddler_custom_features = [
      fdl.Enrichment(
        name='Rag PII',
        enrichment='pii',
        columns=['question'], # one or more columns
        allow_list=['fiddler'], # Optional: list of strings that are white listed
        score_threshold=0.85, # Optional: float value for minimum possible confidence
      ),
    ]

model_spec"
" = fdl.ModelSpec(
    inputs=['question'],
    custom_features=fiddler_custom_features,
)
```

The above example will lead to generation of new columns:

| Column                          | Type | Description                                                                              |
| ------------------------------- | ---- | ---------------------------------------------------------------------------------------- |
| FDL Rag PII (question)          | bool | Whether any PII was detected.                                                            |
| FDL Rag PII (question) Matches  | str  | What matches in raw text were flagged as potential PII (ex. ‚ÄòDouglas MacArthur,Korean‚Äô)? |
| FDL Rag PII (question) Entities | str  | What entites these matches were tagged as (ex. 'PERSON')?                                |

_Note_

PII enrichment is integrated with [Presidio](https://microsoft.github.io/presidio/analyzer/languages/)

***

### Evaluate (Private Preview)

Here is a summary of the three evaluation metrics for natural language generation:

| Metric | Description                                                                "
"                      | Strengths                                    | Limitations                                            |
| ------ | ------------------------------------------------------------------------------------------------ | -------------------------------------------- | ------------------------------------------------------ |
| bleu   | Measures precision of word n-grams between generated and reference texts                         | Simple, fast, widely used                    | Ignores recall, meaning, and word order                |
| rouge  | Measures recall of word n-grams and longest common sequences                                     | Captures more information than BLEU          | Still relies on word matching, not semantic similarity |
| meteor | Incorporates recall, precision, and additional semantic matching based on stems and paraphrasing | More robust and flexible than BLEU and ROUGE | Requires linguistic resources and alignment algorithms |

```python
fiddler_custom_features = [
      fdl.Enrichment(
        name='QA Evaluate',
        enrichment='evaluate',
        columns=['correct_answer', 'generated_answer'],
        config={
            'reference_col': 'correct_answer', # required
            'prediction_col': 'generated_answer', # required
           "
" 'metrics': ..., # optional, default - ['bleu', 'rouge' , 'meteor']
        }
      ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['question'],
    custom_features=fiddler_custom_features,
)
```



The above example generates 6 new columns:

| Column                      | Type  |
| --------------------------- | ----- |
| FDL QA Evaluate (bleu)      | float |
| FDL QA Evaluate (rouge1)    | float |
| FDL QA Evaluate (rouge2)    | float |
| FDL QA Evaluate (rougel)    | float |
| FDL QA Evaluate (rougelsum) | float |
| FDL QA Evaluate (meteor)    | float  |

***

### Textstat (Private Preview)

\*\*Supported Statistics \*\*

| Statistic                       | Description                                                                                      | Usage                                                                |
| ------------------------------- | ------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------- |
| char\_count                     | Total number of"
" characters in text, including everything.                                        | Assessing text length, useful for platforms with character limits.   |
| letter\_count                   | Total number of letters only, excluding numbers, punctuation, spaces.                            | Gauging text complexity, used in readability formulas.               |
| miniword\_count                 | Count of small words (usually 1-3 letters).                                                      | Specific readability analyses, especially for simplistic texts.      |
| words\_per\_sentence            | Average number of words in each sentence.                                                        | Understanding sentence complexity and structure.                     |
| polysyllabcount                 | Number of words with more than three syllables.                                                  | Analyzing text complexity, used in some readability scores.          |
| lexicon\_count                  | Total number of words in the text.                                                               | General text analysis, assessing overall word count.                 |
| syllable\_count                 | Total number of syllables in the text.                                                           | Used in readability formulas, measures text"
" complexity.              |
| sentence\_count                 | Total number of sentences in the text.                                                           | Analyzing text structure, used in readability scores.                |
| flesch\_reading\_ease           | Readability score indicating how easy a text is to read (higher scores = easier).                | Assessing readability for a general audience.                        |
| smog\_index                     | Measures years of education needed to understand a text.                                         | Evaluating text complexity, especially for higher education texts.   |
| flesch\_kincaid\_grade          | Grade level associated with the complexity of the text.                                          | Educational settings, determining appropriate grade level for texts. |
| coleman\_liau\_index            | Grade level needed to understand the text based on sentence length and letter count.             | Assessing readability for educational purposes.                      |
| automated\_readability\_index   | Estimates the grade level needed to comprehend the text.                                         | Evaluating text difficulty for"
" educational materials.                |
| dale\_chall\_readability\_score | Assesses text difficulty based on a list of familiar words for average American readers.         | Determining text suitability for average readers.                    |
| difficult\_words                | Number of words not on a list of commonly understood words.                                      | Analyzing text difficulty, especially for non-native speakers.       |
| linsear\_write\_formula         | Readability formula estimating grade level of text based on sentence length and easy word count. | Simplifying texts, especially for lower reading levels.              |
| gunning\_fog                    | Estimates the years of formal education needed to understand the text.                           | Assessing text complexity, often for business or professional texts. |
| long\_word\_count               | Number of words longer than a certain length (often 6 or 7 letters).                             | Evaluating complexity and sophistication of language used.           |
| monosyllabcount                 | Count"
" of words with only one syllable.                                                           | Readability assessments, particularly for simpler texts.             |

```python
fiddler_custom_features = [
      fdl.Enrichment(
          name='Text Statistics',
          enrichment='textstat',
          columns=['question'],
          config={
          'statistics' : [
              'char_count',
              'dale_chall_readability_score',
            ]
          },
      ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['question'],
    custom_features=fiddler_custom_features,
)
```

The above example leads to the creation of two additional columns:

| Column                                                         | Type  | Description                                      |
| -------------------------------------------------------------- | ----- | ------------------------------------------------ |
| FDL Text Statistics (question) char\_count                     | int   | Character count of string in `question`column.   |
| FDL Text Statistics (question) dale\_chall\_readability\_score | float | Readability score of string in `question`column"
". |

***

### Sentiment (Private Preview)

```python
fiddler_custom_features = [
      fdl.Enrichment(
          name='Question Sentiment',
          enrichment='sentiment',
          columns=['question'],
      ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['question'],
    custom_features=fiddler_custom_features,
)
```

The above example leads to creation of two columns:

| Column                                      | Type   | Description                                  |
| ------------------------------------------- | ------ | -------------------------------------------- |
| FDL Question Sentiment (question) compound  | float  | Raw score of sentiment.                      |
| FDL Question Sentiment (question) sentiment | string | One of `positive`, `negative` and \`neutral. |

***

### Profanity (Private Preview)

```python
fiddler_custom_features = [
      fdl.Enrichment(
            name='Profanity',
            enrichment='profanity',
            columns=['prompt', 'response'],
            config={'output"
"_column_name': 'contains_profanity'},
        ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['prompt', 'response'],
    custom_features=fiddler_custom_features,
)
```



The above example leads to creation of two columns:

| Column                                       | Type | Description                                                                  |
| -------------------------------------------- | ---- | ---------------------------------------------------------------------------- |
| FDL Profanity (prompt) contains\_profanity   | bool | To indicate if input contains profanity in the value of the prompt column.   |
| FDL Profanity (response) contains\_profanity | bool | To indicate if input contains profanity in the value of the response column. |

***

### Answer Relevance (Private Preview)

```python
answer_relevance_config = {
  'prompt' : 'prompt_col',
  'response' : 'response_col',
}

fiddler_custom_features = [
      fdl.Enrichment(
          name = 'Answer Relevance',
          enrichment = 'answer_relevance',
         "
" columns = ['prompt_col', 'response_col'],
          config = answer_relevance_config,
      ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['prompt_col', 'response_col'],
    custom_features=fiddler_custom_features,
)
```

The above example will lead to the generation of a new column

| Column               | Type | Description                                                             |
| -------------------- | ---- | ----------------------------------------------------------------------- |
| FDL Answer Relevance | bool | Binary metric, which is True if `response` is relevant to the `prompt`. |

***

### Faithfulness (Private Preview)

```python
faithfulness_config = {
  'context' : ['doc_0', 'doc_1', 'doc_2'],
  'response' : 'response_col',
}

fiddler_custom_features = [
      fdl.Enrichment(
          name = 'Faithfulness',
          enrichment = 'faithfulness',
          columns = ['doc_0', 'doc_1', 'doc_"
"2', 'response_col'],
          config = faithfulness_config,
      ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['doc_0', 'doc_1', 'doc_2', 'response_col'],
    custom_features=fiddler_custom_features,
)
```

The above example will lead to generation of new column:

| Column           | Type | Description                                                                                               |
| ---------------- | ---- | --------------------------------------------------------------------------------------------------------- |
| FDL Faithfulness | bool | Binary metric, which is True if the facts used in`response` is correctly used from the `context` columns. |

***

### Coherence (Private Preview)

```python
coherence_config = {
  'response' : 'response_col',
}

fiddler_custom_features = [
      fdl.Enrichment(
          name = 'Coherence',
          enrichment = 'coherence',
          columns = ['response_col'],
          config = coherence_config,
      ),
    ]

model_spec = fdl.ModelSpec(
   "
" inputs=['doc_0', 'doc_1', 'doc_2', 'response_col'],
    custom_features=fiddler_custom_features,
)
```

\


The above example will lead to generation of new column:

| Column        | Type | Description                                                                         |
| ------------- | ---- | ----------------------------------------------------------------------------------- |
| FDL Coherence | bool | Binary metric, which is True if`response` makes coherent arguments which flow well. |

***

### Conciseness (Private Preview)

```c
conciseness_config = {
  'response' : 'response_col',
}

fiddler_custom_features = [
      fdl.Enrichment(
          name = 'Conciseness',
          enrichment = 'conciseness',
          columns = ['response'],
          config = coherence_config,
      ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['prompt', 'doc_0', 'doc_1', 'doc_2', 'response'],
    custom_features=fiddler"
"_custom_features,
)
```

The above example will lead to generation of new column:

| Column          | Type | Description                                                                   |
| --------------- | ---- | ----------------------------------------------------------------------------- |
| FDL Conciseness |      | Binary metric, which is True if`response` is concise, and not overly verbose. |

***

### Toxicity (Private Preview)

| Dataset    | PR-AUC | Precision | Recall |
| ---------- | ------ | --------- | ------ |
| Toxic-Chat | 0.4    | 0.64      | 0.24   |

#### Usage

The code snippet shows how to enable toxicity scoring on the `prompt` and `response` columns for each event published to Fiddler.

```python
fiddler_custom_features = [
      fdl.Enrichment(
            name='Toxicity',
            enrichment='toxicity',
            columns=['prompt', 'response'],
        ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['"
"prompt', 'doc_0', 'doc_1', 'doc_2', 'response'],
    custom_features=fiddler_custom_features,
)
```

The above example leads to creation of two columns each for prompt and response that contain the prediction probability and the model decision.

For example for the prompt column following two columns will be generated

| Column                                   | Type  | Description                               |
| ---------------------------------------- | ----- | ----------------------------------------- |
| FDL Toxicity (prompt) toxicity\_prob     | float | Model prediction probability between 0-1. |
| FDL Toxicity (prompt) contains\_toxicity | bool  | Model prediction either 0 or 1.           |

***

### Regex Match (Private Preview)

```python
fiddler_custom_features = [
        fdl.Enrichment(
          name='Regex - only digits',
          enrichment='regex_match',
          columns=['prompt', 'response'],
          config = {
                'regex' : '^\d"
"+$',
            }
        ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['prompt', 'doc_0', 'doc_1', 'doc_2', 'response'],
    custom_features=fiddler_custom_features,
)
```

The above example will lead to generation of new column

| Column                  | Type     | Description                                                                           |
| ----------------------- | -------- | ------------------------------------------------------------------------------------- |
| FDL Regex - only digits | category | Match or No Match, depending on the regex specified in config matching in the string. |

***

### Topic (Private Preview)

```python
fiddler_custom_features = [
      fdl.Enrichment(
          name='Topics',
          enrichment='topic_model',
          columns=['response'],
          config={'topics':['politics', 'economy', 'astronomy']},
      ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['prompt', 'doc_0', 'doc_1', 'doc_2', 'response'],
"
"    custom_features=fiddler_custom_features,
)
```

\


The above example leads to creation of two columns -

| Column                                     | Type         | Description                                                                                                                                                                                                                                                                                                                                                                     |
| ------------------------------------------ | ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| FDL Topics (response) topic\_model\_scores | list\[float] | Indicating probability of the given column in each of the topics specified in the Enrichment config. Each float value indicate probability of the given input classified in the corresponding topic, in the same order as topics. Each value will be between 0 and 1. The sum of values does not equal to 1, as each classification is performed independently of other topics. |
| FDL Topics (response) max\_score\_topic    | string       | Topic with the maximum score from the list of topic names specified in the Enrichment config.                                                                                                                                                                                                                                                                                   |

***

### Banned Keyword Detector (Private Preview)

```python
fiddler"
"_custom_features = [
      fdl.Enrichment(
            name='Banned KW',
            enrichment='banned_keywords',
            columns=['prompt', 'response'],
            config={'output_column_name': 'contains_banned_kw', 'banned_keywords':['nike', 'adidas', 'puma'],},
        ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['prompt', 'doc_0', 'doc_1', 'doc_2', 'response'],
    custom_features=fiddler_custom_features,
)
```

\


The above example leads to creation of two columns -

| Column                                        | Type | Description                                                                                             |
| --------------------------------------------- | ---- | ------------------------------------------------------------------------------------------------------- |
| FDL Banned KW (prompt) contains\_banned\_kw   | bool | To indicate if input contains one of the specified banned keywords in the value of the prompt column.   |
| FDL Banned KW (response) contains\_banned\_kw | bool | To"
" indicate if input contains one of the specified banned keywords in the value of the response column. |

***

### Language Detector (Private Preview)

Language detector leverages [fasttext models](https://fasttext.cc/docs/en/language-identification.html) for language detection.

```python
fiddler_custom_features = [
      fdl.Enrichment(
            name='Language',
            enrichment='language_detection',
            columns=['prompt'],
        ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['prompt'],
    custom_features=fiddler_custom_features,
)
```



The above example leads to creation of two columns -

| Column                                      | Type   | Description                                                    |
| ------------------------------------------- | ------ | -------------------------------------------------------------- |
| FDL Language (prompt) language              | string | Language prediction for input text                             |
| FDL Language (prompt) language\_probability | float  | To indicate the confidence probability of language prediction |

***

### Fast Safety (Private Preview)

The Fast safety enrichment evaluates the safety"
" of the text along ten different dimensions: `illegal, hateful, harassing, racist, sexist, violent, sexual, harmful, unethical, jailbreaking`. These dimensions are all returned by default, but can be selectively chosen as needed. Fast safety is generated through the [Fast Trust Models](doc:llm-based-metrics).

```python
fiddler_custom_features = [
      fdl.Enrichment(
            name='Prompt Safety',
            enrichment='ftl_prompt_safety',
            columns=['prompt'],
            config={'classifiers':['jailbreaking', 'illegal']}
        ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['prompt'],
    custom_features=fiddler_custom_features,
)
```

\


The above example leads to creation of a column for each dimension. -

| Column                                       | Type  | Description                                                                 |
| -------------------------------------------- | ----- | --------------------------------------------------------------------------- |
| FDL Prompt Safety (prompt) `dimension`       | bool  | Binary metric, which is"
" True if the input is deemed unsafe, False otherwise |
| FDL Prompt Safety (prompt) `dimension` score | float | To indicate the confidence probability of safety prediction                |

***

### Fast Faithfulness (Private Preview)

The Fast faithfulness enrichment is designed to evaluate the accuracy and reliability of facts presented in AI-generated text responses. Fast safety is generated through the [Fast Trust Models](doc:llm-based-metrics).

```python
fiddler_custom_features = [
        fdl.Enrichment(
            name='Faithfulness',
            enrichment='ftl_response_faithfulness',
            columns=['context', 'response'],
            config={'context_field':'context',
                    'response_field': 'response'}
        ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['context', 'response'],
    custom_features=fiddler_custom_features,
)
```



The above example leads to creation of two columns -

| Column                          | Type  | Description                                                                                               |
| -------------------------------"
" | ----- | --------------------------------------------------------------------------------------------------------- |
| FDL Faithfulness faithful       | bool  | Binary metric, which is True if the facts used in`response` is correctly used from the `context` columns. |
| FDL Faithfulness faithful score | float | To indicate the confidence probability of faithfulness prediction                                        |

***

### SQL Validation (Private Preview)
{% hint style=""info"" %}
Query validation is syntax based and does not check against any existing schema or databases for validity.
{% endhint %}

The SQL Validation enrichment is designed to evaluate different query dialects for syntax correctness.

```python
# The following dialects are supported
# 'athena', 'bigquery', 'clickhouse', 'databricks', 'doris', 'drill', 'duckdb', 'hive', 'materialize', 'mysql', 'oracle', 'postgres', 'presto', 'prql', 'redshift', 'risingwave', 'snowflake', 'spark', 'spark2"
"', 'sqlite', 'starrocks', 'tableau', 'teradata', 'trino', 'tsql'
fiddler_custom_features = [
        fdl.Enrichment(
            name='SQLValidation',
            enrichment='sql_validation',
            columns=['query_string'],
            config={
                'dialect':'mysql'
            }
        ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['query_string'],
    custom_features=fiddler_custom_features,
)
```

The above example leads to creation of two columns -

| Column                          | Type  | Description                                                                                               |
| ------------------------------- | ----- | --------------------------------------------------------------------------------------------------------- |
| SQL Validator valid       | bool  | True if the query string is syntactically valid for the specified dialect, False if not. |
| SQL Validator errors | str | If syntax errors are found they will be present as a JSON serialized string containing a list of dictionaries describing the errors.  |

***

### JSON Validation (Private Preview)

The JSON"
" Validation enrichment is designed to evaluate strings for correct JSON syntax and optionally against a user-defined schema for validation.

This enrichment uses the [python-jsonschema](https://python-jsonschema.readthedocs.io) library for json schema validation.  The defined `validation_schema` must be a valid python-jsonschema schema.

```python
fiddler_custom_features = [
        fdl.Enrichment(
            name='JSONValidation',
            enrichment='json_validation',
            columns=['json_string'],
            config={
                'strict':'true'
                'validation_schema': {
                    '$schema': 'https://json-schema.org/draft/2020-12/schema',
                    'type': 'object',
                    'properties': {
                        'prop_1': {'type': 'number'}
                        ...
                    },
                    'required': ['prop_1', ...],
                    'additionalProperties': False
                }
            }
        ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['json_string'],
   "
" custom_features=fiddler_custom_features,
)
```
The above example leads to creation of two columns -

| Column                          | Type  | Description                                                                                               |
| ------------------------------- | ----- | --------------------------------------------------------------------------------------------------------- |
| JSON Validator valid       | bool  | String is valid JSON. |
| JSON Validator errors | str | If the string failed to parse to JSON any parsing errors will be returned as a serialized json list of dictionaries,   |

***

### ModelTaskParams

Task parameters given to a particular model.

| Parameter                         | Type         | Default | Description                                                                         |
| --------------------------------- | ------------ | ------- | ----------------------------------------------------------------------------------- |
| binary\_classification\_threshold | float        | None    | Threshold for labels.                                                               |
| target\_class\_order              | list         | None    | Order of target classes.                                                            |
| group\_by                         | str          | None    | Query/session id column for ranking models.                                         |
| top\_k                            | int          | None    | Top k"
" results to consider when computing ranking metrics.                           |
| class\_weights                    | list\[float] | None    | Weight of each class.                                                               |
| weighted\_ref\_histograms         | bool         | None    | Whether baseline histograms must be weighted or not when calculating drift metrics. |

### ModelSchema

Model schema defines the list of columns associated with a model version.

| Parameter       | Type                                      | Default | Description      |
| --------------- | ----------------------------------------- | ------- | ---------------- |
| schema\_version | int                                       | 1       | Schema version.  |
| columns         | list\[[Column](api-methods-30.md#column)] | None    | List of columns. |

### ModelSpec

Model spec defines how model columns are used along with model task.

| Parameter        | Type                                                    | Default | Description                 |
| ---------------- | ------------------------------------------------------- | ------- | --------------------------- |
| schema\_version  | int                                                     | 1       | Schema version."
"             |
| inputs           | list\[str]                                              | None    | Feature columns.            |
| outputs          | list\[str]                                              | None    | Prediction columns.         |
| targets          | list\[str]                                              | None    | Label columns.              |
| decisions        | list\[str]                                              | None    | Decisions columns.          |
| metadata         | list\[str]                                              | None    | Metadata columns            |
| custom\_features | list\[[CustomFeature](api-methods-30.md#customfeature)] | None    | Custom feature definitions. |

### CustomFeature

The base class for derived features such as Multivariate, VectorFeature, etc.

| Parameter   | Type                                                     | Default | Description                                                                                 |
| ----------- | -------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------- |
| name        | str                                                      | None    | The name of the custom feature.                                                             |
| type        | [CustomFeatureType](api-methods-30.md#"
"customfeaturetype) | None    | The type of custom feature. Must be one of the `CustomFeatureType` enum values.             |
| n\_clusters | Optional\[int]                                           | 5       | The number of clusters.                                                                     |
| centroids   | Optional\[List]                                          | None    | Centroids of the clusters in the embedded space. Number of centroids equal to `n_clusters`. |

**Multivariate**

Represents custom features derived from multiple columns.

| Parameter           | Type                                                     | Default                         | Description                                                                                                                                |
| ------------------- | -------------------------------------------------------- | ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| type                | [CustomFeatureType](api-methods-30.md#customfeaturetype) | CustomFeatureType.FROM\_COLUMNS | Indicates this feature is derived from multiple columns.                                                                                   |
| columns             | List\[str]                                               | None                            | List of original columns from which this feature is derived.                                                                               |
| monitor\_components | bool                                                    "
" | False                           | Whether to monitor each column in `columns` as individual feature. If set to `True`, components are monitored and drift will be available. |

**VectorFeature**

Represents custom features derived from a single vector column.

| Parameter      | Type                                                     | Default                        | Description                                                                 |
| -------------- | -------------------------------------------------------- | ------------------------------ | --------------------------------------------------------------------------- |
| type           | [CustomFeatureType](api-methods-30.md#customfeaturetype) | CustomFeatureType.FROM\_VECTOR | Indicates this feature is derived from a single vector column.              |
| source\_column | Optional\[str]                                           | None                           | Specifies the original column if this feature is derived from an embedding. |
| column         | str                                                      | None                           | The vector column name.                                                     |

**TextEmbedding**

Represents custom features derived from text embeddings.

| Parameter | Type                                                     | Default                                 | Description                                                                                                      |
| --------- | -------------------------------------------------------- | --------------------------------------- | ---------------------------------------------------------------------------------------------------------------- |
|"
" type      | [CustomFeatureType](api-methods-30.md#customfeaturetype) | CustomFeatureType.FROM\_TEXT\_EMBEDDING | Indicates this feature is derived from a text embedding.                                                         |
| n\_tags   | Optional\[int]                                           | 5                                       | How many tags(tokens) the text embedding uses in each cluster as the `tfidf` summarization in drift computation. |

**ImageEmbedding**

Represents custom features derived from image embeddings.

| Parameter | Type                                                     | Default                                  | Description                                                |
| --------- | -------------------------------------------------------- | ---------------------------------------- | ---------------------------------------------------------- |
| type      | [CustomFeatureType](api-methods-30.md#customfeaturetype) | CustomFeatureType.FROM\_IMAGE\_EMBEDDING | Indicates this feature is derived from an image embedding. |

**Enrichment**

Represents custom features derived from enrichment.

| Parameter  | Type                                                     | Default                      | Description                                                       |
| ---------- | --------------------------------------------------------"
" | ---------------------------- | ----------------------------------------------------------------- |
| type       | [CustomFeatureType](api-methods-30.md#customfeaturetype) | CustomFeatureType.ENRICHMENT | Indicates this feature is derived from enrichment.                |
| columns    | List\[str]                                               | None                         | List of original columns from which this feature is derived.      |
| enrichment | str                                                      | None                         | A string identifier for the type of enrichment to be applied.     |
| config     | Dict\[str, Any]                                          | None                         | A dictionary containing configuration options for the enrichment. |

### XaiParams

Represents the explainability parameters.

| Parameter                | Type           | Default | Description                                                                     |
| ------------------------ | -------------- | ------- | ------------------------------------------------------------------------------- |
| custom\_explain\_methods | List\[str]     | None    | User-defined explain\_custom methods of the model object defined in package.py. |
| default\_explain\_method | Optional\[str] |"
" NOne    | Default explanation method.                                                     |

### DeploymentParams

Deployment parameters of a particular model.

| Parameter        | Type                                               | Default                                                        | Description                                                                                                                                                                         |
| ---------------- | -------------------------------------------------- | -------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| artifact\_type   | str                                                | [ArtifactType.PYTHON\_PACKAGE](api-methods-30.md#artifacttype) | Type of artifact upload.                                                                                                                                                            |
| deployment\_type | [DeploymentType](api-methods-30.md#deploymenttype) | None                                                           | Type of deployment.                                                                                                                                                                 |
| image\_uri       | Optional\[str]                                     | md-base/python/python-311:1.0.0                                | A Docker image reference. See available images [here](../product-guide/explainability/flexible-model-deployment/). |
| replicas         | Optional\[str]                                     | 1                                                              | The number of replicas running the model. Minimum value: 1 Maximum value: 10 Default"
" value: 1                                                                                       |
| cpu              | Optional\[str]                                     | 100                                                            | The amount of CPU (milli cpus) reserved per replica. Minimum value: 10 Maximum value: 4000 (4vCPUs) Default value: 100                                                              |
| memory           | Optional\[str]                                     | 256                                                            | The amount of memory (mebibytes) reserved per replica. Minimum value: 150 Maximum value: 16384 (16GiB) Default value: 256                                                           |

### RowDataSource

Explainability input source for row data.

| Parameter | Type | Default | Description                        |
| --------- | ---- | ------- | ---------------------------------- |
| row       | Dict | None    | Dictionary containing row details. |

### EventIdDataSource

Explainability input source for event data.

| Parameter | Type                                 | Default | Description                |
| --------- | ------------------------------------ | ------- | -------------------------- |
| event\_id | str"
"                                  | None    | Unique ID for event.       |
| env\_id   | Optional\[Union\[str, UUID]]         | None    | Unique ID for environment. |
| env\_type | [EnvType](api-methods-30.md#envtype) | None    | Environment type.          |

### DatasetDataSource

Reference data source for explainability.

| Parameter    | Type                                 | Default | Description                                  |
| ------------ | ------------------------------------ | ------- | -------------------------------------------- |
| env\_type    | [EnvType](api-methods-30.md#envtype) | None    | Environment type.                            |
| num\_samples | Optional\[int]                       | None    | Number of samples to select for computation. |
| env\_id      | Optional\[Union\[str, UUID]]          | None    | Unique ID for environment.                   |

***

## Helper functions

### set\_logging

Set app logger at given log level.

**Parameters**

| Parameter"
" | Type | Default      | Description                              |
| --------- | ---- | ------------ | ---------------------------------------- |
| level     | int  | logging.INFO | Logging level from python logging module |

**Usage**

```python
set_logging(logging.INFO)
```

**Returns**

None

### group\_by

Group the events by a column. Use this method to form the grouped data for ranking models.

**Parameters**

| Parameter      | Type           | Default | Description                           |
| -------------- | -------------- | ------- | ------------------------------------- |
| df             | pd.DataFrame   | -       | Unique identifier for the AlertRule. |
| group\_by\_col | str            | -       | The column to group the data by.      |
| output\_path   | Optional\[Path | str]    | -                                     |

**Usage**

```python
COLUMN_NAME = 'col_2'

grouped_df = group_by(df=df, group_by_col=COLUMN_NAME)
```

**Returns**

|"
" Return Type  | Description                  |
| ------------ | ---------------------------- |
| pd.Dataframe | Dataframe in grouped format. |
"
"## Environment

{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/environments"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/environments/{env_id}"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/models/{model_id}/environments"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}
"
"## Jobs

{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/jobs/{job_id}"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/jobs"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}
"
"## Model

{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/models"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/models"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/models/{model_id}"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/models/{model_id}"" method=""delete"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/models/{model_id}"" method=""patch"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/model-factory"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/models/{model_id}/deploy-artifact"" method=""put"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/models/{model_id}/deploy-artifact"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/models/{model_id}/deploy-surrogate"" method=""put"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/models/{model_id}/deploy-surrogate"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/models/{model_id}/columns"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/models/{model_id}/columns/{column_id}"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}
"
"## Custom Metrics
{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/custom-metrics"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/custom-metrics"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/custom-metrics/{uuid}"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/custom-metrics/{uuid}"" method=""delete"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/models/{model_id}/custom-metrics"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}
"
"## Explainability
{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/slice-query/parse"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/slice-query/fetch"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/analytics/feature-impact"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/analytics/feature-importance"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/analytics/scores"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/analytics/precompute-feature-impact"" method=""put"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/analytics/precompute-feature-impact"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/analytics/feature-impact/precomputed"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/analytics/precompute-feature-importance"" method=""put"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/analytics/precompute-feature-importance"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/analytics/feature-importance/precomputed"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/predict"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}
"
"## File Upload

{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/files/upload"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/files/multipart-init"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/files/multipart-upload"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/files/multipart-complete"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}
"
"## Projects

{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/projects"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/projects"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/projects/{project_id}"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}
"
"---
title: ""REST API""
slug: ""api""
excerpt: """"
hidden: false
createdAt: ""Mon Aug 19 2024 09:46:33 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Mon Aug 19 2024 09:46:33 GMT+0000 (Coordinated Universal Time)""
---

## API Reference
The Fiddler API is organized around REST. Our API has predictable resource-oriented URLs, accepts form-encoded request bodies, returns JSON-encoded responses, and uses standard HTTP response codes, authentication, and verbs.

## API Response types
Fiddler API returns three kinds of responses

### Normal Response
Normal response are the ones which doesn‚Äôt need to be paginated.
```
{
  api_version: <API version responding back with the response>,
  kind: ""NORMAL"",
  data: <Actual Response Object>
}
```

### Paginated Response
Paginated response contains the relevant items along with pagination data.
```
{
  api_version: <API version responding back with the response>,
  kind: ""PAGINATED"",
  data: {
    page_size: <integer>,
    item_count: <integer>,
    total: <integer>,
    page_count: <integer>,
    page_index: <integer>,
    offset: <integer>,
    items: [<Array of items>]
  }
}
```

### Error Response
In case something goes wrong, error response is returned.
```
{
  api_version: <API version responding back with the response>,
  kind: ""ERROR"",
  error: {
    code: <Error code>,
    message: <string>,
    errors: [
      {
        reason: <string>,
        message: <string>,
        help: <string>
      }
    ]
  }
}
```

Fiddler uses conventional HTTP response codes to indicate the success or failure of an API request.
In general: Codes in the **_2xx_** range indicate success.
Codes in the **_4xx_** range indicate an error that failed given the information provided (e.g., a required parameter was omitted, a charge failed, etc.).
Codes in the **_5xx_** range indicate an error with Fiddler‚Äôs servers (these are rare).

## List of APIs
- [Projects](projects.md)
- [Model](model.md)
- [File Upload](file-upload.md)
- [Custom Metrics](custom-metrics.md)
- [Segments](segments.md)
- [Baseline](baseline.md)
- [Jobs](jobs.md)
- [Alert Rules](alert-rules.md)
- [Environment](environment.md)
- [Explainability](explainability.md)
- [Server Info](server-info.md)

<hr />

## Just getting started?
Check out our [Quickstart Notebooks](../QuickStart\_Notebooks/quick-start.md).
"
"## Baseline

{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/baselines"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/baselines"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/baselines/{baseline_id}"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/baselines/{baseline_id}"" method=""delete"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/models/{model_id}/baselines"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}
"
"## Alert Rules

{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/alert-rules/{alert_id}/records"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/alert-rules/{alert_id}/record-history"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/alert-rules/summary"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/alert-rules"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/alert-rules"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/alert-rules/{alert_id}/notification"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/alert-rules/{alert_id}/notification"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/alert-rules/{alert_id}/notification"" method=""patch"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/alert-rules/{alert_id}"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/alert-rules/{alert_id}"" method=""delete"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/alert-rules/{alert_id}"" method=""patch"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}
"
"## Server Info

{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/server-info"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}
"
"## Segments

{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/segments"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/segments"" method=""post"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/segments/{uuid}"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/segments/{uuid}"" method=""delete"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}


{% swagger src=""../.gitbook/assets/api_v3_ext.yaml"" path=""/v3/models/{model_id}/segments"" method=""get"" %}
[api_v3_ext.yaml](../.gitbook/assets/api_v3_ext.yaml)
{% endswagger %}
"
"# Specifying Custom Features

"
"---
hidden: true
---

# Using Custom Timestamps



{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}
"
"# Retrieving Events



{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}
"
"---
title: Product Tour
slug: product-tour
excerpt: Here's a tour of our product UI!
metadata:
  title: Product Tour | Fiddler Docs
  description: Take a tour of Fiddler AI Observability platform.
  image: []
  robots: index
createdAt: Tue Apr 19 2022 20:09:29 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 20:56:23 GMT+0000 (Coordinated Universal Time)
icon: image
---

# Product Tour

## Video Demo

Watch the video to learn how Fiddler AI Observability provides data science and MLOps teams with a unified platform to monitor, analyze, explain, and improve machine learning models at scale, and build trust in AI.

{% embed url=""https://www.youtube.com/watch?v=9-odGmMvudc"" %}

## Documented UI Tour

When you"
" log in to Fiddler, you are on the Home page and you can visualize monitoring information for your models across all your projects.

* At the top of the page, you will see donut charts for the number of triggered alerts for [Performance](../UI\_Guide/monitoring-ui/performance.md), [Data Drift](../UI\_Guide/monitoring-ui/data-drift.md), and [Data Integrity](../UI\_Guide/monitoring-ui/data-integrity.md).
* To the right of the donut charts, you will find the Bookmarks as well as a Recent Job Status card that lets you keep track of long-running async jobs and whether they have failed, are in progress, or successfully completed.
* The [Monitoring](../UI\_Guide/monitoring-ui/) summary table displays your models across different [projects](../Client\_Guide/create-a-project-and-model.md#create-a-project) along with information on their traffic, drift, and the number of triggered alerts"
".

![](../.gitbook/assets/e8f76d4-image.png)

View all of your bookmarked, Projects, Models, Charts, and Dashboards by clicking ""View All"" on the Bookmarks card on the homepage or navigating directly to Bookmarks via the navigation bar.

![](../.gitbook/assets/5fc2af8-image.png)

Track all of your ongoing and completed model, dataset, and event publish jobs by clicking ""View All"" on the Jobs card on the homepage or navigating directly to the Jobs via the navigation bar.

![](../.gitbook/assets/5d009c7-image.png)

On the side navigation bar, below charts, is the [Projects](../Client\_Guide/create-a-project-and-model.md) Tab. You can click on the Projects tab and it lands on a page that lists all your projects contained within Fiddler. See the [Fiddler Samples](product-tour.md#fiddler-samples) section"
" below for more information on these projects. You can create new projects within the UI (by clicking the ‚ÄúAdd Project‚Äù button) or via the [Fiddler Client](../Python\_Client\_3-x/about-client-3x.md).

![](../.gitbook/assets/8a47f0a-image.png)

**Projects** represent your organization's distinct AI applications or use cases. Within Fiddler, Projects house all the **Models** specific to a given application, and thus serve as a jumping-off point for the majority of Fiddler‚Äôs model monitoring and explainability features.

Go ahead and click on the _bank\_churn_ to navigate to the Project Overview page.

![](../.gitbook/assets/fce6754-image.png)

Here you can see a list of the models contained within the fraud detection project, as well as a project dashboard to which analyze charts can be pinned. Go ahead and click the ‚Äúchurn\_classifier‚Äù model.

![]("
"../.gitbook/assets/a35e820-image.png)

From the Model Overview page, you can view details about the model: its metadata (schema), the files in its model directory, and its features, which are sorted by impact (the degree to which each feature influences the model‚Äôs prediction score).

You can then navigate to the platform's core monitoring and explainability capabilities. These include:

* _**Monitor**_ ‚Äî Track and configure alerts on your model‚Äôs performance, data drift, data integrity, and overall service metrics. Read the [Monitoring](../UI\_Guide/monitoring-ui/) documentation for more details.
* _**Analyze**_ ‚Äî Analyze the behavior of your model in aggregate or with respect to specific segments of your population. Read the [Analytics](../UI\_Guide/analytics-ui/) documentation for more details.
* _**Explain**_ ‚Äî Generate ‚Äúpoint‚Äù or prediction-level explanations on your training or production data for insight into how each model decision"
" was made. Read the [Explainability](../UI\_Guide/explainability-ui-giude/) documentation for more details.

### Fiddler Samples

Fiddler Samples is a set of datasets and models that are preloaded into Fiddler. They represent different data types, model frameworks, and machine learning techniques. See the table below for more details.

| **Project**   | **Model**                       | **Dataset** | **Model Framework** | **Algorithm**       | **Model Task**             | **Explanation Algos** |
| ------------- | ------------------------------- | ----------- | ------------------- | ------------------- | -------------------------- | --------------------- |
| Bank Churn    | Bank Churn                      | Tabular     | scikit-learn        | Random Forest       | Binary Classification      | Fiddler Shapley       |
| Heart Disease | Heart Disease                   | Tabular     | Tensorflow          |                     | Binary Classification      | Fiddler Shapley, IG  "
" |
| IMDB          | Imdb Rnn                        | Text        | Tensorflow          | BiLSTM              | Binary Classfication       | Fiddler Shapley, IG   |
| Iris          | Iris                            | Tabular     | scikit-learn        | Logistic Regression | Multi-class Classification | Fiddler Shapley       |
| Lending       | Logreg-all                      | Tabular     | scikit-learn        | Logistic Regression | Binary Classification      | Fiddler Shapley       |
|               | Logreg-simple                   | Tabular     | scikit-learn        | Logistic Regression | Binary Classification      | Fiddler Shapley       |
|               | Xgboost-simple-sagemaker        | Tabular     | scikit-learn        | XGboost             | Binary Classification      | Fiddler Shapley       |
| Newsgroup     | Christianity Atheism Classifier | Text        | scikit-learn        | Random"
" Forest       | Binary Classification      | Fiddler Shapley       |
| Wine Quality  | Linear Model Wine Regressor     | Tabular     | scikit-learn        | Elastic Net         | Regression                 | Fiddler Shapley       |
|               | DNN Wine Regressor              | Tabular     | Tensorflow          |                     | Regression                 | Fiddler Shapley       |

See the [README](https://github.com/fiddler-labs/fiddler-examples) on GitHub for more information.

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: ""Advanced Explainability""
slug: ""advanced-explainability-1""
excerpt: ""Support for Complex Use Cases""
hidden: true
createdAt: ""Thu Dec 01 2022 19:13:50 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
As a prerequisite for this section, you should be familiar with the [basic model upload](https://dash.readme.com/project/fiddler/v1.3/docs/advanced-explainability) and the components of a Fiddler model package.

# Introduction

Modern machine learning frameworks are extremely flexible and Fiddler's users build powerful models in unique form factors.  ""Advanced Explainability"" in Fiddler refers to a set of capabilities in the model ingestion workflow that support the following less common, but often more interesting and high-value scenarios:

### Multi-Modal Model Inputs

While some models have simple inputs, like ""structured"" vectors of numerical and categorical features, or single text strings; others expect a combination of different data types and shapes. 

Examples Include:

- models evaluating an image based on a text prompt
- credit underwriting models that combine dense tabular features with sequences encoding recent bank transactions or credit history events to make a lending decision

### Unstructured Input Types

Data inputs to models may not conform to simple data types and need to be serialized and deserialized by user code without the expectation that the Fiddler platform understands what that data represents.

Examples Include:

- sparse representations of vectors produced by term-frequency preprocessing
- nested and arbitrary length lists representing event sequences
- encoded binary images or waveform data. 

### Large Feature Counts

Models with large numbers of inputs can make precise approximations of Shaply values computationally prohibitive.  Additionally, representing these inputs in a tabular format may exceed column count limitations or produce unnecessarily large workloads computing sketches of many features whose individual characteristics are not useful.

Examples Include:

- computer vision models where model inputs are pixels
- language models with large token counts
- unstructured models with arbitrary-length sequential s
- inputs more efficiently represented as sparse vectors (e.g. TFIDF vectors; especially when n-grams with n>1 is included).

### Custom Explanations

While Fiddler implements general-purpose explainers, there are some situations where a model developer might prefer to instrument their model with their own explanation algorithm and surface that explanation in the Fiddler UI and through Fiddler's APIs:

Examples include:

- A model developer instruments a PyTorch deep-learning model with the [Captum](https://captum.ai/) library during development and chooses to make those explanations available to other stakeholders
- It may be preferable to use a architecture-specific explainer for performance reasons ‚Äì like using TreeSHAP for a decision tree or GBM.
- Running arbitrary explainability algorithms might be contractually or practically off-limits for a vendor model, but some explainability mechanism is provided by the vendor.

### Explanation Display Customization

# Advanced Explanation Types

# ""GEM"" ‚Äì Generalized Explanation Markup

- Helpers + gem.py
- Custom Example: Ice cream sales
- IG Example: Hybrid Churn
"
"---
title: Data Drift
slug: data-drift
excerpt: UI Guide
createdAt: Tue Apr 19 2022 20:25:14 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Jan 18 2024 00:16:10 GMT+0000 (Coordinated Universal Time)
---

# Data Drift

Model performance can be poor if models trained on a specific dataset encounter different data in production. This is called data drift. In the **Insights** tab for your model, Fiddler gives you a diverse set visuals to explore different metrics.

![](../../.gitbook/assets/007f174-image.png)

Leverage the data drift chart to identify what data is drifting, when it‚Äôs drifting, and how it‚Äôs drifting. This is the first step in identifying possible model performance issues.

![](../../.gitbook/assets/8e36336-image.png)

You can change the time range using the controls in the"
" upper-left in both charts and dashboards:

![](../../.gitbook/assets/934b903-image.png)

### What is being tracked?

* _**Drift Metrics**_
  * **Jensen‚ÄìShannon distance (JSD)**
    * A distance metric calculated between the distribution of a field in the baseline dataset and that same distribution for the time period of interest.
    * For more information on JSD, click [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html).
  * **Population Stability Index (PSI)**
    * A drift metric based on the multinomial classification of a variable into bins or categories. The differences in each bin between the baseline and the time period of interest are then utilized to calculate it as follows:

![](../../.gitbook/assets/0baeb90-psi\_calculation.png)

Here, `B` is the total number of bins, `ActualProp"
"(b)` is the proportion of counts within bin `b` from the target distribution, and `ExpectedProp(b)` is the proportion of counts within bin `b` from the reference distribution. Thus, PSI is a number that ranges from zero to infinity and has a value of zero when the two distributions exactly match.

> üöß Note
>
> Since there is a possibility that a particular bin may be empty, PSI can shoot to infinity. To avoid this, PSI calculation in Fiddler is done such that each bin count is incremented with a base\_count=1. Thus, there might be a slight difference in the PSI values obtained from manual calculations.

* _**Average Values**_ ‚Äì The mean of a field (feature or prediction) over time. This can be thought of as an intuitive drift score.
* _**Drift Analytics**_ ‚Äì You can drill down into the features responsible for the prediction drift using the table at the bottom.
  * _**Feature Impact"
"**_: The contribution of a feature to the model‚Äôs predictions, averaged over the baseline dataset. The contribution is calculated using random ablation feature impact.
  * _**Feature Drift**_: Drift of the feature, calculated using the drift metric of choice.
  * _**Prediction Drift Impact**_: A heuristic calculated using the product of the feature impact and the feature drift. The higher the score, the more this feature is likely to have contributed to the prediction drift.

In the Drift Root Cause Analysis table, you can select a feature to see the feature distribution for both the time period under consideration and the baseline dataset.

![](../../.gitbook/assets/fa3158e-image.png)

### Why is it being tracked?

* Data drift is a great proxy metric for **performance decline**, especially if there is delay in getting labels for production events. (e.g. In a credit lending use case, an actual default may happen after months or years.)
* Monitoring data drift also"
" helps you stay informed about **distributional shifts in the data for features of interest**, which could have business implications even if there is no decline in model performance.

### What do I do next with this information?

* High drift can occur as a result of _data integrity issues_ (bugs in the data pipeline), or as a result of _an actual change in the distribution of data_ due to external factors (e.g. a dip in income due to COVID). The former is more in our control to solve directly. The latter may not be solvable directly, but can serve as an indicator that further investigation (and possible retraining) may be needed.
* You can drill down deeper into the data by examining it in the Analyze tab.

The image below shows how to open the Analyze view for a specific feature and time range identified in the Data Drift page.

![](../../.gitbook/assets/36407f8-Screen\_Shot\_2024-01-17"
"\_at\_7.15.19\_PM.png)

This will bring you to the Analyze tab, where you can then use SQL to slice and dice the data. You can then apply visualizations upon these slices to analyze the model‚Äôs behavior.

![](../../.gitbook/assets/25eca03-Monitor\_Analyze.png)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Monitoring Charts UI
slug: monitoring-charts-ui
excerpt: ''
createdAt: Thu Feb 23 2023 23:06:54 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 23:28:37 GMT+0000 (Coordinated Universal Time)
---

# Monitoring Charts UI

### Getting Started:

To use Fiddler AI‚Äôs monitoring charts, navigate to the Charts tab in the top-level navigation bar on the Fiddler AI platform. Choose between opening a previously saved chart or creating a new chart.

### Create a New Monitoring Chart

To create a new monitoring chart, click on the Add Chart button on the Charts page. Search for and select the [project](../../product-guide/product-concepts.md) to create the chart, and press Add Chart.

![](../../.gitbook/assets/performance\_analytics\_list.png)

### Chart Functions

#### Save & Share

Manually save your chart using"
" the Save button on the top right corner of the chart studio. Copy a link to your chart and share it with other [fiddler accounts who have access](../administration-ui/inviting-users.md) to the project where the chart resides.

#### Global Undo & Redo

Easily control the following actions with the undo and redo buttons:

* Metric query selection
* Time range selections
* Time range selections
* Bin size selections

To learn how to undo actions taken using the chart toolbar, see the Toolbar information in the next section.

### Chart Metric Queries & Filters

#### Metric Query

A metric query enables you to define what model to focus on, and which metrics and columns to plot on your monitoring chart. To get started with the metric query, choose a model of choice. Note: only models within the same project as your chart are accessible.

Once a model is selected, choose a metric type from Performance, Data Drift, Data Integrity, or Traffic metrics and relevant"
" metrics. For example, we may choose to chart accuracy for our binary classification model.

![](../../.gitbook/assets/a46e656-image.png)

#### Charting Multiple Columns

If you choose to chart data drift or data integrity, you can choose to plot up to 20 different columns from the following column categories; inputs, outputs, targets, decisions, metadata, and custom features.

![](../../.gitbook/assets/7d91cc8-image.png)

#### Charting Multiple Metrics or Models

Add up to 6 metric queries that allow you to chart different metrics and/or models in a single chart view.

![](../../.gitbook/assets/4213040-image.png)

#### Chart Filters & Capabilities

There are three major chart filter capabilities, chart filters, chart toolbar, and zoom slider.\
They work together to enable you to best analyze the slices of data that may be worth investigating.

![](../../.gitbook/assets/f58936d-image.png)

####"
" Filters

You can customize your chart view using time range, time zone, and bin size chart filters. The data range can be one of the pre-defined time ranges or a custom range. The bin size selected controls the frequency for which the data is displayed. So selecting Day will show daily data over the date range selected.

#### Toolbar

The charts toolbar is made up of 5 functions:

* Drag to zoom
* Reset zoom
* Toggle to a line chart
* Toggle to a bar chart
* Undo all toolbar actions

> üìò Note: If the zoom reset or toolbar undo is selected, this will also undo any actions taken with the zoom slider.

![](../../.gitbook/assets/0a9224c-image.png)

**Line & Bar Chart Toggle**

You can switch between visualizing your chart as a line or bar chart using the toolbar icons. Click on the line chart icon on the top right of the chart to switch to the line chart view. Likewise,"
" select the bar chart icon in the toolbar to switch to the bar chart view. However, note that these views are only temporary and any settings you specify using the toolbar will not be saved to the chart.

![](../../.gitbook/assets/c8c0e79-image.png)

**Zoom Slider**

You can also use the horizontal zoom bar to zoom, located at the base of the chart. Once you've identified the time range you want to focus on, you can use the zoom bar to drag the range across time. For instance, if you want to analyze your data week-by-week over the past six months, you can use the toolbar or horizontal zoom bar to zoom in on the desired time range and then click and drag the selected range using the base horizontal zoom bar.

![](../../.gitbook/assets/c73c24c-image.png)

#### Breakdown Summary

You can easily visualize your charts' raw data as a table within the fiddler chart studio, or download the"
" content as a CSV for further analysis. If you choose to chart multiple columns, as shown below, you can search for and sort by Model name, Metric name, Column name, or values for a specific date.

![](../../.gitbook/assets/0ddc155-image.png)

### Customize Tab

#### Scale & Range

The Customize tab enables users to adjust the scale and range of the y-axis on their monitoring charts. In the example below, we have adjusted the minimum value of the y-axis for the plotted traffic to make more use of the chart space. For values with large variance, logarithmic scale can be applied to more clearly analyze the chart.

![](../../.gitbook/assets/4926dff-image.png)

#### Y-axis Assignment

Select the y-axis for your metric queries with enhanced flexibility to customize the scale and range for each axis.

![](../../.gitbook/assets/96f49e0-image.png)

{% include ""../../.gitbook/includes/main-doc-footer"
".md"" %}

"
"---
title: Custom Metrics
slug: custom-metrics
excerpt: ''
createdAt: Mon Apr 29 2024 21:31:26 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 23:00:05 GMT+0000 (Coordinated Universal Time)
---

# Custom Metrics

### Overview

Custom metrics offer the flexibility to define metrics that align precisely with your machine learning requirements. Whether it's tracking business KPIs, crafting specialized performance assessments, or computing weighted averages, custom metrics empower you to tailor measurements to your specific needs. Seamlessly integrate these custom metrics throughout the Fiddler platform, leveraging them in dashboards, alerting, and performance tracking.

Craft novel metrics by harnessing a blend of your model's existing dimensions and functions, employing a familiar query language called [Fiddler Query Language (FQL)](../../product-guide/monitoring-platform/fiddler-query-language.md) . This capability enables you to amalgamate features, metadata, predictions, and actual outcomes using a rich array of aggregations, operators, and metric functions, thereby expanding the depth of your analytical insights.

### Adding a Custom Metric

**Note:** To add a Custom Metric using the Python client, see [fdl.CustomMetric](../../Python\_Client\_3-x/api-methods-30.md#create-2)

From the model schema page, you can access the model's custom metrics by clicking the **Custom Metrics** tab at the top of the page. Then click **Add Custom Metric** to add a new Custom Metric. Finally, enter the name, description, and FQL definition for your custom metric and click **Save**.

![](../../.gitbook/assets/1614146-image.png)

### Accessing Custom Metrics in Charts and Alerts

After your custom metric is saved, it can be used in your chart and alert definitions.

#### Charts

Set `Metric Type` to `Custom Metric` and select your desired custom metric.

![](../../.gitbook/assets/94a09f8-image.png)

\\

#### Alerts

When creating a new alert rule, set `Metric Type` to `Custom Metric`, and under the `Metric` field select your desired custom metric or author a new metric to use.

![](../../.gitbook/assets/eaa46b0-image.png)

\\

### Modifying Custom Metrics

Since alerts can be set on Custom Metrics, making modifications to a metric may introduce inconsistencies in alerts.

> üöß Therefore, custom metrics cannot be modified once they are created.

If you'd like to try out a new metric, you can create a new one with a different name and definition.

### Deleting Custom Metrics

To delete a custom metric using the Python client, see [custom\_metric.delete()](../../Python\_Client\_3-x/api-methods-30.md#delete-2). Alternatively, from the custom metrics tab, you can delete a metric by clicking the trash icon next to the metric record.

![](../../.gitbook/assets/cb4a4bd-image.png)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Alerts with Fiddler UI
slug: alerts-with-fiddler-ui
excerpt: ''
createdAt: Wed Feb 07 2024 18:08:16 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 23:12:39 GMT+0000 (Coordinated Universal Time)
---

# Alerts With Fiddler UI

### Overview

Fiddler enables you to set up [alerts](../../product-guide/monitoring-platform/alerts-platform.md) for your model, accessible via the Alerts tab in the navigation bar. The Alerts tab provides views for Triggered Alerts, Alert Rules, and Integrations. You can configure alerts using the Fiddler UI or the [API Client](../../Client_Guide/alerts-with-fiddler-client.md). This page outlines available alert types and provides instructions for setting up and viewing alerts in the Fiddler UI.

![](../../.gitbook/assets/bd02ee"
"8-image.png)

### Setting up Alert Rules

To create a new alert in the Fiddler UI, click Add Alert on the Alerts tab.

1. Fill in the Alert Rule form with basic details like alert name, project, and model.
2. Choose an Alert Type (Traffic, Data Drift, Data Integrity, Performance, Statistic, or Custom Metric) and set up specific metrics, bin size, and columns.
3. Define comparison methods, thresholds, and notification preferences. Click Add Alert Rule to finish.
   1. Learn more about Alert comparisons on the [Alerts Platform Guide](../../product-guide/monitoring-platform/alerts-platform.md).

![](../../.gitbook/assets/52064e5-image.png)

In order to create and configure alerts using the Fiddler API client see [Alert Configuration with Fiddler Client](../../Client_Guide/alerts-with-fiddler-client.md).

### Alert Notification options

You can select the following types of notifications"
" for your alert.

![](../../.gitbook/assets/ee80b90-Screenshot_2023-10-09_at_5.18.21_PM.png)

### Delete an Alert Rule

Delete an existing alert by clicking on the overflow button (‚ãÆ) on the right-hand side of any Alert Rule record and clicking `Delete`. To make any other changes to an Alert Rule, you will need to delete the alert and create a new one with the desired specifications.

![](../../.gitbook/assets/eddf05e-image.png)

### Triggered Alert Revisions

Say goodbye to stale alerts! Triggered Alert Revisions mark a leap forward in alert intelligence, giving you the confidence to act decisively and optimize your operations.

Alerts now adapt to changing data. If new information emerges that alters an alert's severity or value, the alert automatically updates you with highlights in the user interface and revised notifications. This approach empowers you to:

* Make informed decisions based"
" on real-time data: No more relying on outdated or inaccurate alerts.
* Focus on critical issues: Updated alerts prioritize the most relevant information.

![Inspect Alert experience](../../.gitbook/assets/5921286-Screenshot_2024-03-07_at_5.21.04_PM.png)

Inspect Alert experience

![Triggered Alert revision experience](../../.gitbook/assets/7f0aa27-Screenshot_2024-03-07_at_5.21.13_PM.png)

Triggered Alert revision experience

### Sample Alert Email

Here's a sample of an email that's sent if an alert is triggered:

![](../../.gitbook/assets/alert-email-perf-example.png)

### Integrations

The Integrations tab is a read-only view of all the integrations your Admin has enabled for use. As of today, users can configure their Alert Rules to notify them via email or Pager Duty services.

![](../../.gitbook/assets/7462149-image"
".png)

Admins can add new integrations by clicking on the setting cog icon in the main navigation bar and selecting the integration tab of interest.

![](../../.gitbook/assets/6ee3027-Screen_Shot_2022-10-03_at_4.16.00_PM.png)



### Pause alert notification

This feature allows users to temporarily pause and resume notifications for specific alerts without affecting their evaluation and triggering mechanisms. It enhances user experience by providing efficient notification management.\


#### How to Use

**Using the Fiddler User Interface (UI)**

* Locate the Alert Tool:\
  Navigate to the alert rule table and identify the desired alert.
* Toggle Notifications:
  * Click the notification bell icon.
  * The icon updates to indicate the new state (paused or resumed).
* Confirm Action:
  * A loading indicator and a toast notification confirm the action.

**Using the Fiddler Client API**

For programmatic control, use the Fiddler client"
" API's alert-rules method with the enable\_notification argument.

* Details:\
  Refer to the [Fiddler documentation](../../API_Guidelines/alert-rules.md) for a complete explanation of API functionalities.

#### Note

* No Impact on Evaluation:\
  Pausing notifications does not affect the evaluation of alert conditions. The alert tool will continue assessing conditions and triggering alerts as usual.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}
"
"---
title: Performance
slug: performance
excerpt: ''
createdAt: Tue Apr 19 2022 20:25:22 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 22 2024 15:58:55 GMT+0000 (Coordinated Universal Time)
---

# Performance

### What is being tracked?

![](../../.gitbook/assets/ffefe4c-image.png)

* _**Decisions**_ - The post-prediction business decisions made as a result of the model output. Decisions are calculated before [model.publish()](../../Python\_Client\_3-x/api-methods-30.md#publish) (they're not inferred by Fiddler). For binary classification models, a decision is usually determined using a threshold. For multi-class classification models, it's usually determined using the argmax value of the model outputs.
* _**Performance metrics**_
  1. For binary classification models:
     * Accuracy
     * True Positive Rate/Recall
     * False Positive Rate
     * Precision
     * F1 Score
     * AUC
     * AUROC
     * Binary Cross Entropy
     * Geometric Mean
     * Calibrated Threshold
     * Data Count
     * Expected Calibration Error
  2. For multi-class classification models:
     * Accuracy
     * Log loss
  3. For regression models:
     * Coefficient of determination (R-squared)
     * Mean Squared Error (MSE)
     * Mean Absolute Error (MAE)
     * Mean Absolute Percentage Error (MAPE)
     * Weighted Mean Absolute Percentage Error (WMAPE)
  4. For ranking models:
     * Mean Average Precision (MAP)‚Äîfor binary relevance ranking only
     * Normalized Discounted Cumulative Gain (NDCG)

### Why is it being tracked?

* Model performance tells us how well a model is doing on its task. A poorly performing model can have significant business implications.
* The volume of decisions made on the basis of the predictions give visibility into the business impact of the model.

### What steps should I take based on this information?

* For decisions, if there is an increase or decrease in approvals, we can cross-check with the average prediction and prediction drift trendlines on the [Data Drift Tab](data-drift.md). In general, the average prediction value should increase with an increase in the number of approvals, and vice-versa.
* For changes in model performance‚Äîagain, the best way to cross-verify the results is by checking the [Data Drift Tab](data-drift.md) ). Once you confirm that the performance issue is not due to the data, you need to assess if the change in performance is due to temporary factors, or due to longer-lasting issues.
* You can check if there are any lightweight changes you can make to help recover performance‚Äîfor example, you could try modifying the decision threshold.
* Retraining the model with the latest data and redeploying it is usually the solution that yields the best results, although it may be time-consuming and expensive.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Monitoring
slug: monitoring-ui
excerpt: ''
createdAt: Tue Apr 19 2022 20:24:28 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 22:41:41 GMT+0000 (Coordinated Universal Time)
---

# Monitoring

Fiddler Monitoring helps you identify issues with the performance of your ML models after deployment. Fiddler Monitoring has several Metric Types which can be monitored and alerted on:

1. **Traffic**
2. **Data Drift**
3. **Data Integrity**
4. **Performance**
5. **Statistic**
6. **Custom Metric**

The sections contained in this UI Guide provide an overview of using Fiddler's UI for model monitoring.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Data Integrity
slug: data-integrity
excerpt: ''
createdAt: Tue Apr 19 2022 20:25:27 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 22:45:49 GMT+0000 (Coordinated Universal Time)
---

# Data Integrity

ML models are increasingly driven by complex feature pipelines and automated workflows that involve dynamic data. Data is transformed from source to model input, which can result in data inconsistencies and errors.

There are three types of violations that can occur at model inference: **missing feature values**, **type mismatches** (e.g. sending a float input for a categorical feature type) or **range mismatches** (e.g. sending an unknown US State for a State categorical feature).

You can track all these violations in the Data Integrity tab.

### What is being tracked?

![](../../.gitbook/assets/f191dea-image.png)

The time series above tracks the"
" violations of data integrity constraints set up for this model.

* _**Missing value violations**_ ‚Äî The percentage of missing value violations over all features for a given period of time.
* _**Type violations**_ ‚Äî The percentage of data type mismatch violations over all features for a given period of time.
* _**Range violations**_ ‚Äî The percentage of range mismatch violations over all features for a given period of time.
* _**All violating events**_ ‚Äî An aggregation of all the data integrity violations above for a given period of time.

### Why is it being tracked?

* Data integrity issues can cause incorrect data to flow into the model, which can lead to poor model performance and have a negative impact on the business or end-user experience.

### How does it work?

It can be tedious to set up constraints for individual features when they number in the tens or hundreds. To avoid this, you can provide Fiddler with a baseline dataset that's representative of the data you expect your"
" model to infer on in production. This should be sampled from your model's training set, and can be [uploaded to Fiddler using the Python API client](../../Python\_Client\_3-x/api-methods-30.md#publish).

Fiddler will automatically generate constraints based on the distribution of data in this dataset.

* **Missing values**: If a feature has no missing values, then the data integrity violation will be set up to trigger when any missing values are seen. Similarly, if the feature has 50% of its values missing, then the data integrity violation will be set up to trigger when more than 50% of the values encountered are missing in a specified time range.
* **Type mismatch**: A data integrity violation will be triggered when the type of a feature value differs from what was specified for that feature in the baseline dataset.
* **Range mismatch**: For categorical features, a data integrity violation will be triggered when it sees any value other than the ones specified in the"
" baseline. Similarly, for continuous variables, the violation will be triggered if the values are outside the range specified in the baseline.

### What steps should I take with this information?

* The visualization above informs us of the feature-wise breakdown of the violations. The raw counts of the violations are shown in parentheses.
* If there is a spike in violations, or an unexpected violation occurs (such as missing values for a feature that doesn‚Äôt accept a missing value), then a deeper examination of the feature pipeline may be required.
* You can also drill down deeper into the data by examining it in the **Analyze** tab. We can use SQL to slice and dice the data, and try to find the root cause of the issues.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Segments
slug: segments
excerpt: ''
createdAt: Mon Apr 29 2024 22:01:18 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 22:59:29 GMT+0000 (Coordinated Universal Time)
---

# Segments

### Overview

A segment, sometimes referred to as a cohort or slice, represents a distinct subset of model values crucial for performance analysis and troubleshooting. Model segments can be defined using various model dimensions, such as specific time periods or sets of features. Analyzing segments proves invaluable for understanding or troubleshooting specific cohorts of interest, particularly in tasks like bias detection, where overarching datasets might obscure statistical intricacies.

### Adding a Segment

**Note:** To add a Segment using the Python client, see [fdl.Segment.create()](../../Python\_Client\_3-x/api-methods-30.md#create-5)

From the model schema page within the UI, you can access Segments by clicking the **Segments** tab at the top of the page. Then click **Add Segment** to add a new Segment.

![](../../.gitbook/assets/88eabe1-Screen\_Shot\_2024-04-29\_at\_5.48.15\_PM.png)

Finally, enter the Name, Description, and Definition for your Segment and click **Save**.

![](../../.gitbook/assets/d07d080-Screenshot\_2024-04-24\_at\_9.10.28\_AM.png)

### Accessing Segments in Charts and Alerts

After your Segment is saved, it can be selected from Charts and Alerts.

#### Monitoring Charts

![](../../.gitbook/assets/77ebb09-Screen\_Shot\_2024-04-29\_at\_5.51.53\_PM.png)

#### Alerts

![](../../.gitbook/assets/49745d3-Screen\_Shot\_2024-04-29\_at\_5.59.09\_PM.png)

### Modifying Segments

Since alerts can be set on Segments, making modifications to a Segment may introduce inconsistencies in alerts.

> üöß Therefore, **Segments cannot be modified once they are created**.

If you'd like to try out a new Segment, you can create a new one with a different Definition.

### Deleting Segments

To delete a segment using the Python client, see [segment.delete()](../../Python\_Client\_3-x/api-methods-30.md#delete-5). Alternatively, from the segments tab, you can delete a segment by clicking the trash icon next to the segment record.

![](../../.gitbook/assets/5789f6c-image.png)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Traffic
slug: traffic-ui
excerpt: UI Guide
createdAt: Tue Apr 19 2022 20:25:31 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 22:44:23 GMT+0000 (Coordinated Universal Time)
---

# Traffic UI

Traffic as a service metric gives you basic insights into the operational health of your model's service in production.

![](../../.gitbook/assets/d1b7cdf-image.png)

### What is Being Tracked?

* _**Traffic**_ ‚Äî The volume of traffic received by the model over time.

### Why is it Being Tracked?

* Traffic is a basic high-level metric that informs us of the overall model's usage.

### What Steps Should I Take When I See an Outlier?

* A dip or spike in traffic needs to be investigated. For example, a dip could be due to a production model server going down; a spike could be an adversarial attack.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Embedding Visualization Chart Creation
slug: embedding-visualization-chart-creation
excerpt: ''
createdAt: Thu Nov 16 2023 18:09:21 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Dec 08 2023 22:47:56 GMT+0000 (Coordinated Universal Time)
---

# Embedding Visualization Chart Creation

### Creating an embedding visualization Chart

To create an embedding Visualization chart, follow these steps:

1. Navigate to the **Charts** tab in your Fiddler AI instance
2. Click on the **Add Chart** button on the top right
3. In the modal, Select the project that has a [model](../../product-guide/task-types.md) with Custom features
4. Select **Embedding Visualization**.

![](../../.gitbook/assets/umap\_chart\_selection.png)

### Chart Parameters

When creating an embedding visualization chart, you will need to specify the following parameters:

"
"* Model
* Custom Feature Column
* Baseline
* Display Columns
* Sample size
* Number of Neighbors
* Minimum distance

Please see below for details on these parameters.

#### Model

Select the model (with custom features) for which you want to visualize the embeddings.

#### Custom Feature Column

Choose the custom feature column from your dataset that you wish to visualize.

#### Baseline

Define a baseline for comparison. This is optional and will be useful when you want to compare datasets such as a pre-production dataset with a production dataset or two time periods in production.

#### Display Columns

Select the columns that you want to display additional information for when hovering over points in the visualization. These additional display columns will also be available in the data cards when points are selected.

#### Sample Size

Decide on the number of samples you want to include in the visualization for performance and clarity. Currently, a sample size of either 100, 500, or 1000 can be"
" selected. In future releases, we will enable support for larger sample sizes.

#### Number of Neighbors

This parameter controls how UMAP balances local versus global structure in the data. It determines the number of neighboring points used in the manifold approximation. Low values (for example: 5) of this parameter will lead UMAP to focus too much on the local structure losing sight of the big picture, alternatively, bigger values will lead to a focus on the broader data. It is important to experiment on your dataset and use case to identify a value that works best for you.

#### Minimum Distance

Set the minimum distance apart that points are allowed to be in the low-dimensional representation. Smaller values (for example: 0.1) will result in a more clustered embedding, which can highlight finer details.

### Interactions on embedding visualization

#### Choose Different Time Periods

When generating the embedding visualization, you have the flexibility to choose different time periods of production data to analyze. To do this"
":

* Access the time period selector.
* Choose the start and end dates for the time period you are interested in.
* The visualization will update to reflect the embeddings from the selected time range.

![](../../.gitbook/assets/umap\_date\_selection.png)

#### Color By

The 'Color By' feature enriches the visualization by categorizing your data points using different colors based on attributes.

* Find the 'Color By' dropdown in your control panel.
* Choose a categorical feature to color-code the data points. For instance, in the above image, the data points are assigned colors based on a 'target' categorical column. This attribute includes categories like Sandal, Trouser, and Pullover, as indicated in the legend.

Using the 'Color By' feature can assist in uncovering patterns in your data. For instance, in the above image, data points with varying 'target' column values demonstrate clustering, where similar values tend to group together.

You can also select"
" points to delve deeper for further inspection. You may find this ability to interactively color and select data points very useful for root cause analysis.

#### Zoom

Zooming in on the UMAP chart provides a closer look at clusters and individual data points.

* Use the mouse scroll wheel to zoom in or out.
* Click and drag the mouse to move the zoomed-in area around the chart.
* Zooming helps to focus on areas of interest or to distinguish between closely packed points.

#### Selection of Data Points

You can select individual or groups of data points to analyze further.

* Click on a data point to select it. or use the Selector on the top right to select multiple points

![](../../.gitbook/assets/umap\_select\_tool.png)

#### Data cards

* Selected points will be highlighted on the chart and details of the display columns of these cards are displayed in data cards as shown below
* Use this feature to identify and analyze specific data points

In the"
" following example, we use the categorical attribute feedback, which contains three possible values: Like, Dislike, or Null, as indicated in the legend. After applying the 'color by' feature, the user selects specific data points to examine in greater detail. The selected data points are then presented as data cards below.

![](../../.gitbook/assets/umap\_data\_cards.png)

#### Hover on a Data Point

Hovering over a data point reveals additional information about it, providing immediate insight without the need for selection.

* Move the cursor over a data point on the chart.
* A tooltip will appear, displaying the data associated with that point, such as values of different display columns
* Use this feature for a quick look-up of data without altering your current selection on the chart.

### Saving the Chart

Once you're satisfied with your visualization, you can save the chart. This chart can then be added to a Dashboard. This allows you to revisit the UMAP visualization at"
" any time easily either directly going to the Chart or to the dashboard

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Dashboard Interactions
slug: dashboard-interactions
excerpt: ''
metadata:
  title: Dashboard Interactions
  image: []
  robots: index
createdAt: Wed Feb 22 2023 18:06:29 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Dec 08 2023 22:50:59 GMT+0000 (Coordinated Universal Time)
---

# Dashboard Interactions

## Remove a Chart

If you want to remove a chart from your dashboard, simply click on the ""X"" located at the top right of the chart. This will remove the chart from the dashboard, but it will still be available in the saved charts list for future use. If you change your mind and decide to add the chart back to the dashboard, you can simply find it in the saved charts list and add it back to the dashboard at any time.

![](../../.gitbook/assets/91bb601-image.png)

## Edit a Saved Chart

To edit a saved chart, simply click on the chart title within your dashboard. This will open the chart studio in a new tab, where you can make any necessary changes. Once you have made your changes, be sure to select the `Default` time range and then use the Dashboard refresh button to see your updated chart.

![](../../.gitbook/assets/e7d5b04-image.png)

## Zoom

To zoom into a chart within your dashboard, you have two different utilities at your disposal. The first one is located on the top right of the chart component, in the toolbar. After clicking on the **zoom icon**, you can drag your cursor over the data points you wish to zoom into.

![](../../.gitbook/assets/e1a1218-image.png)

You can also use the\*\* horizontal zoom bar \*\*located at the base of the chart to zoom in. Once you've identified the time range you want to focus on, you can use the zoom bar to drag the range across time. For instance, if you want to analyze your data week by week over the past six months, you can use the toolbar or horizontal zoom bar to zoom in on the desired time range and then click and drag the selected range using the base horizontal zoom bar.

![](../../.gitbook/assets/0a13f18-image.png)

## Bar & Line Charts

You can switch between visualizing your chart as a line or a bar chart using the toolbar icons. Click on the line chart icon on the top right of the chart to switch to the line chart view. Likewise, select the bar chart icon in the toolbar to switch to the bar chart view. However,\_ note that these views are only temporary and any settings you specify using the toolbar will not be saved to the dashboard.\_

![](../../.gitbook/assets/31d5896-image.png)

## Undo Chart Toolbar Changes

You can easily restore the changes you applied to your chart using the chart toolbar options, including zoom, switch to line chart, and switch to bar chart. The restore option, which is the last icon in the toolbar, allows you to undo any changes you made and return to the original chart configuration.

![](../../.gitbook/assets/0ec8eb6-image.png)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Dashboards
slug: dashboards-ui
excerpt: ''
createdAt: Tue Feb 21 2023 22:35:31 GMT+0000 (Coordinated Universal Time)
updatedAt: Wed Jan 24 2024 21:33:07 GMT+0000 (Coordinated Universal Time)
---

# Dashboards

### Creating Dashboards

To begin using our dashboard feature, navigate to the dashboard page by clicking on ""Dashboards"" from the top-level navigation bar. On the Dashboards page, you can choose to either select from previously created dashboards or create a new one. This simple process allows you to quickly access your dashboards and begin monitoring your models' performance, data drift, data integrity, and traffic.

![](../../.gitbook/assets/570614f-image.png)

When creating a new dashboard, it's important to note that each dashboard is tied to a specific project space. This means that only models and charts associated with that project can"
" be added to the dashboard. To ensure you're working within the correct project space, select the desired project space before entering the dashboard editor page, then click ""Continue."" This will ensure that you can add relevant charts and models to your dashboard.

![](../../.gitbook/assets/ef961be-image.png)

#### Auto Generated Dashboards

Fiddler will automatically generated model monitoring dashboards for all models registered to the platform. Depending on the task type, these dashboards will include charts spanning from Performance, Traffic, Drift, and Data Integrity metrics.

![Auto-generated model monitoring dashboard](../../.gitbook/assets/8dbf957-image.png)

Auto-generated model monitoring dashboard

Auto-generated dashboard are automatically set as the default dashboards for each model, and can be accessed via the Insights button on the Homepage or Model Schema pages, or alternatively all dashboards are always accessible in the Dashboards list page. Default dashboards and their charts can easily be modified to display the desired set"
" of [monitoring](../../product-guide/monitoring-platform/monitoring-charts-platform.md) and [embedding](../monitoring-ui/embedding-visualization-chart-creation.md) visualizations to meet any use case.

![Accessing the default dashboard from the model schema page via Insights](../../.gitbook/assets/202cd51-Screen\_Shot\_2024-01-18\_at\_4.48.26\_PM.png)

Accessing the default dashboard from the model schema page via Insights

### Add Monitoring Chart

Once you‚Äôve created a dashboard, you can add previously saved monitoring charts that display these metrics over time, making it easy to track changes and identify patterns.

![](../../.gitbook/assets/b862277-image.png)

To create a new monitoring chart for your dashboard, simply select ""New Monitoring Chart"" from the ""Add"" dropdown menu. For more information on creating and customizing monitoring charts, check out our [Monitoring Charts UI Guide](../monitor"
"ing-ui/monitoring-charts-ui.md).

If you'd like to add an existing chart to your dashboard, select ""Saved Charts"" to display a full list of monitoring charts that are available in your project space. This makes it easy to quickly access and add the charts you need to your dashboard for monitoring and reporting purposes.

![](../../.gitbook/assets/2c3857c-image.png)

To further customize your dashboard, you can select the saved monitoring charts of interest by clicking on their respective cards. For instance, you might choose to add charts for Accuracy, Drift, Traffic, and Range Violation to your dashboard for a more comprehensive model overview. By adding these charts to your dashboard, you can quickly access important metrics and visualize your model's performance over time, enabling you to identify trends and patterns that might require further investigation.

### Dashboard Filters

There are three main filters that can be applied to all the charts within dashboards, these include date range, time zone,"
" and bin size.

![](../../.gitbook/assets/0795752-image.png)

#### Date Range

When the `Default` time range is selected, the data range, time zone, and bin size that each monitoring chart was originally saved with will be applied. This enables you to create a dashboard where each chart shows a unique filter set to highlight what matters to each team. Updating the date range will unlock the time zone and bin size filters. You can select from a number of predefined ranges or choose `Custom` to select a start and end date-time.

![](../../.gitbook/assets/960262c-image.png)

#### Bin Size

Bin size controls the frequency at which data is displayed on your monitoring charts. You can select from the following bin sizes: `Hour`, `Day`, `Week`, or `Month`.

> üìò Note: Hour bin sizes are not supported for time ranges above 90 days.
>
> For example, if we select the `6M"
"` data range, we see that the `Hourly` bin selection is disabled. This is disabled to avoid long computation and dashboard loading times.

![](../../.gitbook/assets/93f7576-image.png)

#### Saved Model Updates

If you recently created or updated a saved chart and are not seeing the changes either on the dashboard itself or the Saved Charts list, click the refresh button on the main dashboard studio or within the saved charts list to reflect updates.

![](../../.gitbook/assets/706c198-image.png)

### Model Comparison

With our dashboard feature, you can also compare multiple models side-by-side, making it easy to see which models are performing the best and which may require additional attention. To create model-to-model comparison dashboards, ensure the models you wish to compare belong to the same project. Create the desired charts for each model and then add them to a single dashboard. By creating a single dashboard that tracks the health of all of your models, you can"
" save time and simplify your AI monitoring efforts. With these dashboards, you can easily share insights with your team, management, or stakeholders, and ensure that everyone is up-to-date on your AI performance.

![](../../.gitbook/assets/33b97ae-image.png)

#### Check [Dashboard Utilities ](dashboard-utilities.md)and [Dashboard Interactions](dashboard-interactions.md) pages for more info on dashboard usage.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Dashboard Utilities
slug: dashboard-utilities
excerpt: ''
createdAt: Wed Feb 22 2023 18:05:38 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Dec 08 2023 22:50:45 GMT+0000 (Coordinated Universal Time)
---

# Dashboard Utilities

## Dashboard Name

To rename your dashboard, simply click on the ""Untitled Dashboard"" title on the top-left corner of the dashboard studio. This will allow you to give your dashboard a more descriptive name that reflects its purpose and contents, making it easier to find and manage among your other dashboards.

![](../../.gitbook/assets/5006167-image.png)

Once you've clicked on the ""Untitled Dashboard"" title to rename your dashboard, simply type in the desired name and hit ""Enter"" on your keyboard to save the new name.

![](../../.gitbook/assets/7ec6e36-image.png) ![](../../.gitbook/assets/453ed10-image.png)

If you change your mind and want to discard the changes, simply click anywhere on the page outside of the name box. This will cancel the renaming process and leave the dashboard name as it was before.

## Save, Copy Link, and Delete

You can easily manage your dashboard by using the control panel located on the top left of the dashboard studio. This panel allows you to save your dashboard, copy a link to it, or delete it entirely. By using these controls, you can easily share your dashboard with others or remove it from your collection if it is no longer needed.

![](../../.gitbook/assets/17c9043-image.png)

### Save

It's important to note that dashboards are not automatically saved, so you'll need to manually save your dashboard in order to lock in the current charts and filters. Once you've made the desired changes to your dashboard, simply click the ""Save"" button to save your progress. This will also enable you to share or delete your dashboard as needed. By saving your dashboard frequently, you can ensure that you never lose important information or data visualizations.

![](../../.gitbook/assets/c624c13-image.png)

### Copy Link

If you want to share your dashboard with other users on Fiddler, the first step is to ensure that they have access to the project that the dashboard belongs to. Once you've confirmed that they have access, you can easily share the dashboard by copying the dashboard link and sending it to them. This makes it simple to collaborate and share insights with others who are working on the same project or who have an interest in your findings. Note that you can't share a dashboard link until you've saved the dashboard.

![](../../.gitbook/assets/520189a-image.png)

### Delete

To delete a dashboard, click the overflow button next to the copy link icon. NOTE: Once a dashboard has been deleted it cannot be recovered.

![](../../.gitbook/assets/e768501-image.png)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Inviting Users
slug: inviting-users
excerpt: ''
createdAt: Tue Apr 19 2022 20:07:27 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 22:33:27 GMT+0000 (Coordinated Universal Time)
---

# Adding Users

Fiddler supports both Single Sign-On (SSO) and built-in user authentication called _Email Authentication_. _Email Authentication_ is managed entirely within the Fiddler application by users having the Org Admin [role](../../product-guide/administration-platform.md#understanding-roles). Organizations using solely SSO authentication or a mix of SSO and Email Authentication will still need to invite SSO users to Fiddler following the **Invite a User to Fiddler** process described below.

### Invite a User to Fiddler

> üöß To invite a user to Fiddler, you will need to have the Org Admin [role](../../product-guide/administration-platform.md#understanding-roles). If you do not have access to this role, please contact your Fiddler Org Admin.

To invite a user follow these steps:

1. Go to the **Settings** page
2. Click on the **Access** tab
3. Click on the **Invitations** sub-tab
4. Click on the plus icon on the right

![Invitations sub-tab of the Access tab on the Settings page and the sequence of UI clicks to the invitation button.](../../.gitbook/assets/07a57b4-Screen\_Shot\_2024-04-29\_at\_6.26.39\_PM.png)

Clicking on the invite button opens a modal dialog:

![A modal dialog with an email text box and role dropdown.](../../.gitbook/assets/8e3806f-Screen\_Shot\_2023-04-11\_at\_12.27.32\_PM.png)

Once the invitation has been sent, the user should receive a sign-up link at the email provided.

### Getting an invitation link

In the case where the email address is not associated with a user inbox, you can get the invite link by clicking **Copy invite link** after the invitation has been created.

![Invitations sub-tab of the Access tab on the Settings page showing an invited user's copy invite link.](../../.gitbook/assets/cb5a963-Screen\_Shot\_2024-04-29\_at\_6.32.32\_PM.png)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Settings
slug: settings
excerpt: ''
createdAt: Tue Apr 19 2022 20:26:28 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 22:25:47 GMT+0000 (Coordinated Universal Time)
---

# Application Settings

### Overview

The Settings section captures team setup, permissions, and credentials. You can access the **Settings** page from the user settings on the left navigation bar of the Fiddler UI.

![Fiddler home page showing the user menu open and highlighting the Settings link.](../../.gitbook/assets/728d8bf-Screen\_Shot\_2024-04-29\_at\_6.21.41\_PM.png)

### Key Tabs within Settings

#### General

The **General** tab shows your organization name, your email, and a few other details.

![General tab of the Settings page.](../../.gitbook/assets/"
"3f2e734-general.png)

#### Access

The **Access** tab shows the users, teams, and invitations for everyone in the organization.

**Users**

The **Users** sub-tab shows all the users that are members of this organization.

![Users sub-tab of the Access tab on the Settings page showing a list of users.](../../.gitbook/assets/c8c5bf1-access\_user.png)

**Teams**

The **Teams** tab shows all the teams that have been defined for this organization.

![Teams  sub-tab of the Access tab on the Settings page showing a list of teams](../../.gitbook/assets/settings-access-teams-dir.png)

You can create a team by clicking on the plus (**`+`**) icon on the top-right.

> üöß Note
>
> Only Org Admins can create teams. The plus (**`+`**) icon will not be visible unless you have the Org Admin role.

![](../../.gitbook/assets"
"/b0c4c53-access\_create\_team.png)

**Invitations**

The **Invitations** tab shows all pending user invitations. Invitations that have been accepted no longer appear.

![Invitations sub-tab of the Access tab on the Settings page with a list of user invitations.](../../.gitbook/assets/5cb4046-access\_invitation.png)

You can invite a user by clicking on the plus (**`+`**) icon on the top-right.

> üöß Note
>
> Only Org Admins can invite users. The plus (**`+`**) icon will not be visible unless you have the Org Admin role.

![](../../.gitbook/assets/abb030c-access\_invite\_user.png)

### Credentials

The **Credentials** tab displays user access keys. These access keys are used by Fiddler Python client for authentication. Each Org Admin or Org Member can create a unique key by clicking on **Create Key**.

![Credentials sub-tab"
" of the Access tab on the Settings page with a list of keys and highlighting the create key button.](../../.gitbook/assets/fce7911-credentials.png)

### Webhooks

Webhooks enable you to link Fiddler to your own notification and communication services and have them receive Fiddler alerts as they are triggered. We support Slack webhook integration directly as well as a custom webhook that can be used with any webhook-consuming platform.\
You can manage these webhooks in the 'Webhook Integration' tab.

**Configure a New Slack Webhook**

From the ""Webhook Integrations"" tab, use the + icon on the ""Webhook Integration"" tab to configure a new webhook.

![Create Webhook Service modal dialog](../../.gitbook/assets/a0b33c4-Screenshot\_2023-10-10\_at\_12.46.31\_PM.png)

Follow these steps:

1. Enter a unique webhook name in the **Service Name** textbox
"
"2. Select Slack in the **Provider** dropdown
3. Enter the Slack webhook URL provided by your Slack administrator which will appear similar to this example: [https://hooks.slack.com/services/xxxxxxxxxx](https://hooks.slack.com/services/xxxxxxxxxx)
4. Test the webhook service using the **Test** button
5. Click the **Create** button once the test is successful

Slack documentation on creating webhooks can be reviewed [here](https://api.slack.com/messaging/webhooks).

**Configure a New Custom Webhook**

To configure a webhook for any other platform, follow the same steps listed for the Slack webhook, but instead select **Other** for Provider type and enter the webhook URL provided by the platform's administrator.

1. Enter a unique webhook name in the **Service Name** textbox
2. Select Other in the **Provider** dropdown
3. Enter the webhook URL provided by your platform administrator
4. Test the webhook service using the **"
"Test** button
5. Click the **Create** button once the test is successful

> üöß Custom Webhooks
>
> Note that many platforms will require some amount of configuration in order to properly receive and act on the notifications sent by third party software like Fiddler.

#### Edit or Delete a Webhook

![Webhooks list on the Webhook Integration tab with Delete Webhook and Edit Webhook actions](../../.gitbook/assets/f7be111-Screenshot\_2023-10-09\_at\_4.58.02\_PM.png)

You can manage your webhook from the **Webhook Integrations** tab.

1. Select the webhook that you want to edit/delete using the ""..."" icon towards the right of a webhook integration row.
2. Select the **Delete Webhook** option to delete the webhook

> üöß Deleting a Webhook
>
> You will not be able to delete a webhook that is already linked to alerts. To delete"
" the webhook, you will need to modify the alert and then delete the webhook

3. Select the **Edit Webhook** option to edit the webhook
4. Click the **Test** button to test your changes
5. Click the **Save** button once the test is successful

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Administration
slug: administration-ui
excerpt: ''
createdAt: Tue Apr 19 2022 20:26:16 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Oct 21 2024 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# Administration

This section provides details on how to use the UI for:

* Adding Users to Fiddler
* Authorization and Access Control
* Application Settings

For application-specific information check the [Product Guide on Administration](../../product-guide/administration-platform.md).

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Authorization and Access Control
slug: authorization-and-access-control
excerpt: ''
createdAt: Tue Apr 19 2022 20:26:44 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 22:38:22 GMT+0000 (Coordinated Universal Time)
---

# Authorization and Access Control

Each project supports its own set of permissions for its users. Start by selecting `Project Settings` within a project space. Here you can view which users and teams currently have access to the project or add members and specify their access.

![Access Project Settings to set permissions.](../../.gitbook/assets/d7fdfc5-Screen\_Shot\_2024-04-29\_at\_6.35.16\_PM.png)

Access Project Settings to set permissions.

![Add members or teams with specific access.](../../.gitbook/assets/d79efe9-image.png)

Add members or teams with specific access.

For more details refer to [Administration Page](../../product-guide/administration-platform.md) in the Product Guide.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Project Structure on UI
slug: project-structure-on-ui
excerpt: >-
  This document provides information on how to create projects, models,
  datasets, and project dashboards within Fiddler for organizing and managing
  machine learning models.
hidden: true
metadata:
  description: >-
    This document provides information on how to create projects, models,
    datasets, and project dashboards within Fiddler for organizing and managing
    machine learning models.
  image: []
  robots: index
createdAt: Fri Apr 05 2024 11:20:17 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 11 2024 09:30:14 GMT+0000 (Coordinated Universal Time)
---

# Project Structure On Ui

A project within Fiddler helps organize models under observation. Additionally, a project acts as the authorization unit to govern access to your models. To onboard a model to Fidd"
"ler, you must first have a project with which you wish to associate it.

### Projects

Create a project by clicking on **Projects** and then clicking on **Add Project**.

![](../../.gitbook/assets/8e4b429-Add\_project\_0710.png)

* _**Create New Project**_ ‚Äî A window will pop up where you can enter the project name and click **Create**. Once the project is created, it will be displayed on the projects page.

You can access your projects from the Projects Page.

![Projects Page on Fiddler UI](../../.gitbook/assets/82404e6-Screenshot\_2022-12-27\_at\_1.00.15\_PM.png)

Projects Page on Fiddler UI

### Models

A model in Fiddler represents a machine learning model. A project will have one or more models for the ML task (e.g. a project to predict house prices might contain LinearRegression"
"-HousePredict and RandomForest-HousePredict). For further details refer to the [Models](doc:project-architecture#models) section in the Platform Guide.

You can create a model from the Fiddler Client

![](../../.gitbook/assets/e151df5-Model\_Dashboard.png)

### Datasets

A dataset in Fiddler is a data table containing features, model outputs, and a target for machine learning models. Optionally, you can also upload metadata and ‚Äúdecision‚Äù columns, which can be used to segment the dataset for analyses, track business decisions, and work as protected attributes in bias-related workflows. For more details refer to [Datasets](doc:project-architecture#datasets) in the Platform Guide.

Once you click on a particular project/model, you will be able to see if there are any datasets associated with the model. For example, the bank\_churn project, in the following screenshot, has the bank\_churn dataset. [Datasets"
" are uploaded via the Fiddler client](../../Python_Client_2-x/api-methods-20.md#clientuploaddataset).

![Dataset and Baselines in a model](../../.gitbook/assets/d25c3a1-Screenshot\_2024-04-05\_at\_5.04.09\_PM.png)

Dataset and Baselines in a model

#### Model Schema and Artifacts

A model in Fiddler is simply a directory that contains [model Schema and artifacts](../../product-guide/explainability/artifacts-and-surrogates.md) such as:

* Input, Output, Prediction and Target column(s)
* The model file (e.g. `*.pkl`)
* `package.py`: A wrapper script containing all of the code needed to standardize the execution of the model.

![](../../.gitbook/assets/4b3f2d1-Screenshot\_2024-04-05\_at\_5.06.51\_PM"
".png)

![](../../.gitbook/assets/3897e25-Screenshot\_2024-04-05\_at\_5.04.09\_PM.png)

### Project Dashboard

You can collate specific visualizations under the Project Dashboard. After visualizations are created using the Model Analytics tool, you can pin them to the dashboard, which can then be shared with others.

![](../../.gitbook/assets/232cc47-Screenshot\_2024-04-05\_at\_5.12.51\_PM.png)

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: Surrogate Models
slug: surrogate-models
excerpt: ''
createdAt: Tue Apr 19 2022 20:25:57 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 22 2024 16:10:35 GMT+0000 (Coordinated Universal Time)
---

# Surrogate Models

Fiddler‚Äôs explainability features require a model on the backend that can generate explanations for you.

A surrogate model is an approximation of your model using gradient boosted trees (LightGBM), trained with a general, predefined set of hyperparameters. It serves as a way for Fiddler to generate approximate explanations without you having to upload your actual model artifact.

***

A surrogate model **will be built automatically** for you when you call [`add_surrogate()`](../../Python\_Client\_3-x/api-methods-30.md#add\_surrogate).\
You just need to provide a few pieces of information about how your model operates.

### What you need to specify

* Your model‚Äôs task (regression, binary classification, etc.)
* Your model‚Äôs target column (ground truth labels)
* Your model‚Äôs output column (model predictions)
* Your model‚Äôs feature columns

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Point Explainability
slug: point-explainability
excerpt: ''
createdAt: Tue Apr 19 2022 20:25:41 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Dec 08 2023 22:48:51 GMT+0000 (Coordinated Universal Time)
---

# Point Explainability

Fiddler provides powerful visualizations that can explain your model's behavior. These explanations can be queried at an individual prediction level in the **Explain** tab or within the monitoring context in the **Monitor** tab.

Explanations are available in the UI for structured (tabular) and natural language (NLP) models. They are also supported via API using the [fiddler-client](https://pypi.org/project/fiddler-client/) Python package. Explanations are available for both production and baseline queries.

Fiddler‚Äôs explanations are interactive ‚Äî you can change feature inputs and immediately view an updated prediction and"
" explanation. We have productized several popular **explanation methods** to work fast and at scale:

* SHAP and Fiddler SHAP, using Kernel SHAP implementations, are game-theory based methods. They work for all models, because they only require the ability to ask a model for predictions.
* Integrated Gradients, which is particularly performant for deep learning models with a large number of inputs. It requires the model‚Äôs prediction to be mathematically differentiable, and a prediction gradient must be made available to Fiddler.
* Tree SHAP, is not enabled by default but can be used for Tree-based model. This is a faster and model-specific method to approximate Shapley values.

These methods are discussed in more detail below.

In addition to the previous out of the box explanation methods, Fiddler allows to bring your own explanation method. This can be customized in your model‚Äôs `package.py` wrapper script.

### Tabular Models

For tabular models"
", Fiddler‚Äôs Point Explanation tool shows how any given model prediction can be attributed to its individual input features.

The following is an example of an explanation for a model predicting the likelihood of customer churn:

![](../../.gitbook/assets/b8e4f81-Tabular\_Explain.png)

A brief tour of the features above:

* _**Explanation Method**_: The explanation method is selected from the **Explanation Type** dropdown.
* _**Input Vector**_: The far left column contains the input vector. Each input can be adjusted.
* _**Model Prediction**_: The box in the upper-left shows the model‚Äôs prediction for this input vector.
  * If the model produces multiple outputs (e.g. probabilities in a multiclass classifier), you can click on the prediction field to select and explain any of the output components. This can be particularly useful when diagnosing misclassified examples.
* _**Feature Attributions**_: The colored bars on the right represent how"
" the prediction is attributed to the individual feature inputs.
  * A positive value (blue bar) indicates a feature is responsible for driving the prediction in the positive direction.
  * A negative value (red bar) is responsible for driving the prediction in a negative direction.
* _**Baseline Prediction**_: The thin colored line just above the bars shows the difference between the baseline prediction and the model prediction. The specifics of the baseline calculation vary with the explanation method, but usually it's approximately the mean prediction of the training/reference data distribution (i.e. the dataset specified when importing the model into Fiddler). The baseline prediction represents a typical model prediction.

**Two numbers** accompany each feature‚Äôs attribution bar in the UI.

* _The first number_ is the **attribution**. The sum of these values over all features will always equal the difference between the model prediction and a baseline prediction value.
* _The second number_, the percentage in parentheses, is the **feature attribution divided by the"
" sum of the absolute values of all the feature attributions**. This provides an easy to compare, relative measure of feature strength and directionality (notice that negative attributions have negative percentages) and is bounded by ¬±100%.

> üìò Info
>
> An input box labeled **‚ÄúTop N‚Äù** controls how many attributions are visible at once. If the values don‚Äôt add up as described above, it‚Äôs likely that weaker attributions are being filtered-out by this control.

Finally, it‚Äôs important to note that **feature attributions combine model behavior with characteristics of the data distribution**.

### Language (NLP) Models

For language models, Fiddler‚Äôs Point Explanation provides the word-level impact on the prediction score when using perturbative methods (SHAP and Fiddler); for the Integrated Gradients method, tokenization can be customized in your model‚Äôs `package.py` wrapper script. The explanations are interactive‚Äîedit the text, and the explanation updates immediately.

"
"Here is an example of an explanation of a prediction from a sentiment analysis model:

![](../../.gitbook/assets/970a86b-NLP\_Explain.png)

### Point Explanation Methods: How to Quantify Prediction Impact of a Feature?

**Introduction**

One strategy for explaining the prediction of a machine learning model is to measure the influence that each of its inputs have on the prediction made. This is called Feature Impact.

To measure Feature Impact, **additive attribution methods** can be quite powerful. Fiddler includes:

* **SHAP** and **Fiddler SHAP**, which require only the ability to ask a model for predictions, and are thus suitable across all types of models; no knowledge of the model implementation is necessary.
* **Integrated Gradients**, a method that takes advantage of the gradient vector of the prediction, which is typically available in deep learning models, to efficiently explain complex models with large input dimensionality.

**Additive Attributions**

To explain a"
" prediction with an additive attribution method, we look at how individual features contribute to the _prediction difference_. The prediction difference is a comparison between the prediction as a point in feature space (we refer to this as the _explain-point_), and a counterfactual baseline position (or a distribution of positions), representing an uninteresting or typical model inference.

Each feature is assigned a fraction of the prediction difference for which it is responsible. This fraction is called the feature attribution, and it‚Äôs what we show in our explanations.

Additive attribution methods have the following characteristics:

* The sum of feature attributions always equals the prediction difference.
* Features that have no effect on a model‚Äôs prediction receive a feature attribution of zero.
* Features that have the identical effect receive the same attribution.
* Features with mutual information share the attribution for any effect that information has on the prediction.

Additionally, each of these methods takes into account interactions between the features (e.g. two features that have no effect individually but in combination"
" change the model output). This is explicitly built into the Shapley value formalism, and is captured in the path integral over gradients in Integrated Gradients.

**Shapley Values and their Approximation**

The Shapley value[\[1\]](point-explainability.md#references) (proposed by Lloyd Shapley in 1953) is one way to derive feature attributions. Shapley values distribute the total payoff of a collaborative game across a coalition of cooperating players. They are computed by tabulating the average gain in payoff when a particular player is added to the coalition, over all coalition sizes and permutations of players.

In our case, we consider the ‚Äútotal gains‚Äù to be the prediction value, and a ‚Äúplayer‚Äù is a single model feature. The collaborative ‚Äúgame‚Äù is all of the model features cooperating to form a prediction value.

Why do we create ‚Äúcoalitions‚Äù with only a subset of the features? In some scenarios, it may"
" be appropriate to replace a feature with a zero value when removed from the coalition (e.g. text models where no mask token is available). In others (e.g. models with dense tabular inputs), values are swapped in from a reference distribution or baseline example as a zero value may have a specific meaning (like zero income on a credit application).

Shapley values have desirable properties including:

* _**Linearity**_: If two games are combined, then the total gains correspond to the gains derived from a linear combination of the gains of each game.
* _**Efficiency**_: The sum of the values of all players equals the value of the grand coalition, so that all the gain is distributed among the players. In our case, the efficiency property says _the feature attributions should sum to the prediction value_. The attributions can be positive or negative, since a feature can raise or lower a predicted value.

**Approximating Shapley Values**

Computation of exact Shap"
"ley values can be extremely computationally expensive‚Äîin fact, exponentially so, in the number of input features. Fiddler makes two approximation methods available:

* **SHAP**[\[2\]](point-explainability.md#references) (SHapely Additive exPlanations) approximates Shapley values by sampling coalitions according to a combinatorially weighted kernel (compensating for the number of permutations of features in coalitions of different cardinality). It samples the feature space uniformly between baseline-like feature vectors and explain-point-like feature vectors. This has the effect of downsampling behavior in the immediate vicinity of the explain-point, a region where the model may be saturated or uniform in its prediction, and attributions may not be helpful.
* **Fiddler SHAP**[\[3\]](point-explainability.md#references) builds on the SHAP approach and is optimized for computing distributions of Shapley values for each feature by comparing the explain-point"
" against a distribution of baselines. This makes it possible to compute confidence intervals around the mean attribution for each feature and identify clusters in attribution space where distinct, individually relevant explanations might be important (e.g. ‚Äúyour loan application was rejected for a set of reasons when compared to applications in your region, and for another set of reasons when compared to applications with the same profession‚Äù).

Approximate Shapley value methods can be used to explain nearly any model, since you only need to be able to ask the model for predictions at a variety of positions in the feature space.

**Integrated Gradients**

Another additive attribution method: the Integrated Gradients method.

For models whose prediction is continuous and piecewise differentiable in the feature space, it can be useful to provide additional information through the gradient (slope vector) of a prediction.

Fiddler supports Integrated Gradients (IG)[\[4\]](point-explainability.md#references). In this method, an approximate integral tabulates components of"
" the slope along a linear path from baseline to explain-point, and attributes them to respective input features. This has several advantages:

1. For models with very high dimensional feature volumes (e.g. images, text), where differentiable deep-learning models typically excel, this method can be very performant (O(N) vs. the O(2^n) of the Shapley methods)
2. Attributions can be computed for intermediate layers within the model, providing fine-grained model diagnostics. This is naturally extensible to models with hybrid and multimodal inputs.
3. In comparison to local gradients and saliency methods, the IG path integral samples the large-scale behavior of the model and is resistant to amplifying noise in the possibly saturated region around the explain-point.

### References

1. [https://en.wikipedia.org/wiki/Shapley\_value](https://en.wikipedia.org/wiki/Shapley\_value)
2. S. Lundberg, S Lee. ‚ÄúA Unified Approach"
" to Interpreting Model Predictions.‚Äù NeurIPS, 2017 [http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf)
3. L. Merrick and A. Taly ‚ÄúThe Explanation Game: Explaining Machine Learning Models Using Shapley Values‚Äù [https://arxiv.org/abs/1909.08128](https://arxiv.org/abs/1909.08128)
4. M. Sundararajan, A. Taly, Q. Yan ‚ÄúAxiomatic Attribution for Deep Networks‚Äù [http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf](http://proceedings.mlr.press/v70/sundararajan17a/sundarar"
"ajan17a.pdf)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Explainability
slug: explainability-ui-giude
excerpt: ''
createdAt: Tue Apr 19 2022 20:24:31 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 15 2024 14:42:02 GMT+0000 (Coordinated Universal Time)
---

# Explainability

There are three topics related to Explainability to cover:

* [Point Explainability](point-explainability.md)
* [Global Explainability](global-explainability.md)
* [Surrogate Models](surrogate-models.md)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Global Explainability
slug: global-explainability
excerpt: ''
createdAt: Tue Apr 19 2022 20:25:47 GMT+0000 (Coordinated Universal Time)
updatedAt: Wed Apr 03 2024 20:59:01 GMT+0000 (Coordinated Universal Time)
---

# Global Explainability

Fiddler provides powerful visualizations to describe the impact of features in your model. Feature impact and importance can be found in either the Explain.

Global explanations are available in the UI for **structured (tabular)** and **natural language (NLP)** models, for both classification and regression. They are also supported via API using the Fiddler Python package. Global explanations are available for both production and baseline queries.

### Tabular Models

For tabular models, Fiddler‚Äôs Global Explanation tool shows the impact/importance of the features in the model.

Two global explanation methods are available:

* _**Feature impact**_"
" ‚Äî Gives the average absolute change in the model prediction when a feature is randomly ablated (removed).
* _**Feature importance**_ ‚Äî Gives the average change in loss when a feature is randomly ablated.

Feature impact and importance are displayed as percentages of all attributions.

The following is an example of feature impact for a model predicting the likelihood of successful loan repayment:

![](../../.gitbook/assets/2548d18-Global-Expln-Tabular.png)

### Language (NLP) Models

For language models, Fiddler‚Äôs Global Explanation performs ablation feature impact on a collection of text samples, determining which words have the most impact on the prediction.

> üìò Info
>
> For speed performance, Fiddler uses a random corpus of 200 documents from the dataset. When using the [`get_feature_impact`](ref:api-methods#get\_feature\_impact) function from the Fiddler API client, the number of input sentences can be changed"
" to use a bigger corpus of texts.

Two types of visualization are available:

* _**Word cloud**_ ‚Äî Displays a word cloud of top 150 words from a collection of text for this model. Fiddler provides three options:
  * **Average change**: The average impact of a word in the corpus of documents. This takes into account the impact's directionality.
  * **Average absolute feature impact**: The average absolute impact of a word in the corpus of documents. This only takes the absolute impact of the word into account, and not its directionality.
  * **Occurrences**: The number of times a word is present in the corpus of text.
* _**Bar chart**_ ‚Äî Displays the impact for the **Top N** words. By default, only words with at least 15 occurrences are displayed. This number can be modified in the UI and will be reflected in real time in the bar chart. Fiddler provides two options:
  * **Average change**:"
" The average impact of a word in the corpus of documents. This takes into account the impact's directionality. Since positive and negative directionalities can cancel out, Fiddler provides a histogram of the individual impact, which can be found by clicking on the word.
  * **Average absolute feature impact**: The average absolute impact of a word in the corpus of documents. This only takes the absolute impact of the word into account, and not its directionality.

The following image shows an example of word impact for a sentiment analysis model:

![](../../.gitbook/assets/f02245d-Screen\_Shot\_2023-01-20\_at\_2.39.08\_PM.png)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
Title: Metric Card Creation
slug: metric-card-chart-creation
excerpty: ''
createdAt: Wed Sep 11 2024 20:14:25 GMT+0000 (Coordinated Universal Time)
updateAt: Wed Sep 11 2024 20:14:25 GMT+0000 (Coordinated Universal Time)
---

# Metric Card Creation

## Creating a Metric Card Chart

To create a Metric Card, follow these steps:

* Navigate to the `Charts` tab in your Fiddler AI instance
* Click on the `Add Chart` button on the top right
* In the modal, select a project
* Select **Metric Card**

## Support

Metric card is supported for any model task type and for both production and pre-production data. Metric card allows displaying of Custom Metric, Data Drift, Data Integrity, Performance, Traffic, and Statistic metrics.

## Available Right-Side Controls

| Parameter   | Value                                                                                                                                                                                               |
| ----------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Model       | List of models in the project                                                                                                                                                                       |
| Version     | List of versions for the selected model                                                                                                                                                             |
| Environment | `Production` or `Pre-Production`                                                                                                                                                                    |
| Dataset     | Displayed only if `Pre-Production` is selected. List of pre-production env uploaded for the model version.                                                                                          |
| Metric      | Selecting a metric across Custom Metrics, Data Drift, Data Integrity, Performance, Traffic, and Statistic to display as an aggregated number. Select up to 4 metrics to display on the chart.       |
| Segment     | <p>- Selecting a saved segment<br>- Defining an applied (on the fly) segment. This applied segment isn‚Äôt saved (unless specifically required by the user) and is applied for analysis purposes.</p> |

## Available In-Chart Controls

| Control                  | Model Task            | Value                                                                                        |
| ------------------------ | --------------------- | -------------------------------------------------------------------------------------------- |
| Time range selection     | All                   | Selecting start time and end time or time label for production data. Default is last 30 days |
| Positive class threshold | Binary classification | For performance metrics, select a threshold to be applied for computation. Default is 0.5    |
| Top-K                    | Ranking               | For performance metrics, select a top-k value to be applied for computation. Default is 20.  |

## Saving the Chart

Once you're satisfied with your metric card Chart, you can save it which allows it to be added to Dashboards and viewed individually from the Charts page.

![Root Cause Analyze Experience](../../.gitbook/assets/metric-card-display.png)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
Title: Events Table Examples in RCA
slug: events-table-in-rca
excerpt: ''
hidden: false
---

# Events Table in RCA

Allow visualizing events corresponding to a certain bin in a monitoring chart. Note that this view is to provide example rows used for the computation. The maximum number of rows that can viewed is 1000.

## Analyzing a sample of events

* Navigate to the `Charts` tab in your Fiddler AI instance
* Click on the `Add Chart` button on the top right
* In the modal, select a project
* Select **Monitoring**
* Create a Monitoring chart and click on a time range
* This will display the RCA (Root Cause Analysis) tab
* In RCA, select the `Events Table` tab

## Support

This visualization is supported for any model and data type.

## Represented data

The displayed events are production events coming from the selected model and bin, and segment if it was selected in the monitoring chart.

![Monitoring Chart Configuration](../../.gitbook/assets/monitoring-chart-selection.png) ![Event Table Example](../../.gitbook/assets/data-table-rca-example.png)

## Available Controls

* **Column selection**: On the top right side of the table, select the columns to be displayed. By default all non-vector columns are displayed.

![Event Table Column Selection](../../.gitbook/assets/data-table-column-selection.png)

* **Vector columns**: By default the vector columns are not fetched for latency reasons. Toggle on if vectors need to be fetched.

![Event Table Vectors displayed](../../.gitbook/assets/data-table-vectors.png)

* **Download**: Download the sample events to `CSV` or `PARQUET` format.

![Event Table Vectors Download](../../.gitbook/assets/data-table-download.png)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Analytics
slug: analytics-ui
excerpt: UI Guide
createdAt: Tue Apr 19 2022 20:24:49 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Dec 08 2023 22:49:26 GMT+0000 (Coordinated Universal Time)
---

# Analytics

## Interfaces

### Analytics Charts

There are three supported analytics chart types:

1. _**Performance Analytics**_ ‚Äî Visualize various performance metrics tailored to different task types, such as confusion matrices, prediction scatterplots, and more.
2. _**Feature Analytics**_ ‚Äî Analyze the behavior and relationships between features, helping to identify patterns and insights within the data, using feature distribution, feature correlation, and correlation matrix charts.
3. _**Metric Card**_ ‚Äî Display a single numeric representation of a metric, offering a concise overview of performance, data quality, data integrity, or custom metrics on a card.

### Root Cause Analysis

The Root Cause Analysis (RCA) experience consists of four parts, all based on the FQL segment and time range provided in the monitoring chart:

1. _**Events**_ ‚Äî Browse a sample of 1,000 events.
2. _**Data Drift**_ ‚Äî View a breakdown of drift for your features, along with prediction drift impact.
3. _**Data Integrity**_ ‚Äî Review a summary of data integrity violations, including counts across range, type, and missing value issues.
4. _**Analyze**_ ‚Äî View performance and feature analytics charts.

![Root Cause Analyze Experience](../../.gitbook/assets/root-cause-analysis-analyze.png)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Performance Charts Visualization
slug: performance-charts-visualization
excerpt: ''
createdAt: Mon Apr 29 2024 21:11:48 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 21:11:48 GMT+0000 (Coordinated Universal Time)
---

# Performance Charts Visualization

List of possible performance visualization depending on the model task. To see how to create a Performance chart, visit [this page](performance-charts-creation.md).

## Binary Classification

#### Confusion Matrix

A 2x2 table that shows how many predicted and actual values exist for positive and negative classes. Also referred as an error matrix. The percentage is computed per row.

![](../../.gitbook/assets/confusion\_matrix.png)

#### Receiver Operating Characteristic (ROC) Curve

A graph showing the performance of a classification model at different classification thresholds. Plots the true positive rate (TPR), also known as recall, against the false positive rate (FPR).

![](../../.gitbook/assets/roc.png)

#### Precision-Recall Curve

A graph that plots the precision against the recall for different classification thresholds.

![](../../.gitbook/assets/precision-recall.png)

#### Calibration Plot

A graph that tell us how well the model is calibrated. The plot is obtained by dividing the predictions into 10 quantile buckets (0-10th percentile, 10-20th percentile, etc.). The average predicted probability is plotted against the true observed probability for that set of points.

![](../../.gitbook/assets/calibration-plot.png)

## Multi-class Classification

#### Confusion Matrix

A table that shows how many predicted and actual values exist for different classes. Also referred as an error matrix. The percentage is computed per row (using all classes). In the full view, up to 15 classes can be displayed. In the chart creation mode, up to 12 classes can be displayed. The displayed labels can be controlled in the chart.

![](../../.gitbook/assets/multi-class-confusion-matrix.png)

## Regression

#### Prediction Scatterplot

Plots the predicted values against the actual values. The more closely the plot hugs the `y=x line`, the better the fit of the model.

![](../../.gitbook/assets/prediction-scatterplot.png)

#### Error Distribution

A histogram showing the distribution of errors (differences between model predictions and actuals). The closer to 0 the errors are, the better the fit of the model.

![](../../.gitbook/assets/error-distribution.png)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
Title: Feature Analytics Creation
slug: feature-analytics-chart-creation
excerpty: ''
createdAt: Tue Sep 03 2024 16:30:33 GMT+0000 (Coordinated Universal Time)
updateAt: Wed Oct 16 2024 16:30:33 GMT+0000 (Coordinated Universal Time)
---

# Feature Analytics Creation

## Creating a Feature Analytics Chart

To create a Feature Analytics chart, follow these steps:

* Navigate to the `Charts` tab in your Fiddler AI instance
* Click on the `Add Chart` button on the top right
* In the modal, select a project
* Select **Feature Analytics**

![](../../.gitbook/assets/feature\_distribution\_chart\_selection.png)

## Support

Feature analytics is supported for any model task type, but does not support time stamps or columns of type `string` or `vector`.

## Available Right-Side Controls

| Parameter   | Value                                                                                                                                "
"                                                                                                                                                          |
| ----------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Model       | List of models in the project                                                                                                                                                                                                                                                                  |
| Version     | List of versions for the selected model                                                                                                                                                                                                                                                        |
| Environment | `Production` or `Pre-Production`                                                                                                                                                                                                                                                               |
| Dataset     | Displayed only if `Pre-Production` is selected. List of pre-production env uploaded for the model version.                                                                                                                                                                                     |
| Visual      | List of possible visualizations, Feature Distribution or Feature Correlation.                                                                                                                                                                                                                  |
| Segment     | <p>- Selecting a saved segment<br>- Defining an applied (on the fly) segment. This applied segment isn‚Äôt saved (unless specifically required by the user) and is applied for analysis purposes.</p>                                                                                            |
| Feature     | <p>- Feature Distribution: Select a single column of a supported data type to view the distribution of its values<br>- Feature Correlation: Select two columns to visualize their correlation<br>- Correlation"
" Matrix: Select up to eight columns to visualize their pairwise correlations</p> |

## Available In-Chart Controls

| Control              | Value                                                                                                                      |
| -------------------- | -------------------------------------------------------------------------------------------------------------------------- |
| Time range selection | Selecting start time and end time or time label for production data. Default is last 30 days                               |
| Display              | Select how to display the chart, Histogram or Kernel Density Estimate, depending on the data type of the feature selected. |

## Displays for Feature Distribution

### Kernel Density Estimate

For numeric column types, visualize feature distribution as a Kernel Density Estimate chart. ![Feature distribution chart configured with Kernel Density Estimate rendering](../../.gitbook/assets/feature\_analytics\_kde.png)

### Histogram

Categorical and boolean value may only be displayed as Histograms, but other column types may also be displayed as such. ![Feature distribution chart configured with histogram rendering](../../.gitbook/assets/feature\_analytics\_histogram.png)

## Displays for Feature Correlation

"
"### Line Plot

![Line plot example of Feature Correlation chart](../../.gitbook/assets/correlation-lineplot.png)

### Scatter Plot

![Scatter plot example of Feature Correlation chart](../../.gitbook/assets/correlation-scatterplot.png)

### Stacked Bar Plot

![Stacked bar example of Feature Correlation chart](../../.gitbook/assets/correlation-stackedbar.png)

### Box Plot

![Box plot example of Feature Correlation chart](../../.gitbook/assets/correlation-boxplot.png)

## Correlation Matrix Interactions

Clicking a cell in the correlation matrix opens the Feature Correlation chart, enabling a more detailed analysis of the relationship between the selected features. ![Correlation matrix interactions](../../.gitbook/assets/correlation-matrix-rca.png)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Performance Charts Creation
slug: performance-charts-creation
excerpt: ''
createdAt: Mon Apr 29 2024 21:11:06 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 21:11:28 GMT+0000 (Coordinated Universal Time)
---

# Performance Charts Creation

## Creating a Performance Chart

To create a Performance chart, follow these steps:

* Navigate to the `Charts` tab in your Fiddler AI instance
* Click on the `Add Chart` button on the top right
* In the modal, Select the project that has a model with Custom features
* Select **Performance Analytics**

![](../../.gitbook/assets/feature\_distribution\_chart\_selection.png)

## Available Performance charts

| Model task                 | Available Chart(s)                                                                    |
| -------------------------- | ------------------------------------------------------------------------------------- |
| Binary classification      | <p>- Confusion Matrix<br>- ROC<br>- Precision Recall"
"<br>- Calibration Plot Charts</p> |
| Multi-class Classification | - Confusion Matrix                                                                    |
| Regression                 | <p>- Prediction Scatterplot<br>- Error Distribution</p>                               |
| Ranking / LLM / Not Set    | _Not available_                                                                       |

## Available right side controls

| Parameter       | Value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| --------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Model           | List of models in the project                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| Version         | List of versions for the selected model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Environment     | `Production` or `Pre-Production`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| Dataset         | Displayed only if `Pre-Production` is selected. List of pre-production env uploaded for the model version.                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Visual          | List of possible [performance visualization](performance-charts-visualization.md) depending on the model task.                                                                                                                                                                                                                                                                                                                                                                                                             |
| Segment"
"         | <p>- Selecting a saved segment<br>- Defining an applied (on the fly) segment. This applied segment isn‚Äôt saved (unless specifically required by the user) and is applied for analysis purposes.</p>                                                                                                                                                                                                                                                                                                                        |
| Max Sample size | <p>Integer representing the maximum number of rows used for computing the chart, up to 500,000. If the data selected has less rows, we will use all the available rows with non null target and output(s).<br>Fiddler select the <code>n</code> first number of rows from the selected slice.<br><br><em>Note: Clickhouse is configured using multiple shards, which means slightly different results can be observed if data is only selected on a specific shard (usually when little observation are queried).</em></p> |

## Available in-chart controls

| Control                  | Model Task                 | Value                                                                                              |
| ------------------------ | -------------------------- | -------------------------------------------------------------------------------------------------- |
| Time range selection"
"     | All                        | Selecting start time and end time or time label for production data. Default to last 30 days       |
| Positive class threshold | Binary classification      | Selecting threshold applied for computation / visualization. Default to 0.5                        |
| Displayed labels         | Multi-class Classification | Selecting the labels to display on the confusion matrix (up to 12). Default to the 12 first labels |

## Saving the Chart

Once you're satisfied with your visualization, you can save the chart. This chart can then be added to a Dashboard. This allows you to revisit the Performance visualization at any time easily either directly going to the Chart or to the dashboard.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: ""Platform Security""
slug: ""platform-security""
excerpt: """"
hidden: true
createdAt: ""Tue Nov 29 2022 17:25:17 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
will publish once Murat has something in here
"
"---
title: Supported Browsers
slug: supported-browsers
excerpt: Platform Guide
createdAt: Tue Jan 10 2023 22:16:01 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)
---

# Supported Browsers

Fiddler Product can be accessed through the following supported web browsers:

* Google Chrome
* Firefox
* Safari
* Microsoft Edge

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Administration
slug: administration-platform
excerpt: ''
createdAt: Tue Nov 15 2022 18:09:04 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Dec 08 2023 22:57:55 GMT+0000 (Coordinated Universal Time)
---

# Administration

## Overview
Fiddler supports Role-Based Access Control (RBAC) using resources and roles. This documentation outlines the resources, roles, and permissions available in Fiddler, enabling you to manage access control for your organization.

## Understanding Resources

Resources are entities within Fiddler that users can access and interact with. There are two main resource types:

### Organization Resources
* Organization: Represents your entire Fiddler setup, including projects and users.
* Settings: General information, login details, notification settings, and integration configurations.
* Users: Individual users with accounts in your Fiddler organization.
* Teams: Groups of users within your organization.
 "
" * Each user can be a member of zero or more teams.
  * Team roles are associated with project roles (i.e. teams can be granted\
    **Project Viewer**, **Project Writer**, and/or **Project Admin** permissions for a project).

### Project Resources

* Projects: Contain models, data, and configurations for a specific ML application.
* Models: Machine learning models onboarded to Fiddler for monitoring and explainability.
* Project Settings: Configurations related to project access and user permissions.
* Alerts: Notifications generated by Fiddler based on monitoring data.
* Charts & Dashboards: Visualizations of your model performance and data insights.

## Understanding Roles

Roles define the level of access a user has to Fiddler resources:

### Organization Roles
* Org Admin: Has access to manage users, teams, projects, and organization settings. However, this role cannot read the details of the projects.
  * As an administrator, you can [invite users](../"
"UI_Guide/administration-ui/inviting-users.md).
* Org Member: Limited access to organization settings and cannot create projects.

### Project Roles

* Project Admin: Manages all aspects of a project, including models, settings, alerts, and user access (except deleting the project).
* Project Writer: Can view and edit most project details (models, settings, alerts), but cannot delete the project or invite other users.
* Project Viewer: Can view project details and model content but cannot edit anything except charts and dashboards (read-only access).

## Understanding Permissions

### Permission types

Permission types are used in combination with resources and roles to define the access control rules in Fiddler. Fiddler's RBAC access control uses the following permission types to define the level of access a user has to resources:

* List: This permission allows users to view a list of resources, but does not grant access to view details or interact with the resources in any way. For example, a"
" user with the ""List"" permission for projects can see a list of project names, but cannot view project details or settings.
* Read: This permission enables users to view details of a resource, but does not grant access to edit or modify the resource in any way.
* Create: This permission allows users to create new resources, such as projects, models, or alerts.
* Edit: This permission enables users to modify existing resources, such as updating project settings or editing model configurations.
* Delete: This permission allows users to delete resources, such as deleting a project or a model.

### Organization Level permissions
* Org Admin: Full access to organization settings and resources.
* Org Member: Limited access to organization settings.

![](../.gitbook/assets/Org\_level\_permission.png)

### Project Level permissions

An ‚ÄúOrg Admin‚Äù or ‚ÄúOrg Member‚Äù user can have the below access to the Projects

* Project Admin: Full access to project resources.
* Project Writer: Limited access"
" to project resources, excluding deletion and user invitation.
* Project Viewer: Read-only access to project resources.

![](../.gitbook/assets/Project\_level\_permission.png)

## Getting Started

* The default ""Org Admin"" role is created during Fiddler installation.
* Assign roles to users and teams to control access to resources.
* Use the permissions matrix to understand the access levels for each role.

Click [here](../UI\_Guide/administration-ui/settings.md#teams) for more information on teams.

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Product Concepts
slug: product-concepts
excerpt: Learn the key concept of the Fiddler product
createdAt: Tue Nov 15 2022 18:06:28 GMT+0000 (Coordinated Universal Time)
updatedAt: Tue Apr 30 2024 20:50:32 GMT+0000 (Coordinated Universal Time)
---

# Product Concepts

### What is ML Observability?

ML observability is the modern practice of gaining comprehensive insights into your AI application's performance throughout its lifecycle. It goes beyond simple indicators of good and bad performance by empowering all model stakeholders to understand why a model behaves in a certain manner and how to enhance its performance.\
ML Observability starts with monitoring and alerting on performance issues, but goes much deeper into guiding model owners towards the underlying root cause of model performance issues.

### What is LLM Observability?

LLM observability is the practice of evaluating, monitoring, analyzing, and improving Generative AI or"
" LLM based application across their lifecycle.\
Fiddler provides real-time monitoring on safety metrics like toxicity, bias, and PII and correctness metrics like hallucinations, faithfulness and relevancy.

### Projects

A project within Fiddler represents your organization's distinct AI applications or use cases. It houses all of the model schemas that have been onboarded to Fiddler for the purpose of AI observability. Projects are typically specific to a given business unit, ML application, or use case.\
They serve as a jumping-off point for Fiddler's model monitoring and explainability features.

Additionally, Fiddler projects serve as the primary access control or authorization mechanism. Different users and teams are given access to view model data within Fiddler at the project level.

### Models

#### Model Schemas

In Fiddler, A model schema is the metadata about a model that is being observed.\
Model schemas are onboarded to Fiddler so that Fiddler understand"
" the data under observation.\
Fiddler does not require the model artifact itself to properly observe the performance of the model; however, [model artifacts](explainability/artifacts-and-surrogates.md#Model-Artifacts) can be uploaded to Fiddler to unlock advanced explainability features.\
Instead, we may just need adequate information about the model's schema, or [the model's specification](../Python\_Client\_3-x/api-methods-30.md#modelspec) in order to monitor the model.

> üìò **Working with Model Artifacts**
>
> You can [upload your model artifacts](../Client\_Guide/uploading-model-artifacts.md) to Fiddler to unlock high-fidelity explainability for your model. However, it is not required. If you do not wish to upload your artifact but want to explore explainability with Fiddler, we can build a [surrogate model](explainability/artifacts-and-surrogates.md#surrogate"
"-model) on the backend to be used in place of your artifact.

#### Model Versions

Fiddler offers [model versions](monitoring-platform/model-versions.md) to organize related models, streamlining processes such as model retraining or conducting champion vs. challenger analyses. When retraining a model, instead of uploading a new model instance, you can create a new version of the existing model, retaining its core structure while accommodating necessary updates. These updates can include modifications to the schema, such as adding or removing columns, modifying data types, adjusting value ranges, updating the model specifications, and even refining task parameters or Explainable AI (XAI) settings.

### Data

#### Environments

Within Fiddler, each model is associated with two environments; **pre-production** and **production**.\
These environments assign purpose to the data published to Fiddler, allowing it to distinguish between:

```
* Non-time series data (pre-production datasets, eg. training data)
*"
" Time-series data (production data, eg. inference logs)
```

**Pre-Production Environment**

The pre-production environment contains non-time series chunks of data, called **datasets**.\
Datasets are used primarily for point-in-time analysis or as static baselines for comparison against production data.

**Production Environment**

The production environment contains time series data such as production or inference logs which are the _digital exhaust_ emitted by each decision a model makes.\
This time series data provides the inputs and outputs of each model inference/decision and is what Fiddler analyses and compares against the pre-production data to determine if the model's performance is degrading.

#### Datasets

Datasets within Fiddler are static sets of data that have been uploaded to Fiddler for the purpose of establishing _baselines_.\
A common dataset that is uploaded to Fiddler is the model's training data.

#### Baselines

[Baselines](../Client\_Guide/creating-a-baseline-d"
"ataset.md) are derived from datasets and used by Fiddler to compare the expected data distributions with the live data published to Fiddler.\
A baseline in Fiddler is a set of reference data used to compare the performance of a model for monitoring purposes. The default baseline for all monitoring metrics in Fiddler is typically the model's training data.\
Additional baselines can be added to existing models that are derived from other datasets, historical inferences, or rolling baselines that refer back to data distributions using historical inferences.

#### Segments

[Segments](monitoring-platform/segments.md) are custom filters applied to your data that enable you to analyze metrics for specific subsets of your data population, for example ""People under 50"". Segments help you focus on relevant data for more precise insights. Additionally, you can set alerts on these Segments to stay informed about important changes or trends within these defined subsets.

### Metrics

Metrics are computations performed on data received. F"
"iddler supports the ability to specify custom user-defined metrics [Custom Metrics](monitoring-platform/custom-metrics.md) in addition to five out-of-the-box metric types:

* Data Integrity
* Data Drift
* Performance
* Statistics
* Traffic

### Alerts

[Alerts](monitoring-platform/alerts-platform.md) are user-specified rules which trigger when some condition is met by production data received in Fiddler. Alert rule notifications are sent via email, Slack, custom webhooks, or any combination thereof.

### Enrichments

[Enrichments](llm-monitoring/enrichments-private-preview.md) augment existing columns with additional metrics to monitor different aspects of LLM applications. The new metrics are available for use within the analyze, charting, and alerting functionalities in Fiddler.

### Dashboards and Charts

Fiddler uses customizable [Dashboards](monitoring-platform/dashboards-platform.md) for monitoring and sharing model behavior. A Dashboard is"
" comprised of [Charts](monitoring-platform/monitoring-charts-platform.md) which provide three distinct types of visualizations: monitoring charts, embedding visualizations, and performance analytics. Dashboards consolidate visualizations in one place offering a detailed overview of model performance as well as an entry point for deeper data analysis and root cause identification.

#### Monitoring Charts

Monitoring charts provide a comprehensive view of model performance and support model to model performance analysis. With intuitive displays for data drift, performance metrics, data integrity, traffic patterns, and more, monitoring charts empower users to maintain model accuracy and reliability with ease.

#### Embedding Visualizations

Embedding visualization is a powerful charting tool used to understand and interpret complex relationships in high-dimensional data. Reducing the dimensionality of custom features into a 2D or 3D space makes it easier to identify patterns, clusters, and outliers.

### Jobs

Fiddler [Jobs](https://docs.fiddler.ai/api-integration/api\_guidelines/j"
"obs) are a feature used to track operations such as data publishing or adding model assets such as user artifacts. Jobs are created automatically and can be observed both in the Fiddler UI and polled using the API. Upon successful job completion, the new data or model asset is available for use in the monitoring, alerting, and charting functionalities. If the job fails, users can navigate to the Jobs page and click on ""failure"" for more details on the job failure. Users can then remediate the cause of error or contact the Fiddler team for more support.

### Bookmarks

Bookmarking enables quick access to projects, models, charts, and dashboards. The comprehensive bookmark page enhances your convenience and efficiency in navigating Fiddler.

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Model Task Types
slug: task-types
excerpt: ''
createdAt: Tue Nov 15 2022 18:06:58 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 25 2024 20:45:49 GMT+0000 (Coordinated Universal Time)
---

# Model Task Types

From 23.5 and onwards, Fiddler supports **six** model tasks. These include:

* Binary Classification
* Multi-class Classification
* Regression
* Ranking
* LLM
* Not set

**Binary classification** is the task of classifying the elements of an outcome set into two groups (each called class) on the basis of a classification rule.

Onboarding a Binary classification task in Fiddler requires the following:

* A single output column of type float (range 0-1) which represents the soft output of the model. This column has to be defined.
* A single target column that represents the"
" true outcome. This column has to be defined.
* A list of input features has to be defined.

Typical binary classification problems include:

* Determining whether a customer will churn or not. Here the outcome set has two outcomes: The customer will churn or the customer will not. Further, the outcome can only belong to either of the two classes.
* Determining whether a patient has a disease or not. Here the outcome set has two outcomes: the patient has the disease or does not.

**Multiclass classification** is the task of classifying the elements of an outcome set into three or more groups (each called class) on the basis of a classification rule.

Onboarding a Multiclass classification task in Fiddler requires the following:

* Multiple output columns (one per class) of type float (range 0-1) which represent the soft outputs of the model. Those columns have to be defined.
* A single target column that represents the true outcome. This column has to"
" be defined.
* A list of input features has to be defined.

Typical multiclass classification problems include:

* Determining whether an image is a cat, a dog, or a bird. Here the outcome set has more than two outcomes. Further, the image can only be determined to be one of the three outcomes and it's thus a multiclass classification problem.

**Regression** is the task of predicting a continuous numeric quantity.

Onboarding a Regression task in Fiddler requires the following:

* A single numeric output column that represents the output of the model. This column has to be defined.
* A single numeric target column that represents the true outcome. This column has to be defined.
* A list of input features has to be defined.

Typical regression problems include:

* Determining the average home price based on a given set of housing-related features such as its square footage, number of beds and baths, its location, etc.
* Determining the income of an individual based on"
" features such as age, work location, job sector, etc.

**Ranking** is the task of constructing a rank-ordered list of items given a particular query that seeks some information.

Onboarding a Ranking task in Fiddler requires the following:

* A single numeric output column that represents the output of the model. This column has to be defined.
* A single target column that represents the true outcome. This column has to be defined.
* A list of input features has to be defined.

Typical ranking problems include:

* Ranking documents in information retrieval systems.
* Ranking relevancy of advertisements based on user search queries.

**LLM** is the task dedicated for Large Language Models, a type of transformer model. It represents a deep learning model that can process human languages.

Onboarding an LLM task in Fiddler doesn't require any specific format with regards to the targets/outputs/inputs definition. Those can be defined or not, with any type and no minimum or maximum"
" column has to be defined. However, in that setting, Fiddler doesn't offer XAI functionalities or performance metrics.

Typical LLM problems include:

* Chatbots and Virtual assistants that can answer questions.
* Content creation like articles, blog posts, etc.

**Not set** is the task to choose if a use-case is not covered by the previous tasks or the use-case doesn't need performance metrics. In this setting, the user doesn't have to specify the required data as discussed above to onboard their model.

Onboarding a `NOT_SET` task in Fiddler doesn't require any specific format with regards to the targets/outputs/inputs definition Those can be defined or not, with any type and no minimum or maximum column has to be defined. However, in that setting, Fiddler doesn't offer XAI functionalities or performance metrics.

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Analytics
slug: analytics-eval-platform
excerpt: Platform Guide
createdAt: Wed Feb 01 2023 21:50:06 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Dec 08 2023 22:42:46 GMT+0000 (Coordinated Universal Time)
---

# Analytics

Fiddler‚Äôs industry-first model analytics tool, called Slice and Explain, allows you to perform an exploratory or targeted analysis of model behavior.

1. _**Slice**_ ‚Äî Identify a selection, or slice, of data. Or, you can start with the entire dataset for global analysis.
2. _**Explain**_ ‚Äî Analyze model behavior on that slice using Fiddler‚Äôs visual explanations and other data insights.

Slice and Explain is designed to help data scientists, model validators, and analysts drill down into a model and dataset to see global, local, or instance-level explanations for the model‚Äôs predictions.

Slice and Explain can help you answer questions like:

* What are the key drivers of my model output in a subsection of the data?
* How are the model inputs correlated to other inputs and to the output?
* Where is my model underperforming?
* How is my model performing across the classes in a protected group?

Access Slice and Explain from the Analyze tab for your model. Slice and Explain currently support all tabular models.

**For details on how to use Fiddler Analytics through our interface check the** [**Analytics Page**](../UI\_Guide/analytics-ui/) **on our UI Guide**

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: ""Integrity Constraints""
slug: ""integrity-constraints""
excerpt: """"
hidden: true
createdAt: ""Tue Nov 15 2022 18:07:12 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
What are data integrity constraints
"
"---
title: ""Handling Class Imbalance""
slug: ""handling-class-imbalance""
excerpt: """"
hidden: true
createdAt: ""Tue Nov 15 2022 18:08:49 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Here there will be a general description of class imbalance and the other pages will become housed within this - for now class-imbalanced data page is standalone
"
"---
title: ""Histogram binning""
slug: ""histogram-binning""
excerpt: """"
hidden: true
createdAt: ""Tue Nov 15 2022 18:08:35 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
How we do binning
"
"---
title: Fairness
slug: fairness
excerpt: ''
createdAt: Tue Apr 19 2022 20:24:34 GMT+0000 (Coordinated Universal Time)
updatedAt: Fri Dec 08 2023 22:42:37 GMT+0000 (Coordinated Universal Time)
---

# Fairness

Thanks to Fiddler charts visuals and our [custom metrics](../UI\_Guide/monitoring-ui/custom-metrics.md), you can define your fairness metric of choice to detect model bias on your dataset or production data and set up alerts.

### Definitions of Fairness

Models are trained on real-world examples to mimic past outcomes on unseen data. The training data could be biased, which means the model will perpetuate the biases in the decisions it makes.

While there is not a universally agreed upon definition of fairness, we define a ‚Äòfair‚Äô ML model as a model that does not favor any group of people based on their characteristics.

Ensuring fairness is"
" key before deploying a model in production. For example, in the US, the government prohibited discrimination in credit and real-estate transactions with fair lending laws like the Equal Credit Opportunity Act (ECOA) and the Fair Housing Act (FHAct).

The Equal Employment Opportunity Commission (EEOC) acknowledges 12 factors of discrimination:[\[1\]](fairness.md#reference) age, disability, equal pay/compensation, genetic information, harassment, national origin, pregnancy, race/color, religion, retaliation, sex, sexual harassment. These are what we call protected attributes.

### Fairness Metrics

Among others, some popular metrics are:

* Disparate Impact
* Group Benefit
* Equal Opportunity
* Demographic Parity

The choice of the metric is use case-dependent. An important point to make is that it's impossible to optimize all the metrics at the same time. This is something to keep in mind when analyzing fairness metrics.

### Disparate Impact

Disparate impact is"
" a form of **indirect and unintentional discrimination**, in which certain decisions disproportionately affect members of a protected group.

Mathematically, disparate impact compares the pass rate of one group to that of another.

The pass rate is the rate of positive outcomes for a given group. It's defined as follows:

pass rate = passed / (num of ppl in the group)

Disparate impact is calculated by:

`DI = (pass rate of group 1) / (pass rate of group 2)`

Groups 1 and 2 are interchangeable. Therefore, the following formula can be used to calculate disparate impact:

`DI = min{pr_1, pr_2} / max{pr_1, pr_2}.`

The disparate impact value is between 0 and 1. The Four-Fifths rule states that the disparate impact has to be greater than 80%.

For example:

`pass-rate_1 = 0.3, pass-rate_2 = "
"0.4, DI = 0.3/0.4 = 0.75`

`pass-rate_1 = 0.4, pass-rate_2 = 0.3, DI = 0.3/0.4 = 0.75`

> üìò Info
>
> Disparate impact is the only legal metric available. The other metrics are not yet codified in US law.

### Demographic Parity

Demographic parity states that the proportion of each segment of a protected class should receive the positive outcome at equal rates.

Mathematically, demographic parity compares the pass rate of two groups.

The pass rate is the rate of positive outcome for a given group. It is defined as follow:

`pass rate = passed / (num of ppl in the group)`

If the decisions are fair, the pass rates should be the same.

### Group Benefit

Group benefit aims to measure the rate at which a particular event is predicted to occur within a"
" subgroup compared to the rate at which it actually occurs.

Mathematically, group benefit for a given group is defined as follows:

`Group Benefit = (TP+FP) / (TP + FN).`

Group benefit equality compares the group benefit between two groups. If the two groups are treated equally, the group benefit should be the same.

### Equal Opportunity

Equal opportunity means that all people will be treated equally or similarly and not disadvantaged by prejudices or bias.

Mathematically, equal opportunity compares the true positive rate (TPR) between two groups. TPR is the probability that an actual positive will test positive. The true positive rate is defined as follows:

`TPR = TP/(TP+FN)`

If the two groups are treated equally, the TPR should be the same.

### Intersectional Fairness

We believe fairness should be ensured to all subgroups of the population. We extended the classical metrics (which are defined for two classes) to multiple classes. In addition"
", we allow multiple protected features (e.g. race _and_ gender). By measuring fairness along overlapping dimensions, we introduce the concept of intersectional fairness.

To understand why we decided to go with intersectional fairness, we can take a simple example. In the figure below, we observe that equal numbers of black and white people pass. Similarly, there is an equal number of men and women passing. However, this classification is unfair because we don‚Äôt have any black women and white men that passed, and all black men and white women passed. Here, we observe bias within subgroups when we take race and gender as protected attributes.

![](../.gitbook/assets/21f6b94-intersectional\_fairness.svg)

The EEOC provides examples of past intersectional discrimination/harassment cases.[\[2\]](fairness.md#reference)

For more details about the implementation of the intersectional framework, please refer to this [research paper](https://ar"
"xiv.org/pdf/2101.01673.pdf).

### Model Behavior

In addition to the fairness metrics, we suggest tracking model outcomes and model performance for each subgroup of population.

### Reference

\[^1]: USEEOC article on [_Discrimination By Type_](https://www.eeoc.gov/discrimination-type) \[^2]: USEEOC article on [_Intersectional Discrimination/Harassment_](https://www.eeoc.gov/initiatives/e-race/significant-eeoc-racecolor-casescovering-private-and-federal-sectors#intersectional)

{% include ""../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Embedding Visualizations for LLM Monitoring
slug: embedding-visualization-with-umap
excerpt: >-
  UMAP visualization for understanding unstructured data in high dimensional
  space
createdAt: Tue Nov 14 2023 04:38:29 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 22 2024 18:28:07 GMT+0000 (Coordinated Universal Time)
---

# Embedding Visualizations for LLM Monitoring

\
![](../../.gitbook/assets/e572f49-umap.gif)

### Introduction to embedding visualization

Embedding visualization is a powerful technique used to understand and interpret complex relationships in high-dimensional data. Reducing the dimensionality of custom features into a 2D or 3D space makes it easier to identify patterns, clusters, and outliers.

In Fiddler, high-dimensional data like embeddings and vectors are ingested as a [Custom feature](../../Python\_Client\_3-x/api-methods-30.md#customfeaturetype).

Our goal in this document is to show you how you can visualize and interact with these custom features in 3D to do analysis and expose patterns.

### UMAP Technique for embedding visualization

We utilize the [UMAP](https://umap-learn.readthedocs.io/en/latest/) (Uniform Manifold Approximation and Projection) technique for embedding visualizations. UMAP is a dimensionality reduction technique that is particularly good at preserving the local structure of the data, making it ideal for visualizing embeddings. We reduce the high-dimensional embeddings to a 3D space.

UMAP is supported for both Text and Image embeddings in a [Custom feature](../../Python\_Client\_3-x/api-methods-30.md#customfeaturetype) and you can follow this [guide](../../UI\_Guide/monitoring-ui/embedding-visualization-chart-creation.md) on how to apply a UMAP visualization to your application data.

### UMAP understanding for Generative AI applications

UMAP embedding visualizations are extremely helpful in understanding common themes and topics present in the data corpus for generative AI applications. When evaluating prompts and responses, it is paramount to see which concept clusters are emerging and which clusters are exhibiting the most problems. By further coloring these clusters with a variety of LLM and GenAI correctness and safety metrics, users can identify the clusters with the most issues.

<figure><img src=""../../.gitbook/assets/UMAP cluster.png"" alt=""Identifying a cluster of prompts with Jailbreak attempts via UMAP"" width=""375""><figcaption><p>Identifying a cluster of prompts with Jailbreak attempts via UMAP</p></figcaption></figure>



> üìò To create an embedding visualization chart, follow the UI Guide [here](../../UI\_Guide/monitoring-ui/embedding-visualization-chart-creation.md).

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Selecting the right Enrichments (Private Preview)
slug: selecting-enrichments
excerpt: ''
createdAt: Mon Apr 22 2024 18:53:35 GMT+0000 (Coordinated Universal Time)
updatedAt: Tues Jun 20 2024 20:47:16 GMT+0000 (Coordinated Universal Time)
---

# Selecting Enrichments

Fiddler offers enrichments out of the box to monitor different aspects of LLM applications. Use the below table to select the right enrichment for your specific use case.

This table provides high level information on the metric, the enrichment to use to measure the metric, if the metric uses LLMs, and if so, what LLM it uses.

If you have a use case not covered by the below enrichments out of the box, please contact your administrator.

| Metric                                                                                    | Metric Category | Description                                                                                                                                                                                      | Enrichment                  | LLM Used? | LLM Type"
"                 |
| ----------------------------------------------------------------------------------------- | --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------- | --------- | ------------------------ |
| [Faithfulness](enrichments-private-preview.md#faithfulness-private-preview)               | Hallucination   | This enrichment identifies the accuracy and reliability of facts presented in AI-generated texts                                                                                                 | `faithfulness`              | Yes       | OpenAI                   |
| [Fast Faithfulness](enrichments-private-preview.md#fast-faithfulness-private-preview)     | Hallucination   | This enrichment identifies the accuracy and reliability of facts presented in AI-generated texts. It is generated by Fiddler's Fast Trust Models                                                 | `ftl_response_faithfulness` | Yes       | Fiddler Fast Trust Model |
| [Answer Relevance](enrichments-private-preview.md#answer-relevance-private-preview)       | Hallucination   | This enrichment measures the pertinence of AI-generated responses to their inputs                                                                                                                | `answer_relevance`         "
" | Yes       | OpenAI                   |
| [Conciseness](enrichments-private-preview.md#conciseness-private-preview)                 | Hallucination   | This enrichment evaluates the brevity and clarity of AI-generated responses                                                                                                                      | `conciseness`               | Yes       | OpenAI                   |
| [Coherence](enrichments-private-preview.md#coherence-private-preview)                     | Hallucination   | This enrichment assesses the logical flow and clarity of AI-generated responses                                                                                                                  | `coherence`                 | Yes       | OpenAI                   |
| [Toxicity](enrichments-private-preview.md#toxicity-private-preview)                       | Safety          | This enrichment classifies whether a piece of text is toxic or not                                                                                                                               | `toxicity`                  | Yes       | OpenAI                   |
| [Fast Jailbreak](enrichments-private-preview.md#fast-safety-private-preview)              | Safety          | This enrichment detects the"
" presence of jailbreak attempts by the user. It is generated by Fiddler's Fast Trust Models.                                                                          | `ftl_prompt_safety`         | Yes       | Fiddler Fast Trust Model |
| [Fast Safety](enrichments-private-preview.md#fast-safety-private-preview)                 | Safety          | This enrichment generates 10 different safety metrics to measure texts upon. These metrics are: `illegal, hateful, harassing, racist, sexist, violent, sexual, harmful, unethical, jailbreaking` | `ftl_prompt_safety`         | Yes       | Fiddler Fast Trust Model |
| [PII](enrichments-private-preview.md#personally-identifiable-information-private-preview) | Safety          | This enrichment flags the presence of sensitive information within textual data                                                                                                                  | `pii`                       | No        |                          |
| [Regex Match](enrichments-private-preview.md#regex-match-private-preview)                 | Safety          | This enrichment compares the text"
" with a regular expression string                                                                                                                               | `regex_match`               | No        |                          |
| [Topic](enrichments-private-preview.md#topic-private-preview)                             | Safety          | This enrichment classifies the text into several preset dimensions using a zero-shot classifier                                                                                                  | `topic_model`               | No        |                          |
| [Banned Keywords](enrichments-private-preview.md#banned-keyword-detector-private-preview) | Safety          | This enrichment detects the presence of banned keywords configured by the user                                                                                                                   | `banned_keywords`           | No        |                          |
| [Profanity](enrichments-private-preview.md#profanity-private-preview)                     | Safety          | This enrichment flags the use of offensive or inappropriate language                                                                                                                             | `profanity`                 | No        |                          |
| [Language Detection](enrichments-private-preview.md#language-detector-private-preview)    | Safety          | This enrichment identifies the language of the source text                                                                "
"                                                                       | `language_detection`        | No        |                          |
| [Evaluate](enrichments-private-preview.md#evaluate-private-preview)                       | Text Statistics | This enrichment provides classic text evaluation methods such as BLEU, ROUGE, and Meteor                                                                                                         | `evaluate`                  | No        |                          |
| [Sentiment](enrichments-private-preview.md#sentiment-private-preview)                     | Text Statistics | This enrichment provides sentiment analysis of the target text                                                                                                                                   | `sentiment`                 | No        |                          |
| [TextStat](enrichments-private-preview.md#textstat-private-preview)                       | Text Statistics | This enrichment provides various text statistics such as character/letter count, flesh kinkaid, and others                                                                                       | `textstat`                  | No        |                          |
| [SQLValidation](enrichments-private-preview.md#sql-validation-private-preview)            | Text Validation | Evaluates different query dialects for syntax correctness.                                                                                                                                       | `"
"sql_validation`            | No        |                          |
| [JSONValidation](enrichments-private-preview.md#json-validation-private-preview)          | Text Validation | Validates JSON for correctness and optionally against a user-defined schema.                                                                                                                     | `json_validation`           | No        |                          |

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: LLM based metrics
slug: llm-based-metrics
excerpt: ''
createdAt: Fri Apr 05 2024 18:12:45 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 25 2024 20:38:07 GMT+0000 (Coordinated Universal Time)
---

# LLM Based Metrics

LLM-based metrics use large language models to evaluate the quality of text generated by AI. This approach is much closer to how humans judge text, making these metrics particularly useful for evaluating AI-generated content for use cases such as chatbots, writing assistants, or content creation tools.

LLM-based metrics can adapt to different topics and types of text because LLMS have been trained on a wide range of information, making them a valuable tool for developers and researchers looking to enhance the quality of AI-generated text.

Currently, Fiddler supports two types of LLM-based metrics - OpenAI-based metrics and Fiddler Fast Trust Model metrics.

### OpenAI-based metrics

* These metrics are generated through the OpenAI API, which may introduce latency due to network communication and processing time.
* OpenAI API access token MUST BE provided by the user, which will be configured during onboarding.
* The specific model to be used for these metrics will also be chosen during onboarding.

Currently, the below metrics are OpenAI-based:

* [Answer Relevance](enrichments-private-preview.md#answer-relevance-private-preview)
* [Faithfulness](enrichments-private-preview.md#faithfulness-private-preview)
* [Coherence](enrichments-private-preview.md#coherence-private-preview)
* [Conciseness](enrichments-private-preview.md#conciseness-private-preview)

### Fiddler Fast Trust metrics

* These metrics are generated through Fiddler's in-house, purpose-built LLMs.
* These metrics can be generated in airgapped environments and do not rely on any over-the-network connection to generate such scores.

Currently, the below metrics are Fiddler Fast Trust Model-based:

* [Fast Safety](enrichments-private-preview.md#fast-safety-private-preview)
* [Fast Jailbreak](enrichments-private-preview.md#fast-safety-private-preview)
* [Fast Faithfulness](enrichments-private-preview.md#fast-faithfulness-private-preview)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: LLM Monitoring
slug: llm-monitoring
excerpt: ''
createdAt: Mon Feb 26 2024 20:14:33 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Jun 20 2024 20:36:40 GMT+0000 (Coordinated Universal Time)
---

# LLM Monitoring

Monitoring of Large Language Model (LLM) applications with Fiddler requires publication of the LLM application inputs and outputs, which include prompts, prompt context, response and the source documents retrieved (for RAG-based applications). Fiddler will then generate enrichments, which are LLM trust and safety metrics, for alerting, analysis, or debugging purposes.

Fiddler is a pioneer in the AI Trust domain and, as such, offers the most extensive set of AI safety and trust metrics available today.

### Selecting Enrichments

Fiddler offers many enrichments that each measure different aspects of an LLM application"
". For detailed information about which enrichment to select for any specific use case, visit [this](selecting-enrichments.md) page. Some enrichments use Fast Trust Models to generate these scores.

### Generating Enrichments

In order to generate enrichments, LLM application owners must specify during model onboarding the enrichments to be generated by Fiddler. The enrichment pipeline then generates enrichments for the inputs and outputs of the LLM application as events are published to Fiddler.

\
![Figure 1. The Fiddler Enrichment Framework](../../.gitbook/assets/e6e869b-Screenshot_2024-02-29_at_9.17.29_AM.png)

Figure 1. The Fiddler Enrichment Framework

\
After the raw unstructured inputs and outputs of the LLM application are published to Fiddler, the enrichments framework augments these inputs and outputs with a variety of AI trust and safety metrics. These metrics"
" can be used to monitor the overall health of the LLM application and alert users to any performance degradation.

\
![Figure 2. A Fiddler dashboard showing LLM application performance](../../.gitbook/assets/a1540d1-Screenshot_2024-02-29_at_9.18.22_AM.png)

Figure 2. A Fiddler dashboard showing LLM application performance



With the metrics produced by the enrichment framework, users can monitor LLM application performance over time and conduct root cause analysis when problematic trends are identified.

At the time of model onboarding, application owners can opt in to the various and ever-expanding Fiddler enrichments by specifying [fdl.Enrichment](../../Python_Client_3-x/api-methods-30.md#fdl.enrichment-private-preview) as custom features in the Fiddler [ModelSpec](../../Python_Client_3-x/api-methods-30.md#modelspec) object.

```python
#"
" Automatically generating an embedding for a column named ‚Äúquestion‚Äù

fiddler_custom_features = [
        fdl.Enrichment(
            name='question_embedding',
            enrichment='embedding',
            columns=['question'],
        ),
        fdl.TextEmbedding(
            name='question_cf',
            source_column='question',
            column='question_embedding',
        ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['question'],
    custom_features=fiddler_custom_features,
)
```

The code snippet above illustrates how the [ModelSpec](../../Python_Client_3-x/api-methods-30.md#modelspec) object is configured to opt in to an [embedding enrichment](enrichments-private-preview.md#embedding-private-preview), which is then used to create a [fdl.TextEmbedding](../../Python_Client_3-x/api-methods-30.md#embedding-public-preview) input. This input allows for drift detection and [embedding visualizations with UMAP](embedding-visualization-with-"
"umap.md).

### Enrichments Available

Please reference [fdl.Enrichment](../../Python_Client_3-x/api-methods-30.md#fdlenrichment-private-preview) for a list of available enrichments as of the latest release.



{% include ""../../.gitbook/includes/main-doc-footer.md"" %}
"
"---
title: Enrichments (Private Preview)
slug: enrichments-private-preview
excerpt: ''
createdAt: Mon Apr 22 2024 18:53:35 GMT+0000 (Coordinated Universal Time)
updatedAt: Tues Jun 18 2024 20:47:16 GMT+0000 (Coordinated Universal Time)
---

# Enrichments (Private Preview)

* Enrichments are [custom features](../../Python\_Client\_3-x/api-methods-30.md#customfeature) designed to augment data provided in events.
* Enrichments augment existing columns with new metrics that are defined during model onboarding.
* The new metrics are available for use within the analyze, charting, and alerting functionalities in Fiddler.

Below is an example of an enrichment onboarded onto fiddler. See [enrichments](../../Python\_Client\_3-x/api-methods-30.md#fdlenrichment-private-preview) for more details about"
" all enrichments.

```
fiddler_custom_features = [
        fdl.TextEmbedding(
            name='question_cf',
            source_column='question',
            column='question_embedding',
        ),
    ]

model_spec = fdl.ModelSpec(
    inputs=['question'],
    custom_features=fiddler_custom_features,
)
```

***

### Embedding (Private Preview)

Embeddings are a series of numerical representations (a vector) generated by a model for input text. Each number within the vector represents a different dimension of the text input. The meaning of each number depends on how the embedding generating model was trained.

Fiddler uses publicly available embeddings to power the 3D UMAP experience. Because all of the embeddings should be generated by the same model, the points will naturally cluster together in a way such that can enable quick visual anomaly detection.

In order to create embeddings and leverage them for the UMAP visualization, you must create a new `TextEmbedding` enrichment on this embedding"
" column. If you want to bring your own embeddings onto the Fiddler platform, you can direct Fiddler to consume the embeddings vector directly from your data.

For a list of supported embedding models to choose from as well as usage examples, see [here](../../Python\_Client\_3-x/api-methods-30.md#embedding-private-preview).

***

### Centroid Distance (Private Preview)

Fiddler uses KMeans to determine cluster membership for a given enrichment. The Centroid Distance enrichment provides information about the distance between the selected point and the closest centroid. Centroid Distance is automatically added if the `TextEmbedding` enrichment is created for any given model.

For a usage example, see [here](../../Python\_Client\_3-x/api-methods-30.md#centroid-distance-private-preview).

***

### Personally Identifiable Information (Private Preview)

The PII (Personally Identifiable Information) enrichment is a critical tool designed to detect and flag the presence of sensitive information within textual data"
". Whether user-entered or system-generated, this enrichment aims to identify instances where PII might be exposed, helping to prevent privacy breaches and the potential misuse of personal data. In an era where digital privacy concerns are paramount, mishandling or unintentionally leaking PII can have serious repercussions, including privacy violations, identity theft, and significant legal and reputational damage.

For list of PII Entities and usage, see [here](../../Python\_Client\_3-x/api-methods-30.md#personally-identifiable-information-private-preview).

***

### Evaluate (Private Preview)

This enrichment provides classic Metrics for evaluating QA results like Bleu, Rouge and Meteor.

For supported evaluation metrics and usage, see [here](../../Python\_Client\_3-x/api-methods-30.md#evaluate-private-preview).

***

### Textstat (Private Preview)

The Textstat enrichment generates various text statistics such as character/letter count, flesh kinkaid, and others metrics on the target text column.

"
"For supported statistics and usage, see [here](../../Python\_Client\_3-x/api-methods-30.md#textstat-private-preview).

***

### Sentiment (Private Preview)

The Sentiment enrichment uses NLTK's VADER lexicon to generate a score and corresponding sentiment for all specified columns. To enable set enrichment parameter to `sentiment`.

For usage example, see [here](../../Python\_Client\_3-x/api-methods-30.md#sentiment-private-preview).

***

### Profanity (Private Preview)

The Profanity enrichment is designed to detect and flag the use of offensive or inappropriate language within textual content. This enrichment is essential for maintaining the integrity and professionalism of digital platforms, forums, social media, and any user-generated content areas.

The profanity enrichment uses searches the target text for words from the below two sources

* The Obscenity List from [https://github.com/surge-ai/profanity/blob/main/profanity\_en.csv](https://"
"github.com/surge-ai/profanity/blob/main/profanity\_en.csv)
* Google banned words [https://github.com/coffee-and-fun/google-profanity-words/blob/main/data/en.txt](https://github.com/coffee-and-fun/google-profanity-words/blob/main/data/en.txt)

For usage example, see [here](../../Python\_Client\_3-x/api-methods-30.md#profanity-private-preview).

***

### Toxicity (Private Preview)

The toxicity enrichment classifies whether a piece of text is toxic or not. A RoBERTa based model is fine-tuned with a mix of toxic and non-toxic data. The model predicts score between 0-1 where scores closer to 1 indicate toxicity.

For performance of enrichment and usage example, see [here](../../Python\_Client\_3-x/api-methods-30.md#toxicity-private-preview).

***

### Regex Match (Private Preview)

The Regex Match enrichment is designed"
" to evaluate text responses or content based on their adherence to specific patterns defined by regular expressions (regex). By accepting a regex as input, this metric offers a highly customizable way to check if a string column in the dataset matches the given pattern. This functionality is essential for scenarios requiring precise formatting, specific keyword inclusion, or adherence to particular linguistic structures.

For usage example, see [here](../../Python\_Client\_3-x/api-methods-30.md#regex-match-private-preview).

***

### Topic (Private Preview)

The Topic enrichment leverages the capabilities of Zero Shot Classifier [Zero Shot Classifier](https://huggingface.co/tasks/zero-shot-classification) models to categorize textual inputs into a predefined list of topics, even without having been explicitly trained on those topics. This approach to text classification is known as zero-shot learning, a groundbreaking method in natural language processing (NLP) that allows models to intelligently classify text into categories they haven't encountered during training. It's particularly useful for"
" applications requiring the ability to understand and organize content dynamically across a broad range of subjects or themes.

For usage example, see [here](../../Python\_Client\_3-x/api-methods-30.md#topic-private-preview).

***

### Banned Keyword Detector (Private Preview)

The Banned Keyword Detector enrichment is designed to scrutinize textual inputs for the presence of specified terms, particularly focusing on identifying content that includes potentially undesirable or restricted keywords. This enrichment operates based on a list of terms defined in its configuration, making it highly adaptable to various content moderation, compliance, and content filtering needs.

For usage example, see [here](../../Python\_Client\_3-x/api-methods-30.md#banned-keyword-detector-private-preview).

***

### Language Detector (Private Preview)

The Language Detector enrichment is designed to identify the language of the source text. This enrichment operates from a pretrained text identification model.

For usage example, see [here](../../Python\_Client\_3-x/api-methods"
"-30.md#language-detector-private-preview).

***

### Answer Relevance (Private Preview)

The Answer Relevance enrichment evaluates the pertinence of AI-generated responses to their corresponding prompts. This enrichment operates by assessing whether the content of a response accurately addresses the question or topic posed by the initial prompt, providing a simple yet effective binary outcome: relevant or not relevant. Its primary function is to ensure that the output of AI systems, such as chatbots, virtual assistants, and content generation models, remains aligned with the user's informational needs and intentions.

For usage example, see [here](../../Python\_Client\_3-x/api-methods-30.md#answer-relevance-private-preview).

***

### Faithfulness (Private Preview)

The Faithfulness (Groundedness) enrichment is a binary indicator designed to evaluate the accuracy and reliability of facts presented in AI-generated text responses. It specifically assesses whether the information used in the response correctly aligns with and is grounded in the provided context, often"
" in the form of referenced documents or data. This enrichment plays a critical role in ensuring that the AI's outputs are not only relevant but also factually accurate, based on the context it was given.

For usage example, see [here](../../Python\_Client\_3-x/api-methods-30.md#faithfulness-private-preview).

***

### Coherence (Private Preview)

The Coherence enrichment assesses the logical flow and clarity of AI-generated text responses, ensuring they are structured in a way that makes sense from start to finish. This enrichment is crucial for evaluating whether the content produced by AI maintains a consistent theme, argument, or narrative, without disjointed thoughts or abrupt shifts in topic. Coherence is key to making AI-generated content not only understandable but also engaging and informative for the reader.

For usage example, see [here](../../Python\_Client\_3-x/api-methods-30.md#faithfulness-private-preview).

***

### Conciseness (Private Preview)

The Conciseness"
" enrichment evaluates the brevity and clarity of AI-generated text responses, ensuring that the information is presented in a straightforward and efficient manner. This enrichment identifies and rewards responses that effectively communicate their message without unnecessary elaboration or redundancy. In the realm of AI-generated content, where verbosity can dilute the message's impact or confuse the audience, maintaining conciseness is crucial for enhancing readability and user engagement.

For usage example, see [here](../../Python\_Client\_3-x/api-methods-30.md#conciseness-private-preview).

***

### Fast Safety (Private Preview)

The Fast safety enrichment evaluates the safety of the text along ten different dimensions: `illegal, hateful, harassing, racist, sexist, violent, sexual, harmful, unethical, jailbreaking` Fast safety is generated through the [Fast Trust Models](llm-based-metrics.md).

For usage example, see [here](../../Python\_Client\_3-x/api-methods-30.md#fast-safety-private-preview).

***

"
"### Fast Faithfulness (Private Preview)

The Fast faithfulness enrichment is designed to evaluate the accuracy and reliability of facts presented in AI-generated text responses. Fast safety is generated through the [Fast Trust Models](llm-based-metrics.md).

For usage example, see [here](../../Python\_Client\_3-x/api-methods-30.md#fast-faithfulness-private-preview).

***

### SQL Validation (Private Preview)

The SQL Validation enrichment is designed to evaluate different query dialects for syntax correctness.

For usage example, see [here](../../Python\_Client\_3-x/api-methods-30.md#sql-validation-private-preview).

***

### JSON Validation (Private Preview)

The JSON Validation enrichment is designed to validate JSON for correctness and optionally against a user-defined schema for validation.

For usage example, see [here](../../Python\_Client\_3-x/api-methods-30.md#json-validation-private-preview).

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}



***
"
"---
title: Global Explainability
slug: global-explainability-platform
excerpt: ''
createdAt: Fri Nov 18 2022 22:57:28 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 25 2024 20:45:00 GMT+0000 (Coordinated Universal Time)
---

# Global Explainability

Fiddler provides powerful visualizations to describe the impact of features in your model. Feature impact and importance can be found in either the Explain or Analyze tab.

Global explanations are available in the UI for **structured (tabular)** and **natural language (NLP)** models, for both classification and regression. They are also supported via API using the Fiddler Python package. Global explanations are available for both production and dataset queries.

### Tabular Models

For tabular models, Fiddler‚Äôs Global Explanation tool shows the impact/importance of the features in the model.

Two global explanation methods are available:

* _"
"**Feature importance**_ ‚Äî Gives the average change in loss when a feature is randomly ablated.
* _**Feature impact**_ ‚Äî Gives the average absolute change in the model prediction when a feature is randomly ablated (removed).
  * _**Custom Feature Impact**_
    * Overview
      * The Custom Feature Impact feature empowers you to provide your feature impact scores for your models, leveraging domain-specific knowledge or external data to inform feature importance. This feature enables you to upload custom feature impact data without requiring the corresponding model artifact.
    * How to Use
      * Prepare Your Data
        * Gather feature names and corresponding impact scores for your model.
        * Ensure impact scores are numeric values; negative values indicate inverse relationships.
      * Upload Feature Impact Data
        * Use the provided API endpoint to upload your data.
        * Required parameters:
          * Model UUID: Unique identifier of your model.
          * Feature Names: List of feature names.
          * Impact Scores: List"
" of corresponding impact scores.
      * View Updated Model Information
        * After successful upload, updated feature impact data will be reflected in:
          * Model details page
          * Charts page
          * Explain page
        * Visualize feature impact scores in charts and explanations.
    * Important Notes
      * Error handling: API returns detailed error messages to help resolve issues.
      * Update existing feature impact data by uploading new data for the same model.
      * If you upload feature impact data for a model with an existing artifact, the artifact will be updated.
      * Missing feature impact data may display a tooltip or message; upload data manually or compute using other tools.
    * Methods:
      * `upload_feature_impact`: Accepts a dictionary of feature impact (key-value pairs of features and their impact) and an update flag (True or False).
    * Parameters:
      * `feature_impact_map`: Dictionary (key-value pairs of features and their impact)
      * `update"
"`: Boolean (true or false)
    *   Sample Usage :

        ```python
        PROJECT_NAME = 'YOUR_PROJECT_NAME'
        MODEL_NAME = 'YOUR_MODEL_NAME'
        FEATURE_IMPACT_MAP = {'feature_1': 0.1, 'feature_2': 0.4}
        project = fdl.Project.from_name(name=PROJECT_NAME)
        model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)
        feature_impacts = model.upload_feature_impact(feature_impact_map=FEATURE_IMPACT_MAP, update=False)
        ```

### Language (NLP) Models

For language models, Fiddler‚Äôs Global Explanation performs ablation feature impact on a collection of text samples, determining which words have the most impact on the prediction.

> üìò Info
>
> For speed performance, Fiddler uses a random corpus of 200 documents from the dataset. When using the [`get_feature_importance`](../../Python\_Client\_"
"3-x/api-methods-30.md#get\_feature\_importance) function from the Fiddler API client, the argument `num_refs` can be changed to use a bigger corpus of texts.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Point Explainability
slug: point-explainability-platform
excerpt: ''
createdAt: Fri Nov 18 2022 22:57:20 GMT+0000 (Coordinated Universal Time)
updatedAt: Tue Apr 16 2024 15:24:42 GMT+0000 (Coordinated Universal Time)
---

# Point Explainability

Fiddler provides powerful visualizations that can explain your model's behavior. These explanations can be queried at an individual prediction level in the **Explain** tab, at a model level in the **Analyze** tab or within the monitoring context in the **Monitor** tab.

Explanations are available in the UI for structured (tabular) and natural language (NLP) models. They are also supported via API using the [fiddler-client](https://pypi.org/project/fiddler-client/) Python package. Explanations are available for both production and baseline queries.

Fiddler‚Äôs explanations are interactive"
" ‚Äî you can change feature inputs and immediately view an updated prediction and explanation. We have productized several popular **explanation methods** to work fast and at scale:

* \*\*FiddlerSHAP and SHAP \*\*: using Kernel SHAP implementations, are game-theory-based methods. They work for all models because they only require the ability to ask a model for predictions.
* **Tree SHAP**: is not enabled by default but can be used for Tree-based models. This is a faster and model-specific method to approximate Shapley values.
* **Permutation** : is model-agnostic and can be applied to various types of predictive algorithms, such as linear regression, random forests, support vector machines, and neural networks. By randomly selecting _n_ values for a feature to explain and look at the difference in prediction between the row to explain and the same row with that feature equal to the random value. We obtain the permutation impact for a feature by averaging the _"
"n_ difference in predictions.

For Models with continuous features:

* **Zero-reset**: is a simple method which, for each feature, look at the difference in prediction between the row to explain and the same row with that feature equal to 0. In that case the reference dataset used for explanation is a single row, which considers having a feature equal to 0 is the neutral example.
* **Mean-reset**: is is also a simple method. This time, we use a reference dataset and look at the difference in prediction between the row to explain and the same row with that feature equal to the mean value from the reference dataset.

For Gradient-based models:

* **Integrated Gradients (IG)**: is particularly performant for deep learning models with a large number of inputs. It requires the model‚Äôs prediction to be mathematically differentiable, and a prediction gradient must be made available to Fiddler.
* **Super Integrated Gradients (SIG)**: is a combination of IG and D"
"RISE algorithms developed by the Fiddler to deliver fast explanations for unstructured data coming from models used in computer vision applications.

Custom Explainers:

This allows users to bring in their custom explanation algorithms that can be deployed using Fiddler to server model explanations like the out-of-the-box algorithms shared above.

These methods are discussed in more detail below.

In addition to the previous out-of-the-box explanation methods, Fiddler allows to bring your own explanation method. This can be customized in your model‚Äôs `package.py` wrapper script.

### Tabular Models

For tabular models, Fiddler‚Äôs Point Explanation tool shows how any given model prediction can be attributed to its individual input features.

The following is an example of an explanation for a model predicting the likelihood of customer churn:

![](../../.gitbook/assets/b8e4f81-Tabular\_Explain.png)

A brief tour of the features above:

* _**Explanation Method**_: The explanation method is selected"
" from the **Explanation Type** dropdown.
* _**Input Vector**_: The far left column contains the input vector. Each input can be adjusted.
* _**Model Prediction**_: The box in the upper-left shows the model‚Äôs prediction for this input vector.
  * If the model produces multiple outputs (e.g. probabilities in a multiclass classifier), you can click on the prediction field to select and explain any of the output components. This can be particularly useful when diagnosing misclassified examples.
* _**Feature Attributions**_: The colored bars on the right represent how the prediction is attributed to the individual feature inputs.
  * A positive value (blue bar) indicates a feature is responsible for driving the prediction in the positive direction.
  * A negative value (red bar) is responsible for driving the prediction in a negative direction.
* _**Baseline Prediction**_: The thin colored line just above the bars shows the difference between the baseline prediction and the model prediction. The specifics"
" of the baseline calculation vary with the explanation method, but usually, it's approximately the mean prediction of the training/reference data distribution (i.e. the dataset specified when importing the model into Fiddler). The baseline prediction represents a typical model prediction.

**Two numbers** accompany each feature‚Äôs attribution bar in the UI.

* _The first number_ is the **attribution**. The sum of these values over all features will always equal the difference between the model prediction and a baseline prediction value.
* _The second number_, the percentage in parentheses, is the **feature attribution divided by the sum of the absolute values of all the feature attributions**. This provides an easy to compare, relative measure of feature strength and directionality (notice that negative attributions have negative percentages) and is bounded by ¬±100%.

> üìò Info
>
> An input box labeled **‚ÄúTop N‚Äù** controls how many attributions are visible at once. If the values don‚Äôt add up as described"
" above, it‚Äôs likely that weaker attributions are being filtered-out by this control.

Finally, it‚Äôs important to note that **feature attributions combine model behavior with characteristics of the data distribution**.

### Language (NLP) Models

For language models, Fiddler‚Äôs Point Explanation provides the word-level impact on the prediction score when using perturbative methods (SHAP and Fiddler); for the Integrated Gradients method, tokenization can be customized in your model‚Äôs `package.py` wrapper script. The explanations are interactive‚Äîedit the text, and the explanation updates immediately.

Here is an example of an explanation of a prediction from a sentiment analysis model:

![](../../.gitbook/assets/970a86b-NLP\_Explain.png)

### Point Explanation Methods: How to Quantify Prediction Impact of a Feature?

**Introduction**

One strategy for explaining the prediction of a machine learning model is to measure the influence that each of its inputs have on the prediction made. This"
" is called Feature Impact.

To measure Feature Impact, **additive attribution methods** can be quite powerful. Fiddler includes:

* **SHAP** and **Fiddler SHAP**, which require only the ability to ask a model for predictions, and are thus suitable across all types of models; no knowledge of the model implementation is necessary.
* **Integrated Gradients**, a method that takes advantage of the gradient vector of the prediction, which is typically available in deep learning models, to efficiently explain complex models with large input dimensionality.

**Additive Attributions**

To explain a prediction with an additive attribution method, we look at how individual features contribute to the _prediction difference_. The prediction difference is a comparison between the prediction as a point in feature space (we refer to this as the _explain-point_), and a counterfactual baseline position (or a distribution of positions), representing an uninteresting or typical model inference.

Each feature is assigned a fraction of the prediction difference for which"
" it is responsible. This fraction is called the feature attribution, and it‚Äôs what we show in our explanations.

Additive attribution methods have the following characteristics:

* The sum of feature attributions always equals the prediction difference.
* Features that have no effect on a model‚Äôs prediction receive a feature attribution of zero.
* Features that have the identical effect receive the same attribution.
* Features with mutual information share the attribution for any effect that information has on the prediction.

Additionally, each of these methods takes into account interactions between the features (e.g. two features that have no effect individually but in combination change the model output). This is explicitly built into the Shapley value formalism, and is captured in the path integral over gradients in Integrated Gradients.

**Shapley Values and their Approximation**

The Shapley value[\[1\]](point-explainability-platform.md#references) (proposed by Lloyd Shapley in 1953) is one way to derive feature"
" attributions. Shapley values distribute the total payoff of a collaborative game across a coalition of cooperating players. They are computed by tabulating the average gain in payoff when a particular player is added to the coalition, over all coalition sizes and permutations of players.

In our case, we consider the ‚Äútotal gains‚Äù to be the prediction value, and a ‚Äúplayer‚Äù is a single model feature. The collaborative ‚Äúgame‚Äù is all of the model features cooperating to form a prediction value.

Why do we create ‚Äúcoalitions‚Äù with only a subset of the features? In some scenarios, it may be appropriate to replace a feature with a zero value when removed from the coalition (e.g. text models where no mask token is available). In others (e.g. models with dense tabular inputs), values are swapped in from a reference distribution or baseline example as a zero value may have a specific meaning (like zero income on a credit application).

Shapley values have desirable properties including:

*"
" _**Linearity**_: If two games are combined, then the total gains correspond to the gains derived from a linear combination of the gains of each game.
* _**Efficiency**_: The sum of the values of all players equals the value of the grand coalition, so that all the gain is distributed among the players. In our case, the efficiency property says _the feature attributions should sum to the prediction value_. The attributions can be positive or negative, since a feature can raise or lower a predicted value.

**Approximating Shapley Values**

Computation of exact Shapley values can be extremely computationally expensive‚Äîin fact, exponentially so, in the number of input features. Fiddler makes two approximation methods available:

* **SHAP**[\[2\]](point-explainability-platform.md#references) (SHapely Additive exPlanations) approximates Shapley values by sampling coalitions according to a combinatorially weighted kernel (comp"
"ensating for the number of permutations of features in coalitions of different cardinality). It samples the feature space uniformly between baseline-like feature vectors and explain-point-like feature vectors. This has the effect of downsampling behavior in the immediate vicinity of the explain-point, a region where the model may be saturated or uniform in its prediction, and attributions may not be helpful.
* **Fiddler SHAP**[\[3\]](point-explainability-platform.md#references) builds on the SHAP approach and is optimized for computing distributions of Shapley values for each feature by comparing the explain-point against a distribution of baselines. This makes it possible to compute confidence intervals around the mean attribution for each feature and identify clusters in attribution space where distinct, individually relevant explanations might be important (e.g. ‚Äúyour loan application was rejected for a set of reasons when compared to applications in your region, and for another set of reasons when compared to applications with the same profession‚Äù).

Approximate"
" Shapley value methods can be used to explain nearly any model, since you only need to be able to ask the model for predictions at a variety of positions in the feature space.

**Integrated Gradients**

Another additive attribution method: the Integrated Gradients method.

For models whose prediction is continuous and piecewise differentiable in the feature space, it can be useful to provide additional information through the gradient (slope vector) of a prediction.

Fiddler supports Integrated Gradients (IG)[\[4\]](point-explainability-platform.md#references). In this method, an approximate integral tabulates components of the slope along a linear path from baseline to explain-point, and attributes them to respective input features. This has several advantages:

1. For models with very high dimensional feature volumes (e.g. images, text), where differentiable deep-learning models typically excel, this method can be very performant (O(N) vs. the O(2^n) of the Shapley methods"
")
2. Attributions can be computed for intermediate layers within the model, providing fine-grained model diagnostics. This is naturally extensible to models with hybrid and multimodal inputs.
3. In comparison to local gradients and saliency methods, the IG path integral samples the large-scale behavior of the model and is resistant to amplifying noise in the possibly saturated region around the explain-point.

### References

1. [https://en.wikipedia.org/wiki/Shapley\_value](https://en.wikipedia.org/wiki/Shapley\_value)
2. S. Lundberg, S Lee. ‚ÄúA Unified Approach to Interpreting Model Predictions.‚Äù NeurIPS, 2017 [http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf)
3. L. Merrick and"
" A. Taly ‚ÄúThe Explanation Game: Explaining Machine Learning Models Using Shapley Values‚Äù [https://arxiv.org/abs/1909.08128](https://arxiv.org/abs/1909.08128)
4. M. Sundararajan, A. Taly, Q. Yan ‚ÄúAxiomatic Attribution for Deep Networks‚Äù [http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf](http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Explainability
slug: explainability
excerpt: Overview of Fiddler's explainability tooling for machine learning models.
createdAt: Mon Dec 19 2022 19:01:04 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 15 2024 14:44:55 GMT+0000 (Coordinated Universal Time)
---

# Explainability

Fiddler's Explainability offering provides:

* [Point Explainations](../../UI\_Guide/explainability-ui-giude/point-explainability.md)
* [Global Explainations](../../UI\_Guide/explainability-ui-giude/global-explainability.md)

Explainability can be enabled in two forms

* [User-Provided Model Artifacts](artifacts-and-surrogates.md)
* [Fiddler Generated Surrogate Model](../../UI\_Guide/explainability-ui-giude/surrogate-models.md)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: 'Model: Artifacts, Package, Surrogate'
slug: artifacts-and-surrogates
excerpt: Important terminologies for the ease of use of Fiddler Explainability
createdAt: Tue Nov 15 2022 18:06:36 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 22 2024 18:22:44 GMT+0000 (Coordinated Universal Time)
---

# Model: Artifacts, Package, Surrogate

### Model Artifacts and Model Package

A model in Fiddler is a placeholder that may not need the **model artifacts** for monitoring purposes. However, for explainability, model artifacts are needed.

_Required_ model artifacts include:

* The \*\*[model file](artifacts-and-surrogates.md#model-file) \*\*(e.g. `*.pkl`)
* [`package.py`](artifacts-and-surrogates.md#packagepy-wrapper-script): A wrapper script"
" containing all of the code needed to standardize the execution of the model.

A collection of model artifacts in a directory is referred to as a **model package**. To start, **place your model artifacts in a new directory**. This directory will be the model package you will upload to Fiddler to add or update model artifacts.

While the model file and package.py are required artifacts in a model package, you can also _optionally_ add other artifacts such as any serialized [preprocessing objects](artifacts-and-surrogates.md#preprocessing-objects) needed to transform data before running predictions or after.

In the following, we discuss the various model artifacts.

#### Model File

A model file is a **serialized representation of your model** as a Python object.

Model files can be stored in a variety of formats. Some include

* Pickle (`.pkl`)
* Protocol buffer (`.pb`)
* Hierarchical Data Format/HDF5 (`.h5`)

"
"#### package.py wrapper script

Fiddler‚Äôs artifact upload process is **framework-agnostic**. Because of this, a **wrapper script** is needed to let Fiddler know how to interact with your particular model and framework.

The wrapper script should be named `package.py`, and it should be **placed in the same directory as your model artifact**. Below is an example of what `package.py` should look like.

```python
from pathlib import Path
import pandas as pd

PACKAGE_PATH = Path(__file__).parent

class MyModel:

    def __init__(self):
        """"""
        Here we can load in the model and any other necessary
            serialized objects from the PACKAGE_PATH.
        """"""

    def predict(self, input_df):
        """"""
        The predict() function should return a DataFrame of predictions
            whose columns correspond to the outputs of your model.
        """"""

def get_model():
    return MyModel()
```

The only hard requirements for `package.py`"
" are

* The script must be named `package.py`
* The script must implement a function called `get_model`, which returns a model object
* This model object must implement a function called `predict`, which takes in a pandas DataFrame of model inputs and returns a pandas DataFrame of model predictions

#### Preprocessing objects

Another component of your model package could be any **serialized preprocessing objects** that are used to transform the data before or after making predictions.

You can place these in the model package directory as well.

> üìò Info
>
> For example, in the case that we have a **categorical feature**, we may need to **encode** it as one or more numeric columns before calling the model‚Äôs prediction function. In that case, we may have a serialized transform object called `encoder.pkl`. This should also be included in the model package directory.

#### requirements.txt file

> üìò Info
>
> This is only used starting at 23.1 version with Model"
" Deployment enabled.

Each base image (see [flexible model deployment](flexible-model-deployment/) for more information on base images) comes with a few pre-installed libraries and these can be overridden by specifying `requirements.txt` file inside your model artifact directory where `package.py` is defined.

Add the dependencies to requirements.txt file like this:

```python
scikit-learn==1.0.2  
numpy==1.23.0  
pandas==1.5.0
```

### Surrogate Model

A surrogate model is an approximation of your model intended to make qualitative explainability calculations possible for scenarios where model ingestion is impossible or explainability is an occasional nice-to-have, but not a primary component of a model monitoring workflow.

Fiddler creates a surrogate when you call [`add_surrogate`](../../Python\_Client\_3-x/api-methods-30.md#add\_surrogate). This requires that you've already added a model using [model.create]("
"../../Python\_Client\_3-x/api-methods-30.md#create-3).

> üöß Surrogates can currently only be created for models with tabular input types.

Fiddler produces a surrogate by training a gradient-boosted decision tree (LightGBM) to the ground-truth labels provided and with a general, predefined set of settings.

### Custom dependencies

If your model artifact requires specific packages to run, Fiddler can support this with the Flexible Model Deployment feature. See [here](flexible-model-deployment/) for details on how to configure your model with the correct requirements.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: ""Global Explanations""
slug: ""global-explanations-platform""
excerpt: ""Platform Guide""
hidden: true
createdAt: ""Mon Dec 19 2022 19:29:10 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:41:52 GMT+0000 (Coordinated Universal Time)""
---
Fiddler provides powerful visualizations to describe the impact of features in your model. Feature impact and importance can be found in either the Explain or Analyze tab.

Global explanations are available in the UI for **structured (tabular)** and **natural language (NLP)** models, for both classification and regression. They are also supported via API using the Fiddler Python package. Global explanations are available for both production and dataset queries.

## Tabular Models

For tabular models, Fiddler‚Äôs Global Explanation tool shows the impact/importance of the features in the model.

Two global explanation methods are available:

* **_Feature impact_** ‚Äî Gives the average absolute change in the model prediction when a feature is randomly ablated (removed).
  * **_User-Defined Feature Impact_** - The User-Defined Feature Impact feature allows you to upload custom feature impact for models to execute Global model explanations in Fiddler and see the feature drift impact without the need to compute feature impact in Fiddler
    * Methods: upload_feature_impact: Accepts a dictionary of feature impact (key-value pairs of features and their impact) and an update flag (true or false).
    * Parameters:
      * feature_impact_map: Dictionary (key-value pairs of features and their impact)
      * update: Boolean (true or false)
    * Sample Usage :

```python
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'
DATASET_NAME = 'YOUR_DATASET_NAME'
FEATURE_IMPACT_MAP = {'feature_1': 0.1, 'feature_2': 0.4}

project = fdl.Project.from_name(name=PROJECT_NAME)
model = fdl.Model.from_name(name=MODEL_NAME, project_id=project.id)
dataset = fdl.Dataset.from_name(name=DATASET_NAME, model_id=model.id)

feature_impacts = model.upload_feature_impact(feature_impact_map=FEATURE_IMPACT_MAP, update=False)
```
* **_Feature importance_** ‚Äî Gives the average change in loss when a feature is randomly ablated.

## Language (NLP) Models

For language models, Fiddler‚Äôs Global Explanation performs ablation feature impact on a collection of text samples, determining which words have the most impact on the prediction.

> üìò Info
>
> For speed performance, Fiddler uses a random corpus of 200 documents from the dataset. When using the [`run_feature_importance`](https://api.fiddler.ai/#client-run_feature_importance) function from the Fiddler API client, the argument `n_inputs` can be changed to use a bigger corpus of texts.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Point Explanations""
slug: ""point-explanations""
excerpt: """"
hidden: true
createdAt: ""Fri Dec 16 2022 23:19:37 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:41:45 GMT+0000 (Coordinated Universal Time)""
---
| Model | Input | Type | Default | Reference | Sice | Permutations |
| --- | --- | --- | --- |
| Point Explanations  <br/>SHAP and Fiddler SHAP  | Tabular  | 200  |   |
|   | Text  | 200  |   |
| Global Explanations  <br/>Random Ablation Feature Impact (and Importance for models with tabular inputs)  | Tabular  | 10K  |   |
|   | Text  | 200  |   | 



# How to Quantify"
"slug: ""point-explanations""  Prediction Impact of a Feature?

One approach for explaining the prediction of a machine learning model is to measure the influence that each of its inputs has on the prediction made. This is called Feature Impact.

- SHAP and Fiddler SHAP, using Kernel SHAP implementations, are game-theory based methods. They work for all models, because they only require the ability to ask a model for predictions.
- Integrated Gradients, which is particularly performant for deep learning models with a large number of inputs. It requires the model‚Äôs prediction to be mathematically differentiable, and a prediction gradient must be made available to Fiddler.
- Tree SHAP, is not enabled by default but can be used for Tree-based model. This is a faster and model-specific method to approximate Shapley values.

These methods are discussed in more detail below.

In addition to the previous out of the box explanation methods, Fiddler allows to bring your own explanation method. This can be customized in"
"slug: ""point-explanations""  your model‚Äôs `package.py` wrapper script.

## Tabular Models

## Language (NLP) Models

## Point Explanation Methods:

**Introduction**

To measure Feature Impact, **additive attribution methods** can be quite powerful. Fiddler includes:

- **SHAP** and **Fiddler SHAP**, which require only the ability to ask a model for predictions, and are thus suitable across all types of models; no knowledge of the model implementation is necessary.
- **Integrated Gradients**, a method that takes advantage of the gradient vector of the prediction, which is typically available in deep learning models, to efficiently explain complex models with large input dimensionality.

**Additive Attributions**

To explain a prediction with an additive attribution method, we look at how individual features contribute to the _prediction difference_. The prediction difference is a comparison between the prediction as a point in feature space (we refer to this as the _explain-point_), and a counterfactual baseline position (or"
"slug: ""point-explanations""  a distribution of positions), representing an uninteresting or typical model inference.

Each feature is assigned a fraction of the prediction difference for which it is responsible. This fraction is called the feature attribution, and it‚Äôs what we show in our explanations.

Additive attribution methods have the following characteristics:

- The sum of feature attributions always equals the prediction difference.
- Features that have no effect on a model‚Äôs prediction receive a feature attribution of zero.
- Features that have the identical effect receive the same attribution.
- Features with mutual information share the attribution for any effect that information has on the prediction.

Additionally, each of these methods takes into account interactions between the features (e.g. two features that have no effect individually but in combination change the model output). This is explicitly built into the Shapley value formalism, and is captured in the path integral over gradients in Integrated Gradients.

**Shapley Values and their Approximation**

The Shapley value[<sup>\[1\]</"
"slug: ""point-explanations"" sup>](#references) (proposed by Lloyd Shapley in 1953) is one way to derive feature attributions. Shapley values distribute the total payoff of a collaborative game across a coalition of cooperating players. They are computed by tabulating the average gain in payoff when a particular player is added to the coalition, over all coalition sizes and permutations of players.

In our case, we consider the ‚Äútotal gains‚Äù to be the prediction value, and a ‚Äúplayer‚Äù is a single model feature. The collaborative ‚Äúgame‚Äù is all of the model features cooperating to form a prediction value.

Why do we create ‚Äúcoalitions‚Äù with only a subset of the features? In some scenarios, it may be appropriate to replace a feature with a zero value when removed from the coalition (e.g. text models where no mask token is available). In others (e.g. models with dense tabular inputs), values are swapped in from a reference distribution or baseline example as a zero"
"slug: ""point-explanations""  value may have a specific meaning (like zero income on a credit application).

Shapley values have desirable properties including:

- **_Linearity_**: If two games are combined, then the total gains correspond to the gains derived from a linear combination of the gains of each game.
- **_Efficiency_**: The sum of the values of all players equals the value of the grand coalition, so that all the gain is distributed among the players. In our case, the efficiency property says _the feature attributions should sum to the prediction value_. The attributions can be positive or negative, since a feature can raise or lower a predicted value.

**Approximating Shapley Values**

Computation of exact Shapley values can be extremely computationally expensive‚Äîin fact, exponentially so, in the number of input features. Fiddler makes two approximation methods available:

- **SHAP**[<sup>\[2\]</sup>](#references) (SHapely Additive"
"slug: ""point-explanations""  exPlanations) approximates Shapley values by sampling coalitions according to a combinatorially weighted kernel (compensating for the number of permutations of features in coalitions of different cardinality). It samples the feature space uniformly between baseline-like feature vectors and explain-point-like feature vectors. This has the effect of downsampling behavior in the immediate vicinity of the explain-point, a region where the model may be saturated or uniform in its prediction, and attributions may not be helpful.
- **Fiddler SHAP**[<sup>\[3\]</sup>](#references) builds on the SHAP approach and is optimized for computing distributions of Shapley values for each feature by comparing the explain-point against a distribution of baselines. This makes it possible to compute confidence intervals around the mean attribution for each feature and identify clusters in attribution space where distinct, individually relevant explanations might be important (e.g. ‚Äúyour loan application was rejected for a set of reasons when compared to"
"slug: ""point-explanations""  applications in your region, and for another set of reasons when compared to applications with the same profession‚Äù).

Approximate Shapley value methods can be used to explain nearly any model, since you only need to be able to ask the model for predictions at a variety of positions in the feature space.

**Integrated Gradients**

Another additive attribution method: the Integrated Gradients method.

For models whose prediction is continuous and piecewise differentiable in the feature space, it can be useful to provide additional information through the gradient (slope vector) of a prediction.

Fiddler supports Integrated Gradients (IG)[<sup>\[4\]</sup>](#references). In this method, an approximate integral tabulates components of the slope along a linear path from baseline to explain-point, and attributes them to respective input features. This has several advantages:

1. For models with very high dimensional feature volumes (e.g. images, text), where differentiable deep-learning models typically excel, this method can"
"slug: ""point-explanations""  be very performant (O(N) vs. the O(2^n) of the Shapley methods)
2. Attributions can be computed for intermediate layers within the model, providing fine-grained model diagnostics. This is naturally extensible to models with hybrid and multimodal inputs.
3. In comparison to local gradients and saliency methods, the IG path integral samples the large-scale behavior of the model and is resistant to amplifying noise in the possibly saturated region around the explain-point.

## References

1. <https://en.wikipedia.org/wiki/Shapley_value>
2. S. Lundberg, S Lee. ‚ÄúA Unified Approach to Interpreting Model Predictions.‚Äù NeurIPS, 2017 <http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf>
3. L. Merrick  and A. Taly ‚ÄúThe Explanation Game: Explaining Machine Learning Models Using Shap"
"slug: ""point-explanations"" ley Values‚Äù <https://arxiv.org/abs/1909.08128>
4. M. Sundararajan, A. Taly, Q. Yan ‚ÄúAxiomatic Attribution for Deep Networks‚Äù  <http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf>

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""On-Prem - Manual Flexible Model Deployment (FMD)""
slug: ""manual-flexible-model-deployment""
excerpt: ""This guide is for using FMD for custom on-prem deployments""
hidden: false
createdAt: ""Mon Jul 03 2023 16:06:11 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Mon Apr 22 2024 18:17:46 GMT+0000 (Coordinated Universal Time)""
---
This page outlines how to upload a model artifact or surrogate if you have an on-prem deployment of Fiddler and your deployment doesn't give k8s permissions to create model deployment pods dynamically.

> üìò Note
> 
> Follow this page if you want to upload a model artifact or a surrogate model. For monitoring only models, without artifact uploaded, this is not required.

## Permissions

Model surrogate or artifact upload is going to create a model deployment pod dynamically, one per model with"
"slug: ""manual-flexible-model-deployment""  all required requirements to run the model.

Fiddler need permissions to perform CRUD operations on k8s resources like deployment and service. If this permissions is not provided, then we need a provision to manually spin up the required k8s resources.

In addition, Fiddler offers the ability to run pip install at runtime to install additional dependencies if the image chosen is missing libraries. If this permission is not provided for your deployment, please reach out to the Fiddler team with the list of required of dependencies for your model, we will build an image for you.

## Model on-boarding steps

- Customer calls [add_model_artifact](../../../Python_Client_3-x/api-methods-30.md#add_artifact) with MANUAL deployment type in the [DeploymentParams](../../../Python_Client_3-x/api-methods-30.md#update-model-deployment) object.  
  The model artifact will be stored, but no deployment pod will be created at this stage. Model validation"
"slug: ""manual-flexible-model-deployment""  and feature impact computation will not be performed. Model deployment status will be inactive.

```python
# Specify deployment parameters
deployment_params = fdl.DeploymentParams(
        image_uri=""md-base/python/machine-learning:1.4.0"",
        cpu=250,
        memory=512,
  		  replicas=1)

# Add model artifact
job = model.add_artifact(
  model_dir =  str, #path to your model dirctory with model artifacts and package.py 
  deployment_param = DeploymentParams | None,
) -> AsyncJob
job.wait()
```

- Customer manually creates Model Deployment k8s resources. Please check the [next section](manual-flexible-model-deployment.md#instructions-to-manually-create-model-deployment-k8s-resources) and follow the instructions to create the required k8s resources.
- Customer calls [update_model_deployment](../../../Python_Client_3-x/api-methods-30.md#update-model-deployment) with the"
"slug: ""manual-flexible-model-deployment""  parameter `active=True`.  
  This step will use the model deployment pod previously created, add the model files, validate the model deployment and compute global feature impact. Model will be active and available for XAI features after this step.

```python
model_deployment.cpu = 300
model_deployment.active = True
model_deployment.update()
```

## Instructions to manually create Model Deployment k8s resources

Fiddler will provide a script to create `service.yaml` and `deployment.yaml` files. Customers can review those files and manually apply those on their deployment. A list of environment variable has to be defined in order to run the script.

```shell
./deploy-model-deployment.sh
```

| Environment variables              | Description                                                        |
| :--------------------------------- | :----------------------------------------------------------------- |
| ENDPOINT                           | Fiddler URL for your deployment                                    |
| TOKEN                              | Fiddler Token                                                      |
| ORGANIZATION_NAME                  | The name of the Fiddler"
"slug: ""manual-flexible-model-deployment""  organization                               |
| PROJECT_NAME                       | The name of the project where the MODEL_NAME is located in Fiddler |
| MODEL_NAME                         | The name of the model to create resources for                      |
| IMAGE_PULL_SECRET_NAME             | The image pull secret name (Optional)                              |
| MODEL_DEPLOYMENT_EXTRA_ANNOTATIONS | Custom extra annotations (Optional)                                |
| MODEL_DEPLOYMENT_EXTRA_LABELS      | Custom extra labels (Optional)                                     |
"
"# Flexible Model Deployment

Fiddler supports explainability for models with varying dependencies. This is achieved by running each model in its own dedicated container to provide the resources and dependencies that are unique to that model. For example, if your team has two models developed with the same libraries but using different versions you can run both those models by specifying the exact version they were built with.

> üìò Note
>
> For models that require monitoring features only, there is no need to upload your model artifact or create a surrogate model as these are only used to support explainability features.

***

When adding a model artifact to your Fiddler model (see [add\_artifact](../../../Python\_Client\_3-x/api-methods-30.md#add\_artifact)), you specify the deployment configuration needed to run it using the [DeploymentParams](../../../Python\_Client\_3-x/api-methods-30.md#deploymentparams) argument. Fiddler has a set of starter images from which to select"
" the configuration most appropriate for running your model. These images vary by included libraries and Python versions. Note you can also customize an image by including your own requirements.txt file along with the model artifact package.

#### DeploymentParams Arguments

*   `image_uri`: This is the Docker image used to create a new runtime to serve the model. You can choose a base image from the following list, with the matching requirements for your model:

    | Image URI                         | Dependencies                                                                                                                                                                     |
    | --------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | `md-base/python/python-39:2.0.2`  | <p>fiddler-client==3.0.3<br>flask==2.2.5<br>gevent==23.9.0<br>gunicorn==22.0.0<br>prometheus-flask-exporter==0.21.0<br>pyarrow==14.0.1<br>pydantic==1.10.13"
"</p>                  |
    | `md-base/python/python-310:1.0.0` | <p>fiddler-client==3.0.3<br>flask==2.2.5<br>gevent==23.9.0<br>gunicorn==22.0.0<br>prometheus-flask-exporter==0.21.0<br>pyarrow==14.0.1<br>pydantic==1.10.13</p>                  |
    | `md-base/python/python-311:1.0.0` | <p>fiddler-client==3.0.3<br>flask==2.2.5<br>gevent==23.9.0<br>gunicorn==22.0.0<br>prometheus-flask-exporter==0.21.0<br>pyarrow==14.0.1<br>pydantic==1.10.13</p"
">                  |
    | `md-base/python/java:2.1.0`       | <p>fiddler-client==3.0.3<br>flask==2.2.5<br>gevent==23.9.0<br>gunicorn==22.0.0<br>h2o==3.46.0.5<br>prometheus-flask-exporter==0.21.0<br>pyarrow==14.0.1<br>pydantic==1.10.13</p> |
    | `md-base/python/rpy2:2.0.2`       | <p>fiddler-client==3.0.3<br>flask==2.2.5<br>gevent==23.9.0<br>gunicorn==22.0.0<br>prometheus-flask-exporter==0.21.0<br>pyarrow==14.0.1<br>py"
"dantic==1.10.13<br>rpy2==3.5.1</p>   |

> üìò Image upgrades
>
> These Docker images are upgraded routinely to resolve security vulnerabilities and the image tag is updated accordingly. Unsupported Python versions are not provided.

> üöß Be aware
>
> Model version features are supported with the image versions listed above. Images below 2.x for `python-39`, `java` and `rpy2` will continue to work for existing models using a single version. From 24.5 onwards, model version first class support is added and these require the new model deployment base image tag versions.

Each base image comes with a few pre-installed libraries and these can be overridden and added to by specifying a [requirements.txt](../artifacts-and-surrogates.md#requirementstxt-file) file inside your model artifact directory where [package.py](../artifacts-and-surrogates.md#packagepy-wrapper-script"
") is defined.

Note that the old images `deep-learning` and `machine-learning` are deprecated (All current versions are still working, but we stopped maintaining and upgrading those). We encourage users to select any plain Python image, and add the necessary libraries in `requirements.txt`.

> üöß Be aware
>
> Installing new dependencies at runtime will take time and is prone to network errors.

```
* `replicas`: The number of Docker image replicas running the model.
* `memory`: The amount of memory (mebibytes) reserved per replica. NLP models might need more memory, so ensure to allocate the required amount of resources.
```

> üöß Be aware
>
> Your model might require more memory than the default setting. Please ensure you set a sufficient amount of resources. If you see a `ModelServeError` error when adding a model, it means the current settings were not enough to run your model.

* `cpu`: The amount of"
" CPU (milli cpus) reserved per replica. Both number of features and model complexity can require more CPU allocation.

Both [add\_artifact](../../../Python\_Client\_3-x/api-methods-30.md#add\_artifact) and [update\_artifact](../../../Python\_Client\_3-x/api-methods-30.md#update\_artifact) methods support passing `deployment_params`. For example:

```python
# Specify deployment parameters
deployment_params = fdl.DeploymentParams(
        image_uri=""md-base/python/python-311:1.0.0"",
        cpu=250,
        memory=512,
          replicas=1)

# Add model artifact
job = model.add_artifact(
  model_dir =  str, #path to your model dirctory with model artifacts and package.py
  deployment_param = DeploymentParams | None,
) -> AsyncJob
job.wait()
```

Once the model is added in Fiddler, you can fine-tune the"
" model deployment based on the scaling requirements, using [update\_model\_deployment](../../../Python\_Client\_3-x/api-methods-30.md#update-model-deployment). This function allows you to:

* **Horizontal scaling**: horizontal scaling via replicas parameter. This will create multiple Kubernetes pods internally to handle concurrent requests.
* **Vertical scaling**: Model deployments support vertical scaling via cpu and memory parameters. Some models might need more memory to load the artifacts into memory or process the requests.
* **Scale down**: You may want to scale down the model deployments to avoid allocating the resources when the model is not in use. Use _active_ parameter set to _False_ to scale down the deployment.
* **Scale up**: To scale model deployments back up, set _active_ parameter to _True_.

{% include ""../../../.gitbook/includes/main-doc-footer.md"" %}

"
"# On Prem Manual Flexible Model Deployment Xai

This page outlines how to upload a model artifact or surrogate if you have an on-prem deployment of Fiddler and your deployment doesn't give k8s permissions to create model deployment pods dynamically.

> üìò Note
>
> Follow this page if you want to upload a model artifact or a surrogate model. For monitoring only models, without artifact uploaded, this is not required.

## Permissions

Model surrogate or artifact upload is going to create a model deployment pod dynamically, one per model with all required requirements to run the model.

Fiddler requires permission to perform CRUD operations on k8s resources like deployment and service. If this permission is not provided, then manual provision is required to spin up the required k8s resources.

In addition, Fiddler offers the ability to run `pip install` at runtime to install additional dependencies not included in the chosen image. If this permission is not provided for your deployment, please reach out to"
" the Fiddler team with the list of required of dependencies for your model, and we will build an image for you.

### Model On-boarding Steps

* Call [add\_artifact](../../../Python\_Client\_3-x/api-methods-30.md#add\_artifact) with MANUAL deployment type in the [DeploymentParams](../../../Python\_Client\_3-x/api-methods-30.md#deploymentparams) object. The model artifact will be stored, but no deployment pod will be created at this stage. Model validation and feature impact computation will not be performed. Model deployment status will be inactive.

```python
# Specify deployment parameters
deployment_params = fdl.DeploymentParams(
        image_uri=""md-base/python/machine-learning:1.4.0"",
        cpu=250,
        memory=512,
  		  replicas=1)

# Add model artifact
job = model.add_artifact(
  model_dir =  str, #path to your model dirctory with"
" model artifacts and package.py 
  deployment_param = DeploymentParams | None,
) -> AsyncJob
job.wait()
```

* Manually create Model Deployment k8s resources. Please check the [next section](on-prem-manual-flexible-model-deployment-xai.md#instructions-to-manually-create-model-deployment-k8s-resources) and follow the instructions to create the required k8s resources.
* Call [update\_model\_deployment](../../../Python\_Client\_3-x/api-methods-30.md#update-model-deployment) with the parameter `active=True`. This step will use the model deployment pod previously created, add the model files, validate the model deployment, and compute global feature impact. Model will be active and available for XAI features after this step.

```python
model_deployment.cpu = 300
model_deployment.active = True
model_deployment.update()
```

## Instructions to Manually Create Model Deployment k8s Resources

Fiddler"
" will provide a script to create `service.yaml` and `deployment.yaml` files. Customers can review those files and manually apply those on their deployment. A list of environment variable has to be defined in order to run the script.

```shell
./deploy-model-deployment.sh
```

| Environment variables                 | Description                                                         |
| ------------------------------------- | ------------------------------------------------------------------- |
| ENDPOINT                              | Fiddler URL for your deployment                                     |
| TOKEN                                 | Fiddler Token                                                       |
| ORGANIZATION\_NAME                    | The name of the Fiddler organization                                |
| PROJECT\_NAME                         | The name of the project where the MODEL\_NAME is located in Fiddler |
| MODEL\_NAME                           | The name of the model to create resources for                       |
| IMAGE\_PULL\_SECRET\_NAME             | The image pull secret name (Optional)                               |
| MODEL\_DEPLOYMENT\_EXTRA\_ANNOTATIONS | Custom extra annotations (Optional)                                 |
| MODEL\_"
"DEPLOYMENT\_EXTRA\_LABELS      | Custom extra labels (Optional)                                      |

{% include ""../../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Dashboards
slug: dashboards-platform
excerpt: ''
createdAt: Tue Feb 21 2023 22:34:44 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 21:23:18 GMT+0000 (Coordinated Universal Time)
---

# Dashboards

### Overview

With Fiddler, you can create comprehensive dashboards that bring together all of your monitoring data in one place. This includes monitoring charts for data drift, traffic, data integrity, and performance metrics. Adding monitoring charts to your dashboards lets you create a detailed view of your model's performance. These dashboards can inform your team, management, or stakeholders, and help make data-driven decisions that improve your AI performance. View a list of the [**available metrics for monitoring charts here**](monitoring-charts-platform.md#supported-metric-types).

![Model monitoring dashboard including performance, drift, traffic, custom metrics, and segment analysis.](../../.gitbook/assets/dashboard-chart-churn-example.png)

### Key Features

Dashboards offer a powerful way to analyze the overall health and performance of your models, as well as to compare multiple models.

#### Dashboard Filters

* [Flexible filters](../../UI\_Guide/dashboards-ui/#dashboard-filters) including date range, time zone, and bin size to customize your view

#### Chart Utilities

* [Leverage the chart toolbar ](../../UI\_Guide/dashboards-ui/dashboard-interactions.md#zoom)to zoom into data and toggle between line and bar chart types

#### Automatic & Default Dashboards

* Fiddler automatically creates a [monitoring dashboard](../../UI\_Guide/dashboards-ui/#auto-generated-dashboards) for all your models that can be accessed as Insights throughout the product.

#### [Dashboard Basics](../../UI\_Guide/dashboards-ui/dashboard-utilities.md)

* Perform model-to-model comparison
* Plot drift or data integrity for multiple columns in one view
* Easily save and share your dashboard

Checkout more on the [Dashboards UI Guide](../../UI\_Guide/dashboards-ui/).

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Embedding Visualizations
slug: embedding-visualization-with-umap
excerpt: UMAP visualizations for understanding commonality in high dimensional spaces
createdAt: Wed Apr 17 2024 18:40:25 GMT+0000 (Coordinated Universal Time)
updatedAt: Tue Apr 30 2024 17:05:28 GMT+0000 (Coordinated Universal Time)
---

# Embedding Visualization With Umap

### Overview of Embedding Visualizations

![](../../.gitbook/assets/b874b1a-umap.gif)

Embedding visualization is a powerful technique used to understand and interpret complex relationships in high-dimensional data. Reducing the dimensionality of custom features into a 2D or 3D space makes it easier to identify patterns, clusters, and outliers.

In Fiddler, high-dimensional data like embeddings and vectors are ingested as a [Custom feature](../../Python\_Client\_3-x/api-methods-30.md#customfeature).

Our goal in this document is to visualize these custom features.

### UMAP Technique for Embedding Visualization

We utilize the [UMAP](https://umap-learn.readthedocs.io/en/latest/) (Uniform Manifold Approximation and Projection) technique for embedding visualizations. UMAP is a dimension reduction technique that is particularly good at preserving the local structure of the data, making it ideal for visualizing embeddings. We reduce the high-dimensional embeddings to a 3D space.

UMAP is supported for both Text and Image embeddings in [Custom feature](../../Python\_Client\_3-x/api-methods-30.md#customfeaturetype).

> üìò To create an embedding visualization chart
>
> Follow the UI Guide on [creating the embedding visualization chart here.](../../UI\_Guide/monitoring-ui/embedding-visualization-chart-creation.md)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Model Versions
slug: model-versions
excerpt: ''
createdAt: Thu May 02 2024 15:02:32 GMT+0000 (Coordinated Universal Time)
updatedAt: Wed May 08 2024 19:22:59 GMT+0000 (Coordinated Universal Time)
---

# Model Versions

## Overview

Model versions in Fiddler provide a structured approach to managing related models, offering enhanced efficiency in tasks such as model retraining and champion vs. challenger analyses. Rather than creating entirely new model instances for updates, users can opt for creating versions of existing models, maintaining their foundational structure while accommodating necessary changes. These changes can span schema modifications, including column additions or removals, adjustments to data types and value ranges, updates to model specifications, and refinement of task parameters or Explainable AI (XAI) settings.

## Use Cases

Model versions are particularly useful in scenarios where:

* **Model Retraining**: Models require updating with new data or improved algorithms without disrupting existing deployments.
* **Champion vs. Challenger Analyses**: Comparing the performance of different model versions to identify the most effective one for deployment.
* **Historical Tracking**: Maintaining a clear record of model iterations and changes over time for auditing or analysis purposes.

## Capabilities

With model versions, users can:

* **Maintain Model Lineage**: Easily trace the evolution of models by keeping track of different versions and their respective changes.
* **Efficiently Manage Updates**: Streamline the process of updating models by creating new versions with incremental changes, avoiding the need to re-upload entire model instances.
* **Flexibly Modify Schemas**: Modify model schemas, including column structures, data types, and other specifications, to adapt models to evolving requirements.
* **Adjust Parameters**: Refine task parameters, XAI settings, and other configurations to improve explainability, or tailor the task to better suit the model's purpose.
* **Ensure Consistency**: Ensure consistency in model deployments by managing related models within the same versioning system, facilitating comparisons and deployments.

## Example of Creating a Model Version

Utilizing [from\_name()](../../Python\_Client\_3-x/api-methods-30.md#from\_name-3) and [duplicate()](../../Python\_Client\_3-x/api-methods-30.md#duplicate) methods, we can efficiently create a new model version with modifications based on an existing model. First, we retrieve the existing model by specifying its name, project ID, and version. Subsequently, we duplicate this model while updating its version, transitioning, for instance, from `v3` to `v4`. Within the new version (`v4`), we tailor the value ranges of the 'Age' column to meet our requirements. Finally, the [create()](../../Python\_Client\_3-x/api-methods-30.md#create-3) method is invoked to publish the newly minted model version `v4`.

```python
# Define project ID and model name
PROJECT_ID = 'your_project_id'
MODEL_NAME = 'your_model_name'

# Retrieve the existing model version by name and project ID
existing_model = fdl.Model.from_name(name=MODEL_NAME, project_id=PROJECT_ID, version='v3')

# Duplicate the existing model to create a new version
new_model = existing_model.duplicate(version='v4')

# Modify the schema of the new version
new_model.schema['Age'].min = 18
new_model.schema['Age'].max = 60

# Create the new version of the model
new_model.create()
```

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: ""Traffic""
slug: ""traffic-platform""
excerpt: ""Platform Guide""
hidden: false
createdAt: ""Mon Dec 19 2022 19:28:11 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Mon Apr 29 2024 21:09:25 GMT+0000 (Coordinated Universal Time)""
---
## Overview

Traffic as a service metric gives you basic insights into the operational health of your ML service in production.

![](../../.gitbook/assets/854c3c9-image.png)

## What is being tracked?

- **_Traffic_** ‚Äî The volume of traffic received by the model over time.

## Why is it being tracked?

- Traffic is a basic high-level metric that informs us of the overall system's health.

## What steps should I take when I see an outlier?

- A dip or spike in traffic needs to be investigated. For example, a dip could be due to a production model server going down; a spike could be an adversarial attack.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: Custom Metrics
slug: custom-metrics
excerpt: Define ML monitoring metrics tailored to meet your needs
---

# Custom Metrics

### Overview

Custom metrics offer the capability to define metrics that align precisely with your machine learning requirements. Whether it's tracking business KPIs, crafting specialized performance assessments, or computing weighted averages, custom metrics empower you to tailor measurements to your specific needs. Seamlessly integrate these custom metrics throughout Fiddler, leveraging them in dashboards, alerting, and performance tracking.

Create user-defined metrics by employing a simple query language we call [Fiddler Query Language (FQL)](fiddler-query-language.md). FQL enables you to leverage your model's features, metadata, predictions, and outcomes for new data fields using a rich array of aggregations, operators, and metric functions, thereby expanding the depth of your analytical insights.

### How to Define a Custom Metric

Build custom metrics effortlessly with Fiddler's intuitive Excel-formula-like"
" syntax. 
Once a custom metric is defined, Fiddler distinguishes itself by seamlessly managing time granularity and ranges within the charting, dashboarding, and analytics experience. 
This empowers you to effortlessly adjust time range and granularity without the need to modify your query, ensuring a smooth and efficient analytical experience.

Fiddler Custom Metrics are constructed using the [Fiddler Query Language (FQL)](fiddler-query-language.md).

> üìò Custom metrics must return either:
>
> * an aggregate (produced by aggregate functions or built-in metric functions)
> * a combination of aggregates

### Examples

#### Simple Metric

Given this example use case:

> If an event is a false negative, assign a value of -40. If the event is a false positive, assign a value of -400. If the event is a true positive or true negative, then assign a value of 250.

Create a new Custom Metric with the following FQL formula:

```python"
"
average(if(fn(), -40, if(fp(), -400, 250)))
```

Fiddler offers many convenience functions such as `fp()` and `fn()`.\
Alternatively, we could also identify false positives and false negatives the old fashioned way.

```python
average(if(Prediction < 0.5 and Target == 1, -40, if(Prediction >= 0.5 and Target == 0, -400, 250)))
```

Here, we assume `Prediction` is the name of the output column for a binary classifier and `Target` is the name of our label column

#### Tweedie Loss

In our next example, we provide an example implementation of the Tweedie Loss Function. Here, `Target` is the name of the target column and `Prediction` is the name of the prediction/output column.

```python
average((Target \* Prediction ^ (1 - 0.5)) / (1 - "
"0.5) + Prediction ^ (2 - 0.5) / (2 - 0.5))
```

### Adding a Custom Metric

To learn more about adding a Custom Metric using the Python client, see [fdl.CustomMetric](../../Python\_Client\_3-x/api-methods-30.md#custommetric):

```python
METRIC_NAME = 'YOUR_CUSTOM_METRIC_NAME'
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'

PROJECT = fdl.Project.from_name(name=PROJECT_NAME)
MODEL = fdl.Model.from_name(name=MODEL_NAME, project_id=PROJECT.id)

METRIC = fdl.CustomMetric(
        name=METRIC_NAME,
        model_id=MODEL.id,
        definition=""average(if(\""spend_amount\"">1000, \""spend_amount\"", 0))"", #Use Fiddler Query Language (FQL) to define your custom metrics
        description='Get average spend for users spending over"
" $1000',
    ).create()
```

### Modifying Custom Metrics

Since alerts can be set on Custom Metrics, making modifications to a metric may introduce inconsistencies in alerts.

> üöß Therefore, custom metrics cannot be modified once they are created.

If you'd like to try out a new metric, you can create a new one with a different name and definition.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Alerts
slug: alerts-platform
excerpt: ''
createdAt: Fri Jan 27 2023 19:53:56 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 21:09:03 GMT+0000 (Coordinated Universal Time)
---

# Alerts

### Overview

Fiddler enables users to set up alert rules to track a model's health and performance over time. Fiddler alerts also enable users to dig into triggered alerts and perform root cause analysis to discover what is causing a model to degrade. Users can set up alerts using both the [Fiddler UI](../../UI\_Guide/monitoring-ui/alerts-with-fiddler-ui.md) and the [Fiddler API Client](../../Client\_Guide/alerts-with-fiddler-client.md).

### Supported Metric Types

You can get alerts for the following metrics:

* [**Traffic**](../../UI\_Guide/monitoring-ui/"
"traffic-ui.md)
  * The volume of traffic received by the model over time informs us of the overall system's health.
* [**Statistics**](statistics.md)
  * Metrics which can be used to monitor basic aggregations over columns.
* [**Data Drift**](../../UI\_Guide/monitoring-ui/data-drift.md) ‚Äî Predictions and all features
  * Model performance can be poor if models trained on a specific dataset encounter different data in production.
* [**Data Integrity**](../../UI\_Guide/monitoring-ui/data-integrity.md) ‚Äî All features
  * Three types of violations can occur at model inference: missing feature values, type mismatches (e.g. sending a float input for a categorical feature type) or range mismatches (e.g. sending an unknown US State for a State categorical feature).
* [**Performance**](../../UI\_Guide/monitoring-ui/performance.md)
  * The model performance tells us how well"
" a model performs on its task. A poorly performing model can have significant business implications.

### Alert Configurations

#### Comparison Types

There are two options for alert threshold comparison:

* **Absolute** ‚Äî Compare the metric to an absolute value
  * Example: if traffic for a given hour is less than a threshold of 1,000, trigger alert.
* **Relative** ‚Äî Compare the metric to a previous period
  * Example: if traffic is down 10% or more than it was at the same time one week ago, trigger alert.

#### Alert Rule Priority & Severity

* **Priority**: Whether you're setting up an alert rule to keep tabs on a model in a test environment or for production scenarios, Fiddler has you covered. Easily set the Alert Rule Priority to indicate the importance of any given Alert Rule. Users can select from Low, Medium, and High priorities.
* **Severity**: Up to two threshold values can be specified for additional flexibility. A **Critical"
"** severity threshold value is always required when setting up an Alert Rule, and a **Warning** threshold value is optional.

### Why do we need alerts?

* It‚Äôs not possible to manually track all metrics 24/7.
* Sensible alerts are your first line of defense, and they are meant to warn about issues in production.

### What should I do when I receive an alert?

* Click on the link _Inspect the Alert_ link in the email to go to the Alerts Context Chart.
* Under the chart is a summary of the observed alert metric and the Root Cause Analysis which provides detailed information about the selected data point in the chart above.
* You can also examine the data in the Analyze tab. You can use SQL to slice and dice the data, and use custom visualization tools and operators to make sense of the model‚Äôs behavior within the time range under consideration.

### Sample Alert Email

Here is an example of a triggered Performance alert:

![Triggered alert email example]("
"../../.gitbook/assets/alert-email-perf-example.png)

### Integrations

Fiddler supports the following alert notification integrations:

* Email
* Slack
* PagerDuty
* Webhook

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Performance
slug: performance-tracking-platform
excerpt: Platform Guide
createdAt: Mon Dec 19 2022 19:27:22 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 21:15:09 GMT+0000 (Coordinated Universal Time)
---

# Performance Tracking

### Overview

The model performance tells us how well a model performs on its task. A poorly performing model can have significant business implications.

![](../../.gitbook/assets/ffefe4c-image.png)

### What is being tracked?

_**Performance metrics**_

| Model Task Type       | Metric                                                         | Description                                                                                                                                        |
| --------------------- | -------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| Binary Classification | Accuracy                                                       | (TP + TN) / (TP + TN + FP + FN)                                                                                                                    |
| Binary Classification | True Positive Rate/Recall                                      | TP / (TP + FN)                                                                                                                                     |
| Binary Classification |"
" False Positive Rate                                            | FP / (FP + TN)                                                                                                                                     |
| Binary Classification | Precision                                                      | TP / (TP + FP)                                                                                                                                     |
| Binary Classification | F1 Score                                                       | 2 \* ( Precision \* Recall ) / ( Precision + Recall )                                                                                              |
| Binary Classification | AUROC                                                          | Area Under the Receiver Operating Characteristic (ROC) curve, which plots the true positive rate against the false positive rate                   |
| Binary Classification | Binary Cross Entropy                                           | Measures the difference between the predicted probability distribution and the true distribution                                                   |
| Binary Classification | Geometric Mean                                                 | Square Root of ( Precision \* Recall )                                                                                                             |
| Binary Classification | Calibrated Threshold                                           | A threshold that balances precision and recall at a particular operating point                                                                     |
| Binary Classification | Data Count                                                     | The number of events where target and output are both not NULL. _**This will be used as the denominator when calculating accuracy**_.              |
| Binary"
" Classification | Expected Calibration Error                                     | Measures the difference between predicted probabilities and empirical probabilities                                                                |
| Multi Classification  | Accuracy                                                       | (Number of correctly classified samples) / ( Data Count ). Data Count refers to the number of events where the target and output are both not NULL |
| Multi Classification  | Log Loss                                                       | Measures the difference between the predicted probability distribution and the true distribution, in a logarithmic scale                           |
| Regression            | Coefficient of determination (R-squared)                       | Measures the proportion of variance in the dependent variable that is explained by the independent variables                                       |
| Regression            | Mean Squared Error (MSE)                                       | Average of the squared differences between the predicted and true values                                                                           |
| Regression            | Mean Absolute Error (MAE)                                      | Average of the absolute differences between the predicted and true values                                                                          |
| Regression            | Mean Absolute Percentage Error (MAPE)                          | Average of the absolute percentage differences between the predicted and true values                                                               |
|"
" Regression            | Weighted Mean Absolute Percentage Error (WMAPE)                | The weighted average of the absolute percentage differences between the predicted and true values                                                  |
| Ranking               | Mean Average Precision (MAP)‚Äîfor binary relevance ranking only | Measures the average precision of the relevant items in the top-k results                                                                          |
| Ranking               | Normalized Discounted Cumulative Gain (NDCG)                   | Measures the quality of the ranking of the retrieved items, by discounting the relevance scores of items at lower ranks                            |

### Why is it being tracked?

* Model performance tells us how well a model is doing on its task. A poorly performing model can have significant business implications.
* The volume of decisions made on the basis of the predictions give visibility into the business impact of the model.

### What steps should I take based on this information?

* For changes in model performance‚Äîagain, the best way to cross-verify the results is by checking the [Data Drift Tab](../../UI"
"\_Guide/monitoring-ui/data-drift.md) ). Once you confirm that the performance issue is not due to the data, you need to assess if the change in performance is due to temporary factors, or due to longer-lasting issues.
* You can check if there are any lightweight changes you can make to help recover performance‚Äîfor example, you could try modifying the decision threshold.
* Retraining the model with the latest data and redeploying it is usually the solution that yields the best results, although it may be time-consuming and expensive.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Statistics
slug: statistics
excerpt: ''
createdAt: Thu Oct 05 2023 13:28:07 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 21:10:50 GMT+0000 (Coordinated Universal Time)
---

# Statistics

### Overview

Fiddler supports some simple statistic metrics which can be used to monitor basic aggregations over columns. These can be particularly useful when you have a custom metadata field which you would like to monitor over time in addition to Fiddler's other out-of-the-box metrics.

### What is being tracked?

Specifically, we support:

* **Average**: Takes the arithmetic mean of a numeric column
* **Sum**: Calculates the sum of a numeric column
* **Frequency**: Shows the count of occurrences for each value in a categorical or boolean column

### Monitoring Statistic Metrics

#### Charting Statistic Metrics

These metrics can be accessed in Charts and Alerts by selecting the Statistic Metric Type.

![](../../.gitbook/assets/453a99d-Screen\_Shot\_2023-10-26\_at\_1.37.08\_PM.png)

#### Alerting on Statistic Metrics

Alert rules can be established based on statistics too. Like an alert rule, these can be setup using the Fiddler UI, the Fiddler python client or using Fiddler's RESTful API.

![](../../.gitbook/assets/2b19cf0-Screen\_Shot\_2023-12-19\_at\_2.31.43\_PM.png)

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: ML Monitoring
slug: monitoring-platform
excerpt: ''
createdAt: Tue Nov 15 2022 18:06:49 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 18 2024 16:09:17 GMT+0000 (Coordinated Universal Time)
---

# Monitoring

Fiddler Monitoring helps you identify issues with the performance of your ML and LLM models after deployment. Fiddler Monitoring has five out-of-the-box Metric Types in addition to Custom Metrics which can be monitored and alerted on:

1. **Data Drift**
2. **Performance**
3. **Data Integrity**
4. **Traffic**
5. **Statistic**
6. **Custom Metrics**

These metrics can be visualized using Charts and Dashboards and alerted on for swift root cause analysis.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Vector Monitoring
slug: vector-monitoring-platform
excerpt: '""Patented Fiddler Technology""'
createdAt: Mon Dec 19 2022 19:22:52 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 21:17:39 GMT+0000 (Coordinated Universal Time)
---

# Vector Monitoring

Many modern machine learning systems use input features that cannot be represented as a single number (e.g., text or image data). Such complex features are usually rather represented by high-dimensional vectors which are obtained by applying a vectorization method (e.g., text embeddings generated by NLP models). Furthermore, Fiddler users might be interested in monitoring a group of univariate features together and detecting data drift in multi-dimensional feature spaces.

In order to address the above needs, Fiddler provides a vector monitoring capability which involves enabling users to define ""custom features"", and a novel method for monitoring data drift in multi-dimensional"
" spaces.

Custom features can be defined by grouping columns together in the baseline and inference data. Or, in the case of NLP or image data, custom features can be defined using columns containing embedding vectors.

## Defining Custom Features

Users can use the Fiddler client to define one or more custom features. Custom features can be specified by:

1. a group of dataset columns that need to be monitored together as a vector. (CF1, CF2)
2. a column containing an existing embedding vector along with the source column (CF3, CF4)
3. defining an enrichment that will instruct Fiddler to generate the embedding vector within the product itself (CF5, CF6).

Once a list of custom features is defined and passed to Fiddler (the details of how to use the Fiddler client to define custom features are provided below), Fiddler runs a clustering-based data drift detection algorithm for each custom feature and calculates a corresponding drift value between the baseline"
" and the published events at the selected time period.

```python
CF1 = fdl.CustomFeature.from_columns(['f1','f2','f3'], custom_name = 'vector1')
CF2 = fdl.CustomFeature.from_columns(['f1','f2','f3'], n_clusters=5, custom_name = 'vector2')
CF3 = fdl.TextEmbedding(name='text_embedding',column='embedding_col',source_column='text')
CF4 = fdl.ImageEmbedding(name='image_embedding',column='embedding_col2',source_column='image_url')

CF5 = fdl.Enrichment(
        name='Enrichment Unstructured Embedding',
        enrichment='embedding',
        columns=['doc_col'],
    )
CF6 = fdl.TextEmbedding(
        name='Document TextEmbedding',
        source_column='doc_col',
        column='Enrichment Unstructured Embedding',
        n_tags=10
    )
```

## Passing Custom Features"
" List to ModelSpec

Once the custom features are defined for vector monitoring, they are then defined as a part of the [`fdl.ModelSpec`](../../Python\_Client\_3-x/api-methods-30.md#modelspec) and onboarded to Fiddler.

```python
model_spec = fdl.ModelSpec(
    inputs=[
        'creditscore',
        'geography',
        'gender',
        'age',
        'tenure',
        'balance',
        'numofproducts',
        'hascrcard',
        'isactivemember',
        'estimatedsalary',
        'doc_col'
    ],
    outputs=['predicted_churn'],
    targets=['churn'],
    metadata=['customer_id', 'timestamp']
    custom_features = [CF1,CF2,CF3,CF4,C5,C6]
)

MODEL_NAME = 'my_model'

model = fdl.Model.from_data(
    name=MODEL_NAME,
    project_id=fdl.Project.from"
"_name(PROJECT_NAME).id,
    source=sample_df,
    spec=model_spec,
    task=model_task,
    task_params=task_params,
    event_id_col=id_column,
    event_ts_col=timestamp_column
)

model.create()
```

> üìò Quick Start for NLP Monitoring
>
> Check out our [Quick Start guide for NLP monitoring](../../QuickStart\_Notebooks/simple-nlp-monitoring-quick-start.md) for a fully functional notebook example where we instruct Fiddler to generate the embeddings for unstructured inputs.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Class-Imbalanced Data
slug: class-imbalanced-data
excerpt: ''
createdAt: Tue Jul 05 2022 17:20:48 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 21:19:41 GMT+0000 (Coordinated Universal Time)
---

# Class Imbalanced Data

### Overview

Drift is a measure of how different the production distribution is from the baseline distribution on which the model was trained. In practice, the distributions are approximated using histograms and then compared using divergence metrics like Jensen‚ÄìShannon divergence or Population Stability Index. Generally, when constructing the histograms, every event contributes equally to the bin counts.

However, for scenarios with large class imbalance the minority class‚Äô contribution to the histograms would be minimal. Hence, any change in production distribution with respect to the minority class would not lead to a significant change in the production histograms. Consequently, even if there is a significant change in distribution with respect to the minority class, the drift value would not change significantly.

To solve this issue, Fiddler monitoring provides a way for events to be weighted based on the class distribution. For such models, when computing the histograms, events belonging to the minority class would be up-weighted whereas those belonging to the majority class would be down-weighted.

### Solutions

Fiddler has implemented two solutions for class imbalance use cases.

#### Workflow 1: User provided global class weights

* The user computes the class distribution on baseline data and then provides the class weights via the Model-Info object.
* Class weights can either be manually entered by the user or computed from their dataset
* To tease out drift in a class-imbalanced fraud use case checkout out the [class-imbalanced-notebook](../../QuickStart\_Notebooks/class-imbalance-monitoring-example.md)

#### Workflow 2: User provided event level weights

User provides event level weights as a metadata column in baseline data and provides them while publishing events:

* Users will add a `_weight` column of type metadata in the model's ModelSpec.
* The baseline dataset requires this `_weight` column. Note that all rows must contain valid float values. We expect the user to enforce this assumption.
* Note that the use of weighting parameters requires the presence of a model output in the baseline dataset.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Data Integrity
slug: data-integrity-platform
excerpt: platform guide
createdAt: Mon Dec 19 2022 18:33:03 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 21:13:47 GMT+0000 (Coordinated Universal Time)
---

# Data Integrity

### Overview

ML models are increasingly driven by complex feature pipelines and automated workflows that involve dynamic data. Data is transformed from source to model input which can result in data inconsistencies and errors.

There are three types of violations that can occur at model inference: **missing values**, **type mismatches** (e.g. sending a float input for a categorical feature type) or **range violations** (e.g. sending an unknown US State for a State categorical feature).

You can track all these violations in the Data Integrity tab. The time series shown below tracks the violations of data integrity constraints set up for this model.

### What Is Being"
" Tracked?

![Monitoring chart with missing values, type violations, and range violations](../../.gitbook/assets/dashboard-chart-dataintegrity-example.png)

The time series chart above tracks the violations of data integrity constraints set up for this model. Note that both raw count and percentage are available for data integrity metrics.

* **Any Violation Any Column** ‚Äî The count of missing value violations over all features for a given period of time.
* **% Any Violation Any Column** ‚Äî The count of data type mismatch violations over all features for a given period of time.
* **NULL Count Any Column** ‚Äî The count of range mismatch violations over all features for a given period of time.
* **Range Violation Count Any Column** ‚Äî An aggregation of all the data integrity violations above for a given period of time.
* **Type Violation Count Any Column** ‚Äî An aggregation of all the data integrity violations above for a given period of time.

### Why is it being tracked?

* Data integrity"
" issues can cause incorrect data to flow into the model, which can lead to poor model performance and have a negative impact on the business or end-user experience.

### How does it work?

It can be tedious to set up constraints for individual features when they number in the tens or hundreds. To avoid this, you can provide Fiddler with a baseline dataset that's representative of the data you expect your model to infer on in production. This should be sampled from your model's training set, and can be [uploaded to Fiddler using the Fiddler Python client](../../Python\_Client\_3-x/api-methods-30.md#publish).

Fiddler will automatically generate constraints based on the distribution of data in this dataset.

* **Missing values**: If a feature has no missing values, then the data integrity violation will be set up to trigger when any missing values are seen. Similarly, if the feature has 50% of its values missing, then the data integrity violation will be"
" set up to trigger when more than 50% of the values encountered are missing in a specified time range.
* **Type mismatch**: A data integrity violation will be triggered when the type of a feature value differs from what was specified for that feature in the baseline dataset.
* **Range mismatch**:
  * For categorical features, a data integrity violation will be triggered when it sees any value other than the ones specified in the baseline.
  * For continuous variables, the violation will be triggered if the values are outside the range specified in the baseline.
  * For [vector datatype](../../Python\_Client\_3-x/api-methods-30.md#datatype), a range mismatch will be triggered when a dimension mismatch occurs compared to the expected dimension from the baseline.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Data Drift
slug: data-drift-platform
excerpt: Platform Guide
createdAt: Mon Dec 19 2022 19:26:33 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 21:13:05 GMT+0000 (Coordinated Universal Time)
---

# Data Drift

### Overview

Model performance can be poor if models trained on a specific dataset encounter different data in production. This is called data drift.

![Track data drift for inputs, outputs, and custom features](../../.gitbook/assets/2ab8f7c-image.png)

Track data drift for inputs, outputs, and custom features

### What is being tracked?

Fiddler supports the following:

* _**Drift Metrics**_
  * **Jensen‚ÄìShannon distance (JSD)**
    * A distance metric calculated between the distribution of a field in the baseline dataset and that same distribution for the time period of interest.
    * For more information on JSD, click [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html).
  * **Population Stability Index (PSI)**
    *   A drift metric based on the multinomial classification of a variable into bins or categories. The differences in each bin between the baseline and the time period of interest are then utilized to calculate it as follows:

        > üöß Note
        >
        > There is a possibility that PSI can shoot to infinity. To avoid this, PSI calculation in Fiddler is done such that each bin count is incremented with a base\_count=1. Thus, there might be a slight difference in the PSI values obtained from manual calculations.
* _**Average Values**_ ‚Äì The mean of a field (feature or prediction) over time. This can be thought of as an intuitive drift score.
* _**Drift Analytics**_ ‚Äì You can drill down into the features responsible for the prediction drift using the table at the bottom.
  * _**Feature Impact**_: The contribution of a feature to the model‚Äôs predictions, averaged over the baseline dataset. The contribution is calculated using random ablation feature impact.
  * _**Feature Drift**_: Drift of the feature, calculated using the drift metric of choice.
  * _**Prediction Drift Impact**_: A heuristic calculated using the product of the feature impact and the feature drift. The higher the score, the more this feature is likely to have contributed to the prediction drift.

### Why is it being tracked?

* Data drift is a great proxy metric for **performance decline**, especially if there is delay in getting labels for production events. (e.g. In a credit lending use case, an actual default may happen after months or years.)
* Monitoring data drift also helps you stay informed about **distributional shifts in the data for features of interest**, which could have business implications even if there is no decline in model performance.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Fiddler Query Language (FQL)
slug: fiddler-query-language
excerpt: ''
createdAt: Mon Nov 20 2023 18:46:44 GMT+0000 (Coordinated Universal Time)
updatedAt: Tue Apr 23 2024 15:07:02 GMT+0000 (Coordinated Universal Time)
---

# Fiddler Query Language

## Overview

[Custom Metrics](custom-metrics.md) and [Segments](segments.md) are defined using the **Fiddler Query Language (FQL)**, a flexible set of constants, operators, and functions which can accommodate a large variety of metrics.

## Definitions

| Term               | Definition                                                                             |
| ------------------ | -------------------------------------------------------------------------------------- |
| Row-level function | A function which executes row-wise for a set of data. Returns a value for each row.    |
| Aggregate function | A function which executes across rows. Returns a single value for a given set of rows. |

"
"## FQL Rules

* [Column](../../Python_Client_3-x/api-methods-30.md#column) names can be referenced by name either with double quotes (""my\_column"") or with no quotes (my\_column).
* Single quotes (') are used to represent string values.

## Data Types

FQL distinguishes between three data types:

| Data type | Supported values                                          | Examples                                                        | Supported Model Schema Data Types                                                                                                                                                                       |
| --------- | --------------------------------------------------------- | --------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Number    | Any numeric value (integers and floats are both included) | <p><code>10</code><br><code>2.34</code></p>                     | <p><a href=""../../Python_Client_3-x/api-methods-30.md#datatype""><code>Data.Type.INTEGER</code></a><br><a href=""../../Python_Client_3-x/api-methods-30.md#datatype""><code>DataType.FLOAT</code"
"></a></p>  |
| Boolean   | Only `true` and `false`                                   | <p><code>true</code><br><code>false</code></p>                  | [`DataType.BOOLEAN`](../../Python_Client_3-x/api-methods-30.md#datatype)                                                                                                                                |
| String    | Any value wrapped in single quotes (`'`)                  | <p><code>'This is a string.'</code><br><code>'200.0'</code></p> | <p><a href=""../../Python_Client_3-x/api-methods-30.md#datatype""><code>DataType.CATEGORY</code></a><br><a href=""../../Python_Client_3-x/api-methods-30.md#datatype""><code>DataType.STRING</code></a></p> |

## Constants

| Symbol  | Description                            |
| ------- | -------------------------------------- |
| `true`  | Boolean constant for true expressions  |
| `false` | Boolean constant"
" for false expressions |

## Operators

| Symbol | Description              | Syntax                | Returns   | Examples                                                                                                                                                |
| ------ | ------------------------ | --------------------- | --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `^`    | Exponentiation           | `Number ^ Number`     | `Number`  | <p><code>2.5 ^ 4</code><br><code>(column1 - column2)^2</code></p>                                                                                       |
| `-`    | Unary negation           | `-Number`             | `Number`  | `-column1`                                                                                                                                              |
| `*`    | Multiplication           | `Number * Number`     | `Number`  | <p><code>2 * 10</code><br><code>2 * column1</code><br><code>column1 * column2</code><br><code>sum(column1) * 10</code></p>                              |
| `/`   "
" | Division                 | `Number / Number`     | `Number`  | <p><code>2 / 10</code><br><code>2 / column1</code><br><code>column1 / column2</code><br><code>sum(column1) / 10</code></p>                              |
| `%`    | Modulo                   | `Number % Number`     | `Number`  | <p><code>2 % 10</code><br><code>2 % column1</code><br><code>column1 % column2</code><br><code>sum(column1) % 10</code></p>                              |
| `+`    | Addition                 | `Number + Number`     | `Number`  | <p><code>2 + 2</code><br><code>2 + column1</code><br><code>column1 + column2</code><br"
"><code>average(column1) + 2</code></p>                            |
| `-`    | Subtraction              | `Number - Number`     | `Number`  | <p><code>2 - 2</code><br><code>2 - column1</code><br><code>column1 - column2</code><br><code>average(column1) - 2</code></p>                            |
| `<`    | Less than                | `Number < Number`     | `Boolean` | <p><code>10 &#x3C; 20</code><br><code>column1 &#x3C; 10</code><br><code>column1 &#x3C; column2</code><br><code>average(column2) &#x3C; 5</code></p>     |
| `<=`   | Less than or equal to    | `Number <= Number`   "
" | `Boolean` | <p><code>10 &#x3C;= 20</code><br><code>column1 &#x3C;= 10</code><br><code>column1 &#x3C;= column2</code><br><code>average(column2) &#x3C;= 5</code></p> |
| `>`    | Greater than             | `Number > Number`     | `Boolean` | <p><code>10 > 20</code><br><code>column1 > 10</code><br><code>column1 > column2</code><br><code>average(column2) > 5</code></p>                         |
| `>=`   | Greater than or equal to | `Number >= Number`    | `Boolean` | <p><code>10 >= 20</code><br><code>column1 >= 10</code><br"
"><code>column1 >= column2</code><br><code>average(column2) >= 5</code></p>                     |
| `==`   | Equals                   | `Number == Number`    | `Boolean` | <p><code>10 == 20</code><br><code>column1 == 10</code><br><code>column1 == column2</code><br><code>average(column2) == 5</code></p>                     |
| `!=`   | Does not equal           | `Number != Number`    | `Boolean` | <p><code>10 != 20</code><br><code>column1 != 10</code><br><code>column1 != column2</code><br><code>average(column2) != 5</code></p>                     |
| `not`  | Logical NOT              | `not Boolean`         | `Boolean` | <"
"p><code>not true</code><br><code>not column1</code></p>                                                                                                |
| `and`  | Logical AND              | `Boolean and Boolean` | `Boolean` | <p><code>true and false</code><br><code>column1 and column2</code></p>                                                                                  |
| `or`   | Logical OR               | `Boolean or Boolean`  | `Boolean` | <p><code>true or false</code><br><code>column1 or column2</code></p>                                                                                    |

## Constant functions

| Symbol | Description                                           | Syntax | Returns  | Examples                    |
| ------ | ----------------------------------------------------- | ------ | -------- | --------------------------- |
| `e()`  | Base of the natural logarithm                         | `e()`  | `Number` | `e() == 2.718281828459045`  |
| `pi()` | The ratio of a circle's circumference to its diameter"
" | `pi()` | `Number` | `pi() == 3.141592653589793` |

## Row-level functions

Row-level functions can be applied either to a single value or to a column/row expression (in which case they are mapped element-wise to each value in the column/row expression).

| Symbol                           | Description                                                                                                                                                                                     | Syntax                              | Returns   | Examples                                                                                         |
| -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------- | --------- | ------------------------------------------------------------------------------------------------ |
| `if(condition, value1, value2)`  | <p>Evaluates <code>condition</code> and returns <code>value1</code> if true, otherwise returns <code>value2</code>.<br><code>value1</code> and <code>value2</code> must have the same type.</p> | `if(Boolean, Any, Any)`             | `Any`     | <p><code>if(false, 'yes',"
" 'no') == 'no'</code><br><code>if(column1 == 1, 'yes', 'no')</code></p> |
| `length(x)`                      | Returns the length of string `x`.                                                                                                                                                               | `length(String)`                    | `Number`  | `length('Hello world') == 11`                                                                    |
| `to_string(x)`                   | Converts a value `x` to a string.                                                                                                                                                               | `to_string(Any)`                    | `String`  | <p><code>to_string(42) == '42'</code><br><code>to_string(true) == 'true'</code></p>              |
| `startswith(str, prefix)`        | Returns `true` if `str` starts with `prefix`.                                                                                                                                                   | `startswith(String, String)`        | `Boolean` | `startswith('abcde', 'abc')`                                                                     |
| `substring(str, offset, length)` | Returns"
" a substring of `str` of length `length` from offset `offset`. The first character has an offset of 1.                                                                                   | `substring(String, Number, Number)` | `String`  | `substring('abcde', 2, 3) == 'bcd'`                                                              |
| `match(str, regex)`              | Returns `true` if `str` matches the pattern `regex` in [re2 regular syntax](https://github.com/google/re2/wiki/Syntax).                                                                         | `match(String, String)`             | `Boolean` | `match('abcde', 'a.c.*e')`                                                                       |
| `is_null(x)`                     | Returns `true` if `x` is null, otherwise returns `false`.                                                                                                                                       | `is_null(Any)`                      | `Boolean` | <p><code>is_null('') == true</code><br><code>is_null(""column1"")</code></p>"
"                       |
| `is_not_null(x)`                 | Returns `true` if `x` is not null, otherwise returns `false`.                                                                                                                                   | `is_null(Any)`                      | `Boolean` | <p><code>is_not_null('') == false</code><br><code>`is_not_null(""column1"")</code></p>             |
| `abs(x)`                         | Returns the absolute value of number `x`.                                                                                                                                                       | `abs(Number)`                       | `Number`  | `abs(-3) == 3`                                                                                   |
| `exp(x)`                         | Returns `e^x`, where `e` is the base of the natural logarithm.                                                                                                                                  | `exp(Number)`                       | `Number`  | `exp(1) == 2.718281828459045`                                                                    |
| `log(x)`                         | Returns the natural logarithm (base `e`) of number `x`.                                                                                                                                         |"
" `log(Number)`                       | `Number`  | `log(e) == 1`                                                                                    |
| `log2(x)`                        | Returns the binary logarithm (base `2`) of number `x`.                                                                                                                                          | `log2(Number)`                      | `Number`  | `log2(16) == 4`                                                                                  |
| `log10(x)`                       | Returns the binary logarithm (base `10`) of number `x`.                                                                                                                                         | `log10(Number)`                     | `Number`  | `log10(1000) == 3`                                                                               |
| `sqrt(x)`                        | Returns the positive square root of number `x`.                                                                                                                                                 | `sqrt(Number)`                      | `Number`  | `sqrt(144) == 12`                                                                                |

## Aggregate functions

Every Custom Metric must be wrapped in an aggregate function or be a combination of aggregate functions.

| Symbol       | Description                                                                          | Syntax            | Returns  | Examples"
"                 |
| ------------ | ------------------------------------------------------------------------------------ | ----------------- | -------- | ------------------------ |
| `sum(x)`     | Returns the sum of a numeric column or row expression `x`.                           | `sum(Number)`     | `Number` | `sum(column1 + column2)` |
| `average(x)` | Returns the arithmetic mean/average value of a numeric column or row expression `x`. | `average(Number)` | `Number` | `average(2 * column1)`   |
| `count(x)`   | Returns the number of non-null rows of a column or row expression `x`.               | `count(Any)`      | `Number` | `count(column1)`         |

## Built-in metric functions

| Symbol                               | Description                                                                                                                                                                                                       | Syntax                                                             | Returns  | Examples                                                                                                                                                                                        |
| ------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ | -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `jsd(column, baseline)`              | The Jensen-Shannon"
" distance of column `column` with respect to baseline `baseline`.                                                                                                                               | `jsd(Any, String)`                                                 | `Number` | `jsd(column1, 'my_baseline')`                                                                                                                                                                   |
| `psi(column, baseline)`              | The population stability index of column `column` with respect to baseline `baseline`.                                                                                                                            | `psi(Any, String)`                                                 | `Number` | `psi(column1, 'my_baseline')`                                                                                                                                                                   |
| `null_violation_count(column)`       | Number of rows with null values in column `column`.                                                                                                                                                               | `null_violation_count(Any)`                                        | `Number` | `null_violation_count(column1)`                                                                                                                                                                 |
| `range_violation_count(column)`      | Number of rows with out-of-range values in column `column`.                                                                                                                                                       | `range_violation_count(Any)`                                       | `Number` | `range_violation_count(column1"
")`                                                                                                                                                                |
| `type_violation_count(column)`       | Number of rows with invalid data types in column `column`.                                                                                                                                                        | `type_violation_count(Any)`                                        | `Number` | `type_violation_count(column1)`                                                                                                                                                                 |
| `any_violation_count(column)`        | Number of rows with at least one Data Integrity violation in `column`.                                                                                                                                            | `any_violation_count(Any)`                                         | `Number` | `any_violation_count(column1)`                                                                                                                                                                  |
| `traffic()`                          | Total row count. Includes null rows.                                                                                                                                                                              | `traffic()`                                                        | `Number` | `traffic()`                                                                                                                                                                                     |
| `tp(class)`                          | True positive count. Available for binary classification and multiclass classification models. For multiclass, `class` is used to specify the positive class.                                                     | `tp(class=Optional[String])`                                       | `Number` | <p><code"
">tp()</code><br><code>tp(class='class1')</code></p>                                                                                                                                     |
| `tn(class)`                          | True negative count. Available for binary classification and multiclass classification models. For multiclass, `class` is used to specify the positive class.                                                     | `tn(class=Optional[String])`                                       | `Number` | <p><code>tn()</code><br><code>tn(class='class1')</code></p>                                                                                                                                     |
| `fp(class)`                          | False positive count. Available for binary classification and multiclass classification models. For multiclass, `class` is used to specify the positive class.                                                    | `fp(class=Optional[String])`                                       | `Number` | <p><code>fp()</code><br><code>fp(class='class1')</code></p>                                                                                                                                     |
| `fn(class)`                          | False negative count. Available for binary classification and multiclass classification models. For"
" multiclass, `class` is used to specify the positive class.                                                    | `fn(class=Optional[String])`                                       | `Number` | <p><code>fn()</code><br><code>fn(class='class1')</code></p>                                                                                                                                     |
| `precision(target, threshold)`       | <p>Precision between target and output. Available for binary classification model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p>                       | `precision(target=Optional[Any], threshold=Optional[Number])`      | `Number` | <p><code>precision()</code><br><code>precision(target=column1)</code><br><code>precision(threshold=0.5)</code><br><code>precision(target=column1, threshold=0.5)</code></p>                     |
| `recall(target, threshold)`          | <p>Recall"
" between target and output. Available for binary classification model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p>                          | `recall(target=Optional[Any], threshold=Optional[Number])`         | `Number` | <p><code>recall()</code><br><code>recall(target=column1)</code><br><code>recall(threshold=0.5)</code><br><code>recall(target=column1, threshold=0.5)</code></p>                                 |
| `f1_score(target, threshold)`        | <p>F1 score between target and output. Available for binary classification model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p>                        | `f1_score(target=Optional[Any], threshold=Optional[Number])`       | `Number` | <p><code>f"
"1_score()</code><br><code>f1_score(target=column1)</code><br><code>f1_score(threshold=0.5)</code><br><code>f1_score(target=column1, threshold=0.5)</code></p>                         |
| `fpr(target, threshold)`             | <p>False positive rate between target and output. Available for binary classification model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p>             | `fpr(target=Optional[Any], threshold=Optional[Number])`            | `Number` | <p><code>fpr()</code><br><code>fpr(target=column1)</code><br><code>fpr(threshold=0.5)</code><br><code>fpr(target=column1, threshold=0.5)</code></p>                                             |
| `auroc(target)`                      | <"
"p>Area under the ROC curve between target and output. Available for binary classification model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p>        | `auroc(target=Optional[Any])`                                      | `Number` | <p><code>auroc()</code><br><code>auroc(target=column1)</code></p>                                                                                                                               |
| `geometric_mean(target, threshold)`  | <p>Geometric mean score between target and output. Available for binary classification model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p>            | `geometric_mean(target=Optional[Any], threshold=Optional[Number])` | `Number` | <p><code>geometric_mean()</code><br><code>geometric_mean(target=column1)</code><br><code>ge"
"ometric_mean(threshold=0.5)</code><br><code>geometric_mean(target=column1, threshold=0.5)</code></p> |
| `expected_calibration_error(target)` | <p>Expected calibration error between target and output. Available for binary classification model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p>      | `expected_calibration_error(target=Optional[Any])`                 | `Number` | <p><code>expected_calibration_error()</code><br><code>expected_calibration_error(target=column1)</code></p>                                                                                     |
| `log_loss(target)`                   | <p>Log loss (binary cross entropy) between target and output. Available for binary classification model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p> | `log_loss(target=Optional[Any])`"
"                                   | `Number` | <p><code>log_loss()</code><br><code>log_loss(target=column1)</code></p>                                                                                                                         |
| `calibrated_threshold(target)`       | <p>Optimal threshold value for a high TPR and a low FPR. Available for binary classification model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p>      | `calibrated_threshold(target=Optional[Any])`                       | `Number` | <p><code>calibrated_threshold()</code><br><code>calibrated_threshold(target=column1)</code></p>                                                                                                 |
| `accuracy(target, threshold)`        | <p>Accuracy score between target and outputs. Available for multiclass classification model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p>             | `accuracy(target"
"=Optional[Any], threshold=Optional[Number])`       | `Number` | <p><code>accuracy()</code><br><code>accuracy(target=column1)</code><br><code>accuracy(threshold=0.5)</code><br><code>accuracy(target=column1, threshold=0.5)</code></p>                         |
| `log_loss(target)`                   | <p>Log loss score between target and outputs. Available for multiclass classification model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p>             | `log_loss(target=Optional[Any])`                                   | `Number` | <p><code>log_loss()</code><br><code>log_loss(target=column1)</code></p>                                                                                                                         |
| `r2(target)`                         | <p>R-squared score between target and output. Available for regression model tasks.<br>If <"
"code>target</code> is specified, it will be used in place of the default target column.</p>                            | `r2(target=Optional[Any])`                                         | `Number` | <p><code>r2()</code><br><code>r2(target=column1)</code></p>                                                                                                                                     |
| `mse(target)`                        | <p>Mean squared error between target and output. Available for regression model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p>                         | `mse(target=Optional[Any])`                                        | `Number` | <p><code>mse()</code><br><code>mse(target=column1)</code></p>                                                                                                                                   |
| `mae(target)`                        | <p>Mean absolute error between target and output. Available for regression model tasks.<br>If <code>target</code> is specified, it will be used in"
" place of the default target column.</p>                        | `mae(target=Optional[Any])`                                        | `Number` | <p><code>mae()</code><br><code>mae(target=column1)</code></p>                                                                                                                                   |
| `mape(target)`                       | <p>Mean absolute percentage error between target and output. Available for regression model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p>             | `mape(target=Optional[Any])`                                       | `Number` | <p><code>mape()</code><br><code>mape(target=column1)</code></p>                                                                                                                                 |
| `wmape(target)`                      | <p>Weighted mean absolute percentage error between target and output. Available for regression model tasks.<br>If <code>target</code> is specified, it will be used in place of the default"
" target column.</p>    | `wmape(target=Optional[Any])`                                      | `Number` | <p><code>wmape()</code><br><code>wmape(target=column1)</code></p>                                                                                                                               |
| `map(target)`                        | <p>Mean average precision score. Available for ranking model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p>                                            | `map(target=Optional[Any])`                                        | `Number` | <p><code>map()</code><br><code>map(target=column1)</code></p>                                                                                                                                   |
| `ndcg_mean(target)`                  | <p>Mean normalized discounted cumulative gain score. Available for ranking model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p>                        | `ndcg_mean(target=Optional"
"[Any])`                                  | `Number` | <p><code>ndcg_mean()</code><br><code>ndcg_mean(target=column1)</code></p>                                                                                                                       |
| `query_count(target)`                | <p>Count of ranking queries. Available for ranking model tasks.<br>If <code>target</code> is specified, it will be used in place of the default target column.</p>                                                | `query_count(target=Optional[Any])`                                | `Number` | <p><code>query_count()</code><br><code>query_count(target=column1)</code></p>                                                                                                                   |



{% include ""../../.gitbook/includes/main-doc-dev-footer.md"" %}
"
"---
title: Segments
slug: segments
excerpt: Define ML segments to focus on problematic model cohorts
createdAt: Thu Jan 04 2024 20:06:12 GMT+0000 (Coordinated Universal Time)
updatedAt: Tue May 07 2024 20:18:58 GMT+0000 (Coordinated Universal Time)
---

# Segments

### Overview

A segment, sometimes referred to as a cohort or slice, represents a distinct subset of model values crucial for performance analysis and troubleshooting. Model segments can be defined using various model dimensions, such as specific time periods or sets of features. Analyzing segments proves invaluable for understanding or troubleshooting specific cohorts of interest, particularly in tasks like bias detection, where overarching datasets might obscure statistical intricacies.

### How to Define a Segment

Fiddler makes it easy to define custom segments using either the Fiddler UI or the Fiddler Python client. Instructions for both approaches are covered in more detail below. In either case, Fiddler Segments are constructed using the [Fiddler Query Language (FQL)](fiddler-query-language.md).

You can use any of the constants, operators, and functions mentioned in the page linked above in a Segment definition.

However, every Segment definition must return **a boolean row-level expression**. In other words, each inference will either satisfy the segment expression and thus belong to the segment or it will not.

### Examples

Let us illustrate further by providing a few examples. A segment can be defined by:

* A condition on some column (e.g. `age > 50`)
* A condition on some combination of columns (e.g. `(age / max_age) < 1.0`)

For details on all supported functions, see the [Fiddler Query Language (FQL)](fiddler-query-language.md) page.

### Adding a Segment

To learn more about adding a Segment using the Python client, see [fdl.Segment.create()](../../Python\_Client\_3-x/api-methods-30.md#create-5)

```python
SEGMENT_NAME = 'YOUR_SEGMENT_NAME'
PROJECT_NAME = 'YOUR_PROJECT_NAME'
MODEL_NAME = 'YOUR_MODEL_NAME'

PROJECT = fdl.Project.from_name(name=PROJECT_NAME)
MODEL = fdl.Model.from_name(name=MODEL_NAME, project_id=PROJECT.id)

segment = fdl.Segment(
        name=SEGMENT_NAME,
        model_id=MODEL.id,
        definition=""Age < 60"", #Use Fiddler Query Language (FQL) to define your custom segments
        description='Users with Age under 60',
    ).create()
```

### Applied Segments

When using segments in the UI for Analytics or Monitoring Charts, applied segments offer a flexible way to define segments on the fly for exploratory analysis. These segments are not saved to the model by default but will persist locally if the chart they are applied to is saved.

At any time, an applied segment can be saved to the model. However, once a segment is saved to the model, it cannot be altered.

### Modifying Saved Segments

Since alerts can be set on Segments, making modifications to a Segment may introduce inconsistencies in alerts.

> üöß Therefore, **Saved segments cannot be modified once they are created**.

If you'd like to experiment with a new segment, you can create one with a different definition or use applied segments within charts.

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: Monitoring Charts
slug: monitoring-charts-platform
excerpt: ''
createdAt: Thu Feb 23 2023 22:56:27 GMT+0000 (Coordinated Universal Time)
updatedAt: Mon Apr 29 2024 22:08:06 GMT+0000 (Coordinated Universal Time)
---

# Monitoring Charts

### Overview

Fiddler AI‚Äôs monitoring charts allow you to easily track your models and ensure that they are performing optimally. For any of your models, monitoring charts for data drift, performance, data integrity, or traffic metrics can be displayed using Fiddler Dashboards.

![Plotting traffic on Fiddlers monitoring chart](../../.gitbook/assets/bc3648d-image.png)

### Supported Metric Types

Monitoring charts enable you to plot one of the following metric types for a given model:

* [**Traffic**](../../UI\_Guide/monitoring-ui/traffic-ui.md)
  * The volume of traffic received by the model over time informs us of the overall system's health.
* [**Statistics**](statistics.md)
  * Metrics which can be used to monitor basic aggregations over columns.
* [**Data Drift**](../../UI\_Guide/monitoring-ui/data-drift.md) ‚Äî Predictions and all features
  * Model performance can be poor if models trained on a specific dataset encounter different data in production.
* [**Data Integrity**](../../UI\_Guide/monitoring-ui/data-integrity.md) ‚Äî All features
  * Three types of violations can occur at model inference: missing feature values, type mismatches (e.g. sending a float input for a categorical feature type) or range mismatches (e.g. sending an unknown US State for a State categorical feature).
* [**Performance**](../../UI\_Guide/monitoring-ui/performance.md)
  * The model performance tells us how well a model performs on its task. A poorly performing model can have significant business implications.

### Key Features:

#### Multiple Charting Options

You can [plot up to 20 columns](../../UI\_Guide/monitoring-ui/monitoring-charts-ui.md#chart-metric-queries--filters) and 6 metric queries for a model enabling you to easily perform model-to-model comparisons and plot multiple metrics in a single chart view.

### Embedded Root Cause Analytics

Root cause analysis information covers data drift and data integrity, and performance analytics charts for binary classification, multiclass classification, and regression models.

#### Downloadable CSV Data

You can [easily download a CSV of the raw chart data](../../UI\_Guide/monitoring-ui/monitoring-charts-ui.md#breakdown-summary). This feature allows you to analyze your data further.

#### Advanced Chart Functionality

The monitoring charts feature offers [advanced chart functionalities ](../../UI\_Guide/monitoring-ui/monitoring-charts-ui.md#chart-metric-queries--filters)to provide you with the flexibility to customize your charts and view your data in a way that is most useful to you. Features include:

* Zoom
* Dragging of time ranges
* Toggling between bar and line chart types
* Adjusting the scale between linear and log options
* Adjusting the range of the y-axis

Check out more on the [Monitoring Charts UI Guide](../../UI\_Guide/monitoring-ui/monitoring-charts-ui.md).

{% include ""../../.gitbook/includes/main-doc-footer.md"" %}

"
"---
title: main-doc-dev-footer
---

***

:bulb: Need help? Contact [support](https://fiddlerlabs.zendesk.com/hc/en-us).
"
"---
title: main-doc-footer
---

***

:question: Questions? [Talk](https://www.fiddler.ai/contact-sales) to a product expert or [request](https://www.fiddler.ai/demo) a demo.

:bulb: Need help? Contact [support](https://fiddlerlabs.zendesk.com/hc/en-us).
"
"package.py for R based models```python
import fiddler as fdl
```


```python
print(fdl.__version__)
```

    1.6.2



```python
url = ''
token = ''
org_id = ''

client = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=token, version=2)
```


```python
project_id = 'test_r3'
model_id = 'iris'
dataset_id = 'iris'
```


```python
# client.create_project(project_id=project_id)
```


```python
import pandas as pd
from pathlib import Path
import yaml
```


```python
df = pd.read_csv('test_R/data_r.csv')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>Sepal.Length</th>
      <th>Sepal.Width</th>
      <th>Petal.Length</th>
      <th>Petal.Width</th>
      <th>Species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
  </tbody>
</table>
</div>




```python
dataset_info = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=100)
dataset_info
```




<div style=""border: thin solid rgb(41, 57, 141); padding: 10px;""><h3 style=""text-align: center; margin: auto;"">DatasetInfo
</h3><pre>display_name: 
files: []
</pre><hr>Columns:<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>column</th>
      <th>dtype</th>
      <th>count(possible_values)</th>
      <th>is_nullable</th>
      <th>value_range</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Sepal.Length</td>
      <td>FLOAT</td>
      <td></td>
      <td>False</td>
      <td>4.3 - 7.9</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Sepal.Width</td>
      <td>FLOAT</td>
      <td></td>
      <td>False</td>
      <td>2.0 - 4.4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Petal.Length</td>
      <td>FLOAT</td>
      <td></td>
      <td>False</td>
      <td>1.0 - 6.9</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Petal.Width</td>
      <td>FLOAT</td>
      <td></td>
      <td>False</td>
      <td>0.1 - 2.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Species</td>
      <td>CATEGORY</td>
      <td>3</td>
      <td>False</td>
      <td></td>
    </tr>
  </tbody>
</table>
</div></div>




```python
client.upload_dataset(project_id=project_id, dataset={'baseline': df},
                      dataset_id=dataset_id, info=dataset_info)
```


```python
target = 'Species'
outputs = ['proba_setosa', 'proba_versicolor', 'proba_virginica']
features = list(df.drop(columns=[target]).columns)
    
# Generate ModelInfo
model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id=dataset_id,
    target=target,
    outputs=outputs,
    features=features,
    categorical_target_class_details=['setosa', 'versicolor', 'virginica'],
    model_task=fdl.ModelTask.MULTICLASS_CLASSIFICATION,
)
model_info
```


```python
model_dir = Path('test_R/iris_r')
```


```python
# save model schema
with open(model_dir / 'model.yaml', 'w') as yaml_file:
    yaml.dump({'model': model_info.to_dict()}, yaml_file)
```


```python
%%writefile test_R/iris_r/package.py

from pathlib import Path

import numpy as np
import pandas as pd
import rpy2.robjects as robjects
from rpy2.robjects import numpy2ri, pandas2ri
from rpy2.robjects.packages import importr

pandas2ri.activate()
numpy2ri.activate()
r = robjects.r


class Model:
    """"""
    R Model Loader

    Attributes
    ----------
    model : R object
    """"""

    def __init__(self):
        self.model = None

    def load(self, path):
        """"""
        load the model at `path`
        """"""
        model_rds_path = f'{path}.rds'

        self.model = r.readRDS(model_rds_path)
        
        _ = [importr(dep.strip()) for dep in ['randomForest'] if dep.strip() != '']


        return self

    def predict(self, input_df):
        """"""
        Perform classification on samples in X.

        Parameters
        ----------
        input_df : pandas dataframe, shape (n_samples, n_features)
        Returns
        -------
        pred : array, shape (n_samples)
        """"""

        if self.model is None:
            raise Exception('There is no Model')


        pred = r.predict(self.model, [input_df], type='prob')
        df = pd.DataFrame(np.array(pred), columns=['proba_setosa', 'proba_versicolor', 'proba_virginica'])

        return df


MODEL_PATH = 'iris'
PACKAGE_PATH = Path(__file__).parent


def get_model():
    return Model().load(str(PACKAGE_PATH / MODEL_PATH))
```


```python
client.upload_model_package(artifact_path=model_dir, project_id=project_id, model_id=model_id)
```


```python
client.run_model(project_id=project_id, model_id=model_id, df=df.head())
```


```python
client.run_feature_importance()
```
"
"Once you have added a model on the Fiddler platform using a specific model info object, that is fdl.ModelInfo, you cannot modify aspects such as features, inputs, outputs, model task etc. specified in the model info object. Currently, if you want to change fundamental details about a modelinfo object, then it is advised to create/add a new model with a new modelinfo object."
Custom metrics is an upcoming feature and it is currently not supported.
"Re-uploading in Fiddler essentially means having to delete what was uploaded or ingested earlier, making the updates you want to make, and then following the same steps as before for the specific thing you are looking to upload. So for example, if you want make changes to the baseline dataset you uploaded, you will have to delete the dataset and then make modifications for the datasetinfo object or the dataset itself and then upload the dataset again. As for events, currently there isn't a way for the user to directly delete events. Please contact Fiddler personnell for the same. "
"Currently, only the following fields in [fdl.ModelInfo()](ref:fdlmodelinfo) can be updated:
> 
> - `custom_explanation_names`
> - `preferred_explanation_method`
> - `display_name`
> - `description` "
"AI has been in the limelight thanks to ‚Äårecent AI products like ChatGPT, DALLE- 2, and Stable Diffusion. These breakthroughs reinforce the notion that companies need to double down on their AI strategy and execute on their roadmap to stay ahead of the competition. However, Large Language Models (LLMs) and other generative AI models pose the risk of providing users with inaccurate or biased results, generating adversarial output that‚Äôs harmful to users, and exposing private information used in training. This makes it critical for companies to implement LLMOps practices to ensure generative AI models and LLMs are continuously high-performing, correct, and safe.The Fiddler AI Observability platform helps standardize LLMOps by streamlining LLM workflows from pre-production to production, and creating a continuous feedback loop for improved prompt engineering and LLM fine-tuning.Figure 1: Fiddler AI Observability optimizes LLMs and generative AI for better outcomesPre-production Workflow:Robust evaluation of prompts and models with Fiddler AuditorWe are thrilled to launch Fiddler Auditor today to ensure LLMs perform in a safe and correct fashion.¬†Fiddler Auditor is the first robustness library that leverages LLMs to evaluate robustness of other LLMs. Testing the robustness of LLMs in pre-production is a critical step in LLMOps. It helps identify weaknesses that can result in hallucinations, generate harmful or biased responses, and expose private information. ML and software application teams can now utilize the Auditor to test model robustness by applying perturbations, including adversarial examples, out-of-distribution inputs, and linguistic variations, and obtain a report to analyze the outputs generated by the LLM.A practitioner can evaluate LLMs from OpenAI, Anthropic, and Cohere using the Fiddler Auditor and find areas to improve correctness and performance while minimizing hallucinations. In the example below, we tested OpenAI‚Äôs test-davinci-003 model with the following prompt and the best output it should generate when prompted: Then, we entered five perturbations with linguistic variations, and only one of them generated the desired output as seen in the report below. If the LLM were released for public use as is, users would lose trust in it as the model generates hallucinations for simple paraphrasing, and users could potentially be harmed had they acted on the output generated.Figure 2: Evaluate the robustness of LLMs in a reportThe Fiddler Auditor is on GitHub. Don‚Äôt forget to give us a star if you enjoy using it! ‚≠ê‚ÄçProduction Workflow:Continuous monitoring to ensure optimal experienceTransitioning into production requires continuous monitoring to ensure optimal performance. Earlier this year, we announced how vector monitoring in the Fiddler AI Observability platform can monitor LLM-based embeddings generated by OpenAI, Anthropic, Cohere, and embeddings from other LLMs with a minimal integration effort. Our clustering-based multivariate drift detection algorithm is a novel method for measuring data drift in natural language processing (NLP) and computer vision (CV) models.ML teams can track and share LLM metrics like model performance, latency, toxicity, costs, and other LLM-specific metrics in real-time using custom dashboards and charts. Metrics like toxicity are calculated by using methods from HuggingFace. Early warnings from flexible model monitoring alerts cut through the noise and help teams prioritize on business-critical¬† issues.¬†Figure 3: Track metrics like toxicity in real-time to improve prompt engineering and LLM fine-tuningImproving LLM performance using root cause analysisOrganizations need in-depth visibility into their AI solutions to help improve user satisfaction. Through slice & explain, ML teams can get a 360¬∞ view into the performance of their AI solutions, helping them refine prompt context, and gain valuable inputs for fine-tuning models.Fiddler AI Observability: A Unified Platform for ML and Generative AI¬†Figure 4: The Fiddler AI Observability platformWith these new product enhancements, the Fiddler AI Observability platform is a full stack platform for predictive and generative AI models. ML/AI and engineering teams can standardize their practices for both LLMOps and MLOps through model monitoring, explainable AI, analytics, fairness, and safety.¬†We continue our unwavering mission to partner with companies in their AI journey to build trust into AI. Our product and data science teams have been working with companies that are defining ways to operationalize AI beyond predictive models and successfully implement generative AI models to deliver high performance AI, reduce costs, and be responsible with model governance.We look forward to building more capabilities to help companies standardize their LLMOps and MLOps. "
"LLM means large language model.  A large language model (LLM) is a type of artificial intelligence (AI) algorithm that uses deep learning techniques and massively large data sets to understand, summarize, generate and predict new content."
"The term generative AI, or GenAI, also is closely connected with LLMs, which are, in fact, a type of generative AI that has been specifically architected to help generate text-based content."
"FM, or FMs, means Foundation Models.  Foundation Models are the same as large language models."
