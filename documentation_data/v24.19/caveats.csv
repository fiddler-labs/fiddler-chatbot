text
"package.py for R based models```python
import fiddler as fdl
```


```python
print(fdl.__version__)
```

    1.6.2



```python
url = ''
token = ''
org_id = ''

client = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=token, version=2)
```


```python
project_id = 'test_r3'
model_id = 'iris'
dataset_id = 'iris'
```


```python
# client.create_project(project_id=project_id)
```


```python
import pandas as pd
from pathlib import Path
import yaml
```


```python
df = pd.read_csv('test_R/data_r.csv')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>Sepal.Length</th>
      <th>Sepal.Width</th>
      <th>Petal.Length</th>
      <th>Petal.Width</th>
      <th>Species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
  </tbody>
</table>
</div>




```python
dataset_info = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=100)
dataset_info
```




<div style=""border: thin solid rgb(41, 57, 141); padding: 10px;""><h3 style=""text-align: center; margin: auto;"">DatasetInfo
</h3><pre>display_name: 
files: []
</pre><hr>Columns:<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>column</th>
      <th>dtype</th>
      <th>count(possible_values)</th>
      <th>is_nullable</th>
      <th>value_range</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Sepal.Length</td>
      <td>FLOAT</td>
      <td></td>
      <td>False</td>
      <td>4.3 - 7.9</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Sepal.Width</td>
      <td>FLOAT</td>
      <td></td>
      <td>False</td>
      <td>2.0 - 4.4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Petal.Length</td>
      <td>FLOAT</td>
      <td></td>
      <td>False</td>
      <td>1.0 - 6.9</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Petal.Width</td>
      <td>FLOAT</td>
      <td></td>
      <td>False</td>
      <td>0.1 - 2.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Species</td>
      <td>CATEGORY</td>
      <td>3</td>
      <td>False</td>
      <td></td>
    </tr>
  </tbody>
</table>
</div></div>




```python
client.upload_dataset(project_id=project_id, dataset={'baseline': df},
                      dataset_id=dataset_id, info=dataset_info)
```


```python
target = 'Species'
outputs = ['proba_setosa', 'proba_versicolor', 'proba_virginica']
features = list(df.drop(columns=[target]).columns)
    
# Generate ModelInfo
model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id=dataset_id,
    target=target,
    outputs=outputs,
    features=features,
    categorical_target_class_details=['setosa', 'versicolor', 'virginica'],
    model_task=fdl.ModelTask.MULTICLASS_CLASSIFICATION,
)
model_info
```


```python
model_dir = Path('test_R/iris_r')
```


```python
# save model schema
with open(model_dir / 'model.yaml', 'w') as yaml_file:
    yaml.dump({'model': model_info.to_dict()}, yaml_file)
```


```python
%%writefile test_R/iris_r/package.py

from pathlib import Path

import numpy as np
import pandas as pd
import rpy2.robjects as robjects
from rpy2.robjects import numpy2ri, pandas2ri
from rpy2.robjects.packages import importr

pandas2ri.activate()
numpy2ri.activate()
r = robjects.r


class Model:
    """"""
    R Model Loader

    Attributes
    ----------
    model : R object
    """"""

    def __init__(self):
        self.model = None

    def load(self, path):
        """"""
        load the model at `path`
        """"""
        model_rds_path = f'{path}.rds'

        self.model = r.readRDS(model_rds_path)
        
        _ = [importr(dep.strip()) for dep in ['randomForest'] if dep.strip() != '']


        return self

    def predict(self, input_df):
        """"""
        Perform classification on samples in X.

        Parameters
        ----------
        input_df : pandas dataframe, shape (n_samples, n_features)
        Returns
        -------
        pred : array, shape (n_samples)
        """"""

        if self.model is None:
            raise Exception('There is no Model')


        pred = r.predict(self.model, [input_df], type='prob')
        df = pd.DataFrame(np.array(pred), columns=['proba_setosa', 'proba_versicolor', 'proba_virginica'])

        return df


MODEL_PATH = 'iris'
PACKAGE_PATH = Path(__file__).parent


def get_model():
    return Model().load(str(PACKAGE_PATH / MODEL_PATH))
```


```python
client.upload_model_package(artifact_path=model_dir, project_id=project_id, model_id=model_id)
```


```python
client.run_model(project_id=project_id, model_id=model_id, df=df.head())
```


```python
client.run_feature_importance()
```
"
"Once you have added a model on the Fiddler platform using a specific model info object, that is fdl.ModelInfo, you cannot modify aspects such as features, inputs, outputs, model task etc. specified in the model info object. Currently, if you want to change fundamental details about a modelinfo object, then it is advised to create/add a new model with a new modelinfo object."
Custom metrics is an upcoming feature and it is currently not supported.
"Re-uploading in Fiddler essentially means having to delete what was uploaded or ingested earlier, making the updates you want to make, and then following the same steps as before for the specific thing you are looking to upload. So for example, if you want make changes to the baseline dataset you uploaded, you will have to delete the dataset and then make modifications for the datasetinfo object or the dataset itself and then upload the dataset again. As for events, currently there isn't a way for the user to directly delete events. Please contact Fiddler personnell for the same. "
"Currently, only the following fields in [fdl.ModelInfo()](ref:fdlmodelinfo) can be updated:
> 
> - `custom_explanation_names`
> - `preferred_explanation_method`
> - `display_name`
> - `description` "
"AI has been in the limelight thanks to ‌recent AI products like ChatGPT, DALLE- 2, and Stable Diffusion. These breakthroughs reinforce the notion that companies need to double down on their AI strategy and execute on their roadmap to stay ahead of the competition. However, Large Language Models (LLMs) and other generative AI models pose the risk of providing users with inaccurate or biased results, generating adversarial output that’s harmful to users, and exposing private information used in training. This makes it critical for companies to implement LLMOps practices to ensure generative AI models and LLMs are continuously high-performing, correct, and safe.The Fiddler AI Observability platform helps standardize LLMOps by streamlining LLM workflows from pre-production to production, and creating a continuous feedback loop for improved prompt engineering and LLM fine-tuning.Figure 1: Fiddler AI Observability optimizes LLMs and generative AI for better outcomesPre-production Workflow:Robust evaluation of prompts and models with Fiddler AuditorWe are thrilled to launch Fiddler Auditor today to ensure LLMs perform in a safe and correct fashion. Fiddler Auditor is the first robustness library that leverages LLMs to evaluate robustness of other LLMs. Testing the robustness of LLMs in pre-production is a critical step in LLMOps. It helps identify weaknesses that can result in hallucinations, generate harmful or biased responses, and expose private information. ML and software application teams can now utilize the Auditor to test model robustness by applying perturbations, including adversarial examples, out-of-distribution inputs, and linguistic variations, and obtain a report to analyze the outputs generated by the LLM.A practitioner can evaluate LLMs from OpenAI, Anthropic, and Cohere using the Fiddler Auditor and find areas to improve correctness and performance while minimizing hallucinations. In the example below, we tested OpenAI’s test-davinci-003 model with the following prompt and the best output it should generate when prompted: Then, we entered five perturbations with linguistic variations, and only one of them generated the desired output as seen in the report below. If the LLM were released for public use as is, users would lose trust in it as the model generates hallucinations for simple paraphrasing, and users could potentially be harmed had they acted on the output generated.Figure 2: Evaluate the robustness of LLMs in a reportThe Fiddler Auditor is on GitHub. Don’t forget to give us a star if you enjoy using it! ⭐‍Production Workflow:Continuous monitoring to ensure optimal experienceTransitioning into production requires continuous monitoring to ensure optimal performance. Earlier this year, we announced how vector monitoring in the Fiddler AI Observability platform can monitor LLM-based embeddings generated by OpenAI, Anthropic, Cohere, and embeddings from other LLMs with a minimal integration effort. Our clustering-based multivariate drift detection algorithm is a novel method for measuring data drift in natural language processing (NLP) and computer vision (CV) models.ML teams can track and share LLM metrics like model performance, latency, toxicity, costs, and other LLM-specific metrics in real-time using custom dashboards and charts. Metrics like toxicity are calculated by using methods from HuggingFace. Early warnings from flexible model monitoring alerts cut through the noise and help teams prioritize on business-critical  issues. Figure 3: Track metrics like toxicity in real-time to improve prompt engineering and LLM fine-tuningImproving LLM performance using root cause analysisOrganizations need in-depth visibility into their AI solutions to help improve user satisfaction. Through slice & explain, ML teams can get a 360° view into the performance of their AI solutions, helping them refine prompt context, and gain valuable inputs for fine-tuning models.Fiddler AI Observability: A Unified Platform for ML and Generative AI Figure 4: The Fiddler AI Observability platformWith these new product enhancements, the Fiddler AI Observability platform is a full stack platform for predictive and generative AI models. ML/AI and engineering teams can standardize their practices for both LLMOps and MLOps through model monitoring, explainable AI, analytics, fairness, and safety. We continue our unwavering mission to partner with companies in their AI journey to build trust into AI. Our product and data science teams have been working with companies that are defining ways to operationalize AI beyond predictive models and successfully implement generative AI models to deliver high performance AI, reduce costs, and be responsible with model governance.We look forward to building more capabilities to help companies standardize their LLMOps and MLOps. "
