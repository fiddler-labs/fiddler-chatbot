text
"---
title: ""fdl.FiddlerApi""
slug: ""client-setup""
excerpt: """"
hidden: false
createdAt: ""Fri May 13 2022 14:41:57 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
The Client object is used to communicate with Fiddler.  In order to use the client, you'll need to provide authentication details as shown below.

For more information, see [Authorizing the Client](doc:authorizing-the-client).

[block:parameters]
{
  ""data"": {
    ""h-0"": ""Parameter"",
    ""h-1"": ""Type"",
    ""h-2"": ""Default"",
    ""h-3"": ""Description"",
    ""0-0"": ""url"",
    ""0-1"": ""str"",
    ""0-2"": ""None"",
    ""0-3"": ""The URL used to connect to Fiddler"",
    ""1-0"": ""org_id"",
    ""1-1"": ""str"",
    ""1-2"": ""None"",
    ""1-3"": ""The organization ID for a Fiddler instance. Can be found on the General tab of the Settings page."",
    ""2-0"": ""auth_token"",
    ""2-1"": ""str"",
    ""2-2"": ""None"",
    ""2-3"": ""The authorization token used to authenticate with Fiddler. Can be found on the Credentials tab of the Settings page."",
    ""3-0"": ""proxies"",
    ""3-1"": ""Optional [dict]"",
    ""3-2"": ""None"",
    ""3-3"": ""A dictionary containing proxy URLs."",
    ""4-0"": ""verbose"",
    ""4-1"": ""Optional [bool]"",
    ""4-2"": ""False"",
    ""4-3"": ""If True, client calls will be logged verbosely."",
    ""5-0"": ""verify"",
    ""5-1"": ""Optional  \n[bool]"",
    ""5-2"": ""True"",
    ""5-3"": ""If False, client will allow self-signed SSL certificates from the Fiddler server environment.  If True, the SSL certificates need to be signed by a certificate authority (CA).""
  },
  ""cols"": 4,
  ""rows"": 6,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


> üöß Warning
> 
> If verbose is set to **True**, all information required for debugging will be logged, including the authorization token.

> üìò Info
> 
> To maximize compatibility, **please ensure that your client version matches the server version for your Fiddler instance.**
> 
> When you connect to Fiddler using the code on the right, you'll receive a notification if there is a version mismatch between the client and server.
> 
> You can install a specific version of fiddler-client using pip:  
> `pip install fiddler-client==X.X.X`

```python Connect the Client
import fiddler as fdl

URL = 'https://app.fiddler.ai'
ORG_ID = 'my_org'
AUTH_TOKEN = 'p9uqlkKz1zAA3KAU8kiB6zJkXiQoqFgkUgEa1sv4u58'

client = fdl.FiddlerApi(
"
"slug: ""client-setup""     url=URL,
    org_id=ORG_ID,
    auth_token=AUTH_TOKEN
)
```
```python Connect the Client with self-signed certs
import fiddler as fdl

URL = 'https://app.fiddler.ai'
ORG_ID = 'my_org'
AUTH_TOKEN = 'p9uqlkKz1zAA3KAU8kiB6zJkXiQoqFgkUgEa1sv4u58'

client = fdl.FiddlerApi(
    url=URL,
    org_id=ORG_ID,
    auth_token=AUTH_TOKEN, 
		verify=False
)
```
```Text Connect the Client with Proxies
proxies = {
    'http' : 'http://proxy.example.com:1234',
    'https': 'https://proxy.example.com:5678'
}

client = fdl.FiddlerApi(
    url=URL,
    org_id=ORG_ID,
    auth_token=AUTH_TOKEN, 
		proxies=proxies
)
```

If you want to authenticate with Fiddler without passing this information directly into the function call, you can store it in a file named_ fiddler.ini_, which should be stored in the same directory as your notebook or script.

```python Writing fiddler.ini
%%writefile fiddler.ini

[FIDDLER]
url = https://app.fiddler.ai
org_id = my_org
auth_token = p9uqlkKz1zAA3KAU8kiB6zJkXiQoqFgkUgEa1sv4u58
```

```python Connecting the Client with a fiddler.ini file
client = fdl.FiddlerApi()
```
"
"---
title: ""Customer Churn Prediction""
slug: ""customer-churn-prediction""
excerpt: """"
hidden: false
createdAt: ""Tue May 17 2022 19:12:12 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:05:09 GMT+0000 (Coordinated Universal Time)""
---
Churn prediction is a common use case in the machine learning domain. Churn means ‚Äúleaving the company‚Äù. It is very critical for a business to have an idea about why and when customers are likely to churn. Having a robust and accurate churn prediction model helps businesses to take action to prevent customers from leaving the company. Machine learning models have proved to be effective in detecting churn. However, if left unattended, the performance of churn models can degrade over time leading to losing customers. 

The Fiddler AI Observability platform provides a variety of tools that can be used to monitor, explain, analyze, and improve the performance of your machine learning-based churn model.

In this article we will go over a churn example and how we can mitigate performance degradation in a churn machine learning model.

Refer to the [colab notebook](https://colab.research.google.com/github/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/business-use-cases/churn-usecase/Fiddler_Churn_Use_Case.ipynb) to learn how to -

1. Onboard model on the Fiddler platform
2. Publish events on the Fiddler platform
3. Use the Fiddler API to run explanations

### Example - Model Performance Degradation due to Data Integrity Issues

#### Step 1 - Setting up baseline and publishing production events

Please refer to our [Getting Started guide](doc:product-tour) for a step-by-step walkthrough of how to upload baseline and production data to the Fiddler platform.

#### Step 2 - Monitor Drift

When we check the monitoring dashboard, we notice a drop in the predicted churn value and a rise in the predicted churn drift value. Our next step is to check if this has resulted in a drop in performance.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/2f20fd2-Churn-image1-monitor-drift.png"",
        ""Churn-image1-monitor-drift.png"",
        1999
      ],
      ""align"": ""center"",
      ""caption"": ""Monitor Drift""
    }
  ]
}
[/block]


#### Step 3 - Monitor Performance Metrics

We use **precision, recall, and F1-score** as accuracy metrics for this example. We‚Äôre choosing these metrics as they are suited for classification problems and help us in identifying the number of false positives and false negatives. We notice that although the precision has remained constant, there is a drop in the F1-score and recall, which means that there are a few customers who are likely to churn but the model is not able to predict their outcome correctly. 

There could be a number of reasons for drop in performance, some of them are-

1. Cases of extreme events (Outliers)
2. Data distribution changes
3. Model/Concept drift
4. Pipeline health issues

While **Pipeline health issues** could be due to a component in the Data pipeline failing, the first 3 could be due to changes in data. In order to check that we can go to the **Data Integrity** tab to first check if the incoming data is consistent with the baseline data.

[block:image]
{
  ""images"": [
    {
      ""image"": [
       "
"slug: ""customer-churn-prediction""  ""https://files.readme.io/bb02793-churn-image2-monitor-performance-metrics.png"",
        ""churn-image2-monitor-performance-metrics.png"",
        1999
      ],
      ""align"": ""center"",
      ""caption"": ""Monitor Performance Metrics""
    }
  ]
}
[/block]


#### Step 4 - Data Integrity

Our next step would be to check if this could be due to any data integrity issues. On navigating to the **Data Integrity** tab under the **Monitor** tab, we see that there has been a range violation. On selecting the bins which have the range violations, we notice it is due to the field `numofproducts`. 

It is advised to check all the fields which cause data integrity violations. Since we see a range violation, we can check how much the data has drifted.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/5819966-churn-image3-data-integrity.png"",
        ""churn-image3-data-integrity.png"",
        1999
      ],
      ""align"": ""center"",
      ""sizing"": ""smart"",
      ""caption"": ""Data Integrity""
    }
  ]
}
[/block]


#### Step 5 - Check the impact of drift on ‚Äònumofproducts‚Äô features

Our next step would be to go back to the **Data Drift** tab to measure the amount of drift in the field `numofproducts`. The drift is calculated using **Jensen Shannon Divergence**, which compares the distributions of the two data sets being compared. 

We can select the bin where we see an increase in average value as well as drift. We see a significant increase in the `numofproducts` average value and drift. We can also see there is a difference in the distribution of the baseline and production data which leads to a drift. 

Next step could be to find out if the change in distribution was only for a subsection of data or was it due to other factors like time (seasonality etc.), fault in data reporting (sensor data), change in the unit in which the metric is reported etc.  
Seasonality could be observed by plotting the data across time (provided we have enough data), a fault in data reporting would mean missing values, and change in unit of data would mean change in values for all subsections of data.

In order to investigate if the change was only for a subsection of data, we will go to the **Analyze** tab. We can do this by clicking **Export bin and feature to Analyze**. 

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/1d1a5b3-churn-image4-impact-of-drift.png"",
        ""churn-image4-impact-of-drift.png"",
        1999
      ],
      ""align"": ""center"",
      ""caption"": ""Impact of Drift""
    }
  ]
}
[/block]


#### Step 6 - Root Cause Analysis in the ‚ÄòAnalyze‚Äô tab

In the analyze tab, we will have an auto-generated SQL query based on our selection in the **Monitor** tab, we can also write custom SQL queries to investigate the data. 

We check the distribution of the field `numofproducts` for our selection. We can do this by selecting **Chart Type - Feature Distribution** on the RHS of the tab. 

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/9b1f7d1-Churn-image5-analyze-r"
"slug: ""customer-churn-prediction"" ca-1.png"",
        ""Churn-image5-analyze-rca-1.png"",
        1999
      ],
      ""align"": ""center"",
      ""caption"": ""Root Cause Analysis - 1""
    }
  ]
}
[/block]


We further check the performance of the model for our selection by selecting the **Chart Type - Slice Evaluation**.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/350aef8-Churn-image6-analyze-rca-2.png"",
        ""Churn-image6-analyze-rca-2.png"",
        1578
      ],
      ""align"": ""center"",
      ""caption"": ""Root Cause Analysis - 2""
    }
  ]
}
[/block]


In order to check if the change in the range violation has occurred for a subsection of data, we can plot it against the categorical variable. In our case, we can check distribution of `numofproducts` against `age` and `geography`. For this we can plot a feature correlation plot for two features by querying data and selecting **Chart type - Feature Correlation**.

On plotting the feature correlation plot of `gender` vs `numofprodcuts`, we observe the distribution to be similar.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/4f73274-churn-image6-analyze-rca-2-1.png"",
        ""churn-image6-analyze-rca-2-1.png"",
        512
      ],
      ""align"": ""center"",
      ""caption"": ""Root Cause Analysis - 3""
    }
  ]
}
[/block]


[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/aff3dbf-churn-image6-analyze-rca-2-2.png"",
        ""churn-image6-analyze-rca-2-2.png"",
        464
      ],
      ""align"": ""center"",
      ""caption"": ""Root Cause Analysis - 4""
    }
  ]
}
[/block]


For the sake of this example, let‚Äôs say that state of Hawaii (which is a value in the `geography` field in the data) announced that it has eased restrictions on number of loans, since loans is one of products, our hypothesis is the `numofproducts` would be higher for the state. To test this we will check the feature correlation between `geography` and `numofproducts`.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/e1c31a1-churn-image6-analyze-rca-2-3.png"",
        ""churn-image6-analyze-rca-2-3.png"",
        463
      ],
      ""align"": ""center"",
      ""caption"": ""Root Cause Analysis - 5""
    }
  ]
}
[/block]


We do see higher values for the state of Hawaii as compared to other states. We can further check distribution for the field `numofproducts` just for the state of Hawaii. 

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/6664850-churn-image7--analyze-rca-3.png"",
        ""churn-image7--analyze-rca-3.png"",
        1999
      ],
      ""align"": ""center"",
      ""caption"": ""Root Cause Analysis - 6""
    }
  ]
}
"
"slug: ""customer-churn-prediction"" [/block]


On checking performance for the subset of Hawaii, we see a huge performance drop.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/974b118-churn-image8--analyze-rca-4.png"",
        ""churn-image8--analyze-rca-4.png"",
        1624
      ],
      ""align"": ""center"",
      ""caption"": ""Root Cause Analysis - 7""
    }
  ]
}
[/block]


On the contrary, we see a good performance for the subset of data without the ‚ÄòHawaii‚Äô. 

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/ee29b35-churn-image6-analyze-rca-4-1-1.png"",
        ""churn-image6-analyze-rca-4-1-1.png"",
        924
      ],
      ""align"": ""center"",
      ""caption"": ""Root Cause Analysis - 8""
    }
  ]
}
[/block]


[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/fc54636-churn-image9--analyze-rca-5.png"",
        ""churn-image9--analyze-rca-5.png"",
        1606
      ],
      ""align"": ""center"",
      ""caption"": ""Root Cause Analysis - 9""
    }
  ]
}
[/block]


#### Step 7 - Measuring the impact of the ‚Äònumofproducts‚Äô feature

In order to measure the impact of features - `numofproducts`, we can navigate back to the **Monitor** tab. We can see that the prediction drift impact is highest for `numofproducts` due to its high drift value, which means it is contributing the most to the prediction drift.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/e78d838-churn-image10-impact1.png"",
        ""churn-image10-impact1.png"",
        1999
      ],
      ""align"": ""center"",
      ""caption"": ""Feature Impact - 1""
    }
  ]
}
[/block]


We can further measure the attribution of the feature - `numofproducts` for a single data point. We can select a data point which was incorrectly predicted to not churn (false negative). We can check point explanations for a point from the **Analyze** by running a query or from the **Explain** tab. Below we check point explanations for a data point form analyze tab by clicking the **bulb** symbol from the query results. 

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/026d5cb-churn-image11-impact2.png"",
        ""churn-image11-impact2.png"",
        1654
      ],
      ""align"": ""center"",
      ""caption"": ""Feature Impact - 2""
    }
  ]
}
[/block]


We see that the feature - `numofproducts` attributes significantly towards the data point being predicted not to churn.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/65fd05d-churn-image12-impact3.png"",
        ""churn-image12-impact3.png"",
        1999
      ],
      ""align"": ""center"",
      ""caption"": ""Feature Impact - 3""
    }
  ]
}
[/block]


We"
"slug: ""customer-churn-prediction""  have seen that the performance of the churn model drops due to range violation in one of the features. We can improve the performance by retraining the model with new data but before that we must perform mitigation actions which would help us in preemptively detecting the model performance degradation and inform our retraining frequency.

#### Step 8 - Mitigation Actions

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/6190a63-churn-image13-mitigate.png"",
        ""churn-image13-mitigate.png"",
        1618
      ],
      ""align"": ""center"",
      ""caption"": ""Add to dashboard""
    }
  ]
}
[/block]


1. **Add to dashboard**  
   We can add the chart generated to the dashboard by clicking on **Pin this chart** on the RHS of the Analyze tab. This would help us in monitoring importance aspects of the model.

2. **Add alerts**  
   We can alert users to make sure we are notified the next time there is a performance degradation. For instance, in this example, there was a performance degradation due to range data integrity violation. To mitigate this, we can set up an alert which would notify us in case the percentage range violation exceeds a certain threshold (10% would be a good number in our case). We can also set up alerts on drift values for prediction etc. Check out this [link](doc:alerts-ui) to learn how to set up alerts on Fiddler platform.
"
"---
title: ""Fraud Detection""
slug: ""fraud-detection""
excerpt: ""How to monitor and improve your Fraud Detection ML Models using Fiddler's AI Observability platform""
hidden: false
metadata: 
  title: ""Fraud Detection | Fiddler Docs""
  image: []
  keywords: ""fraud detection""
  robots: ""index""
createdAt: ""Tue Apr 19 2022 20:06:54 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 23:26:54 GMT+0000 (Coordinated Universal Time)""
---
Machine learning-based fraud detection models have been proven to be more effective than humans when it comes to detecting fraud. However, if left unattended, the performance of fraud detection models can degrade over time leading to big losses for the company and dissatisfied customers.  
The **Fiddler AI Observability** platform provides a variety of tools that can be used to monitor, explain, analyze, and improve the performance of your fraud detection model.

## Monitoring

### Drift Detection

- **Class-imbalanced Data** - Fraud use cases suffer from highly imbalanced data. Users can specify model weights on a global or event level to improve drift detection. Please see more information in  [Class-Imbalanced Data](doc:class-imbalanced-data). 

- **Feature Impact** - Tells us the contribution of features to the model's prediction, averaged over the baseline dataset. The contribution is calculated using [random ablation feature impact](https://arxiv.org/pdf/1910.00174.pdf).

- **Feature Drift** - Tells us how much a feature is drifting away from the baseline dataset for the time period of interest. For more information on how drift metrics are calculated, see [Data Drift](doc:data-drift-platform).

- **Prediction Drift Impact** - A heuristic calculated by taking the product of Feature Impact and Feature Drift. The higher the score the more this feature contributed to the prediction value drift.

### Performance Metrics

Accuracy might not be a good measure of model performance in the case of fraud detection as most of the cases are non-fraud. Therefore, we use monitor metrics like: 

1. **Recall** - How many of the non-fraudulent cases were actually detected as fraud? A low recall value might lead to an increased number of cases for review even though all the fraud cases were predicted correctly.
2. **False Positive Rate** - Non-Fraud cases labeled as fraud, high FPR rate leads to dissatisfied customers.

### Data Integrity

- **Range Violations** - This metric shows the percentage of data in the selected production data that has violated the range specified in the baseline data through [`DatasetInfo`](ref:fdldatasetinfo) API.
- **Missing Value Violations** - This metric shows the percentage of missing data for a feature in the selected production data.
- **Type Violations** - This metric shows the percentage of data in the selected production data that has violated the type specified in the baseline data through the DatasetInfo API.

## Explanability

### Point Overview

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/c7249cf-XAI21.gif"",
        ""XAI21.gif"",
        1083
      ],
      ""align"": ""center"",
      ""caption"": ""Point Overview""
    }
  ]
}
[/block]


This tab in the Fiddler AI Observability platform gives an overview for the data point selected. The prediction value for the point along with the strongest positive and negative feature attributions"
"slug: ""fraud-detection"" . We can choose from the explanation types. In the case of fraud detection, we can choose from SHAP, Fiddler SHAP, Mean-reset feature impact, Permutation Feature Impact.

For the data point chosen, ‚Äòcategory‚Äô has the highest positive attribution (35.1%), pushing the prediction value towards fraud, and ‚Äòamt‚Äô has the highest negative attribution(-45.8%), pushing the prediction value towards non-fraud.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/b4704f6-XAI11.png"",
        ""XAI11.png"",
        1807
      ],
      ""align"": ""center"",
      ""caption"": ""Explanation Type""
    }
  ]
}
[/block]


### Feature Attribution

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/9b91f72-XAI22.gif"",
        ""XAI22.gif"",
        1078
      ],
      ""align"": ""center"",
      ""caption"": ""Feature Attribution""
    }
  ]
}
[/block]


The Feature Attribution tab gives us information about how much each feature can be attributed to the prediction value based on the Explanation Type chosen. We can also change the value of a particular feature to measure how much the prediction value changes.  
In the example below we can see that on changing the value of feature ‚Äòamt‚Äô from 110 to 10k the prediction value changes from 0.001 to 0.577 (not fraud to fraud).

### Feature Sensitivity

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/3fba7b9-XAI23.gif"",
        ""XAI23.gif"",
        1073
      ],
      ""align"": ""center"",
      ""caption"": ""Feature Sensitivity""
    }
  ]
}
[/block]


This tab plots the prediction value against the range of values for different features (top n user selected). We can change the value for any feature and measure the resulting prediction sensitivity plot of all other features against the initial sensitivity plot. 

On reducing the value of the ‚Äòamt‚Äô feature below from 331 to 10, we can see that the final prediction sensitivity plot shows a prediction value \< 0.5 for any value of ‚Äòage‚Äô and ‚Äòunique_merchant_card‚Äô. This shows that a lower value for ‚Äòamt‚Äô will result in a prediction value close to 0 (non-fraud)

## Make your Fraud Detections Model better with Fiddler!

Please refer to our [Colab Notebook](https://github.com/fiddler-labs/fiddler-examples/blob/fca984e87ab62b8f0cfb7af4d32192eb22cdc58e/quickstart/Fiddler_Quickstart_Imbalanced_Data.ipynb) for a walkthrough on how to get started with using Fiddler for your fraud detection use case and an interactive demo on usability.

### Overview

It is often the case that a model‚Äôs performance will degrade over time. We can use the Fiddler AI Observability platform to monitor the model‚Äôs performance in production, look at various metrics and also provide explanations to predictions on various data points. In this walkthrough, we will look at a few scenarios common to a fraud model when monitoring for performance. We will show how you would:

1. Get baseline and production data onto the Fiddler Platform
2. Monitor drift for various features
3. Monitor performance metrics associated with fraud detection like recall, false-positive rate
4."
"slug: ""fraud-detection""  Monitor data integrity Issues like range violations
5. Provide point explanations to the mislabelled points
6. Get to the root cause of the issues

### Example - Model Performance Degradation due to Data Integrity Issues

#### Step 1 - Setting up baseline and publishing production events

Please refer to our [`Quick Start Guide`](https://github.com/fiddler-labs/fiddler-examples/blob/fca984e87ab62b8f0cfb7af4d32192eb22cdc58e/quickstart/Fiddler_Quickstart_Simple_Monitoring.ipynb) for a step-by-step walkthrough of how to upload baseline and production data to the Fiddler platform.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/fa8ded4-DatasetReady2.gif"",
        ""DatasetReady2.gif"",
        1064
      ],
      ""align"": ""center"",
      ""caption"": ""Setting up baseline""
    }
  ]
}
[/block]


#### Step 2 - Monitor Drift

Once the production events are published, we can monitor drift for the model output in the ‚Äòdrift‚Äô tab i.e. - pred_is_fraud, which is the probability value of a case being a fraud. Here we can see that the prediction value of pred_is_fraud increased from February 15 to February 16. 

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/2f4bd83-MonitorDrift2.jpg"",
        ""MonitorDrift2.jpg"",
        1221
      ],
      ""align"": ""center"",
      ""caption"": ""Monitor drift""
    }
  ]
}
[/block]


#### Step 3 - Monitor Performance Metrics

Next, To check if the performance has degraded, we can check the performance metrics in the ‚ÄòPerformance‚Äô tab. Here we will monitor the ‚ÄòRecall‚Äô and ‚ÄòFPR‚Äô of the model. We can see that the recall has gone down and FPR has gone up in the same period.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/048968e-ModelPerformance1.png"",
        ""ModelPerformance1.png"",
        2624
      ],
      ""align"": ""center"",
      ""caption"": ""Performance Chart""
    }
  ]
}
[/block]


#### Step 4 - Data Integrity

The performance drop could be due to a change in the quality of the data. To check that we can go to the ‚ÄòData Integrity‚Äô tab to look for Missing Value Violations, Type Violations, Range Violations, etc. We can see the columns ‚ÄòCategory‚Äô suffers range violations. Since this is a ‚Äòcategorical‚Äô column, there is likely a new value that the model did not encounter during training.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/eef991e-DataIntegrity1.png"",
        ""DataIntegrity1.png"",
        3260
      ],
      ""align"": ""center"",
      ""caption"": ""Data Integrity""
    }
  ]
}
[/block]


#### Step 5 - Check the impact of drift

We can go back to the ‚ÄòData Drift‚Äô tab to measure how much the data integrity issue has impacted the prediction. We can select the bin in which the drift increased. The table below shows the Feature Impact, Feature Drift, and Prediction Drift Impact values for the selected bin. We can see that even though"
"slug: ""fraud-detection""  the Feature Impact for ‚ÄòCategory‚Äô value is less than the ‚ÄòAmt‚Äô (Amount) value, because of the drift, its Prediction Drift Impact is more. 

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/328c6b6-DriftImpact1.png"",
        ""DriftImpact1.png"",
        3300
      ],
      ""align"": ""center"",
      ""caption"": ""Drift Impact""
    }
  ]
}
[/block]


We will now move on to check the difference between the production and baseline data for this bin. For this, we can click on ‚ÄòExport bin and feature to Analyze‚Äô. Which will land us on the Analyze tab.

#### Step 6 - Root Cause Analysis in the ‚ÄòAnalyze‚Äô tab

The analyze tab pre-populated the left side of the tab with the query based on our selection. We can also write custom queries to slice the data for analysis.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/31a6110-RCA2.jpg"",
        ""RCA2.jpg"",
        1226
      ],
      ""align"": ""center"",
      ""caption"": ""Analyze Tab""
    }
  ]
}
[/block]


[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/a3e4b27-RCA3.png"",
        ""RCA3.png"",
        1660
      ],
      ""align"": ""center"",
      ""caption"": ""Analyze Query""
    }
  ]
}
[/block]


On the right-hand side of the tab we can build charts on the tabular data based on the results of our custom query. For this RCA we will build a ‚ÄòFeature Distribution‚Äô chart on the ‚ÄòCategory‚Äô column to check the distinct values and also measure the percentage of each value. We can see there are 15 distinct values along with their percentages.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/4996cad-RCA4.png"",
        ""RCA4.png"",
        1634
      ],
      ""align"": ""center"",
      ""caption"": ""Feature Distribution - Production""
    }
  ]
}
[/block]


Next, we will compare the Feature Distribution chart in production data vs the baseline data to find out about the data integrity violation. We can modify the query to obtain data for baseline data and produce a ‚ÄòFeature Distribution‚Äô chart for the same.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/303c243-RCA5.png"",
        ""RCA5.png"",
        1600
      ],
      ""align"": ""center"",
      ""caption"": ""Feature Distribution - Baseline""
    }
  ]
}
[/block]


We can see that the baseline data has just 14 unique values and ‚Äòinsurance‚Äô is not present in baseline data. This ‚ÄòCategory‚Äô value wasn‚Äôt present in the training data and crept in production data likely causing performance degradation.  
Next, we can perform a ‚Äòpoint explanation‚Äô for one such case where the ‚ÄòCategory‚Äô value was ‚ÄòInsurance‚Äô and the prediction was incorrect to measure how much the ‚ÄòCategory‚Äô column contributed to the prediction by looking at its SHAP value.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/c1c1c81-RCA6.png"",
        ""R"
"slug: ""fraud-detection"" CA6.png"",
        1650
      ],
      ""align"": ""center"",
      ""caption"": ""Mislabelled Data Point""
    }
  ]
}
[/block]


We can click on the bulb sign beside the row to produce a point explanation. If we look at example 11, we can see that the output probability value was 0 (predicted as fraud according to the threshold of 0.5) but the actual value was ‚Äònot fraud‚Äô. 

The bulb icon will take us to the ‚ÄòExplain‚Äô tab. Here we can see that the ‚Äòcategory‚Äô value contributed to the model predicting the case as ‚Äòfraud‚Äô.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/16d1150-RCA7.png"",
        ""RCA7.png"",
        3330
      ],
      ""align"": ""center"",
      ""caption"": ""Point Explanation""
    }
  ]
}
[/block]


#### Step 7 - Actions

We discovered that the prediction drift and performance drop were due to the introduction of a new value in the ‚ÄòCategory‚Äô column. We can take steps so that we could identify this kind of issue in the future before it can result in business impact.

##### Setting up Alerts

In the ‚ÄòAnalyze‚Äô tab, we can set up alerts to notify us of as soon as a certain data issue happens. For example, for the case we discussed, we can set up alerts as shown below to alert us when the range violation increases beyond a certain threshold (e.g.-5%).

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/f7ece9a-Alert2.png"",
        ""Alert2.png"",
        1386
      ],
      ""align"": ""center"",
      ""caption"": ""Setting up Alerts""
    }
  ]
}
[/block]


These alerts can further influence the retraining of the ML model, we can retrain the model including the new data so the newly trained model contains the ‚Äòinsurance‚Äô category value. This should result in improved performance.

#### Data Insights

Below we can see the confusion matrix for February 16, 2019 (before drift starts). We can observe a good performance with Recall at 100% and 0.1% FP

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/09c82f2-FraudInsights2.png"",
        ""FraudInsights2.png"",
        1574
      ],
      ""align"": ""center"",
      ""caption"": ""Slice Evaluation - Feb 17""
    }
  ]
}
[/block]


Below we can see the confusion matrix for February 17, 2019 (after drift starts). We can observe a performance drop with Recall at 50% and 9% FP

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/c1c6e39-FraudInsights1.png"",
        ""FraudInsights1.png"",
        1574
      ],
      ""align"": ""center"",
      ""caption"": ""Slice Evaluation - Feb 16""
    }
  ]
}
[/block]


### Conclusion

Undetected fraud cases can lead to losses for the company and customers, not to mention damage reputation and relationship with customers. The Fiddler AI Observability platform can be used to identify the pitfalls in your ML model and mitigate them before they have an impact on your business.

In this walkthrough, we investigated one such issue with"
"slug: ""fraud-detection""  a fraud detection model where a data integrity issue caused the performance of the ML model to drop. 

Fiddler can be used to keep the health of your fraud detection model up by:  

1. Monitoring the drift of the performance metric
2. Monitoring various performance metrics associated with the model
3. Monitoring data integrity issues that could harm the model performance
4. Investigating the features which have drifted/ compromised and analyzing them to mitigate the issue
5. Performing a root cause analysis to identify the exact cause and fix it
6. Diving into point explanations to identify how much the issue has an impact on a particular data point
7. Setting up alerts to make sure the issue does not happen again

We discovered there was an issue with the ‚ÄòCategory‚Äô column, wherein a new value was discovered in the production data. This led to the performance drop in the data likely due to the range violation. We suggest two steps to mitigate this issue:

1. Setting up ‚Äòalerts‚Äô to identify similar issues in data integrity
2. Retraining the ML model after including the new data (with the ground truth labels) to teach the model of the new values
"
"---
title: ""client.update_model_deployment""
slug: ""clientupdate_model_deployment""
excerpt: ""Fine-tune the model deployment based on the scaling requirements""
hidden: false
createdAt: ""Thu Jan 26 2023 15:42:57 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type            | Default | Description                                                            |
| :-------------- | :-------------- | :------ | :--------------------------------------------------------------------- |
| project_id      | str             | None    | The unique identifier for the project.                                 |
| model_id        | str             | None    | The unique identifier for the model.                                   |
| active          | Optional [bool] | None    | Set `False` to scale down model deployment and `True` to scale up.     |
| replicas        | Optional[int]   | None    | The number of replicas running the model.                              |
| cpu             | Optional [int]  | None    | The amount of CPU (milli cpus) reserved per replica.                   |
| memory          | Optional [int]  | None    | The amount of memory (mebibytes) reserved per replica.                 |
| wait            | Optional[bool]  | True    | Whether to wait for the async job to finish (`True`) or not (`False`). |

## Example use cases:

- **Horizontal scaling**: horizontal scaling via replicas parameter. This will create multiple Kubernetes pods internally to handle requests.

  ```python
  PROJECT_NAME = 'example_project'
  MODEL_NAME = 'example_model'


  # Create 3 Kubernetes pods internally to handle requests
  client.update_model_deployment(
      project_id=PROJECT_NAME,
      model_id=MODEL_NAME,
      replicas=3,
  )
  ```

- **Vertical scaling**: Model deployments support vertical scaling via cpu and memory parameters. Some models might need more memory to load the artifacts into memory or process the requests.

  ```python
  PROJECT_NAME = 'example_project'
  MODEL_NAME = 'example_model'

  client.update_model_deployment(
      project_id=PROJECT_NAME,
    	model_id=MODEL_NAME,
      cpu=500,
      memory=1024,
  )
  ```

- **Scale down**: You may want to scale down the model deployments to avoid allocating the resources when the model is not in use. Use active parameters to scale down the deployment.

  ```python
  PROJECT_NAME = 'example_project'
  MODEL_NAME = 'example_model'

  client.update_model_deployment(
      project_id=PROJECT_NAME,
    	model_id=MODEL_NAME,
      active=False,
  )
  ```

- **Scale up**: This will again create the model deployment Kubernetes pods with the resource values available in the database.

  ```python
  PROJECT_NAME = 'example_project'
  MODEL_NAME = 'example_model'

  client.update_model_deployment(
      project_id=PROJECT_NAME,
    	model_id=MODEL_NAME,
      active=True,
  )
  ```

| Return Type | Description                                                        |
| :---------- | :----------------------------------------------------------------- |
| dict        | returns a dictionary, with all related fields for model deployment |

> Supported from server version `23.1` and above with Flexible Model Deployment feature enabled.

```python Response
{
  id: 106548,
  uuid: UUID(""123e4567-e89b-12d3-a456-426614174000""),
  model_id: ""MODEL_NAME"",
  project_id : ""PROJECT_NAME"",
  organization_id: ""ORGANIZATION_NAME"",
"
"slug: ""clientupdate_model_deployment""   artifact_type: ""PYTHON_PACKAGE"",
  deployment_type: ""BASE_CONTAINER"",
  active: True,
  image_uri: ""md-base/python/machine-learning:1.0.0"",
  replicas: 1,
  cpu: 250,
  memory: 512,
  created_by: {
    id: 4839,
    full_name: ""first_name last_name"",
    email: ""example_email@gmail.com"",
  },
  updated_by: {
    id: 4839,
    full_name: ""first_name last_name"",
    email: ""example_email@gmail.com"",
  },
  created_at: datetime(2023, 1, 27, 10, 9, 39, 793829),
  updated_at: datetime(2023, 1, 30, 17, 3, 17, 813865),
  job_uuid: UUID(""539j9630-a69b-98d5-g496-326117174805"")
}
```
"
"---
title: ""client.get_model_deployment""
slug: ""clientget_model_deployment""
excerpt: ""Get model deployment object""
hidden: false
createdAt: ""Thu Jan 26 2023 15:42:31 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type | Default | Description                            |
| :-------------- | :--- | :------ | :------------------------------------- |
| project_id      | str  | None    | The unique identifier for the project. |
| model_id        | str  | None    | The unique identifier for the model.   |

```python
PROJECT_NAME = 'example_project'
MODEL_NAME = 'example_model'

client.get_model_deployment(
    project_id=PROJECT_NAME,
    model_id=MODEL_NAME,
)
```

| Return Type | Description                                                            |
| :---------- | :--------------------------------------------------------------------- |
| dict        | returns a dictionary, with all related fields for the model deployment |

```python Response
{
  id: 106548,
  uuid: UUID(""123e4567-e89b-12d3-a456-426614174000""),
  model_id: ""MODEL_NAME"",
  project_id : ""PROJECT_NAME"",
  organization_id: ""ORGANIZATION_NAME"",
  artifact_type: ""PYTHON_PACKAGE"",
  deployment_type: ""BASE_CONTAINER"",
  active: True,
  image_uri: ""md-base/python/machine-learning:1.0.0"",
  replicas: 1,
  cpu: 250,
  memory: 512,
  created_by: {
    id: 4839,
    full_name: ""first_name last_name"",
    email: ""example_email@gmail.com"",
  },
  updated_by: {
    id: 4839,
    full_name: ""first_name last_name"",
    email: ""example_email@gmail.com"",
  },
  created_at: datetime(2023, 1, 27, 10, 9, 39, 793829),
  updated_at: datetime(2023, 1, 30, 17, 3, 17, 813865),
  job_uuid: UUID(""539j9630-a69b-98d5-g496-326117174805"")
}
```
"
"---
title: ""client.unshare_project""
slug: ""clientunshare_project""
excerpt: ""Unshares a project with a user or team.""
hidden: false
createdAt: ""Wed May 25 2022 15:29:55 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üìò Info
> 
> Administrators and project owners can unshare any project with any user. If you lack the required permissions to unshare a project, contact your organization administrator.

[block:parameters]
{
  ""data"": {
    ""h-0"": ""Input Paraemter"",
    ""h-1"": ""Type"",
    ""h-2"": ""Default"",
    ""h-3"": ""Description"",
    ""0-0"": ""project_id"",
    ""0-1"": ""str"",
    ""0-2"": ""None"",
    ""0-3"": ""The unique identifier for the project."",
    ""1-0"": ""role"",
    ""1-1"": ""str"",
    ""1-2"": ""None"",
    ""1-3"": ""The permissions role being revoked. Can be one of  \n- 'READ'  \n- 'WRITE'  \n- 'OWNER'"",
    ""2-0"": ""user_name"",
    ""2-1"": ""Optional [str]"",
    ""2-2"": ""None"",
    ""2-3"": ""A username with which the project will be revoked. Typically an email address."",
    ""3-0"": ""team_name"",
    ""3-1"": ""Optional [str]"",
    ""3-2"": ""None"",
    ""3-3"": ""A team with which the project will be revoked.""
  },
  ""cols"": 4,
  ""rows"": 4,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


```python Usage
PROJECT_ID = 'example_project'

client.unshare_project(
    project_name=PROJECT_ID,
    role='READ',
    user_name='user@example.com'
)
```
"
"---
title: ""client.list_project_roles""
slug: ""clientlist_project_roles""
excerpt: ""Retrieves the names of users and their permissions roles for a given project.""
hidden: false
createdAt: ""Wed May 25 2022 15:23:56 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Paraemter | Type | Default | Description                            |
| :-------------- | :--- | :------ | :------------------------------------- |
| project_id      | str  | None    | The unique identifier for the project. |

```python Usage
PROJECT_ID = 'example_project'

client.list_project_roles(
    project_id=PROJECT_ID
)
```

| Return Type | Description                                                      |
| :---------- | :--------------------------------------------------------------- |
| dict        | A dictionary of users and their roles for the specified project. |

```python Response
{
    'roles': [
        {
            'user': {
                'email': 'admin@example.com'
            },
            'team': None,
            'role': {
                'name': 'OWNER'
            }
        },
        {
            'user': {
                'email': 'user@example.com'
            },
            'team': None,
            'role': {
                'name': 'READ'
            }
        }
    ]
}
```
"
"---
title: ""client.share_project""
slug: ""clientshare_project""
excerpt: ""Shares a project with a user or team.""
hidden: false
createdAt: ""Wed May 25 2022 15:28:34 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üìò Info
> 
> Administrators can share any project with any user. If you lack the required permissions to share a project, contact your organization administrator.

[block:parameters]
{
  ""data"": {
    ""h-0"": ""Input Paraemter"",
    ""h-1"": ""Type"",
    ""h-2"": ""Default"",
    ""h-3"": ""Description"",
    ""0-0"": ""project_id"",
    ""0-1"": ""str"",
    ""0-2"": ""None"",
    ""0-3"": ""The unique identifier for the project."",
    ""1-0"": ""role"",
    ""1-1"": ""str"",
    ""1-2"": ""None"",
    ""1-3"": ""The permissions role being shared. Can be one of  \n- 'READ'  \n- 'WRITE'  \n- 'OWNER'"",
    ""2-0"": ""user_name"",
    ""2-1"": ""Optional [str]"",
    ""2-2"": ""None"",
    ""2-3"": ""A username with which the project will be shared. Typically an email address."",
    ""3-0"": ""team_name"",
    ""3-1"": ""Optional [str]"",
    ""3-2"": ""None"",
    ""3-3"": ""A team with which the project will be shared.""
  },
  ""cols"": 4,
  ""rows"": 4,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


```python Usage
PROJECT_ID = 'example_project'

client.share_project(
    project_name=PROJECT_ID,
    role='READ',
    user_name='user@example.com'
)
```
"
"---
title: ""client.list_org_roles""
slug: ""clientlist_org_roles""
excerpt: ""Retrieves the names of all users and their permissions roles.""
hidden: false
createdAt: ""Wed May 25 2022 15:21:21 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üöß Warning
> 
> Only administrators can use _client.list_org_roles()_ .

```python Usage
client.list_org_roles()
```

| Return Type | Description                                                |
| :---------- | :--------------------------------------------------------- |
| dict        | A dictionary of users and their roles in the organization. |

```python Response
{
    'members': [
        {
            'id': 1,
            'user': 'admin@example.com',
            'email': 'admin@example.com',
            'isLoggedIn': True,
            'firstName': 'Example',
            'lastName': 'Administrator',
            'imageUrl': None,
            'settings': {'notifyNews': True,
                'notifyAccount': True,
                'sliceTutorialCompleted': True},
            'role': 'ADMINISTRATOR'
        },
        {
            'id': 2,
            'user': 'user@example.com',
            'email': 'user@example.com',
            'isLoggedIn': True,
            'firstName': 'Example',
            'lastName': 'User',
            'imageUrl': None,
            'settings': {'notifyNews': True,
                'notifyAccount': True,
                'sliceTutorialCompleted': True},
            'role': 'MEMBER'
        }
    ],
    'invitations': [
        {
            'id': 3,
            'user': 'newuser@example.com',
            'role': 'MEMBER',
            'invited': True,
            'link': 'http://app.fiddler.ai/signup/vSQWZkt3FP--pgzmuYe_-3-NNVuR58OLZalZOlvR0GY'
        }
    ]
}
```
"
"---
title: ""client.list_teams""
slug: ""clientlist_teams""
excerpt: ""Retrieves the names of all teams and the users and roles within each team.""
hidden: false
createdAt: ""Wed May 25 2022 15:25:05 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
```python Usage
client.list_teams()
```

| Return Type | Description                                                |
| :---------- | :--------------------------------------------------------- |
| dict        | A dictionary containing information about teams and users. |

```python Response
{
    'example_team': {
        'members': [
            {
                'user': 'admin@example.com',
                'role': 'MEMBER'
            },
            {
                'user': 'user@example.com',
                'role': 'MEMBER'
            }
        ]
    }
}
```
"
"---
title: ""About the Fiddler Client 2.0""
slug: ""about-the-fiddler-client""
excerpt: """"
hidden: false
createdAt: ""Mon May 23 2022 15:59:05 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
The Fiddler Client contains many useful methods for sending and receiving data to and from the Fiddler platform.

Fiddler provides a Python Client that allows you to connect to Fiddler directly from a Python notebook or automated pipeline.

Each client function is documented with a description, usage information, and code examples.
"
"---
title: ""About Projects""
slug: ""about-projects""
excerpt: """"
hidden: false
createdAt: ""Mon May 23 2022 16:10:39 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Projects are **used to organize your models and datasets**. Each project can represent a machine learning task (e.g. predicting house prices, assessing creditworthiness, or detecting fraud).

A project **can contain one or more models** (e.g. lin_reg_house_predict, random_forest_house_predict).

For more information on projects, click [here](doc:project-structure).
"
"---
title: ""client.list_projects""
slug: ""clientlist_projects""
excerpt: ""Retrieves the project IDs of all projects accessible by the user.""
hidden: false
createdAt: ""Mon May 23 2022 16:17:06 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
```python Usage
response = client.list_projects()
```

| Return Type | Description                                              |
| :---------- | :------------------------------------------------------- |
| list        | A list containing the project ID string for each project |

```python Response
[
  'project_a',
  'project_b',
  'project_c'
]
```
"
"---
title: ""client.delete_project""
slug: ""clientdelete_project""
excerpt: ""Deletes a specified project.""
hidden: false
createdAt: ""Mon May 23 2022 16:24:42 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters | Type | Default | Description                            |
| :--------------- | :--- | :------ | :------------------------------------- |
| project_id       | str  | None    | The unique identifier for the project. |

```python Usage
PROJECT_ID = 'example_project'

client.delete_project(
    project_id=PROJECT_ID
)
```

| Return Type | Description                                         |
| :---------- | :-------------------------------------------------- |
| bool        | A boolean denoting whether deletion was successful. |

```python Response
True
```

> üöß Caution
> 
> You cannot delete a project without deleting the datasets and the models associated with that project.
"
"---
title: ""client.create_project""
slug: ""clientcreate_project""
excerpt: ""Creates a project using the specified ID.""
hidden: false
createdAt: ""Mon May 23 2022 16:21:29 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters | Type | Default | Description                                                                                                                                                                                                |
| :--------------- | :--- | :------ | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| project_id       | str  | None    | A unique identifier for the project. Must be a lowercase string between 2-30 characters containing only alphanumeric characters and underscores. Additionally, it must not start with a numeric character. |

```python Usage
PROJECT_ID = 'example_project'

client.create_project(
    project_id=PROJECT_ID
)
```

| Return Type | Description                                                                                                     |
| :---------- | :-------------------------------------------------------------------------------------------------------------- |
| dict        | A dictionary mapping project_name to the project ID string specified, once the project is successfully created. |

```python Response
{
    'project_name': 'example_project'
}
```
"
"---
title: ""client.upload_dataset""
slug: ""clientupload_dataset""
excerpt: ""Uploads a dataset from a pandas DataFrame.""
hidden: false
createdAt: ""Mon May 23 2022 18:58:49 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters   | Type                       | Default | Description                                                                                                                                                                                                |
| :----------------- | :------------------------- | :------ | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| project_id         | str                        | None    | The unique identifier for the project.                                                                                                                                                                     |
| dataset            | dict                       | None    | A dictionary mapping dataset slice names to pandas DataFrames.                                                                                                                                             |
| dataset_id         | str                        | None    | A unique identifier for the dataset. Must be a lowercase string between 2-30 characters containing only alphanumeric characters and underscores. Additionally, it must not start with a numeric character. |
| info               | Optional [fdl.DatasetInfo] | None    | The Fiddler [fdl.DatasetInfo()](ref:fdldatasetinfo) object used to describe the dataset.                                                                                                                   |
| size_check_enabled | Optional [bool]            | True    | If True, will issue a warning when a dataset has a large number of rows.                                                                                                                                   |

```python Usage
import pandas as pd

PROJECT_ID = 'example_project'
DATASET_ID = 'example_dataset'

df = pd.read_csv('example_dataset.csv')

dataset_info = fdl.DatasetInfo.from_dataframe(
    df=df
)

client.upload_dataset(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    dataset={
        'baseline': df
    },
    info=dataset_info
)
```

| Return Type | Description                                                     |
| :---------- | :-------------------------------------------------------------- |
| dict        | A dictionary containing information about the uploaded dataset. |

```python Response
{'uuid': '7046dda1-2779-4987-97b4-120e6185cc0b',
 'name': 'Ingestion dataset Upload',
 'info': {'project_name': 'example_model',
  'resource_name': 'acme_data',
  'resource_type': 'DATASET'},
 'status': 'SUCCESS',
 'progress': 100.0,
 'error_message': None,
 'error_reason': None}
```
"
"---
title: ""client.delete_dataset""
slug: ""clientdelete_dataset""
excerpt: ""Deletes a dataset from a project.""
hidden: false
createdAt: ""Mon May 23 2022 19:00:39 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters | Type | Default | Description                            |
| :--------------- | :--- | :------ | :------------------------------------- |
| project_id       | str  | None    | The unique identifier for the project. |
| dataset_id       | str  | None    | A unique identifier for the dataset.   |

```python Usage
PROJECT_ID = 'example_project'
DATASET_ID = 'example_dataset'

client.delete_dataset(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID
)
```

| Return Type | Description                                        |
| :---------- | :------------------------------------------------- |
| str         | A message confirming that the dataset was deleted. |

```python Response
'Dataset deleted example_dataset'
```

> üöß Caution
> 
> You cannot delete a dataset without deleting the models associated with that dataset first.
"
"---
title: ""About Datasets""
slug: ""about-datasets""
excerpt: """"
hidden: false
createdAt: ""Mon May 23 2022 16:27:08 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Datasets (or baseline datasets) are used for making comparisons with production data.

A baseline dataset should be sampled from your model's training set, so it can serve as a representation of what the model expects to see in production.

For more information, see [Uploading a Baseline Dataset](doc:uploading-a-baseline-dataset).

For guidance on how to design a baseline dataset, see [Designing a Baseline Dataset](doc:designing-a-baseline-dataset).
"
"---
title: ""client.list_datasets""
slug: ""clientlist_datasets""
excerpt: ""Retrieves the dataset IDs of all datasets accessible within a project.""
hidden: false
createdAt: ""Mon May 23 2022 16:42:07 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters | Type | Default | Description                            |
| :--------------- | :--- | :------ | :------------------------------------- |
| project_id       | str  | None    | The unique identifier for the project. |

```python Usage
PROJECT_ID = ""example_project""

client.list_datasets(
    project_id=PROJECT_ID
)
```

| Return Type | Description                                               |
| :---------- | :-------------------------------------------------------- |
| list        | A list containing the project ID string for each project. |

```python Response
[
    'dataset_a',
    'dataset_b',
    'dataset_c'
]
```
"
"---
title: ""client.get_dataset_info""
slug: ""clientget_dataset_info""
excerpt: ""Retrieves the DatasetInfo object associated with a dataset.""
hidden: false
createdAt: ""Mon May 23 2022 19:02:48 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters | Type | Default | Description                            |
| :--------------- | :--- | :------ | :------------------------------------- |
| project_id       | str  | None    | The unique identifier for the project. |
| dataset_id       | str  | None    | A unique identifier for the dataset.   |

```python Usage
PROJECT_ID = 'example_project'
DATASET_ID = 'example_dataset'

dataset_info = client.get_dataset_info(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID
)
```

| Return Type     | Description                                                                               |
| :-------------- | :---------------------------------------------------------------------------------------- |
| fdl.DatasetInfo | The [fdl.DatasetInfo()](ref:fdldatasetinfo) object associated with the specified dataset. |

```python Response
#NA
```
"
"---
title: ""CV Monitoring""
slug: ""cv-monitoring""
excerpt: ""Quickstart Notebook""
hidden: false
createdAt: ""2023-01-31T19:44:34.862Z""
updatedAt: ""2023-10-26T00:13:36.905Z""
---
This guide will walk you through the basic steps required to use Fiddler for monitoring computer vision (CV) models. In this notebook we demonstrate how to detect drift in image data using model embeddings using Fiddler's unique Vector Monitoring approach.

Click the following link to get started using Google Colab:

<div class=""colab-box"">
    <a href=""https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/23.5/Fiddler_Quickstart_Image_Monitoring.ipynb"" target=""_blank"">
        <div>
            Open in Google Colab ‚Üí
        </div>
    </a>
    <div>
            <img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" />
    </div>
</div>

# Monitoring Image data using Fiddler Vector Monotoring

In this notebook we present the steps for monitoring images. Fiddler employs a vector-based monitoring approach that can be used to monitor data drift in high-dimensional data such as NLP embeddings, images, video etc. In this notebook we demonstrate how to detect drift in image data using model embeddings.

Fiddler is the pioneer in enterprise Model Performance Management (MPM), offering a unified platform that enables Data Science, MLOps, Risk, Compliance, Analytics, and LOB teams to **monitor, explain, analyze, and improve ML deployments at enterprise scale**. 
Obtain contextual insights at any stage of the ML lifecycle, improve predictions, increase transparency and fairness, and optimize business revenue.

---

You can experience Fiddler's Image monitoring ***in minutes*** by following these quick steps:

1. Connect to Fiddler
2. Load and generate embeddings for CIFAR-10 dataset
3. Upload the vectorized baseline dataset
4. Add metadata about your model 
5. Inject data drift and publish production events
6. Get insights

## Imports


```python
!pip install torch==2.0.0
!pip install torchvision==0.15.1
!pip install -q fiddler-client
```


```python
import numpy as np
import pandas as pd
import random
import time
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torchvision.models import resnet18, ResNet18_Weights
import torchvision
import requests

import fiddler as fdl
print(f""Running Fiddler client version {fdl.__version__}"")
```

# 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using our API client.

---

**We need a few pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your organization ID
3. Your authorization token

The latter two of these can be found by pointing your browser to your Fiddler URL and navigating to the **Settings** page.


```python
URL = ''  # Make sure to include the full URL (including https://).
ORG_ID = ''
AUTH_TOKEN = ''
```

Now just run the following code block to connect to the Fiddler API!


```python
client = fdl.FiddlerApi(
    url=URL,
    org"
"slug: ""cv-monitoring"" _id=ORG_ID,
    auth_token=AUTH_TOKEN,
)
```

Once you connect, you can create a new project by specifying a unique project ID in the client's `create_project` function.


```python
PROJECT_ID = 'image_monitoring'

if not PROJECT_ID in client.list_projects():
    print(f'Creating project: {PROJECT_ID}')
    client.create_project(PROJECT_ID)
else:
    print(f'Project: {PROJECT_ID} already exists')
```

## 2. Generate Embeddings for CIFAR-10 data

In this example, we'll use the popular CIFAR-10 classification dataset and a model based on Resnet-18 architecture. For the purpose of this example we have pre-trained the model. If you'd like to retrain the model you can use the script located here [TODO: Add link]
  
In order to compute data and prediction drift, **Fiddler needs a sample of data that can serve as a baseline** for making comparisons with data in production. When it comes to computing distributional shift for images, Fiddler relies on the model's intermediate representations also known as activations or embeddings. You can read more about our approach [here](https://www.fiddler.ai/blog/monitoring-natural-language-processing-and-computer-vision-models-part-1).

In the the following cells we'll extract these embeddings.


```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Device to be used: {device}')
```

Let us load the pre-trained model


```python
MODEL_URL='https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/models/resnet18_cifar10_epoch5.pth'
MODEL_PATH='resnet18_cifar10_epoch5.pth'

def load_model(device):
    """"""Loads the pre-trained CIFAR-10 model""""""
    model = resnet18()
    model.fc = nn.Sequential(
        nn.Linear(512, 128),
        nn.ReLU(),
        nn.Linear(128, 10),
    )
    
    r = requests.get(MODEL_URL)
    with open(MODEL_PATH,'wb') as f:
        f.write(r.content)
    
    model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(device)))
    model.to(device)
    return model

resnet_model = load_model(device)
```

We'll need the CIFAR-10 dataloaders for this example. Note that running the cell below will download the CIFAR-10 data and load them using torch's dataloaders.


```python
image_transforms = transforms.Compose(
    [
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),
    ]
)
batch_size = 32
trainset = torchvision.datasets.CIFAR10(
    root='./cifar10_data',
    train=True,
    download=True,
    transform=image_transforms
)
trainloader = torch.utils.data.DataLoader(
    trainset,
    batch_size=batch_size,
    shuffle=True, 
    num_workers=2
)

testset = torchvision.datasets.CIFAR10(
    root='./cifar10_data',
    train=False,
    download=True,
    transform=image_transforms
)
testloader = torch.utils.data.DataLoader(
    testset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=2
)
```

***In the cell below we define functions that will extract the 128-dimensional embedding from the FC1 layer of the model***


```python
from copy"
"slug: ""cv-monitoring""  import deepcopy
import matplotlib.pyplot as plt
import numpy as np

import torch
import torch.nn.functional as F
import torchvision.transforms as transforms

torch.manual_seed(0)

CIFAR_CLASSES = (
    'plane', 'car', 'bird', 'cat',
    'deer', 'dog', 'frog',
    'horse', 'ship', 'truck',
)

global view_fc1_output_embeds

def fc1_hook_func(model, input, output):
    global view_fc1_output_embeds
    view_fc1_output_embeds = output

def idx_to_classes(target_arr):
    return [CIFAR_CLASSES[int(i)] for i in target_arr]

def generate_embeddings(model, device, dataloader, n=100_000):
    """"""Generate embeddings for the inout images""""""
    with torch.no_grad():
        model = model.eval()
        fc1_module = model.fc[0]
        fc1_hook = fc1_module.register_forward_hook(fc1_hook_func)
        correct_preds = 0
        images_processed = 0
        try:
            for i, (inputs, labels) in enumerate(dataloader):
                inputs = inputs.to(device)
                labels = labels.to(device)
                outputs = model(inputs)
                outputs_smax = F.softmax(outputs, dim=1)
                _, preds = torch.max(outputs, 1)
                correct_preds += torch.sum(preds == labels.data)
                if i == 0:
                    fc1_embeds = view_fc1_output_embeds.cpu().detach().numpy()
                    output_scores = outputs_smax.cpu().detach().numpy()
                    target = labels.cpu().detach().numpy()
                else:
                    fc1_embeds = np.concatenate((fc1_embeds, view_fc1_output_embeds.cpu().detach().numpy()))
                    output_scores = np.concatenate((output_scores, outputs_smax.cpu().detach().numpy()))
                    target = np.concatenate((target, labels.cpu().detach().numpy()))
                images_processed += outputs.size(0)
                if images_processed >= n:
                    break
        except Exception as e:
            fc1_hook.remove()
            raise
    
    embs = deepcopy(fc1_embeds[:n])
    labels = idx_to_classes(target[:n]) 
    embedding_cols = ['emb_'+str(i) for i in range(128)]
    baseline_embeddings = pd.DataFrame(embs, columns=embedding_cols)
    
    columns_to_combine = baseline_embeddings.columns  
    baseline_embeddings = baseline_embeddings.apply(lambda row: row[columns_to_combine].tolist(), axis=1).to_frame()
    baseline_embeddings = baseline_embeddings.rename(columns={baseline_embeddings.columns[0]: 'embeddings'})
    
    baseline_predictions = pd.DataFrame(output_scores[:n], columns=CIFAR_CLASSES)
    baseline_labels = pd.DataFrame(labels, columns=['target'])
    embeddings_df = pd.concat(
        [baseline_embeddings, baseline_predictions, baseline_labels],
        axis='columns',
        ignore_index=False
    )
    return embeddings_df


def get_cifar_transforms():
    image_transforms = transforms.Compose(
        [
            transforms.ToTensor(),
            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
        ]
    )
    return image_transforms

def get_blur_transforms():
    image_transforms = transforms.Compose(
        [
            transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2)),
            transforms.ToTensor(),
            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),
        ]
"
"slug: ""cv-monitoring""     )
    return image_transforms

def get_brightness_transforms():
    image_transforms = transforms.Compose(
        [
            transforms.ColorJitter(brightness=(0.4, 0.6)),
            transforms.ToTensor(),
            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),
        ]
    )
    return image_transforms

def get_cifar_dataloader(train_data=False, batch_size=32, shuffle_data=False, image_transforms=None):
    if image_transforms is None:
        image_transforms = get_cifar_transforms()
    dataset = torchvision.datasets.CIFAR10(root='./cifar10_data', train=train_data,
                                           download=True, transform=image_transforms)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle_data,
        num_workers=2
    )
    return dataloader

# functions to show an image
def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()
```

We'll now extract the embeddings for training data which will serve as baseline for monitoring.


```python
baseline_df = generate_embeddings(resnet_model, device, trainloader)

# Add a row number as a new column for each cifar10 image and a image_url as hosted by huggingface
baseline_df['image_number'] = baseline_df.reset_index().index
baseline_df['image_url'] = baseline_df.apply(
    lambda row: f""https://datasets-server.huggingface.co/assets/cifar10/--/plain_text/train/{row['image_number']}/img/image.jpg"", 
    axis=1
)
baseline_df.head(5)
```

# 3. Upload the vectorized baseline dataset to Fiddler

Next, let's create a [DatasetInfo](https://docs.fiddler.ai/reference/fdldatasetinfo) object to describe our baseline dataset and then [upload_dataset()](https://docs.fiddler.ai/reference/clientupload_dataset) to Fiddler.


```python
DATASET_ID = 'cifar10_baseline'  # The dataset name in Fiddler platform
dataset_info = fdl.DatasetInfo.from_dataframe(baseline_df, max_inferred_cardinality=100)

if not DATASET_ID in client.list_datasets(project_id=PROJECT_ID):
    print(f'Uploading dataset {DATASET_ID}')
    
    client.upload_dataset(
        project_id=PROJECT_ID,
        dataset_id=DATASET_ID,
        dataset={'baseline': baseline_df},
        info=dataset_info
    )
    print('Finished uploading the baseline dataset.')
else:
    print(f'Dataset: {DATASET_ID} already exists in Project: {PROJECT_ID}.\n'
               'The new dataset is not uploaded. (please use a different name.)')
```

# 4. Add metadata about the model

Next we must tell Fiddler a bit more about our model.  This is done either by calling [.register_model()](https://docs.fiddler.ai/reference/clientregister_model) or [.add_model()](https://docs.fiddler.ai/reference/clientadd_model).  This notebook will use [.add_model()](https://docs.fiddler.ai/reference/clientadd_model) When calling [.add_model()](https://docs.fiddler.ai/reference/clientadd_model), we must pass in a [model_info](https://docs.fiddler.ai/reference/fdlmodel"
"slug: ""cv-monitoring"" info) object to tell Fiddler about our model.  This [model_info](https://docs.fiddler.ai/reference/fdlmodelinfo) object will tell Fiddler about our model's task, inputs, output, target and which features form the image embedding.

Let's first define our Image vector using the API below.


```python
image_embedding_feature = fdl.ImageEmbedding(
    name='image_feature',
    source_column='image_url',
    column='embeddings',
)
```

Now let's define our [model_info](https://docs.fiddler.ai/reference/fdlmodelinfo) object.


```python
# This is a multi-class classification problem
model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION
# name of the column that contains ground truth
features = ['embeddings']
metadata = ['image_url']
target = 'target'

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id=DATASET_ID,
    features=features,
    target=target,
    outputs=CIFAR_CLASSES,
    custom_features=[image_embedding_feature],
    model_task=model_task,
    metadata_cols=metadata,
    description='An example to showcase monitoring Image data using model embeddings.',
    categorical_target_class_details=list(CIFAR_CLASSES),
)
model_info
```

Now we specify a unique model ID and use the client's [add_model](https://docs.fiddler.ai/reference/clientadd_model) function to send this information to Fiddler.


```python
MODEL_ID = 'resnet18'

client.add_model(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    model_id=MODEL_ID,
    model_info=model_info,
)
```

# 5. Inject data drift and publish production events

Netx, we'll inject data drift in form of blurring and brightness-reduction. The following cell illustrates these transforms.


```python
drift_xform_lut = {
    'original': None,
    'blurred': get_blur_transforms(),
    'brightness_reduced': get_brightness_transforms(),
}
for drift_type, xform in drift_xform_lut.items():
    cifar_testloader = get_cifar_dataloader(train_data=False, batch_size=32, image_transforms=xform)
    # get some test images
    dataiter = iter(cifar_testloader)
    images, labels = next(dataiter)

    # show images
    print(f'Image type: {drift_type}')
    imshow(torchvision.utils.make_grid(images))
```

### Publish events to Fiddler

We'll publish events over past 3 weeks. 

- Week 1: We publish CIFAR-10 test set, which would signify no distributional shift
- Week 2: We publish **blurred** CIFAR-10 test set 
- Week 3: We publish **brightness reduce** CIFAR-10 test set 


```python
import time

for i, drift_type in enumerate(['original', 'blurred', 'brightness_reduced']):
    week_days = 6
    xform = drift_xform_lut[drift_type]
    cifar_testloader = get_cifar_dataloader(train_data=False, batch_size=32, image_transforms=xform)
    prod_df = generate_embeddings(resnet_model, device, cifar_testloader)
    week_offset = (2-i)*7*24*60*60*1e3 
    day_offset = 24*60*60*1e3
    print(f'Publishing events with {drift_type} transformation for week {i}.')
   "
"slug: ""cv-monitoring""  for day in range(week_days): 
        now = time.time() * 1000
        timestamp = int(now - week_offset - day*day_offset)
        events_df = prod_df.sample(1000)
        events_df['timestamp'] = timestamp
        client.publish_events_batch(
            project_id=PROJECT_ID,
            model_id=MODEL_ID,
            batch_source=events_df,
            timestamp_field='timestamp',
        )
```

## 6. Get insights

**You're all done!**
  
You can now head to Fiddler URL and start getting enhanced observability into your model's performance.

Fiddler can now track your image drift over time based on the embedding vectors of the images published into the platform.  Please visit your Fiddler environment upon completion to check this out for yourself.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/image_monitoring_1.png"" />
        </td>
    </tr>
</table>



---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
title: ""Explainability with Model Artifact""
slug: ""explainability-with-model-artifact-quickstart-notebook""
excerpt: ""Quickstart Notebook""
hidden: false
createdAt: ""2022-12-13T22:00:20.384Z""
updatedAt: ""2023-10-26T00:14:04.069Z""
---
This guide will walk you through the basic steps required to onboard a model in Fiddler with its model artifact.  When Fiddler is provided with the actual model artifact, it can produce high-fidelity explanations. In contrast, models within Fiddler that use a surrogate model or no model artifact at all provide approximative explainability or no explainability at all.

Click the following link to get started using Google Colab:

<div class=""colab-box"">
    <a href=""https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/23.5/Fiddler_Quickstart_Add_Model_Artifact.ipynb"" target=""_blank"">
        <div>
            Open in Google Colab ‚Üí
        </div>
    </a>
    <div>
            <img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" />
    </div>
</div>

# Adding a Model Artifact

In this notebook, we present the steps for onboarding a model with its model artifact.  When Fiddler is provided with your real model artifact, it can produce high-fidelity explanations.  In contrast, models within Fiddler that use a surrogate model or no model artifact at all provide approximative explainability or no explainability at all.

Fiddler is the pioneer in enterprise Model Performance Management (MPM), offering a unified platform that enables Data Science, MLOps, Risk, Compliance, Analytics, and LOB teams to **monitor, explain, analyze, and improve ML deployments at enterprise scale**. 
Obtain contextual insights at any stage of the ML lifecycle, improve predictions, increase transparency and fairness, and optimize business revenue.

---

You can experience Fiddler's NLP monitoring ***in minutes*** by following these five quick steps:

1. Connect to Fiddler
2. Upload a baseline dataset
3. Upload a model package directory containing the **1) package.py and 2) model artifact**
4. Publish production events
5. Get insights (including high-fidelity explainability, or XAI!)

# 0. Imports


```python
!pip install -q fiddler-client

import fiddler as fdl
import pandas as pd
import yaml
import datetime
import time
from IPython.display import clear_output

print(f""Running Fiddler client version {fdl.__version__}"")
```

# 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using our Python client.

---

**We need a few pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your organization ID
3. Your authorization token

The latter two of these can be found by pointing your browser to your Fiddler URL and navigating to the **Settings** page.


```python
URL = ''  # Make sure to include the full URL (including https://).
ORG_ID = ''
AUTH_TOKEN = ''
```

Now just run the following code block to connect the client to your Fiddler environment.


```python
client = fdl.FiddlerApi(
    url=URL,
"
"slug: ""explainability-with-model-artifact-quickstart-notebook""     org_id=ORG_ID,
    auth_token=AUTH_TOKEN
)
```

Once you connect, you can create a new project by specifying a unique project ID in the client's [create_project](https://docs.fiddler.ai/reference/clientcreate_project) function.


```python
PROJECT_ID = 'simple_model_artifact_upload'

if not PROJECT_ID in client.list_projects():
    print(f'Creating project: {PROJECT_ID}')
    client.create_project(PROJECT_ID)
else:
    print(f'Project: {PROJECT_ID} already exists')
```

# 2. Upload a baseline dataset

In this example, we'll be considering the case where we're a bank and we have **a model that predicts churn for our customers**.  
  
In order to get insights into the model's performance, **Fiddler needs a small  sample of data that can serve as a baseline** for making comparisons with data in production.


---


*For more information on how to design a baseline dataset, [click here](https://docs.fiddler.ai/docs/designing-a-baseline-dataset).*


```python
PATH_TO_BASELINE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/churn_baseline.csv'

baseline_df = pd.read_csv(PATH_TO_BASELINE_CSV)
baseline_df
```

Fiddler uses this baseline dataset to keep track of important information about your data.
  
This includes **data types**, **data ranges**, and **unique values** for categorical variables.

---

You can construct a [DatasetInfo](https://docs.fiddler.ai/reference/fdldatasetinfo) object to be used as **a schema for keeping track of this information** by running the following code block.


```python
dataset_info = fdl.DatasetInfo.from_dataframe(baseline_df, max_inferred_cardinality=100)
dataset_info
```

Then use the client's [upload_dataset](https://docs.fiddler.ai/reference/clientupload_dataset) function to send this information to Fiddler.
  
*Just include:*
1. A unique dataset ID
2. The baseline dataset as a pandas DataFrame
3. The `DatasetInfo` object you just created


```python
DATASET_ID = 'churn_data'

client.upload_dataset(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    dataset={
        'baseline': baseline_df
    },
    info=dataset_info
)
```

Within your Fiddler environment's UI, you should now be able to see the newly created dataset within your project.

## 3. Upload your model package

Now it's time to upload your model package to Fiddler.  To complete this step, we need to ensure we have 2 assets in a directory.  It doesn't matter what this directory is called, but for this example we will call it **/model**.


```python
import os
os.makedirs(""model"")
```

***Your model package directory will need to contain:***
1. A **package.py** file which explains to Fiddler how to invoke your model's prediction endpoint
2. And the **model artifact** itself
3. A **requirements.txt** specifying which python libraries need by package.py

---

### 3.1.a  Create the **model_info** object 

This is done by creating our [model_info](https://docs.fiddler.ai/reference/fdlmodelinfo) object.



```python
# Specify task
model_task = 'binary'

if model_task == 'regression':
    model_task = fdl.ModelTask.REGRESSION
    
elif model_task == 'binary':
    model"
"slug: ""explainability-with-model-artifact-quickstart-notebook"" _task = fdl.ModelTask.BINARY_CLASSIFICATION

elif model_task == 'multiclass':
    model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION

elif model_task == 'ranking':
    model_task = fdl.ModelTask.RANKING
    
metadata_cols = ['gender']
decision_cols = ['decision']
feature_columns = ['creditscore', 'geography', 'age', 'tenure',
       'balance', 'numofproducts', 'hascrcard', 'isactivemember',
       'estimatedsalary']

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=client.get_dataset_info(PROJECT_ID, DATASET_ID),
    model_task=model_task,
    target='churn', 
    categorical_target_class_details='yes',
    features=feature_columns,
    decision_cols = decision_cols,
    metadata_cols = metadata_cols,
    outputs=['predicted_churn'],
    display_name='Random Forest Model',
    description='This is models customer bank churn'
)

model_info
```

### 3.1.b Add Model Information to Fiddler


```python
MODEL_ID = 'customer_churn_rf'

client.add_model(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    model_id=MODEL_ID,
    model_info=model_info
)
```

### 3.2 Create the **package.py** file

The contents of the cell below will be written into our ***package.py*** file.  This is the step that will be most unique based on model type, framework and use case.  The model's ***package.py*** file also allows for preprocessing transformations and other processing before the model's prediction endpoint is called.  For more information about how to create the ***package.py*** file for a variety of model tasks and frameworks, please reference the [Uploading a Model Artifact](https://docs.fiddler.ai/docs/uploading-a-model-artifact#packagepy-script) section of the Fiddler product documentation.


```python
%%writefile model/package.py

import pandas as pd
from pathlib import Path
import os
from sklearn.ensemble import RandomForestClassifier
import pickle as pkl

 
PACKAGE_PATH = Path(__file__).parent
TARGET = 'churn'
PREDICTION = 'predicted_churn'

class Random_Forest:


    def __init__(self, model_path, output_column=None):
        """"""
        :param model_path: The directory where the model is saved.
        :param output_column: list of column name(s) for the output.
        """"""
        self.model_path = model_path
        self.output_column = output_column
        
       
        file_path = os.path.join(self.model_path, 'model.pkl')
        with open(file_path, 'rb') as file:
            self.model = pkl.load(file)
    
    
    def predict(self, input_df):
        return pd.DataFrame(
            self.model.predict_proba(input_df.loc[:, input_df.columns != TARGET])[:,1], 
            columns=self.output_column)
    

def get_model():
    return Random_Forest(model_path=PACKAGE_PATH, output_column=[PREDICTION])
```

### 3.3  Ensure your model's artifact is in the **/model** directory

Make sure your model artifact (*e.g. the model .pkl file*) is also present in the model package directory as well as any dependencies called out in a *requirements.txt* file.  The following cell will move this model's pkl file and requirements.txt file into our */model* directory.


```python
import urllib.request
urllib.request.urlretrieve(""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main"
"slug: ""explainability-with-model-artifact-quickstart-notebook"" /quickstart/models/model.pkl"", ""model/model.pkl"")
urllib.request.urlretrieve(""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/models/requirements.txt"", ""model/requirements.txt"")
```

### 3.4 Define Model Parameters 

This is done by creating our [DEPLOYMENT_PARAMETERS](https://docs.fiddler.ai/reference/fdldeploymentparams) object.


```python
DEPLOYMENT_PARAMETERS = fdl.DeploymentParams(image_uri=""md-base/python/python-39:1.1.0"",  
                                    cpu=100,
                                    memory=256,
                                    replicas=1)
```

### Finally, upload the model package directory

Once the model's artifact is in the */model* directory along with the **pacakge.py** file and requirments.txt the model package directory can be uploaded to Fiddler.


```python
client.add_model_artifact(model_dir='model/', project_id=PROJECT_ID, model_id=MODEL_ID, deployment_params=DEPLOYMENT_PARAMETERS)
```

Within your Fiddler environment's UI, you should now be able to see the newly created model.

# 4. Publish production events

Your model artifact is uploaded.  Now it's time to start publishing some production data! 

Fiddler will **monitor this data and compare it to your baseline to generate powerful insights into how your model is behaving**.  

With the model artifact available to Fiddler, **high-fidelity explanations are also avaialbe**.


---


Each record sent to Fiddler is called **an event**.  An event is just **a dictionary that maps column names to column values**.
  
Let's load in some sample events from a CSV file.  Then we can create an artificial timestamp for the events and publish them to fiddler one by one in a streaming fashion using the Fiddler client's [publish_event](https://docs.fiddler.ai/reference/clientpublish_event) function.


```python
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/hawaii_drift_demo_large.csv'

event_log = pd.read_csv(PATH_TO_EVENTS_CSV)
event_log
```


```python
NUM_EVENTS_TO_SEND = 11500

FIVE_MINUTES_MS = 300000
ONE_DAY_MS = 8.64e+7
NUM_DAYS_BACK_TO_START=39 #set the start of the event data publishing this many days in the past
start_date = round(time.time() * 1000) - (ONE_DAY_MS * NUM_DAYS_BACK_TO_START) 
print(datetime.datetime.fromtimestamp(start_date/1000.0))
```


```python
def event_generator_df():
    for ind, row in event_log.iterrows():
        event_dict = dict(row)
        event_id = event_dict.pop('event_id')
        event_time = start_date + ind * FIVE_MINUTES_MS #publish an event every FIVE_MINUTES_MS
        yield event_id, event_dict, event_time
        
event_queue_df = event_generator_df()

def get_next_event_df():
    return next(event_queue_df)
```


```python
for ind in range(NUM_EVENTS_TO_SEND):
    event_id_tmp, event_dict, event_time = get_next_event_df()
   
    result = client.publish_event(PROJECT_ID,
                                  MODEL_ID,
                                  event_dict,
                                  event_timestamp=event_time,
                                  event_id= event_id_tmp,
                                  update_event= False)
    
    readable_timestamp = datetime.datetime.fromtimestamp(event_time/1000.0)
    clear_output(wait = True)
    
    print(f"
"slug: ""explainability-with-model-artifact-quickstart-notebook"" 'Sending {ind+1} / {NUM_EVENTS_TO_SEND} \n{readable_timestamp} UTC: \n{event_dict}')
    time.sleep(0.001)
```

# 5. Get insights

**You're all done!**
  
Now just head to your Fiddler environment's UI and start getting enhanced monitoring, analytics, and explainability.



---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
title: ""Simple Monitoring""
slug: ""quick-start""
excerpt: ""Quickstart Notebook""
hidden: false
createdAt: ""2022-08-10T15:11:33.699Z""
updatedAt: ""2023-10-26T00:13:05.678Z""
---
This guide will walk you through the basic onboarding steps required to use Fiddler for model monitoring, **using sample data provided by Fiddler**.  

**Note**: This guide does not upload a model artifact or create a surrogate model, both of which are supported by Fiddler.  As a result, this guide won't allow you to explore explainability within the platform.

Click the following link to get started using Google Colab:

<div class=""colab-box"">
    <a href=""https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/23.5/Fiddler_Quickstart_Simple_Monitoring.ipynb"" target=""_blank"">
        <div>
            Open in Google Colab ‚Üí
        </div>
    </a>
    <div>
            <img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" />
    </div>
</div>

[block:html]
{
  ""html"": ""<div class=\""fiddler-cta\"">\n<a class=\""fiddler-cta-link\"" href=\""https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\""><img src=\""https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\"" alt=\""Fiddler Free Trial\""></a>\n</div>""
}
[/block]

# Fiddler Simple Monitoring Quick Start Guide

Fiddler is the pioneer in enterprise Model Performance Management (MPM), offering a unified platform that enables Data Science, MLOps, Risk, Compliance, Analytics, and other LOB teams to **monitor, explain, analyze, and improve ML deployments at enterprise scale**. 
Obtain contextual insights at any stage of the ML lifecycle, improve predictions, increase transparency and fairness, and optimize business revenue.

---

You can start using Fiddler ***in minutes*** by following these 7 quick steps:

1. Imports
2. Connect to Fiddler
3. Upload a baseline dataset
4. Add metadata about your model with Fiddler
5. Set up Alerts and Notifications (Optional)
6. Publish production events
7. Get insights

**Don't have a Fiddler account? [Sign-up for a 14-day free trial](https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral).**

## 1. Imports


```python
!pip install -q fiddler-client

import numpy as np
import pandas as pd
import time as time
import fiddler as fdl

print(f""Running client version {fdl.__version__}"")
```

## 2. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using our API client.


---


**We need a few pieces of information to get started.**
1. The URL you're using to connect to Fiddler


```python
URL = '' # Make sure to include the full URL (including https://). For example, https://abc.xyz.ai
```

2. Your organization ID
3. Your authorization token

Both of these can be found by clicking the URL you entered and navigating"
"slug: ""quick-start""  to the **Settings** page of your Fiddler environment.


```python
ORG_ID = ''
AUTH_TOKEN = ''
```

Now just run the following to connect the client to your Fiddler environment.


```python
client = fdl.FiddlerApi(
    url=URL, 
    org_id=ORG_ID, 
    auth_token=AUTH_TOKEN
)
```

Once you connect, you can create a new project by specifying a unique project ID in the client's `create_project` function.


```python
PROJECT_ID = 'quickstart_example'

client.create_project(PROJECT_ID)
```

You should now be able to see the newly created project on the UI.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/simple_monitoring_1.png"" />
        </td>
    </tr>
</table>

## 3. Upload a baseline dataset

In this example, we'll be considering the case where we're a bank and we have **a model that predicts churn for our customers**.  
We want to know when our model's predictions start to drift‚Äîthat is, **when churn starts to increase** within our customer base.
  
In order to get insights into the model's performance, **Fiddler needs a small  sample of data that can serve as a baseline** for making comparisons with data in production.


---


*For more information on how to design a baseline dataset, [click here](https://docs.fiddler.ai/docs/designing-a-baseline-dataset).*


```python
PATH_TO_BASELINE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/churn_baseline.csv'

baseline_df = pd.read_csv(PATH_TO_BASELINE_CSV)
baseline_df
```

Fiddler uses this baseline dataset to keep track of important information about your data.
  
This includes **data types**, **data ranges**, and **unique values** for categorical variables.

---

You can construct a `DatasetInfo` object to be used as **a schema for keeping track of this information** by running the following code block.


```python
dataset_info = fdl.DatasetInfo.from_dataframe(baseline_df, max_inferred_cardinality=100)
dataset_info
```

Then use the client's [upload_dataset](https://docs.fiddler.ai/reference/clientupload_dataset) function to send this information to Fiddler!
  
*Just include:*
1. A unique dataset ID
2. The baseline dataset as a pandas DataFrame
3. The [DatasetInfo](https://docs.fiddler.ai/reference/fdldatasetinfo) object you just created


```python
DATASET_ID = 'churn_data'

client.upload_dataset(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    dataset={
        'baseline': baseline_df
    },
    info=dataset_info
)
```

If you click on your project in the Fiddler UI, you should now be able to see the newly onboarded dataset.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/simple_monitoring_2.png"" />
        </td>
    </tr>
</table>

## 4. Add metadata about your model

Now it's time to add your model with Fiddler.  We do this by defining a [ModelInfo](https://docs.fiddler.ai/reference/fdlmodelinfo) object.


---


The [ModelInfo](https"
"slug: ""quick-start"" ://docs.fiddler.ai/reference/fdlmodelinfo) object will contain some **information about how your model operates**.
  
*Just include:*
1. The **task** your model is performing (regression, binary classification, etc.)
2. The **target** (ground truth) column
3. The **output** (prediction) column
4. The **feature** columns
5. Any **metadata** columns
6. Any **decision** columns (these measures the direct business decisions made as result of the model's prediction)



```python
# Specify task
model_task = 'binary'

if model_task == 'regression':
    model_task = fdl.ModelTask.REGRESSION
    
elif model_task == 'binary':
    model_task = fdl.ModelTask.BINARY_CLASSIFICATION

elif model_task == 'multiclass':
    model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION
    
elif model_task == 'ranking':
    model_task = fdl.ModelTask.RANKING

    
# Specify column types
features = ['geography', 'gender', 'age', 'tenure', 'balance', 'numofproducts', 'hascrcard', 'isactivemember', 'estimatedsalary']
outputs = ['predicted_churn']
target = 'churn'
decision_cols = ['decision']
metadata_cols = ['customer_id']
    
# Generate ModelInfo
model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id=DATASET_ID,
    model_task=model_task,
    features=features,
    outputs=outputs,
    target=target,
    categorical_target_class_details='yes',
    decision_cols=decision_cols, # Optional
    metadata_cols=metadata_cols, # Optional
    binary_classification_threshold=0.5 # Optional
)
model_info
```

Almost done! Now just specify a unique model ID and use the client's [add_model](https://docs.fiddler.ai/reference/clientadd_model) function to send this information to Fiddler.


```python
MODEL_ID = 'churn_classifier'

client.add_model(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    model_id=MODEL_ID,
    model_info=model_info,
)
```

On the project page, you should now be able to see the newly onboarded model with its model schema.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/simple_monitoring_3.png"" />
        </td>
    </tr>
</table>

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/simple_monitoring_4.png"" />
        </td>
    </tr>
</table>

## 5. Set up Alerts and Notifications (Optional)

Fiddler Client API function [add_alert_rule](https://dash.readme.com/project/fiddler/v1.5/refs/clientadd_alert_rule) allow creating rules to receive email and pagerduty notifications when your data or model predictions deviates from it's expected behavior.

The rules can of **Data Drift, Performance, Data Integrity,** and **Service Metrics** types and they can be compared to **absolute** or **relative** values.

Please refer [our documentation](https://docs.fiddler.ai/docs/alerts) for more information on Alert Rules. 

---
  
Let's set up a few Alert Rules.

The following API call sets up a Data Integrity type rule which triggers an email notification when published events have"
"slug: ""quick-start""  2 or more range violations in any 1 day bin for the ```numofproducts``` column.


```python
notifications_config = client.build_notifications_config(
    emails = ""name@google.com"",
)

client.add_alert_rule(
    name = ""Bank Churn Range Violation Alert1"",
    project_id = PROJECT_ID,
    model_id = MODEL_ID,
    alert_type = fdl.AlertType.DATA_INTEGRITY,
    metric = fdl.Metric.RANGE_VIOLATION,
    bin_size = fdl.BinSize.ONE_DAY, 
    compare_to = fdl.CompareTo.RAW_VALUE,
    #compare_period = None,
    priority = fdl.Priority.HIGH,
    warning_threshold = 2,
    critical_threshold = 3,
    condition = fdl.AlertCondition.GREATER,
    column = ""numofproducts"",
    notifications_config = notifications_config
)
```

The following API call sets up a Performance type rule which triggers an email notification when precision metric is 5% higher than that from 1 hr bin one day ago.


```python
notifications_config = client.build_notifications_config(
    emails = ""name@google.com"",
)
client.add_alert_rule(
    name = ""Bank Churn Performance Alert"",
    project_id = PROJECT_ID,
    model_id = MODEL_ID,
    alert_type = fdl.AlertType.PERFORMANCE,
    metric = fdl.Metric.PRECISION,
    bin_size = fdl.BinSize.ONE_HOUR, 
    compare_to = fdl.CompareTo.TIME_PERIOD,
    compare_period = fdl.ComparePeriod.ONE_DAY,
    warning_threshold = 0.05,
    critical_threshold = 0.1,
    condition = fdl.AlertCondition.GREATER,
    priority = fdl.Priority.HIGH,
    notifications_config = notifications_config
)
```

## 6. Publish production events

Information about your model is added to Fiddler and now it's time to start publishing some production data!  
Fiddler will **monitor this data and compare it to your baseline to generate powerful insights into how your model is behaving**.


---


Each record sent to Fiddler is called **an event**.
  
Let's load in some sample events from a CSV file.


```python
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/churn_events.csv'

production_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be as recent as today 
production_df['timestamp'] = production_df['timestamp'] + (int(time.time() * 1000) - production_df['timestamp'].max())
```

You can use the client's `publish_events_batch` function to start pumping data into Fiddler!
  
*Just include:*
1. The DataFrame containing your events
2. The name of the column containing event timestamps


```python
client.publish_events_batch(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    batch_source=production_df,
    timestamp_field='timestamp',
    id_field='customer_id' # Optional
)
```

## 7. Get insights
  
Return to your Fiddler environment to get enhanced observability into your model's performance.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/simple_monitoring_5.png"" />
        </td>
    </tr>
</table>

**What's Next?**

Try the [NLP Monitoring - Quickstart Notebook](https://docs.fiddler.ai/docs/simple-n"
"slug: ""quick-start"" lp-monitoring-quick-start)

---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
title: ""Explainability with a Surrogate Model""
slug: ""monitoring-xai-quick-start""
excerpt: ""Quickstart Notebook""
hidden: false
createdAt: ""2022-04-19T20:06:49.318Z""
updatedAt: ""2023-10-26T00:13:51.595Z""
---
This guide will walk you through the basic onboarding steps required to use Fiddler for model production monitoring and explainability with a model surrogate, **using data provided by Fiddler**.

Click the following link to get started using Google Colab:

<div class=""colab-box"">
    <a href=""https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/23.5/Fiddler_Quickstart_Surrogate_XAI.ipynb"" target=""_blank"">
        <div>
            Open in Google Colab ‚Üí
        </div>
    </a>
    <div>
            <img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" />
    </div>
</div>

[block:html]
{
  ""html"": ""<div class=\""fiddler-cta\"">\n<a class=\""fiddler-cta-link\"" href=\""https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral\""><img src=\""https://files.readme.io/af83f1a-fiddler-docs-cta-trial.png\"" alt=\""Fiddler Free Trial\""></a>\n</div>""
}
[/block]

# Fiddler Quick Start Guide for Explainability (XAI) with Surrogate Models

Fiddler is not only a powerful observability tool for monitoring the health of your ML models in production but also an explainability tool to peak into your black box models. With the ability to **point explain** and **global explain** your model, Fiddler provides powerful visualizations that can explain your model's behavior. 


---


You can start exploring Fiddler's XAI capabilities by following these five quick steps:

1. Connect to Fiddler
2. Upload a baseline dataset
3. Add your model details to Fiddler
4. Either upload a model artifact or use Fiddler generated surrogate model
5. Publish Production Events
6. Get insights

**Don't have a Fiddler account? [Sign-up for a 14-day free trial](https://www.fiddler.ai/trial?utm_source=fiddler_docs&utm_medium=referral).**

## 0. Imports


```python
!pip install -q fiddler-client

import numpy as np
import pandas as pd
import fiddler as fdl
import time as time

print(f""Running client version {fdl.__version__}"")
```

## 1. Connect to Fiddler

Before you can register your model with Fiddler, you'll need to connect using our API client.


---


**We need a few pieces of information to get started.**
1. The URL you're using to connect to Fiddler


```python
URL = '' # Make sure to include the full URL (including https://).
```


```python
ORG_ID = ''
AUTH_TOKEN = ''
```

2. Your organization ID
3. Your authorization token

Both of these can be found by clicking the URL you entered and navigating to the **Settings** page of your Fiddler environment.

Now just run the following code block to connect to the Fiddler API!


```python
"
"slug: ""monitoring-xai-quick-start"" client = fdl.FiddlerApi(
    url=URL,
    org_id=ORG_ID,
    auth_token=AUTH_TOKEN
)
```

Once you connect, you can create a new project by specifying a unique project ID in the client's `create_project` function.


```python
PROJECT_ID = 'quickstart_surrogate_xai'

client.create_project(PROJECT_ID)
```

You should now be able to see the newly created project on the UI.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/surrogate_xai_1.png"" />
        </td>
    </tr>
</table>

## 2. Upload a baseline dataset

In this example, we'll be considering the case where we're a bank and we have **a model that predicts churn for our customers**.  
We want to explain our model's predictions and **understand the features that impact model predictions** the most.
  
In order to get explainability insights, **Fiddler needs to fiddle with your model**. To do so, we need to add your model details. This includes information about the data used by your model. So, we first start with uploading a small sample of data that can serve as a baseline.


---


*For more information on how to design a baseline dataset, [click here](https://docs.fiddler.ai/pages/user-guide/data-science-concepts/monitoring/designing-a-baseline-dataset/).*


```python
PATH_TO_BASELINE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/churn_baseline.csv'

baseline_df = pd.read_csv(PATH_TO_BASELINE_CSV)
baseline_df
```

Fiddler uses this baseline dataset to keep track of important information about your data.
  
This includes **data types**, **data ranges**, and **unique values** for categorical variables.

---

You can construct a `DatasetInfo` object to be used as **a schema for keeping track of this information** by running the following code block.


```python
dataset_info = fdl.DatasetInfo.from_dataframe(baseline_df, max_inferred_cardinality=100)
dataset_info
```

Then use the client's `upload_dataset` function to send this information to Fiddler!
  
*Just include:*
1. A unique dataset ID
2. The baseline dataset as a pandas DataFrame
3. The `DatasetInfo` object you just created


```python
DATASET_ID = 'churn_data'

client.upload_dataset(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    dataset={
        'baseline': baseline_df
    },
    info=dataset_info
)
```

If you click on your project in the Fiddler UI, you should now be able to see the newly onboarded dataset.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/surrogate_xai_2.png"" />
        </td>
    </tr>
</table>

## 3. Add information about your model

Now it's time to add details about your model with Fiddler. We do so by first creating a **ModelInfo Object** that helps Fiddler understand **how your model operates**.
  
*Just include:*
1. The **task** your model is performing (regression, binary classification, etc.)
2. The **target** (ground truth) column
3. The **"
"slug: ""monitoring-xai-quick-start"" output** (prediction) column
4. The **feature** columns
5. Any **metadata** columns
6. Any **decision** columns (these measures the direct business decisions made as result of the model's prediction)



```python
# Specify task
model_task = 'binary'

if model_task == 'regression':
    model_task = fdl.ModelTask.REGRESSION
    
elif model_task == 'binary':
    model_task = fdl.ModelTask.BINARY_CLASSIFICATION

elif model_task == 'multiclass':
    model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION

    
# Specify column types
target = 'churn'
outputs = ['predicted_churn']
decision_cols = ['decision']
features = ['geography', 'gender', 'age', 'tenure', 'balance', 'numofproducts', 'hascrcard', 'isactivemember', 'estimatedsalary']
    
# Generate ModelInfo
model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id=DATASET_ID,
    model_task=model_task,
    target=target,
    categorical_target_class_details='yes',
    outputs=outputs,
    decision_cols=decision_cols,
    features=features
)
model_info
```

After ModelInfo object is created to save your model information, use the client's *add_model* call to add the generated details about your model. 

**Note:** You will need to specify a unique model ID.


```python
MODEL_ID = 'churn_classifier'

client.add_model(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    model_id=MODEL_ID,
    model_info=model_info
)
```

On the project page, you should now be able to see the newly created model. Notice how without uploading a model artifact or creating surrogate model, you can only explore monitoring capabilities.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/surrogate_xai_3.png"" />
        </td>
    </tr>
</table>

## 4. Either upload your own model or generate a surrogate model

With the above step, your model is added to Fiddler which means that for a given *project_id*, your given *model_id* now holds *ModelInfo* about the model you care about. 

In order to be able to run predictions for explainability analysis, however, you will need to upload your model file. If you just want to explore the XAI capabilities without providing your model to Fiddler, you can also generate a surrogate model which tries to mimic your model based on the details provided. 

Let's instruct Fiddler to generate a surrogate model based on the information (ModelInfo) provided above.


```python
client.add_model_surrogate(
    project_id=PROJECT_ID,
    model_id=MODEL_ID
)
```

Notice that our model schema page now lists our *Artifact Status* as ""Surrogate"" and we now see feature impact scores for our model.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/surrogate_xai_4.png"" />
        </td>
    </tr>
</table>

## 5. Publish Production Events


```python
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/churn_events.csv'

production_df = pd.read_csv(PATH_TO_EVENTS_CSV)
# Shift the timestamps"
"slug: ""monitoring-xai-quick-start""  of the production events to be as recent as today 
production_df['timestamp'] = production_df['timestamp'] + (int(time.time() * 1000) - production_df['timestamp'].max())
```


```python
client.publish_events_batch(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    batch_source=production_df,
    timestamp_field='timestamp',
    id_field='customer_id' # Optional
)
```

## 6. Get insights

**You're all done!**
  
Return to your Fiddler environment to get enhanced monitoring and explainability into the surrogate model.  With a surrogate model or an uploaded model artifact, we can unlock advance observability like global and point explanations, PDP Plots, segment-level feature impacts and more.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/surrogate_xai_5.png"" />
        </td>
    </tr>
</table>

You can also run explanations and/or get feature impact now from the client...


```python
#grab a row from the baseline to run an explanation on
row = production_df.to_dict(orient='records')[0]
row
```


```python
explanation = client.get_explanation(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    input_data_source=fdl.RowDataSource(row=row),
    ref_data_source=fdl.DatasetDataSource(dataset_id=DATASET_ID, num_samples=300),
    explanation_type='FIDDLER_SHAP'
)
explanation
```


```python
feature_impact = client.get_feature_impact(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    data_source=fdl.DatasetDataSource(dataset_id=DATASET_ID, num_samples=200),
    num_iterations=300,
    num_refs=200,
    ci_level=0.90,
)
feature_impact
```



---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
title: ""NLP Monitoring""
slug: ""simple-nlp-monitoring-quick-start""
excerpt: ""Quickstart Notebook""
hidden: false
createdAt: ""2022-08-15T23:29:02.913Z""
updatedAt: ""2023-10-26T00:13:19.991Z""
---
This guide will walk you through the basic steps required to use Fiddler for monitoring NLP models. A multi-class classifier is applied to the 20newsgroup dataset and the text embeddings are monitored using Fiddler's unique Vector Monitoring approach.

Click the following link to get started using Google Colab:

<div class=""colab-box"">
    <a href=""https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/23.5/Fiddler_Quickstart_NLP_OpenAI_Monitoring.ipynb"" target=""_blank"">
        <div>
            Open in Google Colab ‚Üí
        </div>
    </a>
    <div>
            <img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" />
    </div>
</div>

# Monitoring NLP data using Fiddler Vector Monotoring

In this notebook we present the steps for using Fiddler NLP monitoring. Fiddler employs a vector-based monitoring approach that can be used to monitor data drift in multi-dimensional data such as NLP embeddings and images. In this notebook we show a use case for monitoring NLP embeddings to detect drift in text data.

Fiddler is the pioneer in enterprise Model Performance Management (MPM), offering a unified platform that enables Data Science, MLOps, Risk, Compliance, Analytics, and LOB teams to **monitor, explain, analyze, and improve ML deployments at enterprise scale**. 
Obtain contextual insights at any stage of the ML lifecycle, improve predictions, increase transparency and fairness, and optimize business revenue.

---

You can experience Fiddler's NLP monitoring ***in minutes*** by following these five quick steps:

1. Connect to Fiddler
2. Load and vectorize 20Newsgroup data
2. Upload the vectorized baseline dataset
3. Add metadata about your model
4. Publish production events
5. Get insights

## Imports


```python
!pip install -q fiddler-client

import fiddler as fdl
import pandas as pd

print(f""Running Fiddler client version {fdl.__version__}"")
```

# 1. Connect to Fiddler

Before you can add information about your model with Fiddler, you'll need to connect using our API client.

---

**We need a few pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your organization ID
3. Your authorization token

The latter two of these can be found by pointing your browser to your Fiddler URL and navigating to the **Settings** page.


```python
URL = ''  # Make sure to include the full URL (including https://).
ORG_ID = ''
AUTH_TOKEN = ''
```

Now just run the following code block to connect to the Fiddler API!


```python
client = fdl.FiddlerApi(
    url=URL,
    org_id=ORG_ID,
    auth_token=AUTH_TOKEN
)
```

Once you connect, you can create a new project by specifying a unique project ID in the client's `create_project` function.


```python
PROJECT_ID = 'simple_nlp_example'

if not PROJECT_ID"
"slug: ""simple-nlp-monitoring-quick-start""  in client.list_projects():
    print(f'Creating project: {PROJECT_ID}')
    client.create_project(PROJECT_ID)
else:
    print(f'Project: {PROJECT_ID} already exists')
```

# 2. Load and vectorize 20Newsgroup data

In order to get insights into the model's performance, **Fiddler needs a small sample of data that can serve as a baseline** for making comparisons with production inferences (aka. events).

For this model's baseline dataset, we will use the __""20 newsgroups text dataset""__.  This dataset contains around 18,000 newsgroups posts on 20 topics. This dataset is available as one of the standard scikit-learn real-world datasets and can be be fechted directly using scikit-learn.

Let's first load our baseline dataset into a dataframe and then squeeze the ""news"" column into a Series to ready it for vectorization.


```python
PATH_TO_BASELINE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/newsgroup20_baseline_gold.csv'

baseline_df = pd.read_csv(PATH_TO_BASELINE_CSV)
base_series = baseline_df['news'].squeeze()
base_series
```

Great!  Now let's vectorize this NLP data using one of the two methods below.

### Vectorization

Fiddler monitors NLP and CV data by using encoded data in the form of embeddings, or **vectors**.  Before we load our baseline or our event data into the Fiddler platform for monitoring purposes, we must *vectorize* the raw NLP input.  

The follow section provides two methods of vectorizing the NLP data: *TF-IDF vectorization* and *word2vec*.  Please run only 1 method.

***Method 1: TF-IDF vectorization***


```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(sublinear_tf=True, max_features=300, min_df=0.01, max_df=0.9, stop_words='english')
vectorizer.fit(base_series)
tfidf_baseline_sparse = vectorizer.transform(base_series)

# Trasnform our sparse matrix of TFIDF values into a dataframe with an embedding vector as a list of values
df = pd.DataFrame(tfidf_baseline_sparse.toarray().tolist())
columns_to_combine = df.columns  
df_embeddings = df.apply(lambda row: row[columns_to_combine].tolist(), axis=1).to_frame()
df_embeddings = df_embeddings.rename(columns={df_embeddings.columns[0]: 'embeddings'})
df_embeddings
```


***Method 2: word2vec by Spacy***

The following lines show how to use ***word2vec*** embedding from Sacy. In order to run the following cell, you need to install spacy and its pre-trained models like 'en_core_web_lg'. See: https://spacy.io/usage


```python
# import spacy
# nlp = spacy.load('en_core_web_lg')

# s = time.time()
# base_embeddings = base_series.apply(lambda sentence: nlp(sentence).vector)
# print(f' Time to compute embeddings {time.time() - s}')

# baseline_df = pd.DataFrame(base_embeddings.values.tolist())
# baseline_df = baseline_df.rename(columns = {c:'f'+str(c+1) for c in baseline_df.columns})
```

Now that we've vectorized our data, let's drop the unstructured ""news"" column and snap the vectorized data to our original baseline dataframe.


```python
baseline_df = pd.concat([baseline_df,"
"slug: ""simple-nlp-monitoring-quick-start""  df_embeddings], axis=1)
baseline_df
```

# 3. Upload the vectorized baseline dataset to Fiddler

Next, let's create a [DatasetInfo](https://docs.fiddler.ai/reference/fdldatasetinfo) object to describe our baseline dataset and then [upload_dataset()](https://docs.fiddler.ai/reference/clientupload_dataset) to Fiddler.


```python
DATASET_ID = 'simple_newsgroups_1'  # The dataset name in Fiddler platform
dataset_info = fdl.DatasetInfo.from_dataframe(baseline_df)

if not DATASET_ID in client.list_datasets(project_id=PROJECT_ID):
    print(f'Upload dataset {DATASET_ID}')
    
    client.upload_dataset(
        project_id=PROJECT_ID,
        dataset_id=DATASET_ID,
        dataset={'baseline': baseline_df},
        info=dataset_info
    )
    
else:
    print(f'Dataset: {DATASET_ID} already exists in Project: {PROJECT_ID}.\n'
               'The new dataset is not uploaded. (please use a different name.)') 
```

# 4. Add metadata about the model

Next we must tell Fiddler a bit more about our model.  This is done by calling [.add_model()](https://docs.fiddler.ai/reference/clientadd_model).  When calling [.add_model()](https://docs.fiddler.ai/reference/clientadd_model), we must pass in a [model_info](https://docs.fiddler.ai/reference/fdlmodelinfo) object to tell Fiddler about our model.  This [model_info](https://docs.fiddler.ai/reference/fdlmodelinfo) object will tell Fiddler about our model's task, inputs, output, target and which features are apart of the NLP vector created above.

Let's first define our NLP vector using a custom feature of type TextEmbedding.  The custom feature will be called *text_embedding* which groups together our embedding vector column, *embeddings*, and our raw source column, *news*.


```python
CF1 = fdl.TextEmbedding(
    name='text_embedding',
    column='embeddings',
    source_column='news',
    n_clusters=6
)
```

Now let's define our [model_info](https://docs.fiddler.ai/reference/fdlmodelinfo) object.


```python
# Specify task
model_task = 'binary'

if model_task == 'regression':
    model_task = fdl.ModelTask.REGRESSION
    
elif model_task == 'binary':
    model_task = fdl.ModelTask.BINARY_CLASSIFICATION

elif model_task == 'multiclass':
    model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION

elif model_task == 'ranking':
    model_task = fdl.ModelTask.RANKING
    
    
# Specify column types
target = 'target'
outputs = ['predicted_score']
features = baseline_df.columns.drop(['target', 'predicted_score']).tolist()

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info = dataset_info,
    dataset_id = DATASET_ID,
    features = features,
    target = target,
    outputs = outputs,
    custom_features = [CF1],
    model_task=model_task,
    description='An example model to showcase monitoring NLP data by vectorizing the unstructured data.',
    binary_classification_threshold=0.5 #optional
)
model_info
```

And call [.add_model()](https://docs.fiddler.ai/reference/clientadd_model) to tell Fiddler about our model.


```python
MODEL_ID = 'newsgroup_model_v1'"
"slug: ""simple-nlp-monitoring-quick-start""  # choose a different model ID

if not MODEL_ID in client.list_models(project_id=PROJECT_ID):
    client.add_model(
        project_id=PROJECT_ID,
        dataset_id=DATASET_ID,
        model_id=MODEL_ID,
        model_info=model_info
    )
else:
    print(f'Model: {MODEL_ID} already exists in Project: {PROJECT_ID}. Please use a different name.')
```

# 5. Publish production events

Let's publish some production events into Fiddler.  This .csv file already has some manufactured drift introduced to the NLP data by sampling from the newsgroup20 dataset more heavily in certain topics.  


```python
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/newsgroup20_events_gold.csv'

events_df = pd.read_csv(PATH_TO_EVENTS_CSV)

# in the csv file the embeddings are stored in different columns. let's combine them into a list in a new column called 'embeddings'
columns_to_combine = [col for col in events_df.columns if col.startswith('f')]
events_df['embeddings'] = events_df.apply(lambda row: row[columns_to_combine].tolist(), axis=1)
events_df = events_df.drop(columns=columns_to_combine)
events_df
```

Now let's time shift the timestamps in this event dataset so that they are as recent as today's date.


```python
from datetime import datetime

# Timeshifting the timestamp column in the events file so the events are as recent as today
ts_col = 'timestamp'
events_df[ts_col]  = pd.to_datetime(events_df[ts_col], origin='unix', unit='ms')
max_dt = events_df[ts_col].max()
delta = datetime.now() - max_dt
events_df[ts_col] = events_df[ts_col] + pd.to_timedelta(delta.total_seconds(), unit='s')
events_df
```

And, finally, publish our events as a batch.


```python
client.publish_events_batch(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    batch_source=events_df,
    timestamp_field= ts_col
)
```

# 5. Get insights

**You're all done!**
  
Now just head to your Fiddler environment and start getting enhanced observability into your model's performance.

<table>
    <tr>
        <td><img src=""https://github.com/fiddler-labs/fiddler-examples/raw/main/quickstart/images/nlp_monitoring_1.png"" /></td>
    </tr>
</table>

---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
title: ""Ranking Monitoring Example""
slug: ""ranking-model""
hidden: false
createdAt: ""2023-06-16T21:38:41.066Z""
updatedAt: ""2023-10-26T00:14:35.148Z""
---
This notebook will show you how Fiddler enables monitoring and explainability for a Ranking model. This notebook uses a dataset from Expedia that includes shopping and purchase data with information on price competitiveness. The data are organized around a set of ‚Äúsearch result impressions‚Äù, or the ordered list of hotels that the user sees after they search for a hotel on the Expedia website.

Click the following link to try it now with Google Colab:

<div class=""colab-box"">
    <a href=""https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/23.5/Fiddler_Quickstart_Ranking_Model.ipynb"" target=""_blank"">
        <div>
            Open in Google Colab ‚Üí
        </div>
    </a>
    <div>
            <img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" />
    </div>
</div>

# Fiddler Ranking Model Quick Start Guide

Fiddler offer the ability for your teams to observe you ranking models to understand thier performance and catch issues like data drift before they affect your applications.

# Quickstart: Expedia Search Ranking
The following dataset is coming from Expedia. It includes shopping and purchase data as well as information on price competitiveness. The data are organized around a set of ‚Äúsearch result impressions‚Äù, or the ordered list of hotels that the user sees after they search for a hotel on the Expedia website. In addition to impressions from the existing algorithm, the data contain impressions where the hotels were randomly sorted, to avoid the position bias of the existing algorithm. The user response is provided as a click on a hotel. From: https://www.kaggle.com/c/expedia-personalized-sort/overview

# 0. Imports


```python
!pip install lightgbm
```


```python
import pandas as pd
import lightgbm as lgb
import numpy as np
import time as time
import datetime
```

# 1. Connect to Fiddler and Create a Project
First we install and import the Fiddler Python client.


```python
!pip install -q fiddler-client
import fiddler as fdl
print(f""Running client version {fdl.__version__}"")
```

Before you can add information about your model with Fiddler, you'll need to connect using our API client.

---

**We need a few pieces of information to get started.**
1. The URL you're using to connect to Fiddler
2. Your organization ID
3. Your authorization token

The latter two of these can be found by pointing your browser to your Fiddler URL and navigating to the **Settings** page.


```python
URL = ''  # Make sure to include the full URL (including https://).
ORG_ID = ''
AUTH_TOKEN = ''
```

Next we run the following code block to connect to the Fiddler API.


```python
client = fdl.FiddlerApi(url=URL, org_id=ORG_ID, auth_token=AUTH_TOKEN)
```

Once you connect, you can create a new project by specifying a unique project ID in the client's `create_project` function.


```python
PROJECT_ID = 'danny3_search_ranking'

if not PROJECT_ID in client.list"
"slug: ""ranking-model"" _projects():
    print(f'Creating project: {PROJECT_ID}')
    client.create_project(PROJECT_ID)
else:
    print(f'Project: {PROJECT_ID} already exists')
```

# 2. Upload the Baseline Dataset

Now we retrieve the Expedia Dataset as a baseline for this model.


```python
df = pd.read_csv(""https://media.githubusercontent.com/media/fiddler-labs/fiddler-examples/main/quickstart/data/expedia_baseline_data.csv"")
df.head()
```

Fiddler uses this baseline dataset to keep track of important information about your data.
  
This includes **data types**, **data ranges**, and **unique values** for categorical variables.

---

You can construct a `DatasetInfo` object to be used as **a schema for keeping track of this information** by running the following code block.


```python
dataset_info = fdl.DatasetInfo.from_dataframe(df=df, max_inferred_cardinality=100)
dataset_info
```

Then use the client's [upload_dataset](https://docs.fiddler.ai/reference/clientupload_dataset) function to send this information to Fiddler!
  
*Just include:*
1. A unique dataset ID
2. The baseline dataset as a pandas DataFrame
3. The [DatasetInfo](https://docs.fiddler.ai/reference/fdldatasetinfo) object you just created


```python
DATASET_ID = 'expedia_data'
client.upload_dataset(project_id=PROJECT_ID,
                      dataset={'baseline': df},
                      dataset_id=DATASET_ID,
                      info=dataset_info)
```

# 3. Share Model Metadata and Upload the Model


```python
#create model directory to sotre your model files
import os
model_dir = ""model""
os.makedirs(model_dir)
```

### 3.a Adding model metadata to Fiddler
To add a Ranking model you must specify the ModelTask as `RANKING` in the model info object.  

Additionally, you must provide the `group_by` argument that corresponds to the query search id. This `group_by` column should be present either in:
- `features` : if it is used to build and run the model
- `metadata_cols` : if not used by the model 

Optionally, you can give a `ranking_top_k` number (default is 50). This will be the number of results within each query to take into account while computing the performance metrics in monitoring.  

Unless the prediction column was part of your baseline dataset, you must provide the minimum and maximum values predictions can take in a dictionary format (see below).  

If your target is categorical (string), you need to provide the `categorical_target_class_details` argument. If your target is numerical and you don't specify this argument, Fiddler will infer it.   

This will be the list of possible values for the target **ordered**. The first element should be the least relevant target level, the last element should be the most relevant target level.


```python
target = 'binary_relevance'
features = list(df.drop(columns=['binary_relevance', 'score', 'graded_relevance', 'position']).columns)

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=client.get_dataset_info(project_id=PROJECT_ID, dataset_id=DATASET_ID),
    target=target,
    features=features,
    input_type=fdl.ModelInputType.TABULAR,
    model_task=fdl.ModelTask.RANKING,
    outputs={'score':[-5.0, 3.0]},
    group_by='srch_id',
    ranking_top_k=20,
    categorical_target_class_details=[0"
"slug: ""ranking-model"" , 1]
)

# inspect model info and modify as needed
model_info
```


```python
MODEL_ID = 'expedia_model'

if not MODEL_ID in client.list_models(project_id=PROJECT_ID):
    client.add_model(
        project_id=PROJECT_ID,
        dataset_id=DATASET_ID,
        model_id=MODEL_ID,
        model_info=model_info
    )
else:
    print(f'Model: {MODEL_ID} already exists in Project: {PROJECT_ID}. Please use a different name.')
```

### 3.b Create a Model Wrapper Script

Package.py is the interface between Fiddler‚Äôs backend and your model. This code helps Fiddler to understand the model, its inputs and outputs.

You need to implement three parts:
- init: Load the model, and any associated files such as feature transformers.
- transform: If you use some pre-processing steps not part of the model file, transform the data into a format that the model recognizes.
- predict: Make predictions using the model.


```python
%%writefile model/package.py

import pickle
from pathlib import Path
import pandas as pd

PACKAGE_PATH = Path(__file__).parent

class ModelPackage:

    def __init__(self):
        """"""
         Load the model file and any pre-processing files if needed.
        """"""
        self.output_columns = ['score']
        
        with open(PACKAGE_PATH / 'model.pkl', 'rb') as infile:
            self.model = pickle.load(infile)
    
    def transform(self, input_df):
        """"""
        Accepts a pandas DataFrame object containing rows of raw feature vectors. 
        Use pre-processing file to transform the data if needed. 
        In this example we don't need to transform the data.
        Outputs a pandas DataFrame object containing transformed data.
        """"""
        return input_df
    
    def predict(self, input_df):
        """"""
        Accepts a pandas DataFrame object containing rows of raw feature vectors. 
        Outputs a pandas DataFrame object containing the model predictions whose column labels 
        must match the output column names in model info.
        """"""
        transformed_df = self.transform(input_df)
        pred = self.model.predict(transformed_df)
        return pd.DataFrame(pred, columns=self.output_columns)
    
def get_model():
    return ModelPackage()
```

### 3.c Retriving the model files 

To explain a model's inner workigs we need to upload the model artifacts. We will retrive a pre-trained model from the Fiddler Repo that was trained with **lightgbm 2.3.0**


```python
import urllib.request
urllib.request.urlretrieve(""https://github.com/fiddler-labs/fiddler-examples/blob/main/quickstart/models/ranking_model.pkl"", ""model/model.pkl"")
```

### 3.d Upload the model files to Fiddler


Now as a final step in the setup you can upload the model artifact files using `add_model_artifact`. 
   - The `model_dir` is the path for the folder containing the model file(s) and the `package.py` from ther last step.
   - Since each model artifact uploaded to Fiddler gets deployed in its own container, the [deployment params](https://docs.fiddler.ai/reference/fdldeploymentparams) allow us to specify the compute needs and library set of the container.


```python
#Uploading Model files
deployment_params = fdl.DeploymentParams(
    image_uri=""md-base/python/machine-learning:1.1.0"",
    cpu=100,
    memory=256,
    replicas=1,
)

client.add_model_artifact(
    model_dir=model_dir, 
   "
"slug: ""ranking-model""  project_id=PROJECT_ID, 
    model_id=MODEL_ID,
    deployment_params=deployment_params
)
```

# 5. Send Traffic For Monitoring

### 5.a Gather and prepare Production Events
This is the production log file we are going to upload in Fiddler.


```python
df_logs = pd.read_csv('https://media.githubusercontent.com/media/fiddler-labs/fiddler-examples/main/quickstart/data/expedia_logs.csv')
df_logs.tail()
```


```python
#timeshift to move the data to last 29 days
df_logs['time_epoch'] = df_logs['time_epoch'] + (float(time.time()) - df_logs['time_epoch'].max())
```

For ranking, we need to ingest all events from a given query or search ID together. To do that, we need to transform the data to a grouped format.  
You can use the `convert_flat_csv_data_to_grouped` utility function to do the transformation.



```python
df_logs_grouped = fdl.utils.pandas_helper.convert_flat_csv_data_to_grouped(input_data=df_logs, group_by_col='srch_id')
```


```python
df_logs_grouped.head(2)
```

### 5.b Publish events


```python
client.publish_events_batch(project_id=PROJECT_ID,
                            model_id=MODEL_ID,
                            batch_source=df_logs_grouped,
                            timestamp_field='time_epoch')
```

# 7. Get insights


**You're all done!**
  
You can now head to your Fiddler environment and start getting enhanced observability into your model's performance.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/ranking_model_1.png"" />
        </td>
    </tr>
</table>

--------
**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
title: ""Class Imbalance Monitoring Example""
slug: ""class-imbalance-monitoring-example""
hidden: false
createdAt: ""2023-05-08T13:42:46.086Z""
updatedAt: ""2023-10-26T00:14:22.453Z""
---
Many ML use cases, like fraud detection and facial recognition, suffer from what is known as the _class imbalance problem_. This problem exists where a vast majority of the inferences seen by the model belong to only one class, known as the majority class. This makes detecting drift in the minority class very difficult as the ""signal"" is completely outweighed by the shear number of inferences seen in the majority class. 

This guide showcases how Fiddler uses a class weighting parameter to deal with this problem. This notebook will onboard two identical models -- one without class imbalance weighting and one with class imbalance weighting -- to illustrate how drift signals in the minority class are easier to detect once properly amplified by Fiddler's unique class weighting approach..

Click the following link to get started using Google Colab:

<div class=""colab-box"">
    <a href=""https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/23.5/Fiddler_Quickstart_Imbalanced_Data.ipynb"" target=""_blank"">
        <div>
            Open in Google Colab ‚Üí
        </div>
    </a>
    <div>
            <img src=""https://colab.research.google.com/img/colab_favicon_256px.png"" />
    </div>
</div>

# Fiddler Quickstart notebook for a Class Imbalance Example

Many ML use cases, like fraud detection and facial recognition, suffer from what is known as the class imbalance problem.  This problem exists where a vast majority of the inferences seen by the model belong to only one class, known as the majority class.  This makes detecting drift in the minority class very difficult as the ""signal"" is completely outweighed by the shear number of inferences seen in the majority class.  The following notebook showcases how Fiddler uses a class weighting paramater to deal with this problem. This notebook will onboard two identical models -- one without class imbalance weighting and one with class imbalance weighting -- to illustrate how drift signals in the minority class are easier to detect once properly amplified by Fiddler's unique class weighting approach.

1. Connect to Fiddler
2. Upload a baseline dataset for a fraud detection use case
3. Onboard two fraud models to Fiddler -- one with class weighting and one without
4. Publish production events to both models with synthetic drift in the minority class
5. Get Insights -- compare the two onboarding approaches in Fiddler

## 0. Imports


```python
!pip install -q fiddler-client;

import numpy as np
import pandas as pd
import fiddler as fdl
import sklearn
import datetime
import time

print(f""Running client version {fdl.__version__}"")

RANDOM_STATE = 42
```

## 1. Connect to Fiddler


```python
URL = ''  # Make sure to include the full URL (including https://).
ORG_ID = ''
AUTH_TOKEN = ''

PROJECT_ID = 'imbalance_cc_fraud'
MODEL_ID = 'imbalance_cc_fraud'
DATASET_ID = 'imbalance_cc_fraud_baseline'

client = fdl.FiddlerApi(
    url=URL,
    org_id=ORG_ID,
    auth_token=AUTH_TOKEN
)
```


```python
# Create a"
"slug: ""class-imbalance-monitoring-example""  new project within Fiddler
client.create_project(PROJECT_ID)
```

## 2. Upload a baseline dataset for a fraud detection use case



```python
PATH_TO_BASELINE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/imbalance_baseline_data_sample.csv'

baseline_df = pd.read_csv(PATH_TO_BASELINE_CSV)
baseline_df.head()
```


```python
baseline_df['Class'].value_counts()
print('Percentage of minority class: {}%'.format(round(baseline_df['Class'].value_counts()[1]*100/baseline_df.shape[0], 4)))
```


```python
dataset_info = fdl.DatasetInfo.from_dataframe(baseline_df, max_inferred_cardinality=100)
dataset_info
```


```python
client.upload_dataset(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    dataset={
        'baseline': baseline_df
    },
    info=dataset_info
)
```

## 3. Onboard two fraud models to Fiddler -- one with class weighting and one without

Now, we will add two models: 
1. With class weight parameters
2. Without class weight parameters


```python
CLASS_WEIGHT = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(baseline_df['Class']), y=baseline_df['Class']).tolist()
print(f'Computed class-weights: {CLASS_WEIGHT}')

BINARY_THRESHOLD = 0.4
TARGET_COL = 'Class'
OUTPUT_COL = 'prediction_score'
```

Below, we first create a `ModelInfo` object and then onboard (add) the two models to Fiddler -- the first model onboarded with weights defined, the second without weights defined.


```python
for mid in [MODEL_ID + '_weighted', MODEL_ID]:
    
    if 'weighted' in mid:
        weighting_params = fdl.WeightingParams(class_weight=CLASS_WEIGHT)
        print(f'Onboard surrogate model with weighting parameters.')
    else:
        weighting_params = None
        print(f'Onboard surrogate model without weighting parameters.')
    
    target_col = TARGET_COL
    output_col = OUTPUT_COL
    inp_features = set(baseline_df.columns) - set([target_col, output_col])
    
    # Create ModelInfo object
    model_info = fdl.ModelInfo.from_dataset_info(
        dataset_info=dataset_info,
        target=target_col,
        dataset_id= DATASET_ID,
        features=inp_features,
        display_name='Fraud model',
        description='Fraud model with predictions in baseline',
        input_type=fdl.core_objects.ModelInputType.TABULAR,
        model_task=fdl.core_objects.ModelTask.BINARY_CLASSIFICATION,
        outputs=output_col,
        weighting_params=weighting_params,
        binary_classification_threshold=BINARY_THRESHOLD,
        categorical_target_class_details=[0, 1],
    )
    
    # Add Model and create surrogate model
    if mid not in client.list_models(project_id=PROJECT_ID):
        client.add_model(project_id=PROJECT_ID, model_id=mid, dataset_id=DATASET_ID, model_info=model_info)
        client.add_model_surrogate(project_id=PROJECT_ID, model_id=mid)
    else:
        print(f'Model: {mid} already exists in Project: {PROJECT_ID}')
```

## 4. Publish production events to both models with synthetic drift in the minority class


```python
PATH_TO_EVENTS_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/imbalance_production_data.csv'

production_df = pd.read_csv"
"slug: ""class-imbalance-monitoring-example"" (PATH_TO_EVENTS_CSV)

# Shift the timestamps of the production events to be as recent as today 
production_df['timestamp'] = pd.to_datetime(production_df['timestamp'],format='%Y-%m-%d %H:%M:%S')
production_df['timestamp'] = production_df['timestamp'] + (pd.to_datetime(datetime.date.today()) - (production_df['timestamp'].max()))

production_df.head()
```


```python
print('Percentage of minority class: {}%'.format(round(production_df['Class'].value_counts()[1]*100/production_df.shape[0], 4)))
```

We see that the percentage of minority class in production data is > 3 times than that of baseline data. This should create a big drift in the predictions.

We will now publish the same production/event data for both of the models -- the one with class weights and the one without class weights.


```python
for mid in [MODEL_ID + '_weighted', MODEL_ID]:
    t0 = time.time()
    print('Publishing events for Model ID: {}'.format(mid))
    client.publish_events_batch(
        project_id=PROJECT_ID,
        model_id=mid,
        batch_source=production_df,
        timestamp_field='timestamp'
    )
    t1 = time.time()
    dt = t1-t0
    print(f'Time required: {dt} secs for {len(production_df)} events. [{len(production_df)/dt} events/sec]')
```

## 5. Get Insights -- compare the two onboarding approaches in Fiddler

**You're all done!**


In the Fiddler UI, we can the model without the class weights defined the output/prediction drift in the minority class is very hard to detect (`<=0.05`) because it is trumped byt the overwhelming volume of events in the majority class.  If we declare class weights then we see a higher drift which is more correct respresentation if the production data where the ratio of minority is class is 3x.

<table>
    <tr>
        <td>
            <img src=""https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/imabalance_data_1.png"" />
        </td>
    </tr>
</table>



---


**Questions?**  
  
Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.

If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly.
"
"---
title: ""Deploying Fiddler""
slug: ""deploying-fiddler""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:19:47 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:54:04 GMT+0000 (Coordinated Universal Time)""
---
## Deployment Overview

Fiddler runs on most mainstream flavors and configurations of Kubernetes, including OpenShift, Rancher, AWS Elastic Kubernetes Service, Azure Managed Kubernetes Service (AKS), GCP Google Kubernetes Engine, and more.

- **Our premises**‚ÄîFiddler is offered as a fully managed service, deployed within an isolated network and dedicated hardware in the cloud.

- **Your premises**‚ÄîDeploy Fiddler into a Kubernetes cluster running in your own cloud account or data center. Please refer to the [On-prem Technical Requirements](doc:technical-requirements#system-requirements) section for more details.

> üìò Info
> 
> Interested in a Fiddler Cloud or on-premises deployment?  Please contact [sales@fiddler.ai](mailto:sales@fiddler.ai).

## Deploy on cloud

Fiddler cloud deployment uses a managed Kubernetes service to deploy, scale, and manage the application. We'll handle the specifics! Please contact [sales@fiddler.ai](mailto:sales@fiddler.ai)

## Deploy on-premise

- [Technical Requirements](doc:technical-requirements) 
- [Installation Guide](doc:installation-guide)

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Single Sign On with Okta""
slug: ""okta-integration""
excerpt: """"
hidden: false
createdAt: ""Mon Aug 01 2022 15:14:37 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Overview

These instructions will help administrators configure Fiddler to be used with an existing Okta single sign on application.

## Okta Setup:

First, you must create an OIDC based application within Okta. Your application will require a callback URL during setup time. This URL will be provided to you by a Fiddler administrator. Your application should grant ""Authorization Code"" permissions to a client acting on behalf of a user. See the image below for how your setup might look like:

![](https://files.readme.io/b7b67fe-Screen_Shot_2022-08-07_at_10.22.36_PM.png)

This is the stage where you can allow certain users of your organization access to Fiddler through Okta. You can use the ""Group Assignments"" field to choose unique sets of organization members to grant access to. This setup stage will also allow for Role Based Access Control (i.e. RBAC) based on specific groups using your application.

Once your application has been set up, a Fiddler administrator will need to receive the following information and credentials:

- Okta domain
- Client ID
- Client Secret
- Okta Account Type (default or custom)

All of the above can be obtained from your Okta application dashboard, as shown in the pictures below:

![](https://files.readme.io/6442827-Screen_Shot_2022-08-07_at_10.30.03_PM.png)

![](https://files.readme.io/f1dbcf6-Screen_Shot_2022-08-07_at_10.30.15_PM.png)

You can also pass the above information to your Fiddler administrator via your okta.yml file. 

## Logging into Fiddler:

Once a Fiddler administrator has successfully set up a deployment for your organization using your given Okta credentials, you should see the ‚ÄúSign in with SSO‚Äù button enabled. When this button is clicked, you should be navigated to an Okta login screen. Once successfully authenticated, and assuming you have been granted access to Fiddler through Okta, you should be able to login to Fiddler.

![](https://files.readme.io/c96a709-Screen_Shot_2022-08-07_at_10.36.40_PM.png)

NOTES:

1. To be able to login with SSO, it is initially required for the user to register with Fiddler Application. Upon successful registration, the users will be able to login using SSO.
2. The only information Fiddler stores from Okta based logins is a user‚Äôs first name, last name, email address, and OIDC token.
3. Fiddler does not currently support using Okta based login through its API (see fiddler-client). In order to use an Okta based account through Fiddler's API, use a valid access token which can be created and copied on the ‚ÄúCredentials‚Äù tab on Fiddler‚Äôs ‚ÄúSettings‚Äù page.
"
"---
title: ""Routing to Fiddler (on-prem)""
slug: ""routing-to-fiddler-on-prem""
excerpt: """"
hidden: false
createdAt: ""Tue Sep 06 2022 21:46:51 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Fiddler supports a wide range of strategies for routing HTTP traffic from end users to the Fiddler system. A typical on-prem Fiddler deployment includes an HTTP reverse proxy (Envoy) that can be configured as needed to meet your routing needs.

![](https://files.readme.io/fd4b216-image.png)

The diagram above shows some of the deployment configuration options related to routing and TLS, described below. Once Fiddler is installed in your on-prem environment, you may need to take  additional steps to route TCP traffic to the Fiddler Envoy service.

# TLS termination

By default, Fiddler does not perform TLS termination. We find that our customers generally have excellent opinions about how TLS should be terminated, and generally prefer to perform TLS termination using their own network machinery.

## Terminate TLS outside of Fiddler

In a typical production environment, TLS termination will occur outside of Fiddler. Clear HTTP traffic should then be routed to the Fiddler Envoy service at the port specified by `envoy.publicHttpPort`. 

```
envoy:
  terminateTLS: false
  publicHttpPort: ""80""
```

## Terminate TLS within Fiddler

Fiddler can be configured to perform TLS termination using an X509 server certificate and corresponding PKCS #8 private key. The TLS certificate must be valid for the FQDN via which end-users will access the Fiddler platform. Both the server certificate and private key must be available in DER format, and should be placed in a `Secret` within the namespace where Fiddler will be deployed prior to installation. For example:

```
kubectl create secret tls my-tls-secret \
    --cert=path/to/the/cert.pem \
    --key=path/to/the/cert.key
```

The Fiddler Helm chart should be configured to reflect the `Secret` containing the server cert and key. TCP traffic should be routed to the port specified by `envoy.publicHttpsPort`.

```yaml
envoy:
  terminateTLS: true
  tlsSecretName: my-tls-secret
  serverCertKey: tls.crt
  privateKeyKey: tls.key
  publicHttpsPort: ""443""
```

## TLS with Ingress

Kubernetes `Ingress` [supports](https://kubernetes.io/docs/concepts/services-networking/ingress/#tls) specifying a TLS secret on a per-ingress basis. If using an `Ingress` to route traffic to Fiddler, create a `Secret` containing the DER-formatted X509 server certificate and PKCS #8 private key in the namespace where Fiddler will be deployed:

```
kubectl create secret tls my-tls-secret \
    --cert=path/to/the/cert.pem \
    --key=path/to/the/cert.key
```

The Fiddler Helm chart should be configured to enable Ingress with TLS. For example:

```
envoy:
  createIngress: true

ingress:
  tls:
    hosts:
      # The FQDN where Fiddler is accessed by end users.
      - fiddler.acme.com
    secretName: my-tls-secret
```

# Ingress

If the cluster"
"slug: ""routing-to-fiddler-on-prem""  where Fiddler is installed supports `Ingress`, the Fiddler Helm chart can be configured to create an `Ingress` resource that points to its HTTP reverse proxy.

For example, a configuration for use with the Kubernetes NGINX ingress controller might look like:

```yaml
envoy:
  createIngress: true

ingress:
  class: nginx
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/proxy-body-size: ""10240m""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""3600""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""3600""
```

# ClusterIP service

By default, the Fiddler HTTP reverse proxy (Envoy) is exposed as a `ClusterIP` service within the namespace where Fiddler is installed.

To control the port(s) on the `Service` where traffic is handled, set `envoy.publicHttpPort` and/or `envoy.publicHttpsPort` in the Fiddler Helm configuration. For example:

```yaml
envoy:
  serviceType: ClusterIP
  publicHttpPort: ""8080""
  publicHttpsPort: ""8443""
```

# LoadBalancer service

Fiddler can be exposed direcly outside the cluster via a `LoadBalancer` service, if supported. In AWS EKS, for example, this would result in the creation of a Network Load Balancer. See above for details on TLS termination within Fiddler.

```yaml
envoy:
  serviceType: LoadBalancer
  publicHttpPort: ""80""
  publicHttpsPort: ""443""
```

# Headless service

In cases where a more advanced service mesh or service discovery mechanism is used, it may be desirable to expose Fiddler via a headless service. For example:

```yaml
envoy:
  headlessService: true
```
"
"---
title: ""System Architecture""
slug: ""system-architecture""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:19:53 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Tue Dec 12 2023 19:45:58 GMT+0000 (Coordinated Universal Time)""
---
Fiddler deploys into your private cloud's existing Kubernetes clusters, for which the minimum system requirements can be found [here](doc:technical-requirements).  Fiddler supports deployment into Kubernetes in AWS, Azure, or GCP.  All services of the Fiddler platform are containerized in order to eliminate reliance on other cloud services and to reduce the deployment and maintenance friction of the platform.  This includes storage services like object storage and databases as well as system monitoring services like Grafana.  

Updates to the Fiddler containers is accomplished through a shared container registry (that Fiddler is provided access to).  Updates to the containers are orchestrated using Helm charts.

A full-stack deployment of Fiddler is shown in the diagram below. 

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/6675149-Fiddler_Reference_Architecture.png"",
        """",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


The Fiddler system components are deployed within a single namespace on a Kubernetes cluster, using the official Fiddler Helm chart.

- Fiddler core infrastructure relies on persistent volumes provided within the Kubernetes cluster. We recommend using encrypted storage volumes wherever possible.
- Fiddler may be configured to utilize external infrastructure in a self-hosted environment, such as existing PostgresQL servers, but this is not required as all services are containerized by default.
- Full-stack ""any-prem"" Fiddler includes observability infrastructure to monitor Fiddler system health and performance. These mainstream observability components may be integrated with external observability systems to support administration in a self-hosted environment.
- HTTP traffic to the Fiddler system is handled by an L4 or L7 load balancer or other proxy. TLS termination should usually occur outside the Fiddler system.

Once the platform is running, end users can interface with the Fiddler platform using their browser, the [Fiddler Python client](ref:about-the-fiddler-client), or Fiddler's RESTful APIs.
"
"---
title: ""On-prem Installation Guide""
slug: ""installation-guide""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:20:10 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Fiddler can run on most mainstream flavors of Kubernetes, provided that a suitable [storage class](https://kubernetes.io/docs/concepts/storage/storage-classes/) is available to provide POSIX-compliant block storage (see [On-prem Technical Requirements](technical-requirements)).

## Before you start

- Create a namespace where Fiddler will be deployed, or request that a namespace/project be created for you by the team that administers your Kubernetes cluster.
  ```text
  [~] kubectl create ns my-fiddler-ns
  ```

- Identify the name of the storage class(es) that you will use for Fiddler's block storage needs. Consult the team that administers your Kubernetes cluster for guidance if you are not sure which class to use.
  ```
  [~] kubectl get storageclass
  NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
  gp2 (default)   kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  96d
  ```

- If using Kubernetes [ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) to route traffic to Fiddler, identify the name of the ingress class that should be used. Consult the team that administers your Kubernetes cluster for guidance if you are not sure which class to use.
  ```
  [~] kubectl get ingressclass
  NAME    CONTROLLER             PARAMETERS   AGE
  nginx   k8s.io/ingress-nginx   <none>       39d
  ```

## Quick-start any-prem deployment

Follow the steps below for a quick-start deployment of Fiddler on your Kubernetes cluster suitable for demonstration purposes. This configuration assumes that an ingress controller is available the cluster.

1. Create a `Secret` for pulling images from the Fiddler container registry using the YAML manifest provided to you.

   - Verify that the name of the secret is `fiddler-pull-secret`  
     ```yaml
     apiVersion: v1
     kind: Secret
     metadata:
       name: fiddler-pull-secret
     data:
       .dockerconfigjson: [REDACTED]
     type: kubernetes.io/dockerconfigjson
     ```

   - Create the secret in the namespace where Fiddler will be deployed.

     ```
     [~] kubectl -n my-fiddler-ns apply -f fiddler-pull-secret.yaml
     ```

2. Deploy Fiddler using Helm.

   ```
   [~] helm repo add fiddler https://helm.fiddler.ai/stable/fiddler
   [~] helm repo update
   [~] export STORAGE_CLASS=<my-storage-class>
   [~] export INGRESS_CLASS=<my-ingress-class>
   [~] export FIDDLER_FQDN=fiddler.acme.com
   [~] helm upgrade -i -n my-fiddler-ns \
      -f https://helm.fiddler.ai/stable/samples/v2.yaml \
      -f https://helm.fiddler.ai/stable/samples/anyprem.yaml  \
      --set=""graf"
"slug: ""installation-guide"" ana.grafana\.ini.server.root_url=https://${FIDDLER_FQDN}/grafana"" \
      --set=hostname=${FIDDLER_FQDN}  \
      --set=k8s.storage.className=${STORAGE_CLASS} \
      --set=clickhouse.storage.className=${STORAGE_CLASS} \
      --set=zookeeper.storage.className=${STORAGE_CLASS} \
      --set=ingress.class=${INGRESS_CLASS} \
      --wait \
       fiddler fiddler/fiddler
   ```
"
"---
title: ""On-prem Technical Requirements""
slug: ""technical-requirements""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:20:05 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Minimum System Requirements

Fiddler is horizontally scalable to support the throughput requirements for enormous production use-cases. The minimum system requirements below correspond to approximately 20 million inference events monitored per day (~230 EPS) for models with around 100 features, with 90 day retention.

- **Deployment**: Kubernetes namespace in AWS, Azure or GCP
- **Compute**: A minimum of 96 vCPU cores
- **Memory**: 384Gi
- **Persistent volumes**: 500 Gi storage across 10 volumes 
  - POSIX-compliant block storage
  - 125 MB/s recommended
  - 3,000 IOPS recommended
- **Container Registry**: Quay.io or similar
- **Ingress Controller**: Ingress-nginx or AWS/GCP/Azure Load Balancer Controller
- **DNS**: FQDN that resolves to an L4 or L7 load balancer/proxy that provides TLS termination

## Kubernetes Cluster Requirements

As stated above, Fiddler requires a Kubernetes cluster to install into.  The following outlines the requirements for this K8 cluster:

- **Node Groups**:  2 node groups -  1 for core Fiddler services, 1 for Clickhouse (Fiddler's event database)
- **Resources**:
  - Fiddler :  48 vCPUs, 192 Gi
  - Clickhouse :  64 vCPUs, 256 Gi [tagged & tainted]
- **Persistent Volumes**: 500 GB (minimum) /  1 TB (recommended)
- **Instance Sizes**

  | Instance Size | AWS    | Azure      | GCP        |
  | :------------ | :----- | :--------- | :--------- |
  | Minimum       | m5.4xl | Std_D16_v3 | c2d_std_16 |
  | Recommended   | m5.8xl | Std_D32_v3 | c2d_std_32 |
"
"---
title: ""Release 22.11 Notes""
slug: ""release-notes-2211""
type: """"
createdAt: {}
hidden: false
---
This page enumerates the new features and updates in this release of the Fiddler platform.

## Release of Fiddler platform version 22.11:

- Alert authoring and maintenance via the Fiddler Client
- New Add Model APIs via the Fiddler Client

## What's New and Improved:

- **Support for alert authoring and management via the Fiddler Client**
  - Add and delete alert rules
  - Retrieve alert rules and triggered alerts
  - Setup alert notifications via Slack, email, and PagerDuty
  - Learn more through the [API Reference Docs](https://docs.fiddler.ai/v1.5/reference/clientadd_alert_rule) and [User Guide](https://docs.fiddler.ai/v1.5/docs/alerts-client) 
- **Support for new add model APIs via the Fiddler Client **
  - Deprecated `register_model`, now using `add_model` in combination with `add_model_surrogate` instead
  - Deprecated `trigger_pre_computation`
  - Deprecated `upload_model_package`, now using `add_model_artifact`

### Client Version

Client version 1.5 is required for the updates and features mentioned in this release."
"---
title: ""Release 23.2 Notes""
slug: ""release-232""
type: """"
createdAt: {}
hidden: false
---
This page enumerates the new features and updates in Release 23.2 of the Fiddler platform.

## Release of Fiddler platform version 23.2:

- Support for uploading multiple baselines to a model

- Alert context overlay on the chart editor

- Ability to customize scale and range of y-axis on the chart editor

## What's New and Improved:

- **Support for uploading multiple baselines**
  - Flexibility to add baseline datasets or use production data as the baseline.
  - Perform comparisons amongst multiple baselines to understand how different baselines ‚Äî data shifts due seasonality or geography for example ‚Äî may influence model drift and model behavior.
  - Learn more on the [Baselines Platform Guide](doc:fiddler-baselines).

- **Alert context overlay on the chart editor**
  - For absolute alerts, alert context is an overlay on the chart area to easily identify critical and warning thresholds.
  - For relative alerts, Fiddler will automatically plot historic comparison data for additional context on why the alert fired.

- **Customization in the chart editor**
  - Further customize charts by toggling between logarithmic and linear scale, and manually setting the min and max values of the y-axis.
  - Learn more on the [Monitoring Charts](doc:monitoring-charts-ui) page.

### Client Version

Client version 1.8 is required for the updates and features mentioned in this release."
"---
title: ""Release 23.6 Notes""
slug: ""release-236-notes""
type: """"
createdAt: {}
hidden: false
---
This page enumerates the new features and updates in Release 23.6 of the Fiddler platform.

## Release of Fiddler platform version 23.6:

- Embedding visualization with UMAP is now available for all customers

- Support for user-defined metrics via custom metrics

- Improved alert messaging for Webhook notifications

- Improved standalone job status page

## What's New and Improved:

- **Custom Metrics**
  - Ability to define flexible metrics via the UI and client
  - Custom metrics can be used to create monitoring charts and alerts
  - Metrics are defined using the [Fiddler Query Language (FQL)](doc:fiddler-query-language)
  - [Learn more](doc:custom-metrics)
- **Improved Webhook Messaging**
  - Refining ""Why Fiddler Generated"" alerts for Webhook with enhanced details
  - Learn how to enable [webhook notification](doc:alerts-ui#alert-notification-options).
- **Improved Job Status **
  - Access all your models, datasets, and events jobs through the Jobs page in the navigation bar.
  - Learn more on the [Product Tour](doc:product-tour)
- **Embedding Visualization with UMAP**
  - Visualize your custom features with high dimensional data using UMAP
  - [Learn More](doc:embedding-visualization-with-umap)

### Client Version

Client version 2.2+ is required for the updates and features mentioned in this release."
"---
title: ""Release 23.3 Notes""
slug: ""release-233""
type: """"
createdAt: {}
hidden: false
---
This page enumerates the new features and updates in Release 23.3 of the Fiddler platform.

> üìò Platform Release Version 23.3 & Doc v1.8 compatability note
> 
> Note that the documentation version remains v1.8 with this release. The new and improved functionalities are added to their respective pages with the note regarding platform version 23.3 as a requirement.

## Release of Fiddler platform version 23.3:

- Support for added charting up to 6 metrics for one or multiple models 

- Ability to assign metrics to the left or right y-axis in monitoring charts

- Addition of automatically created model monitoring dashboards

- New Root Cause Analysis tab with data drift and data integrity information in monitoring charts 

## What's New and Improved:

- **Multiple metric queries in monitoring charts**
  - Flexibility to add up to 6 metrics queries to visualize multiple metrics or models in one chart.
  - Enables model-to-model comparison in a single chart.
  - Learn more on the [Monitoring Charts Platform Guide](doc:monitoring-charts-platform).

- **Y-axis assignment in monitoring charts**
  - Further, customize charts by assigning metric queries to a left or right y-axis in the customize tab.
  - Learn more on the [Monitoring Charts UI Guide](doc:monitoring-charts-ui).

- **Automatically generated model dashboards**
  - Fiddler will automatically create a model dashboard for all models added to the platform, consisting of charts that display data drift, performance, data integrity, and traffic information.
  - Learn more on the[Dashboards Platform Guide](doc:dashboards-platform).

- **Root cause analysis in monitoring charts**
  - Examine specific timestamps within a monitoring time series chart to reveal the underlying reasons for model underperformance, using visualizations of data drift and data integrity insights.
  - Learn more on the page  [Monitoring Charts UI Guide](doc:monitoring-charts-ui).

### Client Version

Client version 1.8 is required for the updates and features mentioned in this release."
"---
title: ""Release 22.12 Notes""
slug: ""release-notes-2022-2-10""
type: """"
createdAt: {}
hidden: false
---
This page enumerates the new features and updates in this release of the Fiddler platform.

## Release of Fiddler platform version 22.12:

- Scale & performance improvements

- Alert on Metadata Columns

- New API for updating existing model artifacts or surrogate models

## What's New and Improved:

- **Scale and performance improvements for monitoring metrics**
  - Significant service refactoring for faster computing of monitoring metrics

- **Support for setting monitoring alerts on metadata columns**
  - Ability to configure Data Drift and Data Integrity alerts on metadata columns

- **Support for updating existing model artifacts or surrogate models to user-uploaded models**
  - The [`update_model_artifact`](ref:clientupdate_model_artifact) method allows you to modify existing surrogate or user-uploaded models with new user uploaded-models. This will be replacing the previously used `update_model` method
  - Read the [API Reference Documentation](https://docs.fiddler.ai/reference/clientupdate_model_artifact) to learn more

### Client Version

Client version 1.6 is required for the updates and features mentioned in this release."
"---
title: ""Release 23.4 Notes""
slug: ""release-234-notes""
type: """"
createdAt: {}
hidden: false
---
> üìò **Version Numbering Update**
> 
> As of version 23.4, the documentation versioning will mirror the **platform release version numbers**, rather than the** client version numbers**.
> 
> Please refer to the bottom of the release notes for information on which client version is recommended for a given platform release.

## Release of Fiddler platform version 23.4:

- Support for custom features in Charts, Alerts, and Root Cause Analysis
- Ability to set alerts on multiple columns at once
- Support for webhook alert notifications
- Two new monitoring metrics for analyzing numeric columns (Average and Sum)
- Improved Python client usability

## What‚Äôs New and Improved:

- **Custom Features**
  - Custom features can plotted on Charts
  - Alerts can be set on Custom features
- **Alert Rules on Multiple Columns**
  - Streamlined the workflow by enabling users to designate alert rules for up to 20 specified columns
- **Webhook Alert Integration**
  - Webhook is available as a new destination for alert notifications. 
- **Statistic Metrics**
  - Adds a new Metric Type to Charts and Alerts, enabling two new Metrics:
    - Average (takes the average of a numeric Column)
    - Sum (takes the sum of a numeric Column)
  - Learn more on the page [Statistics](doc:statistics)
- **Python client 2.0**
  - Refactors the client structure for improved usability (see API documentation for detailed information)
  - Removed methods:
    - run_feature_importance
    - run_explanation
    - run_fairness
    - run_model
  - New methods:
    - [get_feature_importance](doc:clientget_feature_importance) 
    - [get_feature_impact](doc:clientget_feature_impact) 
    - [get_explanation](doc:clientget_explanation) 
    - [get_fairness](doc:clientget_fairness) 
    - [get_predictions](doc:clientget_predictions) 
  - Updated methods:
    - [get_mutual_information](doc:clientget_mutual_information) 
    - [add_alert_rule](doc:clientget_alert_rule) 
    - [get_alert_rules](doc:clientget_alert_rules) 

### Client Version

Client version 2.0 or above is required for the updates and features mentioned in the 23.4 release."
"---
title: ""Release 23.5 Notes""
slug: ""release-235-notes""
type: """"
createdAt: {}
hidden: false
---
This page enumerates the new features and updates in Release 23.5 of the Fiddler platform.

## Release of Fiddler platform version 23.5:

- Support for new `Vector` data type

- Support for new `Frequency` statistics metric

- Support for new `NOT_SET` task type

- New standalone bookmarks page

- Contact your customer success team to get access and documentation to the below:
  - Addition of LLM task
  - Auto embeddings on Text (Prompt, Response, etc) and Image data 
  - Auto enrichments of Toxicity, PII, and Hallucination on LLM task
  - 3D UMAP visualization of high-dimensional embeddings
  - Support for user-defined metrics -> Custom metrics.  Alerts can be set on these custom metrics and plotted in charts and dashboards.

## What's New and Improved:

- **New Vector input type**
  - You can now create custom features from Vector data type
  - The DI violation functionality on vector data types is now available to quickly detect any issues in your data pipelines. As vector and embedding pipelines are complex and prone to errors, we hope this functionality will be of value.
    - Violation of value - Dimension does not match the expected dimension
    - Violation of nullable - Vector length equals zero or received NULL value
    - Violation of type - Value is not of the type of VECTOR or VECTOR contains a non-numerical type
  - Clusters of Text Embeddings have a short summary to understand which of your clusters have the most problematic drift.
  - Events of a particular cluster can be queried in the 'Analyze' tab of the model. This can be used to pinpoint the most problematic prompts/responses in an LLM scenario or images in a CV scenario. This information can be used to improve your models. 
- **New Task Type for Task-less Models**
  - You can now add models without a task by choosing the `NOT_SET` task type when onboarding your model.
  - Note that task-less models have no restrictions on output and target columns, but performance metrics are disabled when a task is not specified.
- **Standalone Bookmarks Page**
  - Access all your bookmarked projects, models, datasets, charts, and dashboards through the Bookmarks page in the navigation bar.
  - Learn more on the [Product Tour](doc:product-tour)

### Client Version

Client version 2.1+ is required for the updates and features mentioned in this release."
"---
title: ""Release 23.1 Notes""
slug: ""2023-3-31""
type: """"
createdAt: {}
hidden: false
---
This page enumerates the new features and updates in Release 23.1 of the Fiddler platform.

## Release of Fiddler platform version 23.1:

- New monitoring chart editor

- New dashboard reporting tool

- Flexible model deployment options

- Scale & performance improvements

- GitHub samples migration

## What's New and Improved:

- **New flexible monitoring chart editor**
  - Create customized charts for model monitoring metrics like Performance,Data Drift, Data Integrity, and more.
  - Learn more on the [Monitoring Charts Platform Guide](https://docs.fiddler.ai/v1.7/docs/monitoring-charts-platform).

- **New dashboard reporting tool for monitoring charts**
  - Combine the monitoring charts that help track model performance and health in a cohesive dashboard for your reporting needs.
  - Learn more on the [Dashboards Platform Guide](https://docs.fiddler.ai/v1.7/docs/dashboards-platform).

- **Flexible model deployment options**
  - Fiddler now supports flexible model deployment by allowing users to spin up separate k8s pods for each model and varying dependencies for their models.
  - Learn more on the [Flexible Model Deployment](doc:model-deployment) page.

- **Scale and performance improvements**
  - Efficiently register models with 2,000 features within just 30 minutes
  - Performance improvements across Vector Monitoring, Multiclass Classification, and Ranking scenarios.

- **Migrating all Fiddler samples to new GitHub repository**
  - The `fiddler-samples` GitHub repository will be deprecated and replaced by the new [`fiddler-examples`](https://github.com/fiddler-labs/fiddler-examples) repository.

### Client Version

Client version 1.7 is required for the updates and features mentioned in this release."
"---
title: ""client.add_model_artifact""
slug: ""clientadd_model_artifact""
excerpt: ""Adds a model artifact to an existing model""
hidden: false
createdAt: ""Mon Aug 01 2022 03:09:29 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üìò Note
> 
> Before calling this function, you must have already added a model using [`add_model`](/reference/clientadd_model).

| Input Parameter   | Type                                                       | Default | Description                                                                                                                                              |
| :---------------- | :--------------------------------------------------------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------- |
| project_id        | str                                                        | None    | The unique identifier for the project.                                                                                                                   |
| model_id          | str                                                        | None    | A unique identifier for the model.                                                                                                                       |
| model_dir         | str                                                        | None    | A path to the directory containing all of the [model files](doc:artifacts-and-surrogates) needed to run the model.                                       |
| deployment_params | Optional\[[fdl.DeploymentParams](ref:fdldeploymentparams)] | None    | Deployment parameters object for tuning the model deployment spec. Supported from server version `23.1` and above with Model Deployment feature enabled. |

```python python
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'

client.add_model_artifact(  
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    model_dir='model_dir/',
)
```
"
"---
title: ""client.update_model_surrogate""
slug: ""clientupdate_model_surrogate""
excerpt: ""Re-generate surrogate model""
hidden: false
createdAt: ""Mon Jan 30 2023 08:25:06 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üìò Note
> 
> This method call cannot replace user uploaded model done using [add_model_artifact](ref:clientadd_model_artifact). It can only re-generate a surrogate model

This can be used to re-generate a surrogate model for a model

| Input Parameter   | Type                                                       | Default | Description                                                        |
| :---------------- | :--------------------------------------------------------- | :------ | :----------------------------------------------------------------- |
| project_id        | str                                                        | None    | A unique identifier for the project.                               |
| model_id          | str                                                        | None    | A unique identifier for the model.                                 |
| deployment_params | Optional\[[fdl.DeploymentParams](ref:fdldeploymentparams)] | None    | Deployment parameters object for tuning the model deployment spec. |
| wait              | Optional[bool]                                             | True    | Whether to wait for async job to finish(True) or return(False).    |

```python python
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'

client.update_model_surrogate(
    project_id=PROJECT_ID,
    model_id=MODEL_ID
)

# with deployment_params
client.update_model_surrogate(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    deployment_params=fdl.DeploymentParams(cpu=250, memory=500)
)
```

| Return Type | Description    |
| :---------- | :------------- |
| None        | Returns `None` |
"
"---
title: ""About Models""
slug: ""about-models""
excerpt: """"
hidden: false
createdAt: ""Mon May 23 2022 19:03:52 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
A model is a **representation of your machine learning model**. Each model must have an associated dataset to be used as a baseline for monitoring, explainability, and fairness capabilities.

You **do not need to upload your model artifact in order to onboard your model**, but doing so will significantly improve the quality of explanations generated by Fiddler.
"
"---
title: ""client.list_models""
slug: ""clientlist_models""
excerpt: ""Retrieves the model IDs of all models accessible within a project.""
hidden: false
createdAt: ""Mon May 23 2022 19:06:21 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type | Default | Description                            |
| :-------------- | :--- | :------ | :------------------------------------- |
| project_id      | str  | None    | The unique identifier for the project. |

```python Usage
PROJECT_ID = 'example_project'

client.list_models(
    project_id=PROJECT_ID
)
```

| Return Type | Description                                    |
| :---------- | :--------------------------------------------- |
| list        | A list containing the string ID of each model. |

```python Response
[
    'model_a',
    'model_b',
    'model_c'
]
```
"
"---
title: ""client.upload_model_package""
slug: ""clientupload_model_package""
excerpt: ""Registers a model with Fiddler and uploads a model artifact to be used for explainability and fairness capabilities.""
hidden: false
createdAt: ""Mon May 23 2022 19:21:34 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> ‚ùóÔ∏è Not supported with client 2.0 and above
> 
> Please use _client.add_model_artifact()_ going forward.
"
"---
title: ""client.update_model_artifact""
slug: ""clientupdate_model_artifact""
excerpt: ""Update the model artifact of an existing model with artifact (surrogate or customer uploaded)""
hidden: false
createdAt: ""Wed Jan 11 2023 21:01:46 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üìò Note
> 
> Before calling this function, you must have already added a model using [`add_model_surrogate`](/reference/clientadd_model_surrogate) or [`add_model_artifact`](/reference/clientadd_model_artifact)

| Input Parameter   | Type                                                       | Default | Description                                                                                                                                              |
| :---------------- | :--------------------------------------------------------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------- |
| project_id        | str                                                        | None    | The unique identifier for the project.                                                                                                                   |
| model_id          | str                                                        | None    | A unique identifier for the model.                                                                                                                       |
| model_dir         | str                                                        | None    | A path to the directory containing all of the model files needed to run the model.                                                                       |
| deployment_params | Optional\[[fdl.DeploymentParams](ref:fdldeploymentparams)] | None    | Deployment parameters object for tuning the model deployment spec. Supported from server version `23.1` and above with Model Deployment feature enabled. |

```python Usage
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'

client.update_model_artifact(  
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    model_dir='model_dir/',
)
```
"
"---
title: ""client.trigger_pre_computation""
slug: ""clienttrigger_pre_computation""
excerpt: ""Runs a variety of precomputation steps for a model.""
hidden: false
createdAt: ""Mon May 23 2022 19:36:35 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> ‚ùóÔ∏è Not supported with client 2.0 and above
> 
> This method is called automatically now when calling _client.add_model_surrogate()_ or _client.add_model_artifact()_.
"
"---
title: ""client.register_model""
slug: ""clientregister_model""
excerpt: ""Registers a model without uploading an artifact. Requires a** fdl.ModelInfo** object containing information about the model.""
hidden: false
createdAt: ""Mon May 23 2022 19:14:26 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> ‚ùóÔ∏è Not supported with client 2.0 and above
> 
> Please use _client.add_model()_ going forward.
"
"---
title: ""client.update_model""
slug: ""clientupdate_model""
excerpt: ""Replaces the model artifact for a model.""
hidden: false
createdAt: ""Mon May 23 2022 19:26:42 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
For more information, see [Uploading a Model Artifact](doc:uploading-model-artifacts).

> üöß Warning
> 
> This function does not allow for changes in a model's schema. The inputs and outputs to the model must remain the same.

| Input Parameter   | Type         | Default | Description                                                                                                                       |
| :---------------- | :----------- | :------ | :-------------------------------------------------------------------------------------------------------------------------------- |
| project_id        | str          | None    | The unique identifier for the project.                                                                                            |
| model_id          | str          | None    | A unique identifier for the model.                                                                                                |
| model_dir         | pathlib.Path | None    | A path to the directory containing all of the model files needed to run the model.                                                |
| force_pre_compute | bool         | True    | If True, re-run precomputation steps for the model. This can also be done manually by calling **client.trigger_pre_computation**. |

```python Usage
import pathlib

PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'

model_dir = pathlib.Path('model_dir')

client.update_model(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    model_dir=model_dir
)
```

| Return Type | Description                                           |
| :---------- | :---------------------------------------------------- |
| bool        | A boolean denoting whether the update was successful. |

```python Response
True
```
"
"---
title: ""client.add_model_surrogate""
slug: ""clientadd_model_surrogate""
excerpt: ""Adds a surrogate model to an existing a model without uploading an artifact.""
hidden: false
createdAt: ""Mon Aug 01 2022 03:05:32 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üìò Note
> 
> Before calling this function, you must have already added a model using [`add_model`](ref:clientadd_model).

> üöß Surrogate models are not supported for input_type = fdl.ModelInputType.TEXT

| Input Parameter   | Type                                                       | Default | Description                                                        |
| :---------------- | :--------------------------------------------------------- | :------ | :----------------------------------------------------------------- |
| project_id        | str                                                        | None    | A unique identifier for the project.                               |
| model_id          | str                                                        | None    | A unique identifier for the model.                                 |
| deployment_params | Optional\[[fdl.DeploymentParams](ref:fdldeploymentparams)] | None    | Deployment parameters object for tuning the model deployment spec. |

```python
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'

client.add_model_surrogate(
    project_id=PROJECT_ID,
    model_id=MODEL_ID
)

# with deployment_params
client.add_model_surrogate(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    deployment_params=fdl.DeploymentParams(cpu=250, memory=500)
)
```

| Return Type | Description    |
| :---------- | :------------- |
| None        | Returns `None` |
"
"---
title: ""client.add_model""
slug: ""clientadd_model""
excerpt: ""Adds a model to Fiddler without uploading an artifact. Requires a** fdl.ModelInfo** object containing information about the model. Requires dataset to have an **output** column.""
hidden: false
createdAt: ""Mon Aug 01 2022 01:48:09 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type          | Default | Description                                                                                                                                                                                              |
| :-------------- | :------------ | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| project_id      | str           | None    | The unique identifier for the project.                                                                                                                                                                   |
| model_id        | str           | None    | A unique identifier for the model. Must be a lowercase string between 2-30 characters containing only alphanumeric characters and underscores. Additionally, it must not start with a numeric character. |
| dataset_id      | str           | None    | The unique identifier for the dataset.                                                                                                                                                                   |
| model_info      | fdl.ModelInfo | None    | A [fdl.ModelInfo()](ref:fdlmodelinfo) object containing information about the model.                                                                                                                     |

```python Usage
PROJECT_ID = 'example_project'
DATASET_ID = 'example_dataset'
MODEL_ID = 'example_model'

dataset_info = client.get_dataset_info(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID
)

model_task = fdl.ModelTask.BINARY_CLASSIFICATION
model_target = 'target_column'
model_output = 'output_column'
model_features = [
    'feature_1',
    'feature_2',
    'feature_3'
]

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    target=model_target,
    outputs=[model_output],
    model_task=model_task
)

client.add_model(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    model_id=MODEL_ID,
    model_info=model_info
)
```

| Return Type | Description                                    |
| :---------- | :--------------------------------------------- |
| str         | A message confirming that the model was added. |
"
"---
title: ""client.get_model_info""
slug: ""clientget_model_info""
excerpt: ""Retrieves the **ModelInfo** object associated with a model.""
hidden: false
createdAt: ""Mon May 23 2022 19:40:10 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type | Default | Description                                                                                                                                                                                              |
| :-------------- | :--- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| project_id      | str  | None    | The unique identifier for the project.                                                                                                                                                                   |
| model_id        | str  | None    | A unique identifier for the model. Must be a lowercase string between 2-30 characters containing only alphanumeric characters and underscores. Additionally, it must not start with a numeric character. |

```python Usage
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'

model_info = client.get_model_info(
    project_id=PROJECT_ID,
    model_id=MODEL_ID
)
```

| Return Type                       | Description                                                   |
| :-------------------------------- | :------------------------------------------------------------ |
| [fdl.ModelInfo](ref:fdlmodelinfo) | The **ModelInfo** object associated with the specified model. |
"
"---
title: ""client.delete_model""
slug: ""clientdelete_model""
excerpt: ""Deletes a model from a project.""
hidden: false
createdAt: ""Mon May 23 2022 19:31:30 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
For more information, see [Uploading a Model Artifact](doc:uploading-model-artifacts).

| Input Parameter | Type | Default | Description                            |
| :-------------- | :--- | :------ | :------------------------------------- |
| project_id      | str  | None    | The unique identifier for the project. |
| model_id        | str  | None    | A unique identifier for the model      |

```python Usage
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'

client.delete_model(
    project_id=PROJECT_ID,
    model_id=MODEL_ID
)
```
"
"---
title: ""client.get_explanation""
slug: ""clientget_explanation""
excerpt: ""Get explanation for a single observation.""
hidden: false
createdAt: ""Wed Aug 16 2023 11:21:48 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
[block:parameters]
{
  ""data"": {
    ""h-0"": ""Input Parameter"",
    ""h-1"": ""Type"",
    ""h-2"": ""Default"",
    ""h-3"": ""Description"",
    ""0-0"": ""project_id"",
    ""0-1"": ""str"",
    ""0-2"": ""None"",
    ""0-3"": ""A unique identifier for the project."",
    ""1-0"": ""model_id"",
    ""1-1"": ""str"",
    ""1-2"": ""None"",
    ""1-3"": ""A unique identifier for the model."",
    ""2-0"": ""input_data_source"",
    ""2-1"": ""Union\\[[fdl.RowDataSource](ref:fdlrowdatasource), [fdl.EventIdDataSource](ref:fdleventiddatasource)]"",
    ""2-2"": ""None"",
    ""2-3"": ""Type of data source for the input dataset to compute explanation on (RowDataSource, EventIdDataSource). A single row explanation is currently supported."",
    ""3-0"": ""ref_data_source"",
    ""3-1"": ""Optional\\[Union\\[[fdl.DatasetDataSource](ref:fdldatasetdatasource), [fdl.SqlSliceQueryDataSource](ref:fdlsqlslicequerydatasource)] ]"",
    ""3-2"": ""None"",
    ""3-3"": ""Type of data source for the reference data to compute explanation on (DatasetDataSource, SqlSliceQueryDataSource).  \nOnly used for non-text models and the following methods:  \n'SHAP', 'FIDDLER_SHAP', 'PERMUTE', 'MEAN_RESET'"",
    ""4-0"": ""explanation_type"",
    ""4-1"": ""Optional[str]"",
    ""4-2"": ""'FIDDLER_SHAP'"",
    ""4-3"": ""Explanation method name. Could be your custom  \nexplanation method or one of the following method:  \n'SHAP', 'FIDDLER_SHAP', 'IG', 'PERMUTE', 'MEAN_RESET', 'ZERO_RESET'"",
    ""5-0"": ""num_permutations"",
    ""5-1"": ""Optional[int]"",
    ""5-2"": ""300"",
    ""5-3"": ""- For Fiddler SHAP, num_permutations corresponds to the number of coalitions to sample to estimate the Shapley values of each single-reference game.  \n- For the permutation algorithms, num_permutations corresponds to the number of permutations from the dataset to use for the computation."",
    ""6-0"": ""ci_level"",
    ""6-1"": ""Optional[float]"",
    ""6-2"": ""0.95"",
    ""6-3"": ""The confidence level (between 0 and 1)."",
    ""7-0"": ""top_n_class"",
    ""7-1"": ""Optional[int]"",
    ""7-2"": ""None"",
    ""7-3"": ""For multi-class classification models only, specifying if only the n top classes are computed or all classes (when parameter is None).""
  },
  """
"slug: ""clientget_explanation"" cols"": 4,
  ""rows"": 8,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


```python Usage
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'
DATASET_ID = 'example_dataset

# FIDDLER SHAP - Dataset reference data source
row = df.to_dict(orient='records')[0]
client.get_explanation(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    input_data_source=fdl.RowDataSource(row=row),
    ref_data_source=fdl.DatasetDataSource(dataset_id=DATASET_ID, num_samples=300),
    explanation_type='FIDDLER_SHAP',
    num_permutations=200,
    ci_level=0.95,
)

# FIDDLER SHAP - Slice ref data source
row = df.to_dict(orient='records')[0]
query = f'SELECT * from {DATASET_ID}.{MODEL_ID} WHERE sulphates >= 0.8'
client.get_explanation(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    input_data_source=fdl.RowDataSource(row=row),
    ref_data_source=fdl.SqlSliceQueryDataSource(query=query, num_samples=100),
    explanation_type='FIDDLER_SHAP',
    num_permutations=200,
    ci_level=0.95,
)

# FIDDLER SHAP - Multi-class classification (top classes)
row = df.to_dict(orient='records')[0]
client.get_explanation(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    input_data_source=fdl.RowDataSource(row=row),
    ref_data_source=fdl.DatasetDataSource(dataset_id=DATASET_ID),
    explanation_type='FIDDLER_SHAP',
    top_n_class=2
)

# IG (Not available by default, need to be enabled via package.py)
row = df.to_dict(orient='records')[0]
client.get_explanation(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    input_data_source=fdl.RowDataSource(row=row),
    explanation_type='IG',
)
```

| Return Type | Description                                 |
| :---------- | :------------------------------------------ |
| tuple       | A named tuple with the explanation results. |
"
"---
title: ""client.get_feature_importance""
slug: ""clientget_feature_importance""
excerpt: ""Get global feature importance for a model over a dataset or a slice.""
hidden: false
createdAt: ""Wed Aug 16 2023 11:25:39 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type                                                                                                                     | Default | Description                                                                                                               |
| :-------------- | :----------------------------------------------------------------------------------------------------------------------- | :------ | :------------------------------------------------------------------------------------------------------------------------ |
| project_id      | str                                                                                                                      | None    | A unique identifier for the project.                                                                                      |
| model_id        | str                                                                                                                      | None    | A unique identifier for the model.                                                                                        |
| data_source     | Union\[[fdl.DatasetDataSource,](ref:fdldatasetdatasource) [fdl.SqlSliceQueryDataSource](ref:fdlsqlslicequerydatasource)] | None    | Type of data source for the input dataset to compute feature importance on (DatasetDataSource or SqlSliceQueryDataSource) |
| num_iterations  | Optional[int]                                                                                                            | 10000   | The maximum number of ablated model inferences per feature.                                                               |
| num_refs        | Optional[int]                                                                                                            | 10000   | Number of reference points used in the explanation.                                                                       |
| ci_level        | Optional[float]                                                                                                          | 0.95    | The confidence level (between 0 and 1).                                                                                   |
| overwrite_cache | Optional[bool]                                                                                                           | False   | Whether to overwrite the feature importance cached values or not                                                          |

```python Usage
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'
DATASET_ID = 'example_dataset'


# Feature Importance - Dataset data source
feature_importance = client.get_feature_importance(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    data_source=fdl.DatasetDataSource(dataset_id=DATASET_ID, num_samples=200),
    num_iterations=300,
    num_refs=200,
    ci_level=0.90,
)

# Feature Importance - Slice Query data source
query = f'SELECT * FROM {DATASET_ID}.{MODEL_ID} WHERE CreditScore > 700'
feature_importance = client.get_feature_importance(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    data_source=fdl.SqlSliceQueryDataSource(query=query, num_samples=80),
    num_iterations=300,
    num_refs=200,
    ci_level=0.90,
)
```

| Return Type | Description                                    |
| :---------- | :--------------------------------------------- |
| tuple       | A named tuple with the feature impact results. |
"
"---
title: ""client.get_mutual_information""
slug: ""clientget_mutual_information""
excerpt: ""Get Mutual Information for a dataset over a slice.""
hidden: false
createdAt: ""Wed Aug 30 2023 14:27:43 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type           | Default | Description                                                                               |
| :-------------- | :------------- | :------ | :---------------------------------------------------------------------------------------- |
| project_id      | str            | None    | A unique identifier for the project.                                                      |
| dataset_id      | str            | None    | A unique identifier for the dataset.                                                      |
| query           | str            | None    | Slice query to compute Mutual information on.                                             |
| column_name     | str            | None    | Column name to compute mutual information with respect to all the columns in the dataset. |
| normalized      | Optional[bool] | False   | If set to True, it will compute Normalized Mutual Information.                            |
| num_samples     | Optional[int]  | 10000   | Number of samples to select for computation.                                              |

```python Usage
PROJECT_ID = 'example_project'
DATASET_ID = 'example_dataset'
MODEL_ID = 'example_model'

query = f'SELECT * FROM {DATASET_ID}.{MODEL_ID} WHERE CreditScore > 700'
mutual_info = client.get_mutual_information(
  project_id=PROJECT_ID,
  dataset_id=DATASET_ID,
  query=query,
  column_name='Geography',
  normalized=True,
  num_samples=20000,
)
```

| Return Type | Description                                       |
| :---------- | :------------------------------------------------ |
| dict        | A dictionary with the mutual information results. |
"
"---
title: ""client.get_feature_impact""
slug: ""clientget_feature_impact""
excerpt: ""Get global feature impact for a model over a dataset or a slice.""
hidden: false
createdAt: ""Wed Aug 16 2023 11:23:05 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type                                                                                                                     | Default | Description                                                                                                                                                                      |
| :-------------- | :----------------------------------------------------------------------------------------------------------------------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| project_id      | str                                                                                                                      | None    | A unique identifier for the project.                                                                                                                                             |
| model_id        | str                                                                                                                      | None    | A unique identifier for the model.                                                                                                                                               |
| data_source     | Union\[[fdl.DatasetDataSource](ref:fdldatasetdatasource), [fdl.SqlSliceQueryDataSource](ref:fdlsqlslicequerydatasource)] | None    | Type of data source for the input dataset to compute feature impact on (DatasetDataSource or SqlSliceQueryDataSource)                                                            |
| num_iterations  | Optional[int]                                                                                                            | 10000   | The maximum number of ablated model inferences per feature. Used for TABULAR data only.                                                                                          |
| num_refs        | Optional[int]                                                                                                            | 10000   | Number of reference points used in the explanation. Used for TABULAR data only.                                                                                                  |
| ci_level        | Optional[float]                                                                                                          | 0.95    | The confidence level (between 0 and 1). Used for TABULAR data only.                                                                                                              |
| output_columns  | Optional\[List[str]]                                                                                                     | None    | Only used for NLP (TEXT inputs) models. Output column names to compute feature impact on. Useful for Multi-class Classification models. If None, compute for all output columns. |
| min_support     | Optional[int]                                                                                                            | 15      | Only used for NLP (TEXT inputs) models. Specify a minimum support (number of times a specific word was present in the sample data) to retrieve top words. Default to 15.         |
| overwrite_cache | Optional[bool]                                                                                                           | False   | Whether to overwrite the feature impact cached values or not.                                                                                                                    |

```python Usage
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'
DATASET_ID = 'example_dataset'

# Feature Impact for TABULAR data - Dataset Data Source
feature_impact = client.get_feature_impact(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    data_source=fdl.DatasetDataSource(dataset_id=DATASET_ID, num_samples=200),
    num_iterations=300,
    num_refs=200,
    ci_level=0.90,
)

# Feature Impact for TABULAR data - Slice Query data source
query = f'SELECT * FROM {DATASET_ID}.{MODEL_ID} WHERE CreditScore > 700'
feature_impact = client.get_feature_impact(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    data_source=fdl.SqlSliceQueryDataSource(query=query, num_samples=80),
    num_iterations=300,
    num_refs=200,
    ci_level=0.90,
)

# Feature Impact for TEXT data
feature_impact = client.get_feature_impact(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    data_source=fdl.DatasetDataSource(dataset_id=DATASET_ID, num_samples=50),
    output_columns= ['probability_A', 'probability_B'],
  	min"
"slug: ""clientget_feature_impact"" _support=30
)
```

| Return Type | Description                                    |
| :---------- | :--------------------------------------------- |
| tuple       | A named tuple with the feature impact results. |
"
"---
title: ""client.get_predictions""
slug: ""clientget_predictions""
excerpt: ""Runs a model on a pandas DataFrame and returns the predictions.""
hidden: false
createdAt: ""Wed Aug 16 2023 11:19:57 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type          | Default | Description                                                            |
| :-------------- | :------------ | :------ | :--------------------------------------------------------------------- |
| project_id      | str           | None    | A unique identifier for the project.                                   |
| model_id        | str           | None    | A unique identifier for the model.                                     |
| input_df        | pd.DataFrame  | None    | A pandas DataFrame containing model input vectors as rows.             |
| chunk_size      | Optional[int] | 10000   | The chunk size for fetching predictions. Default is 10_000 rows chunk. |

```python Usage
import pandas as pd

PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'

input_df = pd.read_csv('example_data.csv')

# Example without chunk size specified:
predictions = client.get_predictions(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    input_df=input_df,
)


# Example with chunk size specified:
predictions = client.get_predictions(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    input_df=input_df,
    chunk_size=1000,
)
```

| Return Type  | Description                                                                  |
| :----------- | :--------------------------------------------------------------------------- |
| pd.DataFrame | A pandas DataFrame containing model predictions for the given input vectors. |
"
"---
title: ""Alerting Integrations""
slug: ""alerting-integrations""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:18:54 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
"
"---
title: ""ML Platform Integrations""
slug: ""ml-platform-integrations""
excerpt: """"
hidden: false
createdAt: ""Wed Jun 22 2022 14:27:52 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
"
"---
title: ""Data Pipeline Integrations""
slug: ""data-pipeline-integrations""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:18:12 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
"
"---
title: ""PagerDuty Integration""
slug: ""pagerduty""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:19:10 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Fiddler offers powerful alerting tools for monitoring models. By integrating with  
PagerDuty services, you gain the ability to trigger PagerDuty events within your monitoring  
workflow.

> üìò If your organization has already integrated with PagerDuty, then you may skip to the [Setup: In Fiddler](#setup-in-fiddler) section to learn more about setting up PagerDuty within Fiddler.

## Setup: In PagerDuty

1. Within your PagerDuty Team, navigate to **Services** ‚Üí **Service Directory**.

![](https://files.readme.io/0ae47bb-pagerduty_1.png ""pagerduty_1.png"")

2. Within the Service Directory:
   - If you are creating a new service for integration, select **+New Service** and follow the prompts to create your service.
   - Click the **name of the service** you want to integrate with.

![](https://files.readme.io/956dbdf-pagerduty_2.png ""pagerduty_2.png"")

3. Navigate to **Integrations** within your service, and select **Add a new integration to this service**.

![](https://files.readme.io/ca2e4c2-pagerduty_3.png ""pagerduty_3.png"")

4. Enter an **Integration Name**, and under **Integration Type** select the option **Use our API directly**. Then, select the **Add Integration** button to save your new integration. You will be redirected to the Integrations page for your service.

![](https://files.readme.io/0f5d5ae-pagerduty_4.png ""pagerduty_4.png"")

5. Copy the **Integration Key** for your new integration.

![](https://files.readme.io/e144e08-pagerduty_5.png ""pagerduty_5.png"")

## Setup: In Fiddler

1. Within **Fiddler**, navigate to the **Settings** page, and then to the **PagerDuty Integration** menu. If your organization **already has a PagerDuty service integrated with Fiddler**, you will be able to find it in the list of services.

![](https://files.readme.io/8de1a6b-pagerduty_setup_f_1.png ""pagerduty_setup_f_1.png"")

2. If you are looking to integrate with a new service, select the **`+`** box on the top right. Then, enter the name of your service, as well as the Integration Key copied from the end of the [Setup: In PagerDuty](#setup-in-pagerduty) section above. After creation, confirm that your new entry is now in the list of available services.

![](https://files.readme.io/9febb10-pagerduty_setup_f_2.png ""pagerduty_setup_f_2.png"")

> üöß Creating, editing, and deleting these services is an **ADMINSTRATOR**-only privilege. Please contact an **ADMINSTRATOR** within your organization to setup any new PagerDuty services

## PagerDuty Alerts in Fiddler

1. Within the **Projects** page, select"
"slug: ""pagerduty""  the model you wish to use with PagerDuty.

![](https://files.readme.io/d9ad82e-pagerduty_fiddler_1.png ""pagerduty_fiddler_1.png"")

2. Select **Monitor** ‚Üí **Alerts** ‚Üí **Add Alert**.

![](https://files.readme.io/b7118f0-pagerduty_fiddler_2.png ""pagerduty_fiddler_2.png"")

3. Enter the condition you would like to alert on, and under **PagerDuty Services**, select all services you would like the alert to trigger for. Additionally, select the **Severity** of this alert, and hit **Save**.

![](https://files.readme.io/8fbffde-pagerduty_fiddler_3.png ""pagerduty_fiddler_3.png"")

4. After creation, the alert will now trigger for the specified PagerDuty services.

> üìò Info
> 
> Check out the [alerts documentation](doc:alerts-platform) for more information on setting up alerts.

## FAQ

**Can Fiddler integrate with multiple PagerDuty services?**

- Yes. So long as the service is present within **Settings** ‚Üí **PagerDuty Services**, anyone within your organization can select that service to be a recipient for an alert.
"
"---
title: ""BigQuery Integration""
slug: ""bigquery-integration""
excerpt: """"
hidden: false
createdAt: ""Fri May 20 2022 18:53:49 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Using Fiddler on your ML data stored in BigQuery

In this article, we will be looking at loading data from BigQuery tables and using the data for the following tasks-

1. Uploading baseline data to Fiddler
2. Onboarding a model to Fiddler and creating a surrogate
3. Publishing production data to Fiddler

## Step 1 - Enable BigQuery API

Before looking at how to import data from BigQuery to Fiddler, we will first see how to enable BigQuery API. This can be done as follows - 

1. In the GCP platform, Go to the navigation menu -> click APIs & Services. Once you are there, click + Enable APIs and Services (Highlighted below). In the search bar, enter BigQuery API and click Enable.

![](https://files.readme.io/75ca647-Screen_Shot_2022-05-19_at_1.26.33_PM.png ""Screen Shot 2022-05-19 at 1.26.33 PM.png"")

![](https://files.readme.io/3dd5deb-Screen_Shot_2022-05-19_at_3.33.43_PM.png ""Screen Shot 2022-05-19 at 3.33.43 PM.png"")

2. In order to make a request to the API enabled in Step#1, you need to create a service account and get an authentication file for your Jupyter Notebook. To do so, navigate to the Credentials tab under APIs and Services console and click Create Credentials tab, and then Service account under dropdown.

![](https://files.readme.io/ea63eca-Screen_Shot_2022-05-19_at_3.34.24_PM.png ""Screen Shot 2022-05-19 at 3.34.24 PM.png"")

3. Enter the Service account name and description. You can use the BigQuery Admin role under Grant this service account access to the project. Click Done. You can now see the new service account under the Credentials screen. Click the pencil icon beside the new service account you have created and click Add Key to add auth key. Please choose JSON and click CREATE. It will download the JSON file with auth key info. (Download path will be used to authenticate)

![](https://files.readme.io/662315e-Screen_Shot_2022-05-19_at_3.39.24_PM.png ""Screen Shot 2022-05-19 at 3.39.24 PM.png"")

## Step 2 - Import data from BigQuery

We will now use the generated key to connect to BigQuery tables from Jupyter Notebook. 

1. Install the following libraries in the python environment and load them to jupyter-

- Google-cloud
- Google-cloud-bigquery[pandas]
- Google-cloud-storage

2. Set the environment variable using the key that was generated in Step 1

```python
#Set environment variables for your notebook
import os
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '<path to json file>'
```

3. Import Google cloud client and initiate BigQuery service

```python
#Imports google cloud client library and initiates BQ service
from google.cloud import"
"slug: ""bigquery-integration""  bigquery
bigquery_client = bigquery.Client()
```

4. Specify the query which will be used to import the data from BigQuery

```python
#Write Query on BQ
QUERY = """"""
SELECT * FROM `fiddler-bq.fiddler_test.churn_prediction_baseline` 
  """"""
```

5. Read the data using the query and write the data to a pandas dataframe

```python
#Run the query and write result to a pandas data frame
Query_Results = bigquery_client.query(QUERY)
baseline_df = Query_Results.to_dataframe()
```

Now that we have data imported from BigQuery to a dataframe, we can refer to the following pages to

1. [Upload baseline data and onboard a model ](doc:uploading-a-baseline-dataset)
2. [Publish production events ](doc:publishing-batches-of-events)
"
"---
title: ""S3 Integration""
slug: ""integration-with-s3""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 17:40:36 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Pulling a dataset from S3

You may want to **pull a dataset directly from S3**. This may be used either to upload a baseline dataset, or to publish production traffic to Fiddler.

You can use the following code snippet to do so. Just fill out each of the string variables (`S3_BUCKET`, `S3_FILENAME`, etc.) with the correct information.

```python
import boto3
import pandas as pd

S3_BUCKET = 'my_bucket'
S3_FILENAME = 'my_baseline.csv'

AWS_ACCESS_KEY_ID = 'my_access_key'
AWS_SECRET_ACCESS_KEY = 'my_secret_access_key'
AWS_REGION = 'my_region'

session = boto3.session.Session(
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
    region_name=AWS_REGION
)

s3 = session.client('s3')

s3_data = s3.get_object(
    Bucket=S3_BUCKET,
    Key=S3_FILENAME
)['Body']

df = pd.read_csv(s3_data)
```

## Uploading the data to Fiddler

If your goal is to **use this data as a baseline dataset** within Fiddler, you can then proceed to upload your dataset (see [Uploading a Baseline Dataset](doc:uploading-a-baseline-dataset)).

If your goal is to **use this data as a batch of production traffic**, you can then proceed to publish the batch to Fiddler (see [Publishing Batches of Events](doc:publishing-batches-of-events) ). 

## What if I don‚Äôt want to hardcode my AWS credentials?

If you don‚Äôt want to hardcode your credentials, you can **use an AWS profile** instead. For more information on how to create an AWS profile, click [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html).

You can use the following code snippet to point your `boto3` session to the profile of your choosing.

```python
import boto3
import pandas as pd

S3_BUCKET = 'my_bucket'
S3_FILENAME = 'my_baseline.csv'

AWS_PROFILE = 'my_profile'

session = boto3.session.Session(
    profile_name=AWS_PROFILE
)

s3 = session.client('s3')

s3_data = s3.get_object(
    Bucket=S3_BUCKET,
    Key=S3_FILENAME
)['Body']

df = pd.read_csv(s3_data)
```

## What if I don't want to load the data into memory?

If you would rather **save the data to a disk** instead of loading it in as a pandas DataFrame, you can use the following code snippet instead.

```python
import boto3
import pandas as pd
import fiddler as fdl

S3_BUCKET = 'my_bucket'
S3_FILENAME = 'my_baseline.csv'

AWS_ACCESS_KEY_ID = 'my_access_key'
AWS_SECRET_ACCESS_KEY = 'my_secret_access_key'
AWS_REGION = 'my_region'

OUTPUT_FILENAME = 's3_data.csv'

session = boto3.session.Session(
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
    region_name=AWS_REGION
)

s3 = session.client('s3"
"slug: ""integration-with-s3"" ')

s3.download_file(
    Bucket=S3_BUCKET,
    Key=S3_FILENAME,
    Filename=OUTPUT_FILENAME
)
```
"
"---
title: ""Kafka Integration""
slug: ""kafka-integration""
excerpt: """"
hidden: false
createdAt: ""Tue May 23 2023 16:48:07 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Fiddler Kafka connector is a service that connects to a [Kafka topic](https://kafka.apache.org/documentation/#intro_concepts_and_terms) containing production events for a model, and publishes the events to Fiddler.

## Pre-requisites

We assume that the user has an account with Fiddler, has already created a project, uploaded a dataset and onboarded a model. We will need the [url_id, org_id,](doc:client-setup) project_id and model_id to configure the Kafka connector.

## Installation

The Kafka connector runs on Kubernetes within the customer‚Äôs environment. It is packaged as a Helm chart. To install:

```shell
helm repo add fiddler https://helm.fiddler.ai/stable/

helm repo update

kubectl -n kafka create secret generic fiddler-credentials --from-literal=auth=<API-KEY>

helm install fiddler-kafka fiddler/fiddler-kafka \
    --devel \
    --namespace kafka \
    --set fiddler.url=https://<FIDDLER-URL> \
    --set fiddler.org=<ORG> \
    --set fiddler.project_id=<PROJECT-ID> \
    --set fiddler.model_id=<MODEL-ID> \
    --set fiddler.ts_field=timestamp \
    --set fiddler.ts_format=INFER \
    --set kafka.host=kafka \
    --set kafka.port=9092 \
    --set kafka.topic=<KAFKA-TOPIC> \
    --set kafka.security_protocol=SSL \
    --set kafka.ssl_cafile=cafile \
    --set kafka.ssl_certfile=certfile \
    --set kafka.ssl_keyfile=keyfile \
    --set-string kafka.ssl_check_hostname=False

```

This creates a deployment that reads events from the Kafka topic and publishes it to the configured model. The deployment can be scaled as needed. However, if the Kafka topic is not partitioned, scaling will not result in any gains.

## Limitations

1. The connector assumes that there is a single dedicated topic containing production events for a given model. Multiple deployments can be created, one for each model, and scaled independently.
2. The connector assumes that events are published as JSON serialized dictionaries of key-value pairs. Support for other formats can be added on request. As an example, a Kafka message should look like the following:

```json
{
    ‚Äúfeature_1‚Äù: 20.7,
    ‚Äúfeature_2‚Äù: 45000,
    ‚Äúfeature_3‚Äù: true,
    ‚Äúoutput_column‚Äù: 0.79,
    ‚Äútarget_column‚Äù: 1,
    ‚Äúts‚Äù: 1637344470000,
}

```
"
"---
title: ""Snowflake Integration""
slug: ""snowflake-integration""
excerpt: """"
hidden: false
createdAt: ""Wed Jun 22 2022 14:51:45 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Using Fiddler on your ML data stored in Snowflake

In this article, we will be looking at loading data from Snowflake tables and using the data for the following tasks-

1. Uploading baseline data to Fiddler
2. Onboarding a model to Fiddler and creating a surrogate
3. Publishing production data to Fiddler

### Import data from Snowflake

In order to import data from Snowflake to Jupyter notebook, we will use the snowflake library, this can be installed using the following command in your Python environment.

```python
pip install snowflake-connector-python
```

Once the library is installed, we would require the following to establish a connection to Snowflake

- Snowflake Warehouse
- Snowflake Role
- Snowflake Account
- Snowflake User
- Snowflake Password

These can be obtained from your Snowflake account under the ‚ÄòAdmin‚Äô option in the Menu as shown below or by running the queries - 

- Warehouse - select CURRENT_WAREHOUSE()
- Role - select CURRENT_ROLE()
- Account - select CURRENT_ACCOUNT()

'User' and 'Password' are the same as one used for logging into your Snowflake account.

![](https://files.readme.io/c2f4cf4-Screen_Shot_2022-06-14_at_4.17.36_PM.png ""Screen Shot 2022-06-14 at 4.17.36 PM.png"")

Once you have this information, you can set up a Snowflake connector using the following code -

```python
# establish Snowflake connection
connection = connector.connect(user=snowflake_username, 
                               password=snowflake_password, 
                               account=snowflake_account, 
                               role=snowflake_role, 
                               warehouse=snowflake_warehouse
                              )
```

You can then write a custom SQL query and import the data to a pandas dataframe.

```python
# sample SQL query
sql_query = 'select * from FIDDLER.FIDDLER_SCHEMA.CHURN_BASELINE LIMIT 100'

# create cursor object
cursor = connection.cursor()

# execute SQL query inside Snowflake
cursor.execute(sql_query)

baseline_df = cursor.fetch_pandas_all()
```

### Publish Production Events

In order to publish production events from Snowflake, we can load the data to a pandas dataframe and publish it to fiddler using _client.publish_events_batch_ api.

Now that we have data imported from Snowflake to a jupyter notebook, we can refer to the following notebooks to

- [Upload baseline data and onboard a model ](doc:uploading-a-baseline-dataset)
- [Publish production events](doc:publishing-batches-of-events)
"
"---
title: ""Airflow Integration""
slug: ""airflow-integration""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:18:03 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Tue Dec 19 2023 20:40:11 GMT+0000 (Coordinated Universal Time)""
---
Apache Airflow is an open source platform ETL platform to manage company‚Äôs complex  
workflows. Companies are increasingly integrating their ML models pipeline into Airflow DAGs to manage and monitor all the components of their ML model system.

By integrating Fiddler into an existing Airflow DAG, you will be able to train, manage, and onboard your models while  actively monitoring performance, data quality, and troubleshooting degradations across your models.

Fiddler can be easily integrated into your existing airflow DAG for ML model pipeline. A notebook which is used for publishing events can be orchestrated to run as a part of your airflow DAG using a ‚ÄòPapermill Operator‚Äô.

## Steps for the walkthrough

1. Setup airflow on your local or docker, these steps can be followed. [Link](https://airflow.apache.org/docs/apache-airflow/stable/start/index.html)

2. Add your jupyter notebook containing the code for publishing to your airflow home directory. In this example we will use the 2 different notebooks - 

   a. [Notebook to onboard ML model to Fiddler platform](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/master/integration-examples/airflow/notebooks/Fiddler_Churn_Add_Model.ipynb)

   b. [Notebook to push production events to Fiddler platform](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/master/integration-examples/airflow/notebooks/Fiddler_Churn_Event_Publishing.ipynb)

3. Add an orchestration code to your airflow directory, airflow will pick up the orchestration code and construct a DAG as defined. The orchestration code contains the ‚Äòpapermill operator‚Äô to orchestrate the jupyter notebooks which will be used to onboard models and publish events to Fiddler. Please refer to our [orchestration code](https://github.com/fiddler-labs/fiddler-examples/tree/master/integration-examples/airflow/DAGs).

4. The run interval can be set up in orchestration code as ‚Äòschedule_interval‚Äô in the DAG class. This interval can be based on the frequency of training and inference of your ML model.

5. Once the DAGs are set up it can be monitored on the UI. Below we can see dummy DAGs have been set up with placeholder nodes for ‚Äòdata preparation ETL‚Äô and ‚Äòmodel training/inference‚Äô. We have two DAGs - 

   a. To set up Fiddler model registration after preparing baseline data (training pipeline)

   b. To publish events to Fiddler after data preparation and ML model inference (inference pipeline)

## Label Update

An important business use case is integrating Fiddler‚Äôs ‚ÄòLabel Update‚Äô as a part of your ML workflow using Airflow. Label update can be used to update the ground truth feature in your data. This can be done using the ‚Äò‚Äã‚Äãpublish_event‚Äô api, passing the event, event_id parameters, and making the update_event parameter as ‚ÄòTrue‚Äô.  
The code to update label can be found in the [notebook](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/master/integration-examples/airflow/note"
"slug: ""airflow-integration"" books/Fiddler_Churn_Label_Update.ipynb)  
This notebook can be integrated to run as a part of your airflow DAG using the [sample code](https://github.com/fiddler-labs/fiddler-examples/blob/master/integration-examples/airflow/DAGs/fiddler_event_update.py)

## Papermill Operator

```
operator_var = PapermillOperator(
        task_id=""task_name"",
        input_nb=""input_jupyter_notebook"",
        output_nb=""output_jupyter_notebook"",
        parameters={""variable_1"": ""{{ value }}""},
    )
```

## Airflow DAG

Below is an example of Model Registration Airflow DAG run history

![](https://files.readme.io/3fb8a21-model_registration_1.png ""model_registration_1.png"")

Model Registration Airflow DAG flow

![](https://files.readme.io/2891852-model_registration_2.png ""model_registration_2.png"")
"
"---
title: ""SageMaker Integration""
slug: ""sagemaker-integration""
excerpt: """"
hidden: false
createdAt: ""Fri May 13 2022 14:21:38 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
The following Python script can be used to define a AWS Lambda function that can move your SageMaker inference logs from an S3 bucket to a Fiddler environment.

## Setup

In addition to pasting this code into your Lambda function, you will need to ensure the following steps are completed before the integration will work.

1. Make sure your model is actively being served by SageMaker and that you have  [enabled data capture](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-capture.html) for your SageMaker hosted models so that your model inferences are stored in a S3 bucket as JSONL files.
2. Make sure you have a Fiddler trial environment and your SageMaker model is onboarded with Fiddler.  Check out our Getting Started guide for guidance on how to onboard your models.
3. Make sure to specify the environment variables in the ‚ÄúConfiguration‚Äù section of your Lambda function so that the Lambda knows how to connect with your Fiddler environment and so it knows what inputs and outputs to expect in the JSONL files captured by your SageMaker model.

![](https://files.readme.io/3b4cb21-lambda_setup.jpg ""lambda_setup.jpg"")

4. Make sure you have set up a trigger on your Lambda function so that the function is called upon ‚ÄúObject creation‚Äù events in your model‚Äôs S3 bucket.
5. Make sure you paste the following code into your new Lambda function.
6. Make sure that your Lambda function references the Fiddler ARN for the Layer that encapsulates the Fiddler Python client. (`arn:aws:lambda:us-west-2:079310353266:layer:fiddler-client-0814:1`)

## Script

```python
import fiddler as fdl
import json
import boto3
import os
import pandas as pd
import sys
import uuid
from urllib.parse import unquote_plus
import csv
import json
import base64
from io import StringIO
from botocore.vendored import requests

s3_client = boto3.client('s3')
url = os.getenv('FIDDLER_URL')
org = os.getenv('FIDDLER_ORG')
token = os.getenv('FIDDLER_TOKEN')
project = os.getenv('FIDDLER_PROJECT')
model = os.getenv('FIDDLER_MODEL')
timestamp_field = os.getenv('FIDDLER_TIMESTAMP_FIELD', None)  # optional arg
id_field = os.getenv('FIDDLER_ID_FIELD', None)  # optional arg
timestamp_format = os.getenv('FIDDLER_TIMESTAMP_FORMAT', None)  # optional arg
credentials = os.getenv('FIDDLER_AWS_CREDENTIALS', '{}')  # optional arg, json string
string_in_features = os.getenv('FEATURE_INPUTS')
out_feature = os.getenv('MODEL_OUTPUT')

def lambda_handler(event, context):
    for record in event['Records']:

        bucket = record['s3']['bucket']['name']
        key = unquote_plus(record['s3']['object']['key'])
        tmpkey = key.replace('/', '')
        download_path = '/tmp/{}{}'.format(uuid.uuid4(), tmpkey)
        s3_client.download_file(bucket, key"
"slug: ""sagemaker-integration"" , download_path)
        parse_sagemaker_log(download_path)

    
    return {
        'statusCode': 200,
        'body': json.dumps('Successful Lambda Publishing Run')
    }
                  

def parse_sagemaker_log(log_file):
    with open(log_file) as f:

        result = {}
        resultList = []
        in_features= string_in_features.replace(""'"", """").split(',')
        
        for line in f:
            pline = json.loads(line)
            input = pline['captureData']['endpointInput']['data']
            inputstr = StringIO(input)
            output = pline['captureData']['endpointOutput']['data']
            outputstr = StringIO(output)
            outarray = list(csv.reader(outputstr, delimiter=','))
            
            new_outarray = [float(x) for x in outarray[0]]
        
            csvReader = csv.reader(inputstr, delimiter=',')
            j = 0
            for row in csvReader:
                input_dict = {in_features[i]: row[i] for i in range(len(row))}
                
                pred_dict = {out_feature:new_outarray[j]}
                result.update(input_dict)
                result.update(pred_dict)
                result['__event_type'] = 'execution_event'
                resultList.append(result)
                j= j+1

        df = pd.DataFrame(resultList)
        print(""Data frame : "", df)
        publish_event(df, log_file)

def assert_envs():
    """"""
    Asserting presence of required environmental variables:
        - FIDDLER_URL
        - FIDDLER_ORG
        - FIDDLER_TOKEN
        - FIDDLER_PROJECT
        - FIDDLER_MODEL
    """"""
    try:
        assert url is not None, '`FIDDLER_URL` env variable must be set.'
        assert org is not None, '`FIDDLER_ORG` env variable must be set.'
        assert token is not None, '`FIDDLER_TOKEN` env variable must be set.'
        assert project is not None, '`FIDDLER_PROJECT` env variable must be set.'
        assert model is not None, '`FIDDLER_MODEL` env variable must be set.'

        return None
    except Exception as e:
        log(f'ERROR: Env Variable assertion failed: {str(e)}')
        return {
            'statusCode': 500,
            'body': json.dumps(f'ERROR: Env Variable assertion failed: {str(e)}'),
        }

def get_timestamp_format(env_timestamp_format):
    """"""
    Parses environment variable to convert `string` to `enum` value
    """"""
    if env_timestamp_format == 'EPOCH_MILLISECONDS':
        return fdl.FiddlerTimestamp.EPOCH_MILLISECONDS
    elif env_timestamp_format == 'EPOCH_SECONDS':
        return fdl.FiddlerTimestamp.EPOCH_SECONDS
    elif env_timestamp_format == 'ISO_8601':
        return fdl.FiddlerTimestamp.ISO_8601
    else:
        return fdl.FiddlerTimestamp.INFER

def log(out):
    print(out)

def publish_event(df, log_file):
    client = fdl.FiddlerApi(url=url, org_id=org, auth_token=token)

    log(f'Publishing events for file JSON for S3 file ' + str(log_file))
    res = client.publish_events_batch(
                    project_id=project,
                    model_id=model,
                    batch_source=df,
                    data_source=fdl.BatchPublishType.DATAFRAME,
                    timestamp_field=timestamp_field,
                    timestamp_format=get_timestamp_format(timestamp_format)
                    )
    
    log(res)
```
"
"---
title: ""SageMaker ML Integration""
slug: ""sagemaker-ml-integration""
excerpt: """"
hidden: false
createdAt: ""Wed Jun 22 2022 14:28:02 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Tue Dec 19 2023 20:35:05 GMT+0000 (Coordinated Universal Time)""
---
Fiddler offers **seamless integration with Amazon SageMaker**. This guide will walk you through how you can easily upload a model trained with SageMaker into Fiddler.

> üìò Before proceeding with this walkthrough, make sure you have already
> 
> - [Uploaded a baseline dataset](docs:uploading-a-baseline-dataset)
> - Trained a model using SageMaker

## Getting your model from S3

In order to download your model, navigate to the AWS console and go to SageMaker. On the left, click **""Inference""** and go to **""Models""**. Then select the model you want to upload to Fiddler.

![](https://files.readme.io/ae27cba-sagemaker_model_select.png ""sagemaker_model_select.png"")

Copy the **Model data location** to your clipboard.

![](https://files.readme.io/be19325-sagemaker_model_location.png ""sagemaker_model_location.png"")

## Downloading your model with Python

Now, from a Python environment (Jupyter notebook or standard Python script), paste the **Model data location** you copied into a new variable.

```python
MODEL_S3_LOCATION = 's3://fiddler-sagemaker-integration/fiddler-xgboost-sagemaker-demo/xgboost_model/output/sagemaker-xgboost-2022-06-06-15-49-54-626/output/model.tar.gz'
```

Then extract the bucket name and file key into their own variables.

```python
MODEL_S3_BUCKET = 'fiddler-sagemaker-integration'
MODEL_S3_KEY = 'fiddler-xgboost-sagemaker-demo/xgboost_model/output/sagemaker-xgboost-2022-06-06-15-49-54-626/output/model.tar.gz'
```

Let's also import a few packages we will be using.

```python
import numpy as np
import pandas as pd
import boto3
import tarfile
import yaml
import xgboost as xgb
import fiddler as fdl
```

After that, initialize an S3 client with AWS using `boto3`.

```python
AWS_PROFILE = 'my_profile'
AWS_REGION = 'us-west-1'

session = boto3.session.Session(
    profile_name=AWS_PROFILE,
    region_name=AWS_REGION
)

s3_client = session.client('s3')
```

We're ready to download! Just run the following code block.

```python
s3_client.download_file(
    Bucket=MODEL_S3_BUCKET,
    Key=MODEL_S3_KEY,
    Filename='model.tar.gz'
)

tarfile.open('model.tar.gz').extractall('model')
```

This will save the model into a directory called `model`.

!!! note  
    It's important to **keep track of the name of your saved model file**. Check the `model` directory in your local filesystem to see its name.

## Upload your model to Fiddler

Now it's time to connect to Fiddler. For more information on how this is done, see [Authorizing the Client](doc:uploading-model-artifacts).

```python
URL = 'https://app.f"
"slug: ""sagemaker-ml-integration"" iddler.ai'
ORG_ID = 'my_org'
AUTH_TOKEN = 'xtu4g_lReHyEisNg23xJ8IEex0YZEZeeEbTwAsupT0U'

fiddler_client = fdl.FiddlerApi(
    url=URL,
    org_id=ORG_ID,
    auth_token=AUTH_TOKEN
)
```

Then, get the dataset info from your baseline dataset by using [client.get_dataset_info](ref:clientget_dataset_info).

After that, construct a model info object and save it as a `.yaml` file into the `model` directory.

```python
PROJECT_ID = 'example_project'
DATASET_ID = 'example_data'

dataset_info = fiddler_client.get_dataset_info(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID
)

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id=DATASET_ID,
    target='target_column',
    outputs=['output_column']
)

with open('model/model.yaml', 'w') as yaml_file:
    yaml.dump({'model': model_info.to_dict()}, yaml_file)
```

The last step is to write our `package.py`.

```python
%%writefile model/package.py

import numpy as np
import pandas as pd
from pathlib import Path
import xgboost as xgb

import fiddler as fdl

PACKAGE_PATH = Path(__file__).parent

class ModelPackage:

    def __init__(self):
        
        self.model_path = str(PACKAGE_PATH / 'xgboost-model') # This is the name of your model file within the model directory
        self.model = xgb.Booster()
        self.model.load_model(self.model_path)
        
        self.output_columns = ['output_column']
    
    def transform_input(self, input_df):
        return xgb.DMatrix(input_df)
    
    def predict(self, input_df):
        transformed_input = self.transform_input(input_df)
        pred = self.model.predict(transformed_input)
        return pd.DataFrame(pred, columns=self.output_columns)
    
def get_model():
    return ModelPackage()
```

Now, go ahead and upload!

```python
MODEL_ID = 'sagemaker_model'

fiddler_client.upload_model_package(
    artifact_path='model',
    project_id=PROJECT_ID,
    model_id=MODEL_ID
)
```
"
"---
title: ""ML Flow Integration""
slug: ""ml-flow-integration""
excerpt: """"
hidden: false
createdAt: ""Fri Sep 15 2023 18:36:23 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Fiddler allows your team to onboard, monitor, explain, and analyze your models developed with [MLFlow](https://mlflow.org/). 

This guide shows you how to ingest the model metadata and artifacts stored in your MLFlow model registry and use them to set up model observability in the Fiddler Platform:

1. Exporting Model Metadata from MLFlow to Fiddler 
2. Uploading Model Artifacts to Fiddler for XAI

## Adding Model Information

Using the **[MLFlow API](https://mlflow.org/docs/latest/python_api/mlflow.html) ** you can query the model registry and get the **model signature** which describes the inputs and outputs as a dictionary. You can use this dictionary to build out the [ModelInfo](ref:fdlmodelinfo) object required to the model to Fiddler:

```python Python
import mlflow 
from mlflow.tracking import MlflowClient

client = MlflowClient() #initiate MLFlow Client 

#Get the model URI
model_version_info = client.get_model_version(model_name, model_version)
model_uri = client.get_model_version_download_uri(model_name, model_version_info) 

#Get the Model Signature
mlflow_model_info = mlflow.models.get_model_info(model_uri)
model_inputs_schema = model_info.signature.inputs.to_dict()
model_inputs = [ sub['name'] for sub in model_inputs_schema ]
```

Now you can use the model signature to build the Fiddler ModelInfo object:

```python
features = model_inputs

model_task = fdl.ModelTask.BINARY_CLASSIFICATION

model_info = fdl.ModelInfo.from_dataset_info(
	dataset_info = client.get_dataset_info(YOUR_PROJECT,YOUR_DATASET),
	target =  ""TARGET COLUMN"", 
  dataset_id=DATASET_ID,
  model_task=model_task, 
  features=features,
  outputs=['output_column'])
```

## Uploading Model Files

Sharing your [model artifacts](doc:artifacts-and-surrogates#model-artifacts-and-model-package) helps Fiddler explain your models. By leveraging the MLFlow API you can download these model files:

```python
import os  
import mlflow  
from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository

model_name = ""example-model-name""  
model_stage = ""Staging""  # Should be either 'Staging' or 'Production'

mlflow.set_tracking_uri(""databricks"")  
os.makedirs(""model"", exist_ok=True)  
local_path = ModelsArtifactRepository(
  f'models:/{model_name}/{model_stage}').download_artifacts("""", dst_path=""model"")  

print(f'{model_stage} Model {model_name} is downloaded at {local_path}')  
```

Once you have the model file, you can create a [package.py](doc:binary-classification-1) file in this model directory that describes how to access this model.

Finally, you can upload all the model artifacts to Fiddler:

```python
client.add_model_artifact(  
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    model_dir='model/',
)
```

Alternatively, you can skip uploading your model and use Fiddler to generate a [surrogate model](doc"
"slug: ""ml-flow-integration"" :artifacts-and-surrogates#surrogate-model) to get low-fidelity explanations for your model.
"
"---
title: ""Databricks Integration""
slug: ""databricks-integration""
excerpt: """"
hidden: false
createdAt: ""Thu Feb 02 2023 20:38:54 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Fiddler allows your team to monitor, explain and analyze your models developed and deployed in [Databricks Workspace](https://docs.databricks.com/introduction/index.html) by integrating with [MLFlow](https://docs.databricks.com/mlflow/index.html) for model asset management and utilizing Databricks Spark environment for data management. 

To validate and monitor models built on Databricks using Fiddler, you can follow these steps:

1. [Creating a Fiddler Project](doc:databricks-integration#creating-a-fiddler-project)
2. [Uploading a Baseline Dataset](doc:databricks-integration#uploading-a-baseline-dataset)
3. [Adding Model Information ](doc:databricks-integration#adding-model-information)
4. [Uploading Model Files (for Explainability)](doc:databricks-integration#uploading-model-files)
5. [Publishing Events](doc:databricks-integration#publishing-events)
   1. Batch Models 
   2. Live Models 

## Creating a Fiddler Project

Launch a [Databricks notebook](https://docs.databricks.com/notebooks/index.html) from your workspace and run the following code:

```python
!pip install -q fiddler-client
import fiddler as fdl
```

Now that you have the Fiddler library installed, you can connect to your Fiddler environment. Please use the [UI administration guide](doc:administration-ui) to help you find your Fiddler credentials.

```python
URL = """"
ORG_ID = """"
AUTH_TOKEN = """"
client = fdl.FiddlerApi(url=URL, org_id=ORG_ID, auth_token=AUTH_TOKEN)
```

Finally, you can set up a new project using:

```python
client.create_project(""YOUR_PROJECT_NAME"")
```

## Uploading a Baseline Dataset

You can grab your baseline dataset from a[ delta table](https://docs.databricks.com/getting-started/dataframes-python.html) and share it with Fiddler as a baseline dataset:

```python
baseline_dataset = spark.read.table(""YOUR_DATASET"").select(""*"").toPandas()

dataset_info = fdl.DatasetInfo.from_dataframe(baseline_upload, max_inferred_cardinality=100)
  
client.upload_dataset(
  project_id=PROJECT_ID,
  dataset_id=DATASET_ID,
  dataset={'baseline': baseline_upload},
  info=dataset_info)
```

## Adding Model Information

Using the **[MLFlow API](https://docs.databricks.com/reference/mlflow-api.html) ** you can query the model registry and get the **model signature** which describes the inputs and outputs as a dictionary. You can use this dictionary to build out the [ModelInfo](ref:fdlmodelinfo) object required to the model to Fiddler:

```python Python
import mlflow 
from mlflow.tracking import MlflowClient

client = MlflowClient() #initiate MLFlow Client 

#Get the model URI
model_version_info = client.get_model_version(model_name, model_version)
model_uri = client.get_model_version_download_uri(model_name, model_version_info) 

#Get the Model Signature
mlflow_model_info = mlflow.models.get"
"slug: ""databricks-integration"" _model_info(model_uri)
model_inputs_schema = model_info.signature.inputs.to_dict()
model_inputs = [ sub['name'] for sub in model_inputs_schema ]
```

Now you can use the model signature to build the Fiddler ModelInfo object :

```python
features = model_inputs

model_task = fdl.ModelTask.BINARY_CLASSIFICATION

model_info = fdl.ModelInfo.from_dataset_info(
	dataset_info = client.get_dataset_info(YOUR_PROJECT,YOUR_DATASET),
	target =  ""TARGET COLUMN"", 
  dataset_id=DATASET_ID,
  model_task=model_task, 
  features=features,
  outputs=['output_column'])
```

## Uploading Model Files

Sharing your [model artifacts](doc:uploading-model-artifacts) helps Fiddler explain your models. By leveraging the MLFlow API you can download these model files:

```python
import os  
import mlflow  
from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository

model_name = ""example-model-name""  
model_stage = ""Staging""  # Should be either 'Staging' or 'Production'

mlflow.set_tracking_uri(""databricks"")  
os.makedirs(""model"", exist_ok=True)  
local_path = ModelsArtifactRepository(
  f'models:/{model_name}/{model_stage}').download_artifacts("""", dst_path=""model"")  

print(f'{model_stage} Model {model_name} is downloaded at {local_path}')  
```

Once you have the model file, you can create a [package.py](doc:binary-classification-1) file in this model directory that describes how to access this model.

Finally, you can upload all the model artifacts to Fiddler:

```python
client.add_model_artifact(  
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    model_dir='model/',
)
```

Alternatively, you can skip uploading your model and use Fiddler to generate a [surrogate model](doc:surrogate-models-client-guide) to get low-fidelity explanations for your model.

## Publishing Events

Now you can publish all the events from your models. You can do this in two ways:

### Batch Models

If your models run batch processes with your models or your aggregate model outputs over a timeframe, then you can use the table change feed from Databricks to select only the new events and send them to Fiddler:

```python Python
changes_df = spark.read.format(""delta"") \
.option(""readChangeFeed"", ""true"") \
.option(""startingVersion"",last_version) \
.option(""endingVersion"", new_version) \
.table(""inferences"").toPandas()


client.publish_events_batch(
   project_id=PROJECT_ID,
   model_id=MODEL_ID,
   batch_source=changes_df,
   timestamp_field='timestamp')

```

### Live Models

For models with live predictions or real-time applications, you can add the following code snippet to your prediction pipeline and send every event to Fiddler in real-time: 

```python Python
example_event = model_output.toPandas() #turn your model's ouput in a pandas datafram 

client.publish_event(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    event=example_event,
    event_id='event_001',
    event_timestamp=1637344470000)
```

_Support for Inference tables and hosted endpoints is coming soon!_
"
"---
title: ""Datadog Integration""
slug: ""datadog-integration""
excerpt: """"
hidden: false
createdAt: ""Wed Jun 21 2023 15:21:52 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Fiddler offers an integration with Datadog which allows Fiddler and Datadog customers to bring their AI Observability metrics from Fiddler into their centralized Datadog dashboards.  Additionally, a Fiddler license can now be procured through the [Datadog Marketplace](https://www.datadoghq.com/blog/tag/datadog-marketplace/). This [integration](https://www.datadoghq.com/blog/monitor-machine-learning-models-fiddler/) enables you to centralize your monitoring of ML models and the applications that utilize them within one unified platform.

## Integrating Fiddler with Datadog

Instructions for integrating Fiddler with Datadog can be found on the ""Integrations"" section of your Datadog console.  Simply search for ""Fiddler"" and follow the installation instructions provided on the ""Configure"" tab.  Please reach out to [support@fiddler.ai](mailto:support@fiddler.ai) with any issues or questions.

![](https://files.readme.io/3f9dcd9-Screenshot_2023-06-21_at_10.28.17_AM.png)

![](https://files.readme.io/9fa9503-Screenshot_2023-06-21_at_10.31.54_AM.png)

![](https://files.readme.io/218dfc2-Screenshot_2023-06-21_at_10.45.14_AM.png)
"
"---
title: ""client.publish_events_batch""
slug: ""clientpublish_events_batch""
excerpt: ""Publishes a batch of events to Fiddler asynchronously.""
hidden: false
createdAt: ""Mon May 23 2022 20:30:23 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
[block:parameters]
{
  ""data"": {
    ""h-0"": ""Input Parameter"",
    ""h-1"": ""Type"",
    ""h-2"": ""Default"",
    ""h-3"": ""Description"",
    ""0-0"": ""project_id"",
    ""0-1"": ""str"",
    ""0-2"": ""None"",
    ""0-3"": ""The unique identifier for the project."",
    ""1-0"": ""model_id"",
    ""1-1"": ""str"",
    ""1-2"": ""None"",
    ""1-3"": ""A unique identifier for the model."",
    ""2-0"": ""batch_source"",
    ""2-1"": ""Union[pd.Dataframe, str]"",
    ""2-2"": ""None"",
    ""2-3"": ""Either a pandas DataFrame containing a batch of events, or the path to a file containing a batch of events. Supported file types are  \n_ CSV (.csv)  \n_ Parquet (.pq)  \n  \n- Pickled DataFrame (.pkl)"",
    ""3-0"": ""id_field"",
    ""3-1"": ""Optional [str]"",
    ""3-2"": ""None"",
    ""3-3"": ""The field containing event IDs for events in the batch.  If not specified, Fiddler will generate its own ID, which can be retrived using the **get_slice** API."",
    ""4-0"": ""update_event"",
    ""4-1"": ""Optional [bool]"",
    ""4-2"": ""None"",
    ""4-3"": ""If True, will only modify an existing event, referenced by _id_field_.  If an ID is provided for which there is no event, no change will take place."",
    ""5-0"": ""timestamp_field"",
    ""5-1"": ""Optional [str]"",
    ""5-2"": ""None"",
    ""5-3"": ""The field containing timestamps for events in the batch. The format of these timestamps is given by _timestamp_format_. If no timestamp is provided for a given row, the current time will be used."",
    ""6-0"": ""timestamp_format"",
    ""6-1"": ""Optional [fdl.FiddlerTimestamp]"",
    ""6-2"": ""fdl.FiddlerTimestamp.INFER"",
    ""6-3"": ""The format of the timestamp passed in _event_timestamp_. Can be one of  \n-fdl.FiddlerTimestamp.INFER  \n  \n- fdl.FiddlerTimestamp.EPOCH_MILLISECONDS  \n- fdl.FiddlerTimestamp.EPOCH_SECONDS  \n- fdl.FiddlerTimestamp.ISO_8601"",
    ""7-0"": ""data_source"",
    ""7-1"": ""Optional [fdl.BatchPublishType]"",
    ""7-2"": ""None"",
    ""7-3"": ""The location of the data source provided. By default, Fiddler will try to infer the value. Can be one of  \n  \n- fdl.BatchPublishType.DATAFRAME  \n- fdl.BatchPublish"
"slug: ""clientpublish_events_batch"" Type.LOCAL_DISK  \n- fdl.BatchPublishType.AWS_S3"",
    ""8-0"": ""casting_type"",
    ""8-1"": ""Optional [bool]"",
    ""8-2"": ""False"",
    ""8-3"": ""If True, will try to cast the data in event to be in line with the data types defined in the model's **ModelInfo** object."",
    ""9-0"": ""credentials"",
    ""9-1"": ""Optional [dict]"",
    ""9-2"": ""None"",
    ""9-3"": ""A dictionary containing authorization information for AWS or GCP.  \n  \nFor AWS, the expected keys are  \n  \n- 'aws_access_key_id'  \n- 'aws_secret_access_key'  \n- 'aws_session_token'For GCP, the expected keys are  \n  \n- 'gcs_access_key_id'  \n- 'gcs_secret_access_key'  \n- 'gcs_session_token'"",
    ""10-0"": ""group_by"",
    ""10-1"": ""Optional [str]"",
    ""10-2"": ""None"",
    ""10-3"": ""The field used to group events together when computing performance metrics (for ranking models only).""
  },
  ""cols"": 4,
  ""rows"": 11,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


```python Usage
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'

df_events = pd.read_csv('events.csv')

client.publish_events_batch(
        project_id=PROJECT_ID,
        model_id=MODEL_ID,
        batch_source=df_events,
        timestamp_field='inference_date')
```

| Return Type | Description                                                            |
| :---------- | :--------------------------------------------------------------------- |
| dict        | A dictionary object which reports the result of the batch publication. |

```python Example Response
{'status': 202,
 'job_uuid': '4ae7bd3a-2b3f-4444-b288-d51e07b6736d',
 'files': ['ssoqj_tmpzmczjuob.csv'],
 'message': 'Successfully received the event data. Please allow time for the event ingestion to complete in the Fiddler platform.'}
```
"
"---
title: ""About Event Publication""
slug: ""publish_event""
excerpt: """"
hidden: false
createdAt: ""Fri May 13 2022 14:36:29 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Event publication is the process of sending your model's prediction logs, or events, to the Fiddler platform.  Using the [Fiddler Client](ref:about-the-fiddler-client), events can be published in batch or streaming mode.  Using these events, Fiddler will calculate metrics around feature drift, prediction drift, and model performance.  These events are also stored in Fiddler to allow for ad hoc segment analysis.  Please read the sections that follow to learn more about how to use the Fiddler Client for event publication.
"
"---
title: ""client.publish_event""
slug: ""clientpublish_event""
excerpt: ""Publishes a single production event to Fiddler asynchronously.""
hidden: false
createdAt: ""Mon May 23 2022 19:53:24 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
[block:parameters]
{
  ""data"": {
    ""h-0"": ""Input Parameter"",
    ""h-1"": ""Type"",
    ""h-2"": ""Default"",
    ""h-3"": ""Description"",
    ""0-0"": ""project_id"",
    ""0-1"": ""str"",
    ""0-2"": ""None"",
    ""0-3"": ""The unique identifier for the project."",
    ""1-0"": ""model_id"",
    ""1-1"": ""str"",
    ""1-2"": ""None"",
    ""1-3"": ""A unique identifier for the model. Must be a lowercase string between 2-30 characters containing only alphanumeric characters and underscores. Additionally, it must not start with a numeric character."",
    ""2-0"": ""event"",
    ""2-1"": ""dict"",
    ""2-2"": ""None"",
    ""2-3"": ""A dictionary mapping field names to field values. Any fields found that are not present in the model's **ModelInfo** object will be dropped from the event."",
    ""3-0"": ""event_id"",
    ""3-1"": ""Optional [str]"",
    ""3-2"": ""None"",
    ""3-3"": ""A unique identifier for the event. If not specified, Fiddler will generate its own ID, which can be retrived using the **get_slice** API."",
    ""4-0"": ""update_event"",
    ""4-1"": ""Optional [bool]"",
    ""4-2"": ""None"",
    ""4-3"": ""If True, will only modify an existing event, referenced by event_id. If no event is found, no change will take place."",
    ""5-0"": ""event_timestamp"",
    ""5-1"": ""Optional [int]"",
    ""5-2"": ""None"",
    ""5-3"": ""The name of the  timestamp input field for when the event took place. The format of this timestamp is given by _timestamp_format_. If no timestamp input is provided, the current time will be used."",
    ""6-0"": ""timestamp_format"",
    ""6-1"": ""Optional [fdl.FiddlerTimestamp]"",
    ""6-2"": ""fdl.FiddlerTimestamp.INFER"",
    ""6-3"": ""The format of the timestamp passed in _event_timestamp_. Can be one of  \n- fdl.FiddlerTimestamp.INFER  \n- fdl.FiddlerTimestamp.EPOCH_MILLISECONDS  \n- fdl.FiddlerTimestamp.EPOCH_SECONDS  \n- fdl.FiddlerTimestamp.ISO_8601"",
    ""7-0"": ""casting_type"",
    ""7-1"": ""Optional [bool]"",
    ""7-2"": ""False"",
    ""7-3"": ""If True, will try to cast the data in event to be in line with the data types defined in the model's **ModelInfo** object."",
    ""8-0"": ""dry_run"",
    ""8-1"": ""Optional [bool]"",
    ""8-2"": ""False"",
    ""8-3"": ""If True"
"slug: ""clientpublish_event"" , the event will not be published, and instead a report will be generated with information about any problems with the event. Useful for debugging issues with event publishing.""
  },
  ""cols"": 4,
  ""rows"": 9,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


```python Usage
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'

example_event = {
    'feature_1': 20.7,
    'feature_2': 45000,
    'feature_3': True,
    'output_column': 0.79,
    'target_column': 1
}

client.publish_event(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    event=example_event,
    event_id='event_001',
    event_timestamp=1637344470000
)
```

| Return Type | Description                                                                          |
| :---------- | :----------------------------------------------------------------------------------- |
| str         | returns a string with a UUID acknowledging that the event was successfully received. |

```Text Example Response
'66cfbeb6-5651-4e8b-893f-90286f435b8d'
```
"
"---
title: ""client.publish_events_batch_schema""
slug: ""clientpublish_events_batch_schema""
excerpt: ""Publishes a batch of events to Fiddler asynchronously using a schema for locating fields within complex data structures.""
hidden: false
createdAt: ""Mon May 23 2022 20:50:05 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
[block:parameters]
{
  ""data"": {
    ""h-0"": ""Input Parameter"",
    ""h-1"": ""Type"",
    ""h-2"": ""Default"",
    ""h-3"": ""Description"",
    ""0-0"": ""batch_source"",
    ""0-1"": ""Union[pd.Dataframe, str]"",
    ""0-2"": ""None"",
    ""0-3"": ""Either a pandas DataFrame containing a batch of events, or the path to a file containing a batch of events. Supported file types are  \n  \n- CSV (.csv)"",
    ""1-0"": ""publish_schema"",
    ""1-1"": ""dict"",
    ""1-2"": ""None"",
    ""1-3"": ""A dictionary used for locating fields within complex or nested data structures."",
    ""2-0"": ""data_source"",
    ""2-1"": ""Optional [fdl.BatchPublishType]"",
    ""2-2"": ""None"",
    ""2-3"": ""The location of the data source provided. By default, Fiddler will try to infer the value. Can be one of  \n  \n- fdl.BatchPublishType.DATAFRAME  \n- fdl.BatchPublishType.LOCAL_DISK  \n- fdl.BatchPublishType.AWS_S3"",
    ""3-0"": ""credentials"",
    ""3-1"": ""Optional [dict]"",
    ""3-2"": ""None"",
    ""3-3"": ""A dictionary containing authorization information for AWS or GCP.  \n  \nFor AWS, the expected keys are  \n  \n- 'aws_access_key_id'  \n- 'aws_secret_access_key'  \n- 'aws_session_token'For GCP, the expected keys are  \n  \n- 'gcs_access_key_id'  \n- 'gcs_secret_access_key'  \n- 'gcs_session_token'"",
    ""4-0"": ""group_by"",
    ""4-1"": ""Optional [str]"",
    ""4-2"": ""None"",
    ""4-3"": ""The field used to group events together when computing performance metrics (for ranking models only).""
  },
  ""cols"": 4,
  ""rows"": 5,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


```python Usage
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'

path_to_batch = 'events_batch.avro'

schema = {
    '__static': {
        '__project': PROJECT_ID,
        '__model': MODEL_ID
    },
    '__dynamic': {
        'feature_1': 'features/feature_1',
        'feature_2': 'features/feature_2',
        'feature_3': 'features/feature_3',
        'output_column': 'outputs/output_column',
        'target_column': 'targets/target_column'
      ORG = '__org'
13      MODEL = '__model'
14      PROJECT = '__project'
15"
"slug: ""clientpublish_events_batch_schema""       TIMESTAMP = '__timestamp'
16      DEFAULT_TIMESTAMP = '__default_timestamp'
17      TIMESTAMP_FORMAT = '__timestamp_format'
18      EVENT_ID = '__event_id'
19      IS_UPDATE_EVENT = '__is_update_event'
20      STATUS = '__status'
21      LATENCY = '__latency'
22      ITERATOR_KEY = '__iterator_key'
    }
}

client.publish_events_batch_schema(
    batch_source=path_to_batch,
    publish_schema=schema
)
```

| Return Type | Description                                                            |
| :---------- | :--------------------------------------------------------------------- |
| dict        | A dictionary object which reports the result of the batch publication. |

```python Example Response
{'status': 202,
 'job_uuid': '5ae7bd3a-2b3f-4444-b288-d51e098a01d',
 'files': ['rroqj_tmpzmczjttb.csv'],
 'message': 'Successfully received the event data. Please allow time for the event ingestion to complete in the Fiddler platform.'}
```
"
"---
title: ""fdl.WindowSize""
slug: ""fdlwindowsize""
excerpt: ""Enum for supported window sizes as seconds""
hidden: false
createdAt: ""Wed Feb 08 2023 23:50:58 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Enum                     | Value  |
| :----------------------- | :----- |
| fdl.WindowSize.ONE_HOUR  | 3600   |
| fdl.WindowSize.ONE_DAY   | 86400  |
| fdl.WindowSize.ONE_WEEK  | 604800 |
| fdl.WindowSize.ONE_MONTH | 259200 |

```c Usage
from fiddler import BaselineType, WindowSize

PROJECT_NAME = 'example_project'
BASELINE_NAME = 'example_rolling'
DATASET_NAME = 'example_validation'
MODEL_NAME = 'example_model'

client.add_baseline(
  project_id=PROJECT_NAME,
  model_id=MODEL_NAME,
  baseline_id=BASELINE_NAME,
  type=BaselineType.ROLLING_PRODUCTION,
  offset=WindowSize.ONE_MONTH, # How far back to set our window
  window_size=WindowSize.ONE_WEEK, # Size of the sliding window
)
```
"
"---
title: ""fdl.Priority""
slug: ""fdlpriority""
excerpt: ""Priority identifiers used on Alert Rules""
hidden: false
createdAt: ""Tue Jan 31 2023 07:30:12 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
**This field can be used to prioritize the alert rules by adding an identifier - low, medium, and high to help users better categorize them on the basis of their importance. Following are the Priority Enums:**

| Enums               | Values |
| :------------------ | :----- |
| fdl.Priority.HIGH   | HIGH   |
| fdl.Priority.MEDIUM | MEDIUM |
| fdl.Priority.LOW    | LOW    |

```coffeescript Usage
import fiddler as fdl

client.add_alert_rule(
    name = ""perf-gt-5prec-1hr-1d-ago"",
    project_name = 'project-a',
    model_name = 'model-a',
    alert_type = fdl.AlertType.PERFORMANCE, 
    metric = fdl.Metric.PRECISION,
    bin_size = fdl.BinSize.ONE_HOUR, 
    compare_to = fdl.CompareTo.TIME_PERIOD,
    compare_period = fdl.ComparePeriod.ONE_DAY,
    warning_threshold = 0.05,
    critical_threshold = 0.1,
    condition = fdl.AlertCondition.GREATER,
    priority = fdl.Priority.HIGH, <---
    notifications_config = notifications_config
)
```
```coffeescript Outputs
[AlertRule(alert_rule_uuid='9b8711fa-735e-4a72-977c-c4c8b16543ae',
           organization_name='some_org_name',
           project_id='project-a',
           model_id='model-a',
           name='perf-gt-5prec-1hr-1d-ago',
           alert_type=AlertType.PERFORMANCE, 
           metric=Metric.PRECISION,
           priority=Priority.HIGH, <----
           compare_to='CompareTo.TIME_PERIOD,
           compare_period=ComparePeriod.ONE_DAY,
           compare_threshold=None,
           raw_threshold=None,
           warning_threshold=0.05,
           critical_threshold=0.1,
           condition=AlertCondition.GREATER,
           bin_size=BinSize.ONE_HOUR)]
```
"
"---
title: ""fdl.ModelInfo""
slug: ""fdlmodelinfo""
excerpt: ""Stores information about a model.""
hidden: false
createdAt: ""Wed Feb 08 2023 17:29:05 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
[block:parameters]
{
  ""data"": {
    ""h-0"": ""Input Parameters"",
    ""h-1"": ""Type"",
    ""h-2"": ""Default"",
    ""h-3"": ""Description"",
    ""0-0"": ""display_name"",
    ""0-1"": ""str"",
    ""0-2"": """",
    ""0-3"": ""A display name for the model."",
    ""1-0"": ""input_type"",
    ""1-1"": ""fdl.ModelInputType"",
    ""1-2"": """",
    ""1-3"": ""A **ModelInputType** object containing the input type of the model."",
    ""2-0"": ""model_task"",
    ""2-1"": ""fdl.ModelTask"",
    ""2-2"": """",
    ""2-3"": ""A **ModelTask** object containing the model task."",
    ""3-0"": ""inputs"",
    ""3-1"": ""list"",
    ""3-2"": """",
    ""3-3"": ""A list of **Column** objects corresponding to the inputs (features) of the model."",
    ""4-0"": ""outputs"",
    ""4-1"": ""list"",
    ""4-2"": """",
    ""4-3"": ""A list of **Column** objects corresponding to the outputs (predictions) of the model."",
    ""5-0"": ""metadata"",
    ""5-1"": ""Optional [list]"",
    ""5-2"": ""None"",
    ""5-3"": ""A list of **Column** objects corresponding to any metadata fields."",
    ""6-0"": ""decisions"",
    ""6-1"": ""Optional [list]"",
    ""6-2"": ""None"",
    ""6-3"": ""A list of **Column** objects corresponding to any decision fields (post-prediction business decisions)."",
    ""7-0"": ""targets"",
    ""7-1"": ""Optional [list]"",
    ""7-2"": ""None"",
    ""7-3"": ""A list of **Column** objects corresponding to the targets (ground truth) of the model."",
    ""8-0"": ""framework"",
    ""8-1"": ""Optional [str]"",
    ""8-2"": ""None"",
    ""8-3"": ""A string providing information about the software library and version used to train and run this model."",
    ""9-0"": ""description"",
    ""9-1"": ""Optional [str]"",
    ""9-2"": ""None"",
    ""9-3"": ""A description of the model."",
    ""10-0"": ""datasets"",
    ""10-1"": ""Optional [list]"",
    ""10-2"": ""None"",
    ""10-3"": ""A list of the dataset IDs used by the model."",
    ""11-0"": ""mlflow_params"",
    ""11-1"": ""Optional [fdl.MLFlowParams]"",
    ""11-2"": ""None"",
    ""11-3"": ""A **MLFlowParams** object containing information about MLFlow parameters."",
    ""12-0"": ""model_deployment_params"",
    ""12-1"": """
"slug: ""fdlmodelinfo"" Optional [fdl.ModelDeploymentParams]"",
    ""12-2"": ""None"",
    ""12-3"": ""A **ModelDeploymentParams** object containing information about model deployment."",
    ""13-0"": ""artifact_status"",
    ""13-1"": ""Optional [fdl.ArtifactStatus]"",
    ""13-2"": ""None"",
    ""13-3"": ""An **ArtifactStatus** object containing information about the model artifact."",
    ""14-0"": ""preferred_explanation_method"",
    ""14-1"": ""Optional [fdl.ExplanationMethod]"",
    ""14-2"": ""None"",
    ""14-3"": ""An **ExplanationMethod** object that specifies the default explanation algorithm to use for the model."",
    ""15-0"": ""custom_explanation_names"",
    ""15-1"": ""Optional [list]"",
    ""15-2"": ""[ ]"",
    ""15-3"": ""A list of names that can be passed to the _explanation_name \\_argument of the optional user-defined \\_explain_custom_ method of the model object defined in _package.py._"",
    ""16-0"": ""binary_classification_threshold"",
    ""16-1"": ""Optional [float]"",
    ""16-2"": "".5"",
    ""16-3"": ""The threshold used for classifying inferences for binary classifiers."",
    ""17-0"": ""ranking_top_k"",
    ""17-1"": ""Optional [int]"",
    ""17-2"": ""50"",
    ""17-3"": ""Used only for ranking models. Sets the top _k_ results to take into consideration when computing performance metrics like MAP and NDCG."",
    ""18-0"": ""group_by"",
    ""18-1"": ""Optional [str]"",
    ""18-2"": ""None"",
    ""18-3"": ""Used only for ranking models.  The column by which to group events for certain performance metrics like MAP and NDCG."",
    ""19-0"": ""fall_back"",
    ""19-1"": ""Optional [dict]"",
    ""19-2"": ""None"",
    ""19-3"": ""A dictionary mapping a column name to custom missing value encodings for that column."",
    ""20-0"": ""target_class_order"",
    ""20-1"": ""Optional [list]"",
    ""20-2"": ""None"",
    ""20-3"": ""A list denoting the order of classes in the target. This parameter is **required** in the following cases:  \n  \n_- Binary classification tasks_: If the **target** is of type _string_, you must tell Fiddler which class is considered the positive class for your **output** column. You need to provide a list with two elements. The 0th element by convention is considered the negative class, and the 1st element is considered the positive class.  When your **target** is _boolean_, you don't need to specify this argument. By default Fiddler considers `True` as the positive class. In case your target is _numerical_, you don't need to  specify this argument, by default Fiddler considers the higher of the two possible values as the positive class.  \n  \n- _Multi-class classification tasks_: You must tell Fiddler which class corresponds to which output by giving an ordered list of classes. This order should be the same as the order of the outputs.  \n  \n- _Ranking tasks_: If the target is of type _string_, you must provide a list of all the possible target values in the order"
"slug: ""fdlmodelinfo""  of relevance. The first element will be considered as the least relevant grade and the last element from the list will be considered the most relevant grade.  \n In the case your target is _numerical_, Fiddler considers the smallest value to be the least relevant grade and the biggest value from the list will be considered the most relevant grade."",
    ""21-0"": ""\\*\\*kwargs"",
    ""21-1"": """",
    ""21-2"": """",
    ""21-3"": ""Additional arguments to be passed.""
  },
  ""cols"": 4,
  ""rows"": 22,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


```python Usage
inputs = [
    fdl.Column(
        name='feature_1',
        data_type=fdl.DataType.FLOAT
    ),
    fdl.Column(
        name='feature_2',
        data_type=fdl.DataType.INTEGER
    ),
    fdl.Column(
        name='feature_3',
        data_type=fdl.DataType.BOOLEAN
    )
]

outputs = [
    fdl.Column(
        name='output_column',
        data_type=fdl.DataType.FLOAT
    )
]

targets = [
    fdl.Column(
        name='target_column',
        data_type=fdl.DataType.INTEGER
    )
]

model_info = fdl.ModelInfo(
    display_name='Example Model',
    input_type=fdl.ModelInputType.TABULAR,
    model_task=fdl.ModelTask.BINARY_CLASSIFICATION,
    inputs=inputs,
    outputs=outputs,
    targets=targets
)
```
"
"---
title: ""fdl.BinSize""
slug: ""fdlbinsize""
excerpt: ""Supported Bin Size values for Alert Rules""
hidden: false
createdAt: ""Tue Jan 31 2023 07:28:35 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:07 GMT+0000 (Coordinated Universal Time)""
---
**This field signifies the durations for which fiddler monitoring calculates the metric values **

[block:parameters]
{
  ""data"": {
    ""h-0"": ""Enums"",
    ""h-1"": ""Values"",
    ""0-0"": ""fdl.BinSize.ONE_HOUR"",
    ""0-1"": ""3600 \\* 1000 millisecond  \ni.e one hour"",
    ""1-0"": ""fdl.BinSize.ONE_DAY"",
    ""1-1"": ""86400 \\* 1000 millisecond  \ni.e one day"",
    ""2-0"": ""fdl.BinSize.SEVEN_DAYS"",
    ""2-1"": ""604800 \\* 1000 millisecond  \ni.e seven days""
  },
  ""cols"": 2,
  ""rows"": 3,
  ""align"": [
    ""left"",
    ""left""
  ]
}
[/block]


```coffeescript Usage
import fiddler as fdl

client.add_alert_rule(
    name = ""perf-gt-5prec-1hr-1d-ago"",
    project_name = 'project-a',
    model_name = 'model-a',
    alert_type = fdl.AlertType.PERFORMANCE, 
    metric = fdl.Metric.PRECISION,
    bin_size = fdl.BinSize.ONE_HOUR, <----
    compare_to = fdl.CompareTo.TIME_PERIOD,
    compare_period = fdl.ComparePeriod.ONE_DAY,
    warning_threshold = 0.05,
    critical_threshold = 0.1,
    condition = fdl.AlertCondition.GREATER,
    priority = fdl.Priority.HIGH,
    notifications_config = notifications_config
)
```
```coffeescript Outputs
[AlertRule(alert_rule_uuid='9b8711fa-735e-4a72-977c-c4c8b16543ae',
           organization_name='some_org_name',
           project_id='project-a',
           model_id='model-a',
           name='perf-gt-5prec-1hr-1d-ago',
           alert_type=AlertType.PERFORMANCE, 
           metric=Metric.PRECISION,
           priority=Priority.HIGH,
           compare_to='CompareTo.TIME_PERIOD,
           compare_period=ComparePeriod.ONE_DAY,
           compare_threshold=None,
           raw_threshold=None,
           warning_threshold=0.05,
           critical_threshold=0.1,
           condition=AlertCondition.GREATER,
           bin_size=BinSize.ONE_HOUR)] <-----
```
"
"---
title: ""fdl.CustomFeature""
slug: ""fdlcustomfeature""
excerpt: ""This is the base class that all other custom features inherit from.  It's flexible enough to accommodate different types of derived features.  Note: All of the derived feature classes (e.g., Multivariate, VectorFeature, etc.) inherit from CustomFeature and thus have its properties, in addition to their specific ones.""
hidden: false
createdAt: ""Tue Oct 24 2023 03:47:43 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type                                      | Default | Description                                                                                                                       |
| :-------------- | :---------------------------------------- | :------ | :-------------------------------------------------------------------------------------------------------------------------------- |
| name            | str                                       | None    | The name of the custom feature.                                                                                                   |
| type            | [CustomFeatureType](fdlcustomfeaturetype) | None    | The type of custom feature. Must be one of the `CustomFeatureType` enum values.                                                   |
| n_clusters      | Optional[int]                             | 5       | The number of clusters.                                                                                                           |
| centroids       | Optional[List]                            | None    | Centroids of the clusters in the embedded space. Number of centroids equal to `n_clusters`.                                       |
| columns         | Optional\[List[str]]                      | None    | For `FROM_COLUMNS` type, represents the original columns from which the feature is derived.                                       |
| column          | Optional[str]                             | None    | Used for vector-derived features, the original vector column name.                                                                |
| source_column   | Optional[str]                             | None    | Specifies the original column name for embedding-derived features.                                                                |
| n_tags          | Optional[int]                             | 5       | For `FROM_TEXT_EMBEDDING` type, represents the number of tags for each cluster in the `tfidf` summarization in drift computation. |

```python Usage
# use from_columns helper function to generate a custom feature combining multiple numeric columns

feature = fdl.CustomFeature.from_columns(
    name='my_feature',
    columns=['column_1', 'column_2'],
    n_clusters=5
)
```
"
"---
title: ""fdl.ComparePeriod""
slug: ""fdlcompareperiod""
excerpt: ""Supported Relative comparison values time period""
hidden: false
createdAt: ""Tue Jan 31 2023 07:23:51 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
**Required when compare_to = CompareTo.TIME_PERIOD, this field is used to set when comparing against the same bin for a previous time period. Choose from the following:**

[block:parameters]
{
  ""data"": {
    ""h-0"": ""Enums"",
    ""h-1"": ""values"",
    ""0-0"": ""fdl.ComparePeriod.ONE_DAY"",
    ""0-1"": ""86400000 millisecond  \ni.e 1 day"",
    ""1-0"": ""fdl.ComparePeriod.SEVEN_DAYS"",
    ""1-1"": ""604800000 millisecond  \ni.e 7 days"",
    ""2-0"": ""fdl.ComparePeriod.ONE_MONTH"",
    ""2-1"": ""2629743000 millisecond  \ni.e 30 days"",
    ""3-0"": ""fdl.ComparePeriod.THREE_MONTHS"",
    ""3-1"": ""7776000000 millisecond  \ni.e 90 days""
  },
  ""cols"": 2,
  ""rows"": 4,
  ""align"": [
    ""left"",
    ""left""
  ]
}
[/block]


```coffeescript Usage
import fiddler as fdl

client.add_alert_rule(
    name = ""perf-gt-5prec-1hr-1d-ago"",
    project_name = 'project-a',
    model_name = 'model-a',
    alert_type = fdl.AlertType.PERFORMANCE, 
    metric = fdl.Metric.PRECISION,
    bin_size = fdl.BinSize.ONE_HOUR, 
    compare_to = fdl.CompareTo.TIME_PERIOD,
    compare_period = fdl.ComparePeriod.ONE_DAY, <----
    warning_threshold = 0.05,
    critical_threshold = 0.1,
    condition = fdl.AlertCondition.GREATER,
    priority = fdl.Priority.HIGH,
    notifications_config = notifications_config
)
```
```coffeescript Outputs
[AlertRule(alert_rule_uuid='9b8711fa-735e-4a72-977c-c4c8b16543ae',
           organization_name='some_org_name',
           project_id='project-a',
           model_id='model-a',
           name='perf-gt-5prec-1hr-1d-ago',
           alert_type=AlertType.PERFORMANCE, 
           metric=Metric.PRECISION,
           priority=Priority.HIGH,
           compare_to='CompareTo.TIME_PERIOD,
           compare_period=ComparePeriod.ONE_DAY, <----
           compare_threshold=None,
           raw_threshold=None,
           warning_threshold=0.05,
           critical_threshold=0.1,
           condition=AlertCondition.GREATER,
           bin_size=BinSize.ONE_HOUR)]
```
"
"---
title: ""fdl.AlertType""
slug: ""fdlalerttype""
excerpt: ""Supported Alert Types for Alert Rules""
hidden: false
createdAt: ""Tue Jan 31 2023 07:35:02 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Enum Value                    | Description                    |
| :---------------------------- | :----------------------------- |
| fdl.AlertType.DATA_DRIFT      | For drift alert type           |
| fdl.AlertType.PERFORMANCE     | For performance alert type     |
| fdl.AlertType.DATA_INTEGRITY  | For data integrity alert type  |
| fdl.AlertType.SERVICE_METRICS | For service metrics alert type |
| fdl.AlertType.STATISTIC       | For statistics of a feature    |

```text Usage
client.add_alert_rule(
    name = ""perf-gt-5prec-1hr-1d-ago"",
    project_name = 'project-a',
    model_name = 'model-a',
    alert_type = fdl.AlertType.PERFORMANCE, <---
    metric = fdl.Metric.PRECISION,
    bin_size = fdl.BinSize.ONE_HOUR, 
    compare_to = fdl.CompareTo.TIME_PERIOD,
    compare_period = fdl.ComparePeriod.ONE_DAY,
    warning_threshold = 0.05,
    critical_threshold = 0.1,
    condition = fdl.AlertCondition.GREATER,
    priority = fdl.Priority.HIGH,
    notifications_config = notifications_config
)
```
```Text Outputs
[AlertRule(alert_rule_uuid='9b8711fa-735e-4a72-977c-c4c8b16543ae',
           organization_name='some_org_name',
           project_id='project-a',
           model_id='model-a',
           name='perf-gt-5prec-1hr-1d-ago',
           alert_type=AlertType.PERFORMANCE, <---
           metric=Metric.PRECISION,
           priority=Priority.HIGH,
           compare_to='CompareTo.TIME_PERIOD,
           compare_period=ComparePeriod.ONE_DAY,
           compare_threshold=None,
           raw_threshold=None,
           warning_threshold=0.05,
           critical_threshold=0.1,
           condition=AlertCondition.GREATER,
           bin_size=BinSize.ONE_HOUR)]
```
"
"---
title: ""fdl.ModelInputType""
slug: ""fdlmodelinputtype""
excerpt: ""Represents supported model input types""
hidden: false
createdAt: ""Wed May 25 2022 14:54:42 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Enum Value                 | Description         |
| :------------------------- | :------------------ |
| fdl.ModelInputType.TABULAR | For tabular models. |
| fdl.ModelInputType.TEXT    | For text models.    |

```python Usage
model_input_type = fdl.ModelInputType.TABULAR
```
"
"---
title: ""fdl.Webhook""
slug: ""fdlwebhook""
excerpt: ""Represents the Webhook Config.""
hidden: false
createdAt: ""Thu Sep 21 2023 12:48:43 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter   | Type | Default | Description                                                                   |
| :---------------- | :--- | :------ | :---------------------------------------------------------------------------- |
| name              | str  | None    | A unique name for the webhook.                                                |
| url               | str  | None    | The webhook url used for sending notification messages.                       |
| provider          | str  | None    | The platform that provides webhooks functionality. Only ‚ÄòSLACK‚Äô is supported. |
| uuid              | str  | None    | A unique identifier for the webhook.                                          |
| organization_name | str  | None    | The name of the organization in which the webhook is created.                 |

```python Usage
webhook = fdl.Webhook(
    name='data_integrity_violations_channel',
    url='https://hooks.slack.com/services/T9EAVLUQ5/P982J/G8ISUczk37hxQ15C28d',
    provider='SLACK',
  	uuid='74a4fdcf-34eb-4dc3-9a79-e48e14cca686',
    organization_name='some_org',
)
```

Example Response:

```python Response
Webhook(name='data_integrity_violations_channel',
    url='https://hooks.slack.com/services/T9EAVLUQ5/P982J/G8ISUczk37hxQ15C28d',
    provider='SLACK',
  	uuid='74a4fdcf-34eb-4dc3-9a79-e48e14cca686',
    organization_name='some_org',
)
```
"
"---
title: ""fdl.DataType""
slug: ""fdldatatype""
excerpt: ""Represents supported data types.""
hidden: false
createdAt: ""Wed May 25 2022 14:58:47 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Enum Value            | Description            |
| :-------------------- | :--------------------- |
| fdl.DataType.FLOAT    | For floats.            |
| fdl.DataType.INTEGER  | For integers.          |
| fdl.DataType.BOOLEAN  | For booleans.          |
| fdl.DataType.STRING   | For strings.           |
| fdl.DataType.CATEGORY | For categorical types. |
| fdl.DataType.VECTOR   | For vector types       |

```python Usage
data_type = fdl.DataType.FLOAT
```
"
"---
title: ""Data Source""
slug: ""data-source""
excerpt: ""Data Source type for data to be used in the explainability api's.""
hidden: false
createdAt: ""Wed Aug 30 2023 14:39:46 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
"
"---
title: ""fdl.WeightingParams""
slug: ""fdlweightingparams""
excerpt: """"
hidden: false
createdAt: ""Wed Jul 06 2022 13:41:14 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Holds weighting information for class imbalanced models which can then be passed into a [fdl.ModelInfo](/reference/fdlmodelinfo) object. Please note that the use of weighting params requires the presence of model outputs in the baseline dataset.

| Input Parameters              | Type        | Default | Description                                                                                            |
| :---------------------------- | :---------- | :------ | :----------------------------------------------------------------------------------------------------- |
| class_weight                  | List[float] | None    | List of floats representing weights for each of the classes. The length must equal the no. of classes. |
| weighted_reference_histograms | bool        | True    | Flag indicating if baseline histograms must be weighted or not when calculating drift metrics.         |
| weighted_surrogate_training   | bool        | True    | Flag indicating if weighting scheme should be used when training the surrogate model.                  |

```python Usage
import pandas as pd
import sklearn.utils
import fiddler as fdl

df = pd.read_csv('example_dataset.csv')
computed_weight = sklearn.utils.class_weight.compute_class_weight(
        class_weight='balanced',
        classes=np.unique(df[TARGET_COLUMN]),
        y=df[TARGET_COLUMN]
    ).tolist()
weighting_params =  fdl.WeightingParams(class_weight=computed_weight)
dataset_info = fdl.DatasetInfo.from_dataframe(df=df)

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    features=[
        'feature_1',
        'feature_2',
        'feature_3'
    ],
    outputs=['output_column'],
    target='target_column',
    weighting_params=weighting_params,
    input_type=fdl.ModelInputType.TABULAR,
    model_task=fdl.ModelTask.BINARY_CLASSIFICATION
)
```
"
"---
title: ""fdl.Column""
slug: ""fdlcolumn""
excerpt: ""Represents a column of a dataset.""
hidden: false
createdAt: ""Wed May 25 2022 15:03:57 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type                            | Default | Description                                                                              |
| :-------------- | :------------------------------ | :------ | :--------------------------------------------------------------------------------------- |
| name            | str                             | None    | The name of the column                                                                   |
| data_type       | [fdl.DataType](ref:fdldatatype) | None    | The [fdl.DataType](ref:fdldatatype) object corresponding to the data type of the column. |
| possible_values | Optional [list]                 | None    | A list of unique values used for categorical columns.                                    |
| is_nullable     | Optional [bool]                 | None    | If True, will expect missing values in the column.                                       |
| value_range_min | Optional [float]                | None    | The minimum value used for numeric columns.                                              |
| value_range_max | Optional [float]                | None    | The maximum value used for numeric columns.                                              |

```python Usage
column = fdl.Column(
    name='feature_1',
    data_type=fdl.DataType.FLOAT,
    value_range_min=0.0,
    value_range_max=80.0
)
```
"
"---
title: ""fdl.CompareTo""
slug: ""fdlcompareto""
excerpt: ""Metrics comparison criteria""
hidden: false
createdAt: ""Tue Jan 31 2023 07:26:58 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
**Whether the metric value is to be compared against a static value or the same time bin from a previous time period(set using compare_period\[[ComparePeriod](ref:fdlcompareperiod)]).**

[block:parameters]
{
  ""data"": {
    ""h-0"": ""Enums"",
    ""h-1"": ""Value"",
    ""0-0"": ""fdl.CompareTo.RAW_VALUE"",
    ""0-1"": ""When comparing to  \nan absolute value"",
    ""1-0"": ""fdl.CompareTo.TIME_PERIOD"",
    ""1-1"": ""When comparing to the same  \nbin size from a previous time period""
  },
  ""cols"": 2,
  ""rows"": 2,
  ""align"": [
    ""left"",
    ""left""
  ]
}
[/block]


```coffeescript Usage
import fiddler as fdl

client.add_alert_rule(
    name = ""perf-gt-5prec-1hr-1d-ago"",
    project_name = 'project-a',
    model_name = 'binary_classification_model-a',
    alert_type = fdl.AlertType.PERFORMANCE,
    metric = fdl.Metric.PRECISION,
    bin_size = fdl.BinSize.ONE_HOUR, 
    compare_to = fdl.CompareTo.TIME_PERIOD, <----
    compare_period = fdl.ComparePeriod.ONE_DAY,
    warning_threshold = 0.05,
    critical_threshold = 0.1,
    condition = fdl.AlertCondition.GREATER,
    priority = fdl.Priority.HIGH,
    notifications_config = notifications_config
)
```
```coffeescript Outputs
[AlertRule(alert_rule_uuid='9b8711fa-735e-4a72-977c-c4c8b16543ae',
           organization_name='some_org_name',
           project_id='project-a',
           model_id='binary_classification_model-a',
           name='perf-gt-5prec-1hr-1d-ago',
           alert_type=AlertType.PERFORMANCE,
           metric=Metric.PRECISION,
           priority=Priority.HIGH,
           compare_to='CompareTo.TIME_PERIOD, <---
           compare_period=ComparePeriod.ONE_DAY,
           compare_threshold=None,
           raw_threshold=None,
           warning_threshold=0.05,
           critical_threshold=0.1,
           condition=AlertCondition.GREATER,
           bin_size=BinSize.ONE_HOUR)]
```
"
"---
title: ""fdl.DeploymentParams""
slug: ""fdldeploymentparams""
excerpt: ""Represents the deployment parameters for a model""
hidden: false
createdAt: ""Wed Jan 11 2023 22:32:19 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> Supported from server version `23.1` and above with Model Deployment feature enabled.

[block:parameters]
{
  ""data"": {
    ""h-0"": ""Input Parameter"",
    ""h-1"": ""Type"",
    ""h-2"": ""Default"",
    ""h-3"": ""Description"",
    ""0-0"": ""image_uri"",
    ""0-1"": ""Optional[str]"",
    ""0-2"": ""md-base/python/machine-learning:1.0.1"",
    ""0-3"": ""Reference to the docker image to create a new runtime to serve the model.  \n  \nCheck the available images on the [Model Deployment](doc:model-deployment) page."",
    ""1-0"": ""replicas"",
    ""1-1"": ""Optional[int]"",
    ""1-2"": ""1"",
    ""1-3"": ""The number of replicas running the model.  \n  \nMinimum value: 1  \nMaximum value: 10  \nDefault value: 1"",
    ""2-0"": ""memory"",
    ""2-1"": ""Optional[int]"",
    ""2-2"": ""256"",
    ""2-3"": ""The amount of memory (mebibytes) reserved per replica.  \n  \nMinimum value: 150  \nMaximum value: 16384 (16GiB)  \nDefault value: 256"",
    ""3-0"": ""cpu"",
    ""3-1"": ""Optional[int]"",
    ""3-2"": ""100"",
    ""3-3"": ""The amount of CPU (milli cpus) reserved per replica.  \n  \nMinimum value:  10  \nMaximum value: 4000 (4vCPUs)  \nDefault value: 100""
  },
  ""cols"": 4,
  ""rows"": 4,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


```python Usage
deployment_params = fdl.DeploymentParams(
        image_uri=""md-base/python/machine-learning:1.1.0"",
        cpu=250,
        memory=512,
  		  replicas=1,
)
```

> üìò What parameters should I set for my model?
> 
> Setting the right parameters might not be straightforward and Fiddler is here to help you.
> 
> The parameters might vary depending the number of input features used, the pre-processing steps used and the model itself.
> 
> This table is helping you defining the right parameters

1. **Surrogate Models guide**

| Number of input features | Memory (mebibytes) | CPU (milli cpus) |
| :----------------------- | :----------------- | :--------------- |
| \< 10                    | 250 (default)      | 100 (default)    |
| \< 20                    | 400                | 300              |
| \< 50                    | 600                | 400              |
| \<100                    | 850                | 900              |
| \<200                    | 160"
"slug: ""fdldeploymentparams"" 0               | 1200             |
| \<300                    | 2000               | 1200             |
| \<400                    | 2800               | 1300             |
| \<500                    | 2900               | 1500             |

2. **User Uploaded guide**

For uploading your artifact model, refer to the table above and increase the memory number, depending on your model framework and complexity. Surrogate models use lightgbm framework. 

For example, an NLP model for a TEXT input might need memory set at 1024 or higher and CPU at 1000.

> üìò Usage Reference
> 
> See the usage with:
> 
> - [add_model_artifact](ref:clientadd_model_artifact)
> - [add_model_surrogate](ref:clientadd_model_surrogate)
> - [update_model_artifact](ref:clientupdate_model_artifact)
> - [update_model_surrogate](ref:clientupdate_model_surrogate)
> 
> Check more about the [Model Deployment](doc:model-deployment) feature set.
"
"---
title: ""fdl.DatasetInfo""
slug: ""fdldatasetinfo""
excerpt: ""Stores information about a dataset.""
hidden: false
createdAt: ""Tue May 24 2022 15:05:38 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
For information on how to customize these objects, see [Customizing Your Dataset Schema](doc:customizing-your-dataset-schema).

| Input Parameters | Type            | Default | Description                                                                |
| :--------------- | :-------------- | :------ | :------------------------------------------------------------------------- |
| display_name     | str             | None    | A display name for the dataset.                                            |
| columns          | list            | None    | A list of **fdl.Column** objects containing information about the columns. |
| files            | Optional [list] | None    | A list of strings pointing to CSV files to use.                            |
| dataset_id       | Optional [str]  | None    | The unique identifier for the dataset                                      |
| \*\*kwargs       |                 |         | Additional arguments to be passed.                                         |

```python Usage
columns = [
    fdl.Column(
        name='feature_1',
        data_type=fdl.DataType.FLOAT
    ),
    fdl.Column(
        name='feature_2',
        data_type=fdl.DataType.INTEGER
    ),
    fdl.Column(
        name='feature_3',
        data_type=fdl.DataType.BOOLEAN
    ),
    fdl.Column(
        name='output_column',
        data_type=fdl.DataType.FLOAT
    ),
    fdl.Column(
        name='target_column',
        data_type=fdl.DataType.INTEGER
    )
]

dataset_info = fdl.DatasetInfo(
    display_name='Example Dataset',
    columns=columns
)
```
"
"---
title: ""fdl.Metric""
slug: ""fdlmetric""
excerpt: ""Supported Metric for different Alert Types in Alert Rules""
hidden: false
createdAt: ""Tue Jan 31 2023 07:32:12 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Tue Dec 19 2023 21:20:06 GMT+0000 (Coordinated Universal Time)""
---
**Following is the list of metrics, with corresponding alert type and model task, for which an alert rule can be created.**

[block:parameters]
{
  ""data"": {
    ""h-0"": ""Enum Values"",
    ""h-1"": ""Supported for [Alert Types](ref:fdlalerttype)  \n([ModelTask ](ref:fdlmodeltask)restriction if any)"",
    ""h-2"": ""Description"",
    ""0-0"": ""fdl.Metric.SUM"",
    ""0-1"": ""fdl.AlertType.STATISTIC"",
    ""0-2"": ""Sum of all values of a column across all events"",
    ""1-0"": ""fdl.Metric.AVERAGE"",
    ""1-1"": ""fdl.AlertType.STATISTIC"",
    ""1-2"": ""Average value of a column across all events"",
    ""2-0"": ""fdl.Metric.FREQUENCY"",
    ""2-1"": ""fdl.AlertType.STATISTIC"",
    ""2-2"": ""Frequency count of a specific value in a categorical column"",
    ""3-0"": ""fdl.Metric.PSI"",
    ""3-1"": ""fdl.AlertType.DATA_DRIFT"",
    ""3-2"": ""Population Stability Index"",
    ""4-0"": ""fdl.Metric.JSD"",
    ""4-1"": ""fdl.AlertType.DATA_DRIFT"",
    ""4-2"": ""Jensen‚ÄìShannon divergence"",
    ""5-0"": ""fdl.Metric.MISSING_VALUE"",
    ""5-1"": ""fdl.AlertType.DATA_INTEGRITY"",
    ""5-2"": ""Missing Value"",
    ""6-0"": ""fdl.Metric.TYPE_VIOLATION"",
    ""6-1"": ""fdl.AlertType.DATA_INTEGRITY"",
    ""6-2"": ""Type Violation"",
    ""7-0"": ""fdl.Metric.RANGE_VIOLATION"",
    ""7-1"": ""fdl.AlertType.DATA_INTEGRITY"",
    ""7-2"": ""Range violation"",
    ""8-0"": ""fdl.Metric.TRAFFIC"",
    ""8-1"": ""fdl.AlertType.SERVICE_METRICS"",
    ""8-2"": ""Traffic Count"",
    ""9-0"": ""fdl.Metric.ACCURACY"",
    ""9-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.BINARY_CLASSIFICATION,  \nfdl.ModelTask.MULTICLASS_CLASSIFICATION)"",
    ""9-2"": ""Accuracy"",
    ""10-0"": ""fdl.Metric.RECALL"",
    ""10-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.BINARY_CLASSIFICATION)"",
    ""10-2"": ""Recall"",
    ""11-0"": ""fdl.Metric.FPR"",
    ""11-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.BINARY_CLASSIFICATION)"",
    ""11-2"": ""False Positive Rate"",
    ""12-0"": "" fdl.Metric.PREC"
"slug: ""fdlmetric"" ISION"",
    ""12-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.BINARY_CLASSIFICATION)"",
    ""12-2"": ""Precision"",
    ""13-0"": ""fdl.Metric.TPR"",
    ""13-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.BINARY_CLASSIFICATION)"",
    ""13-2"": ""True Positive Rate"",
    ""14-0"": ""fdl.Metric.AUC"",
    ""14-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.BINARY_CLASSIFICATION)"",
    ""14-2"": ""Area under the ROC Curve"",
    ""15-0"": ""fdl.Metric.F1_SCORE"",
    ""15-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.BINARY_CLASSIFICATION)"",
    ""15-2"": ""F1 score"",
    ""16-0"": ""fdl.Metric.ECE"",
    ""16-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.BINARY_CLASSIFICATION)"",
    ""16-2"": ""Expected Calibration Error"",
    ""17-0"": ""fdl.Metric.R2"",
    ""17-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.REGRESSION)"",
    ""17-2"": ""R Squared"",
    ""18-0"": ""fdl.Metric.MSE"",
    ""18-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.REGRESSION)"",
    ""18-2"": ""Mean squared error"",
    ""19-0"": ""fdl.Metric.MAPE"",
    ""19-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.REGRESSION)"",
    ""19-2"": ""Mean Absolute Percentage Error"",
    ""20-0"": ""fdl.Metric.WMAPE"",
    ""20-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.REGRESSION)"",
    ""20-2"": ""Weighted Mean Absolute Percentage Error"",
    ""21-0"": ""fdl.Metric.MAE"",
    ""21-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.REGRESSION)"",
    ""21-2"": ""Mean Absolute Error"",
    ""22-0"": ""fdl.Metric.LOG_LOSS"",
    ""22-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.MULTICLASS_CLASSIFICATION)"",
    ""22-2"": ""Log Loss"",
    ""23-0"": ""fdl.Metric.MAP"",
    ""23-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.RANKING)"",
    ""23-2"": ""Mean Average Precision"",
    ""24-0"": ""fdl.Metric.MEAN_NDCG"",
    ""24-1"": ""fdl.AlertType.PERFORMANCE  \n(fdl.ModelTask.RANKING)"",
    ""24-2"": ""Normalized Discounted Cumulative Gain""
  },
  ""cols"": 3,
  ""rows"": 25,
  ""align"": [
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


```coffeescript Usage
import fiddler as fdl

client.add_alert_rule(
    name = ""perf-gt-5prec-1hr-1d-ago"",
    project_name = 'project-a',
    model_name = 'binary_classification_model-a',
    alert_type = fdl.AlertType.PERFORMANCE,
    metric"
"slug: ""fdlmetric""  = fdl.Metric.PRECISION, <----
    bin_size = fdl.BinSize.ONE_HOUR, 
    compare_to = fdl.CompareTo.TIME_PERIOD,
    compare_period = fdl.ComparePeriod.ONE_DAY,
    warning_threshold = 0.05,
    critical_threshold = 0.1,
    condition = fdl.AlertCondition.GREATER,
    priority = fdl.Priority.HIGH,
    notifications_config = notifications_config
)
```
```coffeescript Outputs
[AlertRule(alert_rule_uuid='9b8711fa-735e-4a72-977c-c4c8b16543ae',
           organization_name='some_org_name',
           project_id='project-a',
           model_id='binary_classification_model-a',
           name='perf-gt-5prec-1hr-1d-ago',
           alert_type=AlertType.PERFORMANCE,
           metric=Metric.PRECISION, <---
           priority=Priority.HIGH,
           compare_to='CompareTo.TIME_PERIOD,
           compare_period=ComparePeriod.ONE_DAY,
           compare_threshold=None,
           raw_threshold=None,
           warning_threshold=0.05,
           critical_threshold=0.1,
           condition=AlertCondition.GREATER,
           bin_size=BinSize.ONE_HOUR)]
```
"
"---
title: ""fdl.BaselineType""
slug: ""fdlbaselinetype""
excerpt: ""Enum for different types of baselines""
hidden: false
createdAt: ""Wed Feb 01 2023 00:05:29 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Enum                                | Description                                                                               |
| :---------------------------------- | :---------------------------------------------------------------------------------------- |
| fdl.BaselineType.PRE_PRODUCTION     | Used for baselines on uploaded datasets.They can be training or validation datasets.      |
| fdl.BaselineType.STATIC_PRODUCTION  | Used to describe a baseline on production events of a model between a specific time range |
| fdl.BaselineType.ROLLING_PRODUCTION | Used to describe a baseline on production events of a model relative to the current time  |

```c Usage
from fiddler import BaselineType

PROJECT_NAME = 'example_project'
BASELINE_NAME = 'example_rolling'
DATASET_NAME = 'example_validation'
MODEL_NAME = 'example_model'

client.add_baseline(
  project_id=PROJECT_NAME,
  model_id=MODEL_NAME,
  baseline_id=BASELINE_NAME,
  type=BaselineType.PRE_PRODUCTION,
  dataset_id=DATASET_NAME,
)
```
"
"---
title: ""fdl.AlertCondition""
slug: ""fdlalertcondition""
excerpt: ""Alert conditions for comparisons""
hidden: false
createdAt: ""Tue Jan 31 2023 07:25:05 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
**If condition = fdl.AlertCondition.GREATER/LESSER is specified, and an alert is triggered every time the metric value is greater/lesser than the specified threshold.**

| Enum                       | Value   |
| :------------------------- | :------ |
| fdl.AlertCondition.GREATER | greater |
| fdl.AlertCondition.LESSER  | lesser  |

```coffeescript Usage
import fiddler as fdl

client.add_alert_rule(
    name = ""perf-gt-5prec-1hr-1d-ago"",
    project_name = 'project-a',
    model_name = 'model-a',
    alert_type = fdl.AlertType.PERFORMANCE, 
    metric = fdl.Metric.PRECISION,
    bin_size = fdl.BinSize.ONE_HOUR, 
    compare_to = fdl.CompareTo.TIME_PERIOD,
    compare_period = fdl.ComparePeriod.ONE_DAY,
    warning_threshold = 0.05,
    critical_threshold = 0.1,
    condition = fdl.AlertCondition.GREATER, <-----
    priority = fdl.Priority.HIGH,
    notifications_config = notifications_config
)
```
```coffeescript Outputs
[AlertRule(alert_rule_uuid='9b8711fa-735e-4a72-977c-c4c8b16543ae',
           organization_name='some_org_name',
           project_id='project-a',
           model_id='model-a',
           name='perf-gt-5prec-1hr-1d-ago',
           alert_type=AlertType.PERFORMANCE, <---
           metric=Metric.PRECISION,
           priority=Priority.HIGH,
           compare_to='CompareTo.TIME_PERIOD,
           compare_period=ComparePeriod.ONE_DAY,
           compare_threshold=None,
           raw_threshold=None,
           warning_threshold=0.05,
           critical_threshold=0.1,
           condition=AlertCondition.GREATER, <-----
           bin_size=BinSize.ONE_HOUR)]
```
"
"---
title: ""fdl.CustomFeatureType""
slug: ""fdlcustomfeaturetype""
excerpt: ""This is an enumeration defining the types of custom features that can be created.""
hidden: false
createdAt: ""Tue Oct 24 2023 04:13:43 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Enum                 | Value                                                     |
| :------------------- | :-------------------------------------------------------- |
| FROM_COLUMNS         | Represents custom features derived directly from columns. |
| FROM_VECTOR          | Represents custom features derived from a vector column.  |
| FROM_TEXT_EMBEDDING  | Represents custom features derived from text embeddings.  |
| FROM_IMAGE_EMBEDDING | Represents custom features derived from image embeddings. |
"
"---
title: ""fdl.ModelTask""
slug: ""fdlmodeltask""
excerpt: ""Represents supported model tasks""
hidden: false
createdAt: ""Wed May 25 2022 14:56:32 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Enum Value                              | Description                                       |
| :-------------------------------------- | :------------------------------------------------ |
| fdl.ModelTask.REGRESSION                | For regression models.                            |
| fdl.ModelTask.BINARY_CLASSIFICATION     | For binary classification models                  |
| fdl.ModelTask.MULTICLASS_CLASSIFICATION | For multiclass classification models              |
| fdl.ModelTask.RANKING                   | For ranking classification models                 |
| fdl.ModelTask.LLM                       | For LLM models.                                   |
| fdl.ModelTask.NOT_SET                   | For other model tasks or no model task specified. |

```python Usage
model_task = fdl.ModelTask.BINARY_CLASSIFICATION
```
"
"---
title: ""fdl.DatasetInfo.to_dict""
slug: ""fdldatasetinfoto_dict""
excerpt: ""Converts a DatasetInfo object to a dictionary.""
hidden: false
createdAt: ""Tue May 24 2022 15:13:18 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Return Type | Description                                                                                  |
| :---------- | :------------------------------------------------------------------------------------------- |
| dict        | A dictionary containing information from the [fdl.DatasetInfo()](ref:fdldatasetinfo) object. |

```python Usage
import pandas as pd

df = pd.read_csv('example_dataset.csv')

dataset_info = fdl.DatasetInfo.from_dataframe(df=df, max_inferred_cardinality=100)

dataset_info_dict = dataset_info.to_dict()
```

```python Response
{
    'name': 'Example Dataset',
    'columns': [
        {
            'column-name': 'feature_1',
            'data-type': 'float'
        },
        {
            'column-name': 'feature_2',
            'data-type': 'int'
        },
        {
            'column-name': 'feature_3',
            'data-type': 'bool'
        },
        {
            'column-name': 'output_column',
            'data-type': 'float'
        },
        {
            'column-name': 'target_column',
            'data-type': 'int'
        }
    ],
    'files': []
}
```
"
"---
title: ""fdl.DatasetInfo.from_dataframe""
slug: ""fdldatasetinfofrom_dataframe""
excerpt: ""Constructs a DatasetInfo object from a pandas DataFrame.""
hidden: false
createdAt: ""Tue May 24 2022 15:10:45 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters         | Type                       | Default | Description                                                                                                                                  |
| :----------------------- | :------------------------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------- |
| df                       | Union [pd.Dataframe, list] |         | Either a single pandas DataFrame or a list of DataFrames. If a list is given, all dataframes must have the same columns.                     |
| display_name             | str                        | ' '     | A display_name for the dataset                                                                                                               |
| max_inferred_cardinality | Optional [int]             | 100     | If specified, any string column containing fewer than _max_inferred_cardinality_ unique values will be converted to a categorical data type. |
| dataset_id               | Optional [str]             | None    | The unique identifier for the dataset                                                                                                        |

```python Usage
import pandas as pd

df = pd.read_csv('example_dataset.csv')

dataset_info = fdl.DatasetInfo.from_dataframe(df=df, max_inferred_cardinality=100)
```

| Return Type     | Description                                                                                      |
| :-------------- | :----------------------------------------------------------------------------------------------- |
| fdl.DatasetInfo | A [fdl.DatasetInfo()](ref:fdldatasetinfo) object constructed from the pandas Dataframe provided. |
"
"---
title: ""fdl.DatasetInfo.from_dict""
slug: ""fdldatasetinfofrom_dict""
excerpt: ""Converts a dictionary to a DatasetInfo object.""
hidden: false
createdAt: ""Tue May 24 2022 15:15:40 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters  | Type | Default | Description                           |
| :---------------- | :--- | :------ | :------------------------------------ |
| deserialized_json | dict |         | The dictionary object to be converted |

```python Usage
import pandas as pd

df = pd.read_csv('example_dataset.csv')

dataset_info = fdl.DatasetInfo.from_dataframe(df=df, max_inferred_cardinality=100)

dataset_info_dict = dataset_info.to_dict()

new_dataset_info = fdl.DatasetInfo.from_dict(
    deserialized_json={
        'dataset': dataset_info_dict
    }
)
```

| Return Type     | Description                                                                       |
| :-------------- | :-------------------------------------------------------------------------------- |
| fdl.DatasetInfo | A [fdl.DatasetInfo()](ref:fdldatasetinfo) object constructed from the dictionary. |
"
"---
title: ""fdl.ModelInfo.to_dict""
slug: ""fdlmodelinfoto_dict""
excerpt: ""Converts a Model object to a dictionary.""
hidden: false
createdAt: ""Tue May 24 2022 15:54:52 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Return Type | Description                                                                              |
| :---------- | :--------------------------------------------------------------------------------------- |
| dict        | A dictionary containing information from the [fdl.ModelInfo()](ref:fdlmodelinfo) object. |

```python Usage
import pandas as pd

df = pd.read_csv('example_dataset.csv')

dataset_info = fdl.DatasetInfo.from_dataframe(
    df=df
)

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    features=[
        'feature_1',
        'feature_2',
        'feature_3'
    ],
    outputs=[
        'output_column'
    ],
    target='target_column',
    input_type=fdl.ModelInputType.TABULAR,
    model_task=fdl.ModelTask.BINARY_CLASSIFICATION
)

model_info_dict = model_info.to_dict()
```

```python Response
{
    'name': 'Example Model',
    'input-type': 'structured',
    'model-task': 'binary_classification',
    'inputs': [
        {
            'column-name': 'feature_1',
            'data-type': 'float'
        },
        {
            'column-name': 'feature_2',
            'data-type': 'int'
        },
        {
            'column-name': 'feature_3',
            'data-type': 'bool'
        },
        {
            'column-name': 'target_column',
            'data-type': 'int'
        }
    ],
    'outputs': [
        {
            'column-name': 'output_column',
            'data-type': 'float'
        }
    ],
    'datasets': [],
    'targets': [
        {
            'column-name': 'target_column',
            'data-type': 'int'
        }
    ],
    'custom-explanation-names': []
}
```
"
"---
title: ""fdl.ModelInfo.from_dataset_info""
slug: ""fdlmodelinfofrom_dataset_info""
excerpt: ""Constructs a ModelInfo object from a DatasetInfo object.""
hidden: false
createdAt: ""Wed Feb 08 2023 17:27:59 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
[block:parameters]
{
  ""data"": {
    ""h-0"": ""Input Parameters"",
    ""h-1"": ""Type"",
    ""h-2"": ""Default"",
    ""h-3"": ""Description"",
    ""0-0"": ""dataset_info"",
    ""0-1"": ""[fdl.DatasetInfo()](ref:fdldatasetinfo)"",
    ""0-2"": """",
    ""0-3"": ""The **DatasetInfo** object from which to construct the **ModelInfo** object."",
    ""1-0"": ""target"",
    ""1-1"": ""str"",
    ""1-2"": """",
    ""1-3"": ""The column to be used as the target (ground truth)."",
    ""2-0"": ""model_task"",
    ""2-1"": ""[fdl.ModelTask](ref:fdlmodeltask)"",
    ""2-2"": ""None"",
    ""2-3"": ""A **ModelTask** object containing the model task."",
    ""3-0"": ""dataset_id"",
    ""3-1"": ""Optional [str]"",
    ""3-2"": ""None"",
    ""3-3"": ""The unique identifier for the dataset."",
    ""4-0"": ""features"",
    ""4-1"": ""Optional [list]"",
    ""4-2"": ""None"",
    ""4-3"": ""A list of columns to be used as features."",
    ""5-0"": ""custom_features"",
    ""5-1"": ""Optional\\[List\\[[CustomFeature](fdlcustomfeature)]]"",
    ""5-2"": ""None"",
    ""5-3"": ""List of Custom Features definitions for a model. Objects of type [Multivariate](fdlmultivariate), [Vector](fdlvectorfeature), [ImageEmbedding](fdlimageembedding) or [TextEmbedding](fdltextembedding) derived from [CustomFeature](fdlcustomfeature) can be provided."",
    ""6-0"": ""metadata_cols"",
    ""6-1"": ""Optional [list]"",
    ""6-2"": ""None"",
    ""6-3"": ""A list of columns to be used as metadata fields."",
    ""7-0"": ""decision_cols"",
    ""7-1"": ""Optional [list]"",
    ""7-2"": ""None"",
    ""7-3"": ""A list of columns to be used as decision fields."",
    ""8-0"": ""display_name"",
    ""8-1"": ""Optional [str]"",
    ""8-2"": ""None"",
    ""8-3"": ""A display name for the model."",
    ""9-0"": ""description"",
    ""9-1"": ""Optional [str]"",
    ""9-2"": ""None"",
    ""9-3"": ""A description of the model."",
    ""10-0"": ""input_type"",
    ""10-1"": ""Optional [fdl.ModelInputType]"",
    ""10-2"": ""fdl.ModelInputType.TABULAR"",
    ""10-3"": ""A **ModelInputType** object containing the input type of"
"slug: ""fdlmodelinfofrom_dataset_info""  the model."",
    ""11-0"": ""outputs"",
    ""11-1"": ""Optional [list]"",
    ""11-2"": """",
    ""11-3"": ""A list of **Column** objects corresponding to the outputs (predictions) of the model."",
    ""12-0"": ""targets"",
    ""12-1"": ""Optional [list]"",
    ""12-2"": ""None"",
    ""12-3"": ""A list of **Column** objects corresponding to the targets (ground truth) of the model."",
    ""13-0"": ""model_deployment_params"",
    ""13-1"": ""Optional [fdl.ModelDeploymentParams]"",
    ""13-2"": ""None"",
    ""13-3"": ""A **ModelDeploymentParams** object containing information about model deployment."",
    ""14-0"": ""framework"",
    ""14-1"": ""Optional [str]"",
    ""14-2"": ""None"",
    ""14-3"": ""A string providing information about the software library and version used to train and run this model."",
    ""15-0"": ""datasets"",
    ""15-1"": ""Optional [list]"",
    ""15-2"": ""None"",
    ""15-3"": ""A list of the dataset IDs used by the model."",
    ""16-0"": ""mlflow_params"",
    ""16-1"": ""Optional [fdl.MLFlowParams]"",
    ""16-2"": ""None"",
    ""16-3"": ""A **MLFlowParams** object containing information about MLFlow parameters."",
    ""17-0"": ""preferred_explanation_method"",
    ""17-1"": ""Optional [fdl.ExplanationMethod]"",
    ""17-2"": ""None"",
    ""17-3"": ""An **ExplanationMethod** object that specifies the default explanation algorithm to use for the model."",
    ""18-0"": ""custom_explanation_names"",
    ""18-1"": ""Optional [list]"",
    ""18-2"": ""[ ]"",
    ""18-3"": ""A list of names that can be passed to the _explanation_name \\_argument of the optional user-defined \\_explain_custom_ method of the model object defined in _package.py._"",
    ""19-0"": ""binary_classification_threshold"",
    ""19-1"": ""Optional [float]"",
    ""19-2"": "".5"",
    ""19-3"": ""The threshold used for classifying inferences for binary classifiers."",
    ""20-0"": ""ranking_top_k"",
    ""20-1"": ""Optional [int]"",
    ""20-2"": ""50"",
    ""20-3"": ""Used only for ranking models. Sets the top _k_ results to take into consideration when computing performance metrics like MAP and NDCG."",
    ""21-0"": ""group_by"",
    ""21-1"": ""Optional [str]"",
    ""21-2"": ""None"",
    ""21-3"": ""Used only for ranking models.  The column by which to group events for certain performance metrics like MAP and NDCG."",
    ""22-0"": ""fall_back"",
    ""22-1"": ""Optional [dict]"",
    ""22-2"": ""None"",
    ""22-3"": ""A dictionary mapping a column name to custom missing value encodings for that column."",
    ""23-0"": ""categorical_target_class_details"",
    ""23-1"": ""Optional \\[Union[list, int, str]]"",
    ""23-2"": ""None"",
    ""23-3"": ""A list denoting the order of classes in"
"slug: ""fdlmodelinfofrom_dataset_info""  the target. This parameter is **required** in the following cases:  \n  \n_- Binary classification tasks_: If the **target** is of type _string_, you must tell Fiddler which class is considered the positive class for your **output** column. If you provide a single element, it is considered the positive class. Alternatively, you can provide a list with two elements. The 0th element by convention is considered the negative class, and the 1st element is considered the positive class.  When your **target** is _boolean_, you don't need to specify this argument. By default Fiddler considers `True` as the positive class. In case your target is _numerical_, you don't need to  specify this argument, by default Fiddler considers the higher of the two possible values as the positive class.  \n  \n- _Multi-class classification tasks_: You must tell Fiddler which class corresponds to which output by giving an ordered list of classes. This order should be the same as the order of the outputs.  \n  \n- _Ranking tasks_: If the target is of type _string_, you must provide a list of all the possible target values in the order of relevance. The first element will be considered as the least relevant grade and the last element from the list will be considered the most relevant grade.  \nIn the case your target is _numerical_, Fiddler considers the smallest value to be the least relevant grade and the biggest value from the list will be considered the most relevant grade.""
  },
  ""cols"": 4,
  ""rows"": 24,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


```python Usage
import pandas as pd

df = pd.read_csv('example_dataset.csv')

dataset_info = fdl.DatasetInfo.from_dataframe(
    df=df
)

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    features=[
        'feature_1',
        'feature_2',
        'feature_3'
    ],
    outputs=[
        'output_column'
    ],
    target='target_column',
    input_type=fdl.ModelInputType.TABULAR,
    model_task=fdl.ModelTask.BINARY_CLASSIFICATION
)
```

| Return Type   | Description                                                                                                                |
| :------------ | :------------------------------------------------------------------------------------------------------------------------- |
| fdl.ModelInfo | A [fdl.ModelInfo()](ref:fdlmodelinfo) object constructed from the [fdl.DatasetInfo()](ref:fdldatasetinfo) object provided. |
"
"---
title: ""fdl.ModelInfo.from_dict""
slug: ""fdlmodelinfofrom_dict""
excerpt: ""Converts a dictionary to a ModelInfo object.""
hidden: false
createdAt: ""Tue May 24 2022 15:56:13 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters  | Type | Default | Description                           |
| :---------------- | :--- | :------ | :------------------------------------ |
| deserialized_json | dict |         | The dictionary object to be converted |

```python Usage
import pandas as pd

df = pd.read_csv('example_dataset.csv')

dataset_info = fdl.DatasetInfo.from_dataframe(
    df=df
)

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    features=[
        'feature_1',
        'feature_2',
        'feature_3'
    ],
    outputs=[
        'output_column'
    ],
    target='target_column',
    input_type=fdl.ModelInputType.TABULAR,
    model_task=fdl.ModelTask.BINARY_CLASSIFICATION
)

model_info_dict = model_info.to_dict()

new_model_info = fdl.ModelInfo.from_dict(
    deserialized_json={
        'model': model_info_dict
    }
)
```

| Return Type   | Description                                                                   |
| :------------ | :---------------------------------------------------------------------------- |
| fdl.ModelInfo | A [fdl.ModelInfo()](ref:fdlmodelinfo) object constructed from the dictionary. |
"
"---
title: ""fdl.RowDataSource""
slug: ""fdlrowdatasource""
excerpt: ""Provides the single row to use for point explanation.""
hidden: false
createdAt: ""Wed Aug 30 2023 14:41:13 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type | Default | Description                            |
| :-------------- | :--- | :------ | :------------------------------------- |
| row             | dict | None    | Single row to explain as a dictionary. |

```python Usage
row = df.to_dict(orient='records')[0]

data_source = fdl.RowDataSource(
    row=row,
)
```
"
"---
title: ""fdl.EventIdDataSource""
slug: ""fdleventiddatasource""
excerpt: ""Indicates the single row to use for point explanation given the Event ID.""
hidden: false
createdAt: ""Wed Aug 30 2023 14:41:28 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type | Default | Description                                                                                                                |
| :-------------- | :--- | :------ | :------------------------------------------------------------------------------------------------------------------------- |
| event_id        | str  | None    | Single event id corresponding to the row to explain.                                                                       |
| dataset_name    | str  | None    | The dataset name if the event is located in the dataset table or 'production' if the event if part of the production data. |

```python Usage
DATASET_ID = 'example_dataset'

# In Dataset table
data_source = fdl.EventIdDataSource(
    event_id='xGhys7-83HgdtsoiuYTa872',
  	dataset_name=DATASET_ID,
)

# In Production table
data_source = fdl.EventIdDataSource(
    event_id='xGhys7-83HgdtsoiuYTa872',
  	dataset_name='production',
)
```
"
"---
title: ""fdl.SqlSliceQueryDataSource""
slug: ""fdlsqlslicequerydatasource""
excerpt: ""Indicates the slice of data to use as a source data for explainability computations.""
hidden: false
createdAt: ""Wed Aug 30 2023 14:40:59 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type           | Default | Description                                           |
| :-------------- | :------------- | :------ | :---------------------------------------------------- |
| query           | str            | None    | Slice query defining the data to use for computation. |
| num_samples     | Optional [int] | None    | Number of samples to select for computation.          |

```python Usage
DATASET_ID = 'example_dataset'
MODEL_ID = 'example_model'

query = f'SELECT * FROM {DATASET_ID}.{MODEL_ID} WHERE CreditScore > 700'
data_source = fdl.SqlSliceQueryDataSource(
    query=query,
  	num_samples=500,
)
```
"
"---
title: ""fdl.DatasetDataSource""
slug: ""fdldatasetdatasource""
excerpt: ""Indicates the dataset to use as a source data for explainability computations.""
hidden: false
createdAt: ""Wed Aug 30 2023 14:40:41 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 14 2023 15:05:02 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type           | Default | Description                                                                 |
| :-------------- | :------------- | :------ | :-------------------------------------------------------------------------- |
| dataset_id      | str            | None    | The unique identifier for the dataset.                                      |
| source          | Optional[str]  | None    | The source file name. If not specified, using all sources from the dataset. |
| num_samples     | Optional [int] | None    | Number of samples to select for computation.                                |

```python Usage
DATASET_ID = 'example_dataset'

data_source = fdl.DatasetDataSource(
    dataset_id=DATASET_ID
  	source='baseline.csv',
  	num_samples=500,
)
```
"
"---
title: ""fdl.TextEmbedding""
slug: ""fdltextembedding""
excerpt: ""Represents custom features derived from text embeddings.""
hidden: false
createdAt: ""Tue Oct 24 2023 04:09:57 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type                                      | Default                                                                                    | Description                                                                                                          |
| :-------------- | :---------------------------------------- | :----------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------- |
| type            | [CustomFeatureType](fdlcustomfeaturetype) | CustomFeatureType.FROM_TEXT_EMBEDDING                                                      | Indicates this feature is derived from a text embedding.                                                             |
| source_column   | str                                       | Required                                                                                   | Specifies the column name where text data (e.g. LLM prompts) is stored                                               |
| column          | str                                       | Required                                                                                   | Specifies the column name where the embeddings corresponding to source_col are stored                                |
| n_tags          | Optional[int]                             | 5                                                                                          | How many tags(tokens) the text embedding are used in each cluster as the `tfidf` summarization in drift computation. |
| n_clusters      | Optional[int]                             | 5                                                                                          | The number of clusters.                                                                                              |
| centroids       | Optional[List]                            | Centroids of the clusters in the embedded space. Number of centroids equal to `n_clusters` | Centroids of the clusters in the embedded space. Number of centroids equal to `n_clusters`                           |

```python Usage
text_embedding_feature = TextEmbedding(
    name='text_custom_feature',
    source_column='text_column',
    column='text_embedding',
    n_tags=10
)
```
"
"---
title: ""fdl.VectorFeature""
slug: ""fdlvectorfeature""
excerpt: ""Represents custom features derived from a single vector column.""
hidden: false
createdAt: ""Tue Oct 24 2023 04:08:01 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type                                      | Default                                                                                    | Description                                                                                |
| :-------------- | :---------------------------------------- | :----------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------- |
| type            | [CustomFeatureType](fdlcustomfeaturetype) | CustomFeatureType.FROM_VECTOR                                                              | Indicates this feature is derived from a single vector column.                             |
| source_column   | Optional[str]                             | None                                                                                       | Specifies the original column if this feature is derived from an embedding.                |
| column          | str                                       | None                                                                                       | The vector column name.                                                                    |
| n_clusters      | Optional[int]                             | 5                                                                                          | The number of clusters.                                                                    |
| centroids       | Optional[List]                            | Centroids of the clusters in the embedded space. Number of centroids equal to `n_clusters` | Centroids of the clusters in the embedded space. Number of centroids equal to `n_clusters` |

```python Usage
vector_feature = fdl.VectorFeature(
    name='vector_feature',
    column='vector_column'
)
```
"
"---
title: ""fdl.Multivariate""
slug: ""fdlmultivariate""
excerpt: ""Represents custom features derived from multiple columns.""
hidden: false
createdAt: ""Tue Oct 24 2023 04:05:45 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter    | Type                                      | Default                                                                                    | Description                                                                                                                                |
| :----------------- | :---------------------------------------- | :----------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------- |
| type               | [CustomFeatureType](fdlcustomfeaturetype) | CustomFeatureType.FROM_COLUMNS                                                             | Indicates this feature is derived from multiple columns.                                                                                   |
| columns            | List[str]                                 | None                                                                                       | List of original columns from which this feature is derived.                                                                               |
| n_clusters         | Optional[int]                             | 5                                                                                          | The number of clusters.                                                                                                                    |
| centroids          | Optional[List]                            | Centroids of the clusters in the embedded space. Number of centroids equal to `n_clusters` | Centroids of the clusters in the embedded space. Number of centroids equal to \`n_clusters                                                 |
| monitor_components | bool                                      | False                                                                                      | Whether to monitor each column in `columns` as individual feature. If set to `True`, components are monitored and drift will be available. |

```python Usage
multivariate_feature = fdl.Multivariate(
    name='multi_feature',
    columns=['column_1', 'column_2']
)
```
"
"---
title: ""fdl.ImageEmbedding""
slug: ""fdlimageembedding""
excerpt: ""Represents custom features derived from image embeddings.""
hidden: false
createdAt: ""Tue Oct 24 2023 04:11:24 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:07 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type                                      | Default                                                                                    | Description                                                                                |
| :-------------- | :---------------------------------------- | :----------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------- |
| type            | [CustomFeatureType](fdlcustomfeaturetype) | CustomFeatureType.FROM_IMAGE_EMBEDDING                                                     | Indicates this feature is derived from an image embedding.                                 |
| source_column   | str                                       | Required                                                                                   | URL where image data is stored                                                             |
| column          | str                                       | Required                                                                                   | Specifies the column name where embeddings corresponding to source_col are stored.         |
| n_clusters      | Optional[int]                             | 5                                                                                          | The number of clusters                                                                     |
| centroids       | Optional[List]                            | Centroids of the clusters in the embedded space. Number of centroids equal to `n_clusters` | Centroids of the clusters in the embedded space. Number of centroids equal to `n_clusters` |

```python Usage
image_embedding_feature = fdl.ImageEmbedding(
    name='image_feature',
    source_column='image_url',
    column='image_embedding',
)
```
"
"---
title: ""client.get_fairness""
slug: ""clientget_fairness""
excerpt: ""Get fairness analysis on a dataset or a slice.""
hidden: false
createdAt: ""Wed Aug 16 2023 11:16:57 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:07 GMT+0000 (Coordinated Universal Time)""
---
> üöß Only Binary classification models with categorical protected attributes are currently supported.

| Input Parameter    | Type                                                                                                                     | Default | Description                                                                                             |
| :----------------- | :----------------------------------------------------------------------------------------------------------------------- | :------ | :------------------------------------------------------------------------------------------------------ |
| project_id         | str                                                                                                                      | None    | The unique identifier for the project.                                                                  |
| model_id           | str                                                                                                                      | None    | The unique identifier for the model.                                                                    |
| data_source        | Union\[[fdl.DatasetDataSource](ref:fdldatasetdatasource), [fdl.SqlSliceQueryDataSource](ref:fdlsqlslicequerydatasource)] | None    | DataSource for the input dataset to compute fairness on (DatasetDataSource or SqlSliceQueryDataSource). |
| protected_features | list[str]                                                                                                                | None    | A list of protected features.                                                                           |
| positive_outcome   | Union[str, int, float, bool]                                                                                             | None    | Value of the positive outcome (from the target column) for Fairness analysis.                           |
| score_threshold    | Optional [float]                                                                                                         | 0.5     | The score threshold used to calculate model outcomes.                                                   |

```python Usage
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'
DATASET_ID = 'example_dataset'

# Fairness - Dataset data source
fairness_metrics = client.get_fairness(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    data_source=fdl.DatasetDataSource(dataset_id=DATASET_ID, num_samples=200),
    protected_features=['feature_1', 'feature_2'],
    positive_outcome='Approved',
    score_threshold=0.6
)

# Fairness - Slice Query data source
query = f'SELECT * FROM {DATASET_ID}.{MODEL_ID} WHERE CreditSCore > 700'
fairness_metrics = client.get_fairness(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    data_source=fdl.SqlSliceQueryDataSource(query=query, num_samples=200),
    protected_features=['feature_1', 'feature_2'],
    positive_outcome='Approved',
    score_threshold=0.6
)
```

| Return Type | Description                                      |
| :---------- | :----------------------------------------------- |
| dict        | A dictionary containing fairness metric results. |
"
"---
title: ""Dashboards""
slug: ""dashboards-ui""
excerpt: """"
hidden: false
createdAt: ""Tue Feb 21 2023 22:35:31 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:50:41 GMT+0000 (Coordinated Universal Time)""
---
## Creating Dashboards

To begin using our dashboard feature, navigate to the dashboard page by clicking on ""Dashboards"" from the top-level navigation bar. On the Dashboards page, you can choose to either select from previously created dashboards or create a new one. This simple process allows you to quickly access your dashboards and begin monitoring your models' performance, data drift, data integrity, and traffic.

![](https://files.readme.io/570614f-image.png)

When creating a new dashboard, it's important to note that each dashboard is tied to a specific project space. This means that only models and charts associated with that project can be added to the dashboard. To ensure you're working within the correct project space, select the desired project space before entering the dashboard editor page, then click ""Continue."" This will ensure that you can add relevant charts and models to your dashboard.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/ef961be-image.png"",
        null,
        """"
      ],
      ""align"": ""center"",
      ""sizing"": ""400px""
    }
  ]
}
[/block]


## Add Monitoring Chart

Once you‚Äôve created a dashboard, you can add previously saved monitoring charts that display these metrics over time, making it easy to track changes and identify patterns.

![](https://files.readme.io/b862277-image.png)

To create a new monitoring chart for your dashboard, simply select ""New Monitoring Chart"" from the ""Add"" dropdown menu. For more information on creating and customizing monitoring charts, check out our Monitoring Charts UI Guide.

If you'd like to add an existing chart to your dashboard, select ""Saved Charts"" to display a full list of monitoring charts that are available in your project space. This makes it easy to quickly access and add the charts you need to your dashboard for monitoring and reporting purposes.

![](https://files.readme.io/2c3857c-image.png)

To further customize your dashboard, you can select the saved monitoring charts of interest by clicking on their respective cards. For instance, you might choose to add charts for Accuracy, Drift, Traffic, and Range Violation to your dashboard for a more comprehensive model overview. By adding these charts to your dashboard, you can quickly access important metrics and visualize your model's performance over time, enabling you to identify trends and patterns that might require further investigation.

## Dashboard Filters

There are three main filters that can be applied to all the charts within dashboards, these include date range, time zone, and bin size. 

![](https://files.readme.io/0795752-image.png)

### Date Range

When the `Default` time range is selected, the data range, time zone, and bin size that each monitoring chart was originally saved with will be applied. This enables you to create a dashboard where each chart shows a unique filter set to highlight what matters to each team. Updating the date range will unlock the time zone and bin size filters. You can select from a number of predefined ranges or choose `Custom` to select a start and end date-time.

![](https://files.readme.io/960262c-image.png)

### Bin Size

Bin size controls the frequency at which data is displayed on your monitoring charts"
"slug: ""dashboards-ui"" . You can select from the following bin sizes: `Hour`, `Day`, `Week`, or `Month`. 

> üìò Note: Hour bin sizes are not supported for time ranges above 90 days.
> 
> For example, if we select the `6M` data range, we see that the `Hourly` bin selection is disabled. This is disabled to avoid long computation and dashboard loading times.

![](https://files.readme.io/93f7576-image.png)

### Saved Model Updates

If you recently created or updated a saved chart and are not seeing the changes either on the dashboard itself or the Saved Charts list, click the refresh button on the main dashboard studio or within the saved charts list to reflect updates.

![](https://files.readme.io/706c198-image.png)

## Model Comparison

With our dashboard feature, you can also compare multiple models side-by-side, making it easy to see which models are performing the best and which may require additional attention. To create model-to-model comparison dashboards, ensure the models you wish to compare belong to the same project. Create the desired charts for each model and then add them to a single dashboard. By creating a single dashboard that tracks the health of all of your models, you can save time and simplify your AI monitoring efforts. With these dashboards, you can easily share insights with your team, management, or stakeholders, and ensure that everyone is up-to-date on your AI performance.

![](https://files.readme.io/33b97ae-image.png)

### Check [Dashboard Utilities ](doc:dashboard-utilities)and [Dashboard Interactions](doc:dashboard-interactions) pages for more info on dashboard usage.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Monitoring""
slug: ""monitoring-ui""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:24:28 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:45:28 GMT+0000 (Coordinated Universal Time)""
---
Fiddler Monitoring helps you identify issues with the performance of your ML models after deployment. Fiddler Monitoring has five Metric Types which can be monitored and alerted on:

1. **Data Drift**
2. **Performance**
3. **Data Integrity**
4. **Traffic**
5. **Statistic**

## Integrate with Fiddler Monitoring

Integrating Fiddler monitoring is a four-step process:

1. **Upload dataset**

   Fiddler needs a dataset to be used as a baseline for monitoring. A dataset can be uploaded to Fiddler using our UI and Python package. For more information, see:

   - [client.upload_dataset()](ref:clientupload_dataset) 

2. **Onboard model**

   Fiddler needs some specifications about your model in order to help you troubleshoot production issues. Fiddler supports a wide variety of model formats. For more information, see:

   - [client.add_model()](ref:clientadd_model)

3. **Configure monitoring for this model**

   You will need to configure bins and alerts for your model. These will be discussed in details below.

4. **Send traffic from your live deployed model to Fiddler**

   Use the Fiddler SDK to send us traffic from your live deployed model.

## Publish events to Fiddler

In order to send traffic to Fiddler, use the [`publish_event`](ref:clientpublish_event) API from the Fiddler SDK. Here is a sample of the API call:

```python Publish Event
import fiddler as fdl
	fiddler_api = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=token)
	# Publish an event
	fiddler_api.publish_event(
		project_id='bank_churn',
		model_id='bank_churn',
		event={
			""CreditScore"": 650,      # data type: int
			""Geography"": ""France"",   # data type: category
			""Gender"": ""Female"",
			""Age"": 45,
			""Tenure"": 2,
			""Balance"": 10000.0,      # data type: float
			""NumOfProducts"": 1,
			""HasCrCard"": ""Yes"",
			""isActiveMember"": ""Yes"",
			""EstimatedSalary"": 120000,
			""probability_churned"": 0.105,
      ""churn"": 1
		},
		event_id=‚Äôsome_unique_id‚Äô, #optional
		update_event=False, #optional
		event_timestamp=1511253040519 #optional
	)
```

The `publish_event` API can be called in real-time right after your model inference. 

> üìò Info
> 
> You can also publish events as part of a batch call after the fact using the `publish_events_batch` API (click [here](ref:clientpublish_events_batch) for more information). In this case, you will need to send Fiddler the original event timestamps as to accurately populate the time series charts.

Following is a description of all the parameters for `publish_event`:

- `project_id`: Project ID for the project this event belongs to.

- `model_id"
"slug: ""monitoring-ui"" `: Model ID for the model this event belongs to.

- `event`: The actual event as an array. The event can contain:

  - Inputs
  - Outputs
  - Target
  - Decisions (categorical only)
  - Metadata

- `event_id`: A user-generated unique event ID that Fiddler can use to join inputs/outputs to targets/decisions/metadata sent later as an update.

- `update_event`: A flag indicating if the event is a new event (insertion) or an update to an existing event. When updating an existing event, it's required that the user sends an `event_id`.

- `event_timestamp`: The timestamp at which the event (or update) occurred, represented as a UTC timestamp in milliseconds. When updating an existing event, use the time of the update, i.e., the time the target/decision were generated and not when the model predictions were made.

## Updating events

Fiddler supports partial updates of events for your **target** column. This can be useful when you don‚Äôt have access to the ground truth for your model at the time the model's prediction is made. Other columns can only be sent at insertion time (with `update_event=False`).

Set `update_event=True` to indicate that you are updating an existing event. You only need to provide the decision, metadata, and/or target fields that you want to change‚Äîany fields you leave out will remain as they were before the update.

**Example**

Here‚Äôs an example of using the publish event API to update an existing event:

```python Update Existing Event
import fiddler as fdl

fiddler_api = fdl.FiddlerApi(
	url=url,
	org_id=org_id,
	auth_token=token
)

fiddler_api.publish_event(
	project_id='bank_churn',
	model_id='bank_churn',
	event = {
		'churn': 0,    # data type: category
	},
	event_id=‚Äôsome_unique_id‚Äô,
	update_event=True
)
```

The above `publish_event` call will tell Fiddler to update the target (`'churn': 0`) of an existing event  (`event_id='some_unique_id'`).

Once you‚Äôve used the SDK to send Fiddler your live event data, that data will show up under the **Monitor** tab in the Fiddler UI:

![](https://files.readme.io/978d0c7-Monitor_dashboard.png ""Monitor_dashboard.png"")

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Evaluation""
slug: ""evaluation-ui""
excerpt: ""UI Guide""
hidden: false
createdAt: ""Tue Apr 19 2022 20:24:53 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:50:04 GMT+0000 (Coordinated Universal Time)""
---
Model performance evaluation is one of the key tasks in the ML model lifecycle. A model's performance indicates how successful the model is at making useful predictions on data.

Once your trained model is loaded into Fiddler, click on **Evaluate** to see its performance.

![](https://files.readme.io/2eac9b7-Model_Eval.png ""Model_Eval.png"")

## Regression Models

To measure model performance for regression tasks, we provide some useful performance metrics and tools.

![](https://files.readme.io/e7e7a01-Model_Regression.png ""Model_Regression.png"")

- **_Root Mean Square Error (RMSE)_**
  - Measures the variation between the predicted and the actual value.
  - RMSE = SQRT[Sum of all observation (predicted value - actual value)^2/number of observations]
- **_Mean Absolute Error (MAE)_**
  - Measures the average magnitude of the error in a set of predictions, without considering their direction.
  - MAE = Sum of all observation[Abs(predicted value - actual value)]/number of observations
- **_Coefficient of Determination (R<sup>2</sup>)_**
  - Measures how much better the model's predictions are than just predicting a single value for all examples.
  - R<sup>2</sup> = variance explained by the model / total variance
- **_Prediction Scatterplot_**
  - Plots the predicted values against the actual values. The more closely the plot hugs the y=x line, the better the fit of the model.
- **_Error Distribution_**
  - A histogram showing the distribution of errors (differences between model predictions and actuals). The closer to 0 the errors are, the better the fit of the model.

## Classification Models

To measure model performance for classification tasks, we provide some useful performance metrics and tools.

![](https://files.readme.io/b60acfb-Model_Classification.png ""Model_Classification.png"")

- **_Precision_**
  - Measures the proportion of positive predictions which were correctly classified.
- **_Recall_**
  - Measures the proportion of positive examples which were correctly classified.
- **_Accuracy_**
  - Measures the proportion of all examples which were correctly classified.
- **_F1-Score_**
  - Measures the harmonic mean of precision and recall. In the multi-class classification case, Fiddler computes micro F1-Score.
- **_AUC_**
  - Measures the area under the Receiver Operating Characteristic (ROC) curve.
- **_Log Loss_**
  - Measures the performance of a classification model where the prediction input is a probability value between 0 and 1. The goal of the ML model is to minimize this value.
- **_Confusion Matrix_**
  - A table that shows how many predicted and actual values exist for different classes. Also referred as an error matrix.
- **_Receiver Operating Characteristic (ROC) Curve_**
  - A graph showing the performance of a classification model at different classification thresholds. Plots the true positive rate (TPR), also known as recall, against the false positive rate (FPR).
- **_Precision-Recall Curve_**
  - A graph that"
"slug: ""evaluation-ui""  plots the precision against the recall for different classification thresholds.
- **_Calibration Plot_**
  - A graph that tell us how well the model is calibrated. The plot is obtained by dividing the predictions into 10 quantile buckets (0-10th percentile, 10-20th percentile, etc.). The average predicted probability is plotted against the true observed probability for that set of points.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Administration""
slug: ""administration-ui""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:26:16 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
This section provides details on how to use the UI for:

- Fiddler settings
- Inviting Users
- Project architecture and organization
- Access/Authorization details.

For platform-specific information check the [Platform Guide on Administration](doc:administration-platform).
"
"---
title: ""Fairness""
slug: ""fairness-ui""
excerpt: ""UI Guide""
hidden: false
createdAt: ""Tue Dec 20 2022 17:16:23 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:50:12 GMT+0000 (Coordinated Universal Time)""
---
In the context of [intersectional fairness](doc:fairness#intersectional-fairness), we compute the [fairness metrics](doc:fairness#fairness-metrics) for each subgroup. The values should be similar among subgroups. If there exists some bias in the model, we display the min-max ratio, which takes the minimum value divided by the maximum value for a given metric. If this ratio is close to 1, then the metric is very similar among subgroups. The figure below gives an example of two protected attributes, Gender and Education, and the Equal Opportunity metric.

![](https://files.readme.io/906df04-intersectional_metrics.svg ""intersectional_metrics.svg"")

For the[ Disparate Impact metric](doc:fairness#disparate-impact), we don‚Äôt display a min-max ratio but an absolute min. The intersectional version of this metric is a little different. For a given subgroup, take all possible permutations of 2 subgroups and then display the minimum. If the absolute minimum is greater than 80%, then all combinations are greater than 80%.

## Model Behavior

In addition to the fairness metrics, we provide information about model outcomes and model performance for each subgroup. In the platform, you can see a visualization like the one below by default. You have the option to display the same numbers in a table for a deeper analysis.

![](https://files.readme.io/e03e620-model_behavior_1.svg ""model_behavior_1.svg"")

![](https://files.readme.io/ca0c5be-model_behavior_2.svg ""model_behavior_2.svg"")

## Dataset Fairness

Finally, we provide a section for dataset fairness, with a mutual information matrix and a label distribution. Note that this is a pre-modeling step.

![](https://files.readme.io/c96f7fd-data_fairness.svg ""data_fairness.svg"")

Mutual information gives information about existing dependence in your dataset between the protected attributes and the remaining features. We are displaying Normalized Mutual Information (NMI). This metric is symmetric, and has values between 0 and 1, where 1 means perfect dependency.

![](https://files.readme.io/a946365-mutual_info.svg ""mutual_info.svg"")

For more details about the implementation of the intersectional framework, please refer to this [research paper](https://arxiv.org/pdf/2101.01673.pdf).

## Reference

[^1]\: USEEOC article on [_Discrimination By Type_](https://www.eeoc.gov/discrimination-type)  
[^2]\:  USEEOC article on [_Intersectional Discrimination/Harassment_](https://www.eeoc.gov/initiatives/e-race/significant-eeoc-racecolor-casescovering-private-and-federal-sectors#intersectional)

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Explainability""
slug: ""explainability""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:24:31 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:48:17 GMT+0000 (Coordinated Universal Time)""
---
There are three topics related to Explainability to cover:

- [Point Explainability](doc:point-explainability) 
- [Global Explainability](doc:global-explainability) 
- [Surrogate Models](doc:surrogate-models)
"
"---
title: ""Analytics""
slug: ""analytics-ui""
excerpt: ""UI Guide""
hidden: false
createdAt: ""Tue Apr 19 2022 20:24:49 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:49:26 GMT+0000 (Coordinated Universal Time)""
---
## Interface

The **Analyze** tab has three parts:

1. **_Slice Query box_** _(top-left)_ ‚Äî Accepts a SQL query as input, allowing quick access to the slice.
2. **_Data table_** _(bottom-left)_ ‚Äî Lets you browse instances of data returned by the query.
3. **_Charts column_** _(right)_ ‚Äî Allows you to view explanations for the slice and choose from a range of rich visualizations for different data insights.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/92cacf9-Screen_Shot_2023-10-04_at_4.11.10_PM.png"",
        ""S_E_Landing.png"",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


**Workflow**

1. Write a SQL query in the **Slice Query** box and click **Run**.

![](https://files.readme.io/a76a852-S_E_Step_2.png ""S_E_Step_2.png"")

2. View the data returned by the query in the **Data** table.

![](https://files.readme.io/8771686-S_E_Step_3.png ""S_E_Step_3.png"")

3. Explore a variety of visualizations using the **Explanations** column on the right.

![](https://files.readme.io/3d16c4e-S_E_Step_4.png ""S_E_Step_4.png"")

## SQL Queries

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/23c4424-Screen_Shot_2023-10-04_at_4.11.50_PM.png"",
        ""S_E_First_Time.png"",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


The **Slice Query** box lets you:

1. Write a SQL query
2. Search and auto-complete field names (i.e. your dataset, the names of your inputs or outputs)
3. Run the SQL query

In the UI, you will see examples for different types of queries:

- Example query to analyze your dataset:

```
select * from ""your_dataset_id.your_model_id"" limit 100
```

- Example query to analyze production traffic:

```
select * FROM production.""your_model_id""
where fiddler_timestamp between '2020-10-20 00:00:00' AND '2020-10-20 12:00:00'limit 100
```

> üöß Note
> 
> Only read-only SQL operations are supported. Slices are auto-detected based on your model, dataset, and query. Certain SQL operations like aggregations and joins might not result in a valid slice.

## Data

If the query successfully returns a slice, the results display in the **Data** table below the **Slice Query** box.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/9236f3d-Screen_Shot_2023-10-04_at_4.11."
"slug: ""analytics-ui"" 10_PM.png"",
        ""S_E_Data.png"",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


You can view all data rows and their values or download the data as a CSV file to plug it into another system. By clicking on **Explain** (light bulb icon) in any row in the table, you can access explanations for that individual input (more on this in the next section).

## Explanations

The Analyze tab offers a variety of powerful visualizations to quickly let you analyze and explain slices of your dataset.

1. [**Feature Correlation**](#feature-correlation) ‚Äî View the correlation between model inputs and/or outputs.
2. [**Feature Distribution**](#feature-distribution) ‚Äî Visualize the distribution of an input or output.
3. [**Feature Impact**](#feature-impact) ‚Äî Understand the aggregate impact of model inputs to the output.
4. [**Partial Dependence Plot**](#partial-dependence-plot-pdp) ‚Äî Understand the aggregate impact of a single model input in its output.
5. [**Slice Evaluation**](#slice-evaluation) ‚Äî View the model metrics for a given slice.
6. [**Dataset Details**](#dataset-details) ‚Äî Analyze statistical qualities of the dataset.

You can also access the following _point explanation methods_ by clicking on **Explain** (light bulb icon) for a given data point:

1. [**Point Overview**](#point-overview) ‚Äî Get an overview of the model inputs responsible for a prediction.
2. [**Feature Attribution**](#feature-attribution) ‚Äî Understand how responsible each model input is for the model output.
3. [**Feature Sensitivity**](#feature-sensitivity) ‚Äì Understand how changes in the model‚Äôs input values will impact the model‚Äôs output.

> üìò Info
> 
> For more information on point explanations, click [here](doc:point-explainability).

## Feature Correlation

The feature correlation visualization plots a single variable against another variable. This plot helps identify any visual clusters that might be useful for further analysis. This visualization supports integer, float, and categorical variables.

![](https://files.readme.io/e36a237-S_E_Correlation.png ""S_E_Correlation.png"")

## Feature Distribution

The feature distribution visualization is one of the most basic plots, used for viewing how the data is distributed for a particular variable. This plot helps surface any data abnormalities or data insights to help root-cause issues or drive further analysis.

![](https://files.readme.io/26e2658-S_E_Distribution.png ""S_E_Distribution.png"")

## Feature Impact

This visualization provides the feature impact of the dataset (global explanation) or the selected slice (local explanation), showcasing the overall sensitivity of the model output to each feature (more on this in the [Global Explainability](doc:global-explainability) section). We calculate Feature Impact by randomly intervening on every input using ablations and noting the average absolute change in the prediction.

A high impact suggests that the model‚Äôs behavior on a particular slice is sensitive to changes in feature values. Feature impact only provides the absolute impact of the input‚Äînot its directionality. Since positive and negative directionality can cancel out, we recommend using a Partial Dependence Plot (PDP) to understand how an input impacts the output in aggregate.

![](https://files.readme.io/09cb939-S_E_FeatureImpact.png ""S_E_FeatureImpact.png"")

## Partial Dependence Plot (PDP)

Partial dependence plots show the marginal effect of a selected model input on the model output"
"slug: ""analytics-ui"" . This plot helps understand whether the relationship between the input and the output is linear, monotonic, or more complex.

![](https://files.readme.io/e1c0f84-PDP.png ""PDP.png"")

## Slice Evaluation

The slice evaluation visualization gives you key model performance metrics and plots, which can be helpful to identify performance issues or model bias on protected classes. In addition to key metrics, you get a confusion matrix along with precision recall, ROC, and calibration plots. This visualization supports classification, regression, and multi-class models.

![](https://files.readme.io/96aa3d0-Slice_Evaluation.png ""Slice_Evaluation.png"")

## Dataset Details

This visualization provides statistical details of your dataset to help you understand the data‚Äôs distribution and correlations.

Select a target variable to see the dependence between that variable and the others, measured by [mutual information (MI)](https://en.wikipedia.org/wiki/Mutual_information). A low MI is an indicator of low correlation between two variables, and can be used to decide if particular variables should be dropped from the model.

![](https://files.readme.io/69f1a0a-Dataset_details_1.png ""Dataset_details_1.png"")

![](https://files.readme.io/d3a97a8-Dataset_details_2.png ""Dataset_details_2.png"")

![](https://files.readme.io/cd0b499-Dataset_details_3.png ""Dataset_details_3.png"")

## Point Overview

> üìò Info
> 
> To view this visualization, click on **Explain** (light bulb icon) for any row in the **Data** table.

This visualization provides a human-readable overview of a point explanation.

![](https://files.readme.io/335714a-Explain_Overview.png ""Explain_Overview.png"")

## Feature Attribution

> üìò Info
> 
> To view this visualization, click on **Explain** (light bulb icon) for any row in the **Data** table.

Feature attributions can help you understand which model inputs were responsible for arriving at the model output for a particulat prediction.

When you want to check how the model is behaving for one prediction instance, use this visualization first.

More information is available on the [Point Explainability](doc:point-explainability) page.

![](https://files.readme.io/08d409f-Explain_Chart.png ""Explain_Chart.png"")

## Feature Sensitivity

> üìò Info
> 
> To view this visualization, click on **Explain** (light bulb icon) for any row in the **Data** table.

This visualization helps you understand how changes in the model‚Äôs input values could impact the model‚Äôs prediction for this instance.

**_ICE plots_**

On initial load, the visualization shows an Individual Conditional Expectation (ICE) plot for each model input.

![](https://files.readme.io/ac5f0b2-WhatIF_Chart.png ""WhatIF_Chart.png"")

ICE plots shows how the model prediction is affected by changes in an input for a particular instance. They‚Äôre computed by changing the value of an input‚Äîwhile keeping all other inputs constant‚Äîand plotting the resulting predictions.

Recall the [partial dependence plots](#partial-dependence-plot-pdp) discussed earlier, which showed the average effect of the feature across the entire slice. In essence, the PDP is the average of all the ICE plots. The PDP can mask interactions at the instance level, which an ICE plot will capture.

You can update any input value to see its impact on the model output, and then"
"slug: ""analytics-ui""  view the updated ICE plots for the changed input values.

This is a powerful technique for performing counterfactual analysis of a model prediction. When you plot the updated ICE plots, you‚Äôll see two lines (or sets of bars in the case of categorical inputs).

In the image below, the solid line is the original ICE plot, and the dotted line is the ICE plot using the updated input values. Comparing these two sets of plots can help you understand if the model‚Äôs behavior changes as expected with a hypothetical model input.

![](https://files.readme.io/9311aea-WhatIF_After.png ""WhatIF_After.png"")

## Dashboard

Once visualizations are created, you can pin them to the project dashboard, which can be shared with others.

To pin a chart, click on the thumbtack icon and click **Send**. If the **Update with Query** option is enabled, the pinned chart will update automatically whenever the underlying query is changed on the **Analyze** tab.

![](https://files.readme.io/c4247d1-Pinning_Chart.png ""Pinning_Chart.png"")

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Alerts with Fiddler UI""
slug: ""alerts-ui""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:25:34 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:47:40 GMT+0000 (Coordinated Universal Time)""
---
Fiddler allows you to set up [alerts](doc:alerts-platform) for your model. View your alerts by clicking on the Alerts tab in the navigation bar. The Alerts tab presents three views: Triggered Alerts, Alert Rules, and Integrations. Users can set up alerts using both the Fiddler UI and the Fiddler API Client. This page introduces the available alert types, and how to set up and view alerts in the Fiddler UI. For instructions about how to use the Fiddler API client for alert configuration see [Alert Configuration with Fiddler Client](doc:alerts-client).

![](https://files.readme.io/1730387-image.png)

## Setting up Alert Rules

To create a new alert using the Fiddler UI, click the **Add Alert** button on the top-right corner of any screen on the Alerts tab. 

![](https://files.readme.io/78537d3-image.png)

In the Alert Rule form, provide the basic information such as the desired alert name, and the project and model of interest. 

![](https://files.readme.io/8418e4f-image.png)

Next, select the Alert Type you would like to monitor. Users can select from Performance, Data Drift, Data Integrity, or Traffic monitors. For this example, we'll set up a Data Drift alert to measure distribution drift.

![](https://files.readme.io/d51ca30-image.png)

Once an Alert Type is selected, users can choose a metric corresponding to the Alert Type for which to set the alert on. For our Data Drift alert, we will use JSD (Jensen‚ÄìShannon distance) as our metric. The next consideration are the bin size, which is the duration for which fiddler monitoring calculates the metric values, and the column to apply this monitor on. Users can select up to 20 columns from the following column categories; Inputs, Outputs, Targets, Metadata, Decisions, and Custom Features. Let's choose a 1 hour bin and the CreditScore column for this example. 

![](https://files.readme.io/033e061-image.png)

Next, users can focus on the alerts comparison method. Learn more about Alert comparisons on the [Alerts Platform Guide](doc:alerts-platform). For our example, we will select the Relative comparison option, and compare to the same time 7 days back. Users can select the alert condition as well as a Warning and Critical threshold. We will ask for an alert when the production data is greater than 10%.

![](https://files.readme.io/cb3f4b0-image.png)

Finally user can set the alert rules priority- how important this alert is to a customers work streams, along with how to get notified of triggered alerts. 

![](https://files.readme.io/0e75a9e-image.png)

 Last, click **Add Alert Rule** when you're done. In order to create and configure alerts using the Fiddler API client see [Alert Configuration with Fiddler Client](doc:fiddler-ui).

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/7a1b03e-Screenshot_2023"
"slug: ""alerts-ui"" -10-10_at_12.00.27_PM.png"",
        null,
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


### Alert Notification options

You can select the following types of notifications for your alert.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/ee80b90-Screenshot_2023-10-09_at_5.18.21_PM.png"",
        """",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


### Alert Rules Tab

Once an alert rule is created it can be viewed in the **Alert Rules** tab. This view enables you to view all alert rules across any project and model at a glance.

![](https://files.readme.io/ec2fde7-image.png)

A few high-level details from the alert rule definition are displayed in the table, but users can select to view the full alert definition by selecting the overflow button (‚ãÆ) on the right-hand side of any Alert Rule record and clicking `View All Details`. 

![](https://files.readme.io/0e1dbdc-image.png)

Delete an existing alert by clicking on the overflow button (‚ãÆ) on the right-hand side of any Alert Rule record and clicking `Delete`. To make any other changes to an Alert Rule, you will need to delete the alert and create a new one with the desired specifications. 

![](https://files.readme.io/eddf05e-image.png)

### Multi-column alerts

If you have [configured multi-column alerts](ref:clientadd_alert_rule#examples) using the Fiddler client, then the columns on which the alert is set will be visible in the ""Alert rules"" tab

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/d86c97a-Screenshot_2023-10-09_at_5.31.52_PM.png"",
        """",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


## 

## Visualizations

Throughout the Alert Rules, Triggered Alerts, and Home pages users will see references to the monitors they set up. These visualizations include Alert Rule priority, threshold severities, and more.

### Alert Rule Priority

Alert rule priority allows users to specify how important an alert rule is to their workflows, learn more on the [Alerts Platform Guide](doc:alerts-platform).

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/4f87100-image.png"",
        null,
        """"
      ],
      ""align"": ""center"",
      ""sizing"": ""300px""
    }
  ]
}
[/block]


### Threshold Severity

Users can specify Warning and Critical thresholds as additional customization on their monitors, learn more on the [Alerts Platform Guide](doc:alerts-platform).

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/664e72e-image.png"",
        null,
        """"
      ],
      ""align"": ""center"",
      ""sizing"": ""300px""
    }
  ]
}
[/block]


### Alert Summary

On the Fiddler home page, users can get a summary glance of their triggered alerts, categorized by Alert Type. This view allows users to easily navigate to their degraded models.

![](https://files.readme.io/3f76938-image.png)

## View Triggered Alerts on Fiddler

The"
"slug: ""alerts-ui""  Triggered Alerts view gives a single pane of glass experience where you can view all triggered alerts across any Project and Model. Easily apply time filters to see alerts that fired in a desired range, or customize the table to only show columns that matter the most to you. This view aggregates all triggered alerts by alert rule, where the number of times a given alert rule has been triggered is called out by the `Count` column. Explore the triggered alerts further by clicking on the `Monitor` button to further diagnose your model and data.

![](https://files.readme.io/30a5ab5-Screen_Shot_2022-10-03_at_3.39.32_PM.png)

## Sample Alert Email

Here's a sample of an email that's sent if an alert is triggered:

![](https://files.readme.io/9dfc566-Monitor_Alert_Email_0710.png ""Monitor_Alert_Email_0710.png"")

## Integrations

The Integrations tab is a read-only view of all the integrations your Admin has enabled for use. As of today, users can configure their Alert Rules to notify them via email or Pager Duty services.

![](https://files.readme.io/7462149-image.png)

Admins can add new integrations by clicking on the setting cog icon in the main navigation bar and selecting the integration tab of interest.

![](https://files.readme.io/6ee3027-Screen_Shot_2022-10-03_at_4.16.00_PM.png)

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Data Drift""
slug: ""data-drift""
excerpt: ""UI Guide""
hidden: false
createdAt: ""Tue Apr 19 2022 20:25:14 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:47:07 GMT+0000 (Coordinated Universal Time)""
---
Model performance can be poor if models trained on a specific dataset encounter different data in production. This is called data drift. In the **Monitor** tab for your model, Fiddler gives you a visual way to explore data drift and identify what data is drifting, when it‚Äôs drifting, and how it‚Äôs drifting. This is the first step in identifying possible model performance issues.

![](https://files.readme.io/0d04342-Monitoring-DataDrift.png ""Monitoring-DataDrift.png"")

You can change the time range using the controls in the upper-right:

![](https://files.readme.io/d5809f8-Monitoring-TimeRange.png ""Monitoring-TimeRange.png"")

## What is being tracked?

- **_Drift Metrics_**
  - **Jensen‚ÄìShannon distance (JSD)**
    - A distance metric calculated between the distribution of a field in the baseline dataset and that same distribution for the time period of interest.
    - For more information on JSD, click [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html).
  - **Population Stability Index (PSI)**
    - A drift metric based on the multinomial classification of a variable into bins or categories. The differences in each bin between the baseline and the time period of interest are then utilized to calculate it as follows:

![](https://files.readme.io/0baeb90-psi_calculation.png ""psi_calculation.png"")

Here, `B` is the total number of bins, `ActualProp(b)` is the proportion of counts within bin `b` from the target distribution, and `ExpectedProp(b)` is the proportion of counts within bin `b` from the reference distribution. Thus, PSI is a number that ranges from zero to infinity and has a value of zero when the two distributions exactly match.

> üöß Note
> 
> Since there is a possibility that a particular bin may be empty, PSI can shoot to infinity. To avoid this, PSI calculation in Fiddler is done such that each bin count is incremented with a base_count=1. Thus, there might be a slight difference in the PSI values obtained from manual calculations.

- **_Average Values_** ‚Äì The mean of a field (feature or prediction) over time. This can be thought of as an intuitive drift score.
- **_Drift Analytics_** ‚Äì You can drill down into the features responsible for the prediction drift using the table at the bottom.
  - **_Feature Impact_**: The contribution of a feature to the model‚Äôs predictions, averaged over the baseline dataset. The contribution is calculated using random ablation feature impact.
  - **_Feature Drift_**: Drift of the feature, calculated using the drift metric of choice.
  - **_Prediction Drift Impact_**: A heuristic calculated using the product of the feature impact and the feature drift. The higher the score, the more this feature is likely to have contributed to the prediction drift.

In the Drift Analytics table, you can select a feature to see the feature distribution for both the time period under consideration and the baseline dataset. If it‚Äôs a numerical feature, you will also see a time series of the average feature value over"
"slug: ""data-drift""  time.

![](https://files.readme.io/63a452e-Monitor_DriftAnaly.png ""Monitor_DriftAnaly.png"")

## Why is it being tracked?

- Data drift is a great proxy metric for **performance decline**, especially if there is delay in getting labels for production events. (e.g. In a credit lending use case, an actual default may happen after months or years.)
- Monitoring data drift also helps you stay informed about **distributional shifts in the data for features of interest**, which could have business implications even if there is no decline in model performance.

## What do I do next with this information?

- High drift can occur as a result of _data integrity issues_ (bugs in the data pipeline), or as a result of _an actual change in the distribution of data_ due to external factors (e.g. a dip in income due to COVID). The former is more in our control to solve directly. The latter may not be solvable directly, but can serve as an indicator that further investigation (and possible retraining) may be needed.
- You can drill down deeper into the data by examining it in the Analyze tab. 

The image below shows how to open the Analyze view for a specific feature and time range identified in the Data Drift page.

![](https://files.readme.io/8a699e1-Monitor_DDrift_Analyze.png ""Monitor_DDrift_Analyze.png"")

This will bring you to the Analyze tab, where you can then use SQL to slice and dice the data.  You can then apply visualizations upon these slices to analyze the model‚Äôs behavior.

![](https://files.readme.io/25eca03-Monitor_Analyze.png ""Monitor_Analyze.png"")

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Monitoring Charts UI""
slug: ""monitoring-charts-ui""
excerpt: """"
hidden: false
createdAt: ""Thu Feb 23 2023 23:06:54 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:47:48 GMT+0000 (Coordinated Universal Time)""
---
## Getting Started:

To use Fiddler AI‚Äôs monitoring charts, navigate to the Charts tab in the top-level navigation bar on the Fiddler AI platform. Choose between opening a previously saved chart or creating a new chart.

## Create a New Monitoring Chart

To create a new monitoring chart, click on the Add Chart button on the top right of the screen.

![](https://files.readme.io/2c98736-image.png)

Search for and select the [project](doc:project-structure) to create the chart, and press Add Chart.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/5e5cc8e-image.png"",
        null,
        """"
      ],
      ""align"": ""center"",
      ""sizing"": ""400px""
    }
  ]
}
[/block]


## Chart Functions

![](https://files.readme.io/c5b2029-image.png)

### Chart Properties

Before the first save, you can change the project space to ensure you‚Äôre focusing on the right models and data. After confirming the project selection, you can choose to name and add a description to your chart.

### Save & Share

Manually save your chart using the Save button on the top right corner of the chart studio. Copy a link to your chart and share it with other [fiddler accounts who have access](doc:inviting-users) to the project where the chart resides.

### Global Undo & Redo

Easily control the following actions with the undo and redo buttons:

- Metric query selection
- Time range selections
- Time range selections
- Bin size selections

To learn how to undo actions taken using the chart toolbar, see the Toolbar information in the next section.

## Chart Metric Queries & Filters

### Metric Query

A metric query enables you to define what model to focus on, and which metrics and columns to plot on your monitoring chart. To get started with the metric query, choose a model of choice. Note: only models within the same project as your chart are accessible.

Once a model is selected, choose a metric type from Performance, Data Drift, Data Integrity, or Traffic metrics and relevant metrics. For example, we may choose to chart accuracy for our binary classification model. 

![](https://files.readme.io/a46e656-image.png)

### Charting Multiple Columns

If you choose to chart data drift or data integrity, you can choose to plot up to 20 different columns from the following column categories; inputs, outputs, targets, decisions, metadata, and custom features.

![](https://files.readme.io/7d91cc8-image.png)

### Charting Multiple Metrics or Models

Add up to 6 metric queries that allow you to chart different metrics and/or models in a single chart view.

![](https://files.readme.io/4213040-image.png)

### Chart Filters & Capabilities

There are three major chart filter capabilities, chart filters, chart toolbar, and zoom slider.  
They work together to enable you to best analyze the slices of data that may be worth investigating. 

![](https://files.readme.io/f58936d-image.png)

### Filters

You can customize your chart view using time range, time zone"
"slug: ""monitoring-charts-ui"" , and bin size chart filters. The data range can be one of the pre-defined time ranges or a custom range. The bin size selected controls the frequency for which the data is displayed. So selecting Day will show daily data over the date range selected.

### Toolbar

The charts toolbar is made up of 5 functions:

- Drag to zoom
- Reset zoom
- Toggle to a line chart
- Toggle to a bar chart
- Undo all toolbar actions

> üìò Note: If the zoom reset or toolbar undo is selected, this will also undo any actions taken with the zoom slider.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/0a9224c-image.png"",
        null,
        """"
      ],
      ""align"": ""center"",
      ""sizing"": ""400px""
    }
  ]
}
[/block]


#### Zoom

To utilize the drag-to-zoom functionality, click on the associated icon, it should turn blue on selection. Once selected, move your mouse over the chart area and drag it to zoom into the data points of interest. If you want to return to the original view, you can leverage the Reset Zoom button, which is the icon directly to the right of the drag-to-zoom functionality.

![](https://files.readme.io/ed8ef2b-image.png)

#### Line & Bar Chart Toggle

You can switch between visualizing your chart as a line or bar chart using the toolbar icons. Click on the line chart icon on the top right of the chart to switch to the line chart view. Likewise, select the bar chart icon in the toolbar to switch to the bar chart view. However, note that these views are only temporary and any settings you specify using the toolbar will not be saved to the chart.

![](https://files.readme.io/c8c0e79-image.png)

#### Zoom Slider

You can also use the horizontal zoom bar to zoom, located at the base of the chart. Once you've identified the time range you want to focus on, you can use the zoom bar to drag the range across time. For instance, if you want to analyze your data week-by-week over the past six months, you can use the toolbar or horizontal zoom bar to zoom in on the desired time range and then click and drag the selected range using the base horizontal zoom bar.

![](https://files.readme.io/c73c24c-image.png)

### Breakdown Summary

You can easily visualize your charts' raw data as a table within the fiddler chart studio, or download the content as a CSV for further analysis. If you choose to chart multiple columns, as shown below, you can search for and sort by Model name, Metric name, Column name, or values for a specific date.

![](https://files.readme.io/0ddc155-image.png)

## Customize Tab

### Scale & Range

 The Customize tab enables users to adjust the scale and range of the y-axis on their monitoring charts. In the example below, we have adjusted the minimum value of the y-axis for the plotted traffic to make more use of the chart space. For values with large variance, logarithmic scale can be applied to more clearly analyze the chart.

![](https://files.readme.io/4926dff-image.png)

### Y-axis Assignment

Select the y-axis for your metric queries with enhanced flexibility to customize the scale and range for each axis.

![](https://files.readme.io/96f49e0-image.png)

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk"
"slug: ""monitoring-charts-ui""  to a product expert
"
"---
title: ""Performance""
slug: ""performance""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:25:22 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:47:14 GMT+0000 (Coordinated Universal Time)""
---
## What is being tracked?

![](https://files.readme.io/4a646d4-qs_monitoring.png ""qs_monitoring.png"")

- **_Decisions_** - The post-prediction business decisions made as a result of the model output. Decisions are calculated before [client.publish_event()](ref:clientpublish_event) (they're not inferred by Fiddler). For binary classification models, a decision is usually determined using a threshold. For multi-class classification models, it's usually determined using the argmax value of the model outputs.

- **_Performance metrics_**
  1. For binary classification models:
     - Accuracy
     - True Positive Rate/Recall
     - False Positive Rate
     - Precision
     - F1 Score
     - AUC
     - AUROC
     - Binary Cross Entropy
     - Geometric Mean
     - Calibrated Threshold
     - Data Count
     - Expected Calibration Error
  2. For multi-class classification models:
     - Accuracy
     - Log loss
  3. For regression models:
     - Coefficient of determination (R-squared)
     - Mean Squared Error (MSE)
     - Mean Absolute Error (MAE)
     - Mean Absolute Percentage Error (MAPE)
     - Weighted Mean Absolute Percentage Error (WMAPE)
  4. For ranking models:
     - Mean Average Precision (MAP)‚Äîfor binary relevance ranking only
     - Normalized Discounted Cumulative Gain (NDCG)

## Why is it being tracked?

- Model performance tells us how well a model is doing on its task. A poorly performing model can have significant business implications.
- The volume of decisions made on the basis of the predictions give visibility into the business impact of the model.

## What steps should I take based on this information?

- For decisions, if there is an increase or decrease in approvals, we can cross-check with the average prediction and prediction drift trendlines on the [Data Drift Tab](doc:data-drift). In general, the average prediction value should increase with an increase in the number of approvals, and vice-versa.
- For changes in model performance‚Äîagain, the best way to cross-verify the results is by checking the [Data Drift Tab](doc:data-drift) ). Once you confirm that the performance issue is not due to the data, you need to assess if the change in performance is due to temporary factors, or due to longer-lasting issues.
- You can check if there are any lightweight changes you can make to help recover performance‚Äîfor example, you could try modifying the decision threshold.
- Retraining the model with the latest data and redeploying it is usually the solution that yields the best results, although it may be time-consuming and expensive.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Data Integrity""
slug: ""data-integrity""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:25:27 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:47:23 GMT+0000 (Coordinated Universal Time)""
---
ML models are increasingly driven by complex feature pipelines and automated workflows that involve dynamic data. Data is transformed from source to model input, which can result in data inconsistencies and errors.

There are three types of violations that can occur at model inference: **missing feature values**, **type mismatches** (e.g. sending a float input for a categorical feature type) or **range mismatches** (e.g. sending an unknown US State for a State categorical feature).

You can track all these violations in the Data Integrity tab. 

## What is being tracked?

![](https://files.readme.io/8a59eb0-Monitoring_DataIntegrity.png ""Monitoring_DataIntegrity.png"")

The time series above tracks the violations of data integrity constraints set up for this model.

- **_Missing value violations_** ‚Äî The percentage of missing value violations over all features for a given period of time.
- **_Type violations_** ‚Äî The percentage of data type mismatch violations over all features for a given period of time.
- **_Range violations_** ‚Äî The percentage of range mismatch violations over all features for a given period of time.
- **_All violating events_** ‚Äî An aggregation of all the data integrity violations above for a given period of time.

## Why is it being tracked?

- Data integrity issues can cause incorrect data to flow into the model, which can lead to poor model performance and have a negative impact on the business or end-user experience. 

## How does it work?

It can be tedious to set up constraints for individual features when they number in the tens or hundreds. To avoid this, you can provide Fiddler with a baseline dataset that's representative of the data you expect your model to infer on in production. This should be sampled from your model's training set, and can be [uploaded to Fiddler using the Python API client](ref:clientupload_dataset).

Fiddler will automatically generate constraints based on the distribution of data in this dataset.

- **Missing values**: If a feature has no missing values, then the data integrity violation will be set up to trigger when any missing values are seen. Similarly, if the feature has 50% of its values missing, then the data integrity violation will be set up to trigger when more than 50% of the values encountered are missing in a specified time range.
- **Type mismatch**: A data integrity violation will be triggered when the type of a feature value differs from what was specified for that feature in the baseline dataset.
- **Range mismatch**: For categorical features, a data integrity violation will be triggered when it sees any value other than the ones specified in the baseline. Similarly, for continuous variables, the violation will be triggered if the values are outside the range specified in the baseline.

## What steps should I take with this information?

- The visualization above informs us of the feature-wise breakdown of the violations. The raw counts of the violations are shown in parentheses.
- If there is a spike in violations, or an unexpected violation occurs (such as missing values for a feature that doesn‚Äôt accept a missing value), then a deeper examination of the feature pipeline may be required.
- You can also drill down deeper into the data by examining it in the **Analyze** tab. We can use SQL to slice and dice the data, and try to find"
"slug: ""data-integrity""  the root cause of the issues.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Traffic""
slug: ""traffic-ui""
excerpt: ""UI Guide""
hidden: false
createdAt: ""Tue Apr 19 2022 20:25:31 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:47:31 GMT+0000 (Coordinated Universal Time)""
---
Traffic as a service metric gives you basic insights into the operational health of your model's service in production.

![](https://files.readme.io/d2c1eaa-Screenshot_2023-02-01_at_5.13.34_PM.png)

## What is being tracked?

- **_Traffic_** ‚Äî The volume of traffic received by the model over time.

## Why is it being tracked?

- Traffic is a basic high-level metric that informs us of the overall model's usage.

## What steps should I take when I see an outlier?

- A dip or spike in traffic needs to be investigated. For example, a dip could be due to a production model server going down; a spike could be an adversarial attack.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Embedding Visualization Chart Creation""
slug: ""embedding-visualization-chart-creation""
excerpt: """"
hidden: false
createdAt: ""Thu Nov 16 2023 18:09:21 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:47:56 GMT+0000 (Coordinated Universal Time)""
---
## Creating an embedding visualization Chart

To create an embedding Visualization chart, follow these steps:

1. Navigate to the **Charts** tab in your Fiddler AI instance
2. Click on the **Add Chart** button on the top right
3. In the modal, Select the project that has a [model](doc:task-types) with Custom features
4. Select **Embedding Visualization**.

![](https://files.readme.io/1551e99-image.png)

## Chart Parameters

When creating an embedding visualization chart, you will need to specify the following parameters:

- Model
- Custom Feature Column
- Baseline
- Display Columns
- Sample size
- Number of Neighbors
- Minimum distance

Please see below for details on these parameters.

### Model

Select the model (with custom features) for which you want to visualize the embeddings.

### Custom Feature Column

Choose the custom feature column from your dataset that you wish to visualize. 

### Baseline

Define a baseline for comparison. This is optional and will be useful when you want to compare datasets such as a pre-production dataset with a production dataset or two time periods in production.

### Display Columns

Select the columns that you want to display additional information for when hovering over points in the visualization. These additional display columns will also be available in the data cards when points are selected.

### Sample Size

Decide on the number of samples you want to include in the visualization for performance and clarity. Currently, a sample size of either 100, 500, or 1000 can be selected. In future releases, we will enable support for larger sample sizes.

### Number of Neighbors

This parameter controls how UMAP balances local versus global structure in the data. It determines the number of neighboring points used in the manifold approximation. Low values (for example: 5) of this parameter will lead UMAP to focus too much on the local structure losing sight of the big picture, alternatively, bigger values will lead to a focus on the broader data. It is important to experiment on your dataset and use case to identify a value that works best for you.

### Minimum Distance

Set the minimum distance apart that points are allowed to be in the low-dimensional representation. Smaller values (for example: 0.1) will result in a more clustered embedding, which can highlight finer details. 

## Interactions on embedding visualization

### Choose Different Time Periods

When generating the embedding visualization, you have the flexibility to choose different time periods of production data to analyze. To do this:

- Access the time period selector.
- Choose the start and end dates for the time period you are interested in.
- The visualization will update to reflect the embeddings from the selected timeframe.

![](https://files.readme.io/43aa41f-image.png)

### Color By

The 'Color By' feature enriches the visualization by categorizing your data points using different colors based on attributes.

- Find the 'Color By' dropdown in your control panel.
- Choose a categorical feature to color-code the data points. For instance, in the above image, the data points are assigned colors based on a 'target' categorical column. This attribute includes categories like Sandal, Trouser, and Pullover, as indicated in the legend"
"slug: ""embedding-visualization-chart-creation"" .

Using the 'Color By' feature can assist in uncovering patterns in your data. For instance, in the above image, data points with varying 'target' column values demonstrate clustering, where similar values tend to group together.

You can also select points to delve deeper for further inspection. You may find this ability to interactively color and select data points very useful for root cause analysis.

### Zoom

Zooming in on the UMAP chart provides a closer look at clusters and individual data points.

- Use the mouse scroll wheel to zoom in or out.
- Click and drag the mouse to move the zoomed-in area around the chart.
- Zooming helps to focus on areas of interest or to distinguish between closely packed points.

### Selection of Data Points

You can select individual or groups of data points to analyze further.

- Click on a data point to select it. or use the Selector on the top right to select multiple points

![](https://files.readme.io/de52e8a-image.png)

### Data cards

- Selected points will be highlighted on the chart and details of the display columns of these cards are displayed in data cards as shown below
- Use this feature to identify and analyze specific data points

In the following example, we use the categorical attribute feedback, which contains three possible values: Like, Dislike, or Null, as indicated in the legend. After applying the 'color by' feature, the user selects specific data points to examine in greater detail. The selected data points are then presented as data cards below.

![](https://files.readme.io/0392cee-image.png)

### Hover on a Data Point

Hovering over a data point reveals additional information about it, providing immediate insight without the need for selection.

- Move the cursor over a data point on the chart.
- A tooltip will appear, displaying the data associated with that point, such as values of different display columns
- Use this feature for a quick look-up of data without altering your current selection on the chart.

## Saving the Chart

Once you're satisfied with your visualization, you can save the chart. This chart can then be added to a Dashboard. This allows you to revisit the UMAP visualization at any time easily either directly going to the Chart or to the dashboard

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Dashboard Interactions""
slug: ""dashboard-interactions""
excerpt: """"
hidden: false
metadata: 
  title: ""Dashboard Interactions""
  image: []
  robots: ""index""
createdAt: ""Wed Feb 22 2023 18:06:29 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:50:59 GMT+0000 (Coordinated Universal Time)""
---
## Dashboard Interactions

### Remove a Chart

If you want to remove a chart from your dashboard, simply click on the ""X"" located at the top right of the chart. This will remove the chart from the dashboard, but it will still be available in the saved charts list for future use. If you change your mind and decide to add the chart back to the dashboard, you can simply find it in the saved charts list and add it back to the dashboard at any time.

![](https://files.readme.io/91bb601-image.png)

### Edit a Saved Chart

To edit a saved chart, simply click on the chart title within your dashboard. This will open the chart studio in a new tab, where you can make any necessary changes. Once you have made your changes, be sure to select the `Default` time range and then use the Dashboard refresh button to see your updated chart.

![](https://files.readme.io/e7d5b04-image.png)

### Zoom

To zoom into a chart within your dashboard, you have two different utilities at your disposal. The first one is located on the top right of the chart component, in the toolbar. After clicking on the **zoom icon**, you can drag your cursor over the data points you wish to zoom into.

![](https://files.readme.io/e1a1218-image.png)

You can also use the** horizontal zoom bar **located at the base of the chart to zoom in. Once you've identified the time range you want to focus on, you can use the zoom bar to drag the range across time. For instance, if you want to analyze your data week by week over the past six months, you can use the toolbar or horizontal zoom bar to zoom in on the desired time range and then click and drag the selected range using the base horizontal zoom bar.

![](https://files.readme.io/0a13f18-image.png)

### Bar & Line Charts

You can switch between visualizing your chart as a line or a bar chart using the toolbar icons. Click on the line chart icon on the top right of the chart to switch to the line chart view. Likewise, select the bar chart icon in the toolbar to switch to the bar chart view. However,_ note that these views are only temporary and any settings you specify using the toolbar will not be saved to the dashboard._

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/31d5896-image.png"",
        null,
        """"
      ],
      ""align"": ""center"",
      ""sizing"": ""600px""
    }
  ]
}
[/block]


### Undo Chart Toolbar Changes

You can easily restore the changes you applied to your chart using the chart toolbar options, including zoom, switch to line chart, and switch to bar chart. The restore option, which is the last icon in the toolbar, allows you to undo any changes you made and return to the original chart configuration.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/0ec8eb6-image.png"",
        null,
        """"
      ],
     "
"slug: ""dashboard-interactions""  ""align"": ""center"",
      ""sizing"": ""600px""
    }
  ]
}
[/block]


‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Dashboard Utilities""
slug: ""dashboard-utilities""
excerpt: """"
hidden: false
createdAt: ""Wed Feb 22 2023 18:05:38 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:50:45 GMT+0000 (Coordinated Universal Time)""
---
## Dashboards Utilities

### Dashboard Name

To rename your dashboard, simply click on the ""Untitled Dashboard"" title on the top-left corner of the dashboard studio. This will allow you to give your dashboard a more descriptive name that reflects its purpose and contents, making it easier to find and manage among your other dashboards.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/5006167-image.png"",
        null,
        """"
      ],
      ""align"": ""center"",
      ""sizing"": ""400px""
    }
  ]
}
[/block]


Once you've clicked on the ""Untitled Dashboard"" title to rename your dashboard, simply type in the desired name and hit ""Enter"" on your keyboard to save the new name.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/7ec6e36-image.png"",
        null,
        """"
      ],
      ""align"": ""center"",
      ""sizing"": ""400px""
    }
  ]
}
[/block]


[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/453ed10-image.png"",
        null,
        """"
      ],
      ""align"": ""center"",
      ""sizing"": ""400px""
    }
  ]
}
[/block]


If you change your mind and want to discard the changes, simply click anywhere on the page outside of the name box. This will cancel the renaming process and leave the dashboard name as it was before.

### Save, Copy Link, and Delete

You can easily manage your dashboard by using the control panel located on the top left of the dashboard studio. This panel allows you to save your dashboard, copy a link to it, or delete it entirely. By using these controls, you can easily share your dashboard with others or remove it from your collection if it is no longer needed.

![](https://files.readme.io/17c9043-image.png)

#### Save

It's important to note that dashboards are not automatically saved, so you'll need to manually save your dashboard in order to lock in the current charts and filters. Once you've made the desired changes to your dashboard, simply click the ""Save"" button to save your progress. This will also enable you to share or delete your dashboard as needed. By saving your dashboard frequently, you can ensure that you never lose important information or data visualizations.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/c624c13-image.png"",
        null,
        """"
      ],
      ""align"": ""center"",
      ""sizing"": ""400px""
    }
  ]
}
[/block]


#### Copy Link

If you want to share your dashboard with other users on Fiddler, the first step is to ensure that they have access to the project that the dashboard belongs to. Once you've confirmed that they have access, you can easily share the dashboard by copying the dashboard link and sending it to them. This makes it simple to collaborate and share insights with others who are working on the same project or who have an interest in your findings. Note that you can't share a dashboard link"
"slug: ""dashboard-utilities""  until you've saved the dashboard.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/520189a-image.png"",
        null,
        """"
      ],
      ""align"": ""center"",
      ""sizing"": ""400px""
    }
  ]
}
[/block]


#### Delete

To delete a dashboard, click the overflow button next to the copy link icon. NOTE: Once a dashboard has been deleted it cannot be recovered.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/e768501-image.png"",
        null,
        """"
      ],
      ""align"": ""center"",
      ""sizing"": ""400px""
    }
  ]
}
[/block]


‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Inviting Users""
slug: ""inviting-users""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:07:27 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:44:57 GMT+0000 (Coordinated Universal Time)""
---
## Invite a user to Fiddler

> üöß To invite a user to Fiddler, you will need [Administrator permissions](doc:authorization-and-access-control). If you do not have access to an Administrator account, please contact your server administrator.

Inviting a user is easy. From anywhere on the Fiddler UI, just follow these four steps:

1. Go to the **Settings** page.
2. Click on the **Access** tab.
3. Click on the **Invitations** section.
4. Click on the plus icon on the right.

![](https://files.readme.io/3bd55c1-invite_a_user.png ""invite_a_user.png"")

When you click on the plus icon, an invite popup screen will appear as follows:

![](https://files.readme.io/8e3806f-Screen_Shot_2023-04-11_at_12.27.32_PM.png)

Once the invitation has been sent, the user should receive a signup link at the email provided.

## Getting an invitation link

In the case where the email address is not associated with an inbox, you can get the invite link by clicking **Copy invite link** after the invitation has been created.

![](https://files.readme.io/25b8659-get_invite_link.png ""get_invite_link.png"")

## What if I'm using SSO?

Whether you are using normal sign-on or single sign-on, **the process for inviting users is the same**.

If using SSO, a user should still sign up using their invitation link. Once they have created their account, their SSO login will be enabled.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Settings""
slug: ""settings""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:26:28 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:44:51 GMT+0000 (Coordinated Universal Time)""
---
![](https://files.readme.io/d937de2-Home_Page.png ""Home_Page.png"")

The Settings section captures team setup, permissions, and credentials. You can access the **Settings** page from the left menu of the Fiddler UI at all times.

These are the key tabs in **Settings**.

## General

The **General** tab shows your organization name, ID, email, and a few other details. The organization ID is needed when accessing Fiddler from the Fiddler Python API client.

![](https://files.readme.io/3f2e734-general.png ""general.png"")

## Access

The **Access** tab shows the users, teams, and invitations for everyone in the organization.

### Users

The **Users** tab shows all the users that are part of this organization.

![](https://files.readme.io/c8c5bf1-access_user.png ""access_user.png"")

### Teams

The **Teams** tab shows all the teams that are part of this organization.

![](https://files.readme.io/8cba270-access_team.png ""access_team.png"")

You can create a team by clicking on the plus (**`+`**) icon on the top-right.

> üöß Note
> 
> Only Administrators can create teams. The plus (**`+`**) icon will not be visible unless you have Administrator permissions.

![](https://files.readme.io/b0c4c53-access_create_team.png ""access_create_team.png"")

### Invitations

The **Invitations** tab shows all pending user invitations.

![](https://files.readme.io/5cb4046-access_invitation.png ""access_invitation.png"")

You can invite a user by clicking on the plus (**`+`**) icon on the top-right.

> üöß Note
> 
> Only Administrators can invite users. The plus (**`+`**) icon will not be visible unless you have Administrator permissions.

![](https://files.readme.io/abb030c-access_invite_user.png ""access_invite_user.png"")

## Credentials

The **Credentials** tab displays user access keys. These access keys are used by Fiddler Python client for authentication. Each Administrator or Member can create a unique key by clicking on **Create Key**.

![](https://files.readme.io/fce7911-credentials.png ""credentials.png"")

## Webhook Integrations

Webhook integrations allow you to configure Slack or other common webhook-based solutions to get notified by Fiddler. The ""Webhook Integration"" tab allows for managing the integrations.![](https://files.readme.io/69ad0d9-Screenshot_2023-10-09_at_4.41.55_PM.png ""credentials.png"")

### Configure a new Webhook integration

From the ""Webhook Integrations"" tab, use the + icon on the ""Wehbook integrations"" tab to configure a new webhook.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/a0b33c4-Screenshot_2023-10-10_at_12.46.31_PM.png"",
        null,
        """"
      ],
      ""align"": ""center"",
      ""sizing"": ""50% ""
    }
  ]
"
"slug: ""settings"" }
[/block]


You will need to specify the following. 

1. A unique webhook name in the ""Service name"" option. E.g: Fiddler_webhook 
2. Select your webhook service provider e.g: Slack
3. URL for the service provider where you want to read the messages from Fiddler in your webhook-enabled service. A valid URL : <https://hooks.slack.com/services/xxxxxxxxxx>
4. You can test the webhook service using the ""Test"" button after you have specified all the details.

### Edit or Delete a Webhook

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/f7be111-Screenshot_2023-10-09_at_4.58.02_PM.png"",
        """",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


You can manage your webhook from the ""Webhook Integrations"" tab. 

1. Select the webhook that you want to edit/delete using the ""..."" icon towards the right of a webhook integration row.
2. Select the ""Delete Webhook"" option to delete the webhook

> üöß Deleting a Webhook
> 
> You will not be able to delete a webhook that is already linked to alerts. To delete the webhook, you will need to modify the alert and then delete the webhook

3. Select the Edit option to edit the webhook. You will be prompted with the pre-filled details of the webhook service configured.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Project Structure on UI""
slug: ""project-structure""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:26:33 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:45:05 GMT+0000 (Coordinated Universal Time)""
---
Supervised machine learning involves identifying a predictive task, finding data to enable that task, and building a model using that data. Fiddler captures this workflow with project, dataset, and model entities.

## Projects

A project represents a machine learning task (e.g. predicting house prices, assessing creditworthiness, or detecting fraud).

A project can contain one or more models for the ML task (e.g. LinearRegression-HousePredict, RandomForest-HousePredict).

Create a project by clicking on **Projects** and then clicking on **Add Project**.

![](https://files.readme.io/8e4b429-Add_project_0710.png ""Add_project_0710.png"")

- **_Create New Project_** ‚Äî A window will pop up where you can enter the project name and click **Create**. Once the project is created, it will be displayed on the projects page.

You can access your projects from the Projects Page.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/82404e6-Screenshot_2022-12-27_at_1.00.15_PM.png"",
        null,
        ""Projects Page on Fiddler UI""
      ],
      ""align"": ""center"",
      ""caption"": ""Projects Page on Fiddler UI""
    }
  ]
}
[/block]


## Datasets

A dataset in Fiddler is a data table containing features, model outputs, and a target for machine learning models. Optionally, you can also upload metadata and ‚Äúdecision‚Äù columns, which can be used to segment the dataset for analyses, track business decisions, and work as protected attributes in bias-related workflows. For more details refer to [Datasets](doc:project-architecture#datasets) in the Platform Guide.

Once you click on a particular project, you will be able to see if there are any datasets associated with the project. For example, the bank_churn project, in the following screenshot, has the bank_churn dataset. [Datasets are uploaded via the Fiddler client](ref:clientupload_dataset). 

![](https://files.readme.io/3fa7700-Screenshot_2022-12-27_at_1.05.05_PM.png)

## Models

A model in Fiddler represents a machine learning model. A project will have one or more models for the ML task (e.g. a project to predict house prices might contain LinearRegression-HousePredict and RandomForest-HousePredict). For further details refer to the [Models](doc:project-architecture#models) section in the Platform Guide.

![](https://files.readme.io/e151df5-Model_Dashboard.png ""Model_Dashboard.png"")

### Model Artifacts

At its most basic level, a model in Fiddler is simply a directory that contains [model artifacts](doc:artifacts-and-surrogates) such as:

- The model file (e.g. `*.pkl`)
- `package.py`: A wrapper script containing all of the code needed to standardize the execution of the model.

![](https://files.readme.io/7170489-Model_Details.png ""Model_Details.png"")

![](https://files.readme.io/2b3d52"
"slug: ""project-structure"" e-Model_Details_1.png ""Model_Details_1.png"")

## Project Dashboard

You can collate specific visualizations under the Project Dashboard. After visualizations are created using the Model Analytics tool, you can pin them to the dashboard, which can then be shared with others.

![](https://files.readme.io/b7cb9ce-Chart_Dashboard.png ""Chart_Dashboard.png"")

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Authorization and Access Control""
slug: ""authorization-and-access-control""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:26:44 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:45:12 GMT+0000 (Coordinated Universal Time)""
---
## Project Roles

Each project supports its own set of permissions for its users.

![](https://files.readme.io/caf2bc9-project_settings.png ""project_settings.png"")

![](https://files.readme.io/97b71c4-project_settings_add.png ""project_settings_add.png"")

For more details refer to [Administration Page](doc:administration-platform) in the Platform Guide.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Surrogate Models""
slug: ""surrogate-models""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:25:57 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:49:04 GMT+0000 (Coordinated Universal Time)""
---
Fiddler‚Äôs explainability features require a model on the backend that can generate explanations for you.

A surrogate model is an approximation of your model using gradient boosted trees (LightGBM), trained with a general, predefined set of hyperparameters. It serves as a way for Fiddler to generate approximate explanations without you having to upload your actual model artifact.

***

A surrogate model **will be built automatically** for you when you call  [`add_model_surrogate`](/reference/clientadd_model_surrogate).  
You just need to provide a few pieces of information about how your model operates.

## What you need to specify

- Your model‚Äôs task (regression, binary classification, etc.)
- Your model‚Äôs target column (ground truth labels)
- Your model‚Äôs output column (model predictions)
- Your model‚Äôs feature columns

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Point Explainability""
slug: ""point-explainability""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:25:41 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:48:51 GMT+0000 (Coordinated Universal Time)""
---
Fiddler provides powerful visualizations that can explain your model's behavior. These explanations can be queried at an individual prediction level in the **Explain** tab, at a model level in the **Analyze** tab or within the monitoring context in the **Monitor** tab.

Explanations are available in the UI for structured (tabular) and natural language (NLP) models. They are also supported via API using the [fiddler-client](https://pypi.org/project/fiddler-client/) Python package. Explanations are available for both production and baseline queries.

Fiddler‚Äôs explanations are interactive ‚Äî you can change feature inputs and immediately view an updated prediction and explanation. We have productized several popular **explanation methods** to work fast and at scale:

- SHAP and Fiddler SHAP, using Kernel SHAP implementations, are game-theory based methods. They work for all models, because they only require the ability to ask a model for predictions.
- Integrated Gradients, which is particularly performant for deep learning models with a large number of inputs. It requires the model‚Äôs prediction to be mathematically differentiable, and a prediction gradient must be made available to Fiddler.
- Tree SHAP, is not enabled by default but can be used for Tree-based model. This is a faster and model-specific method to approximate Shapley values.

These methods are discussed in more detail below.

In addition to the previous out of the box explanation methods, Fiddler allows to bring your own explanation method. This can be customized in your model‚Äôs `package.py` wrapper script.

## Tabular Models

For tabular models, Fiddler‚Äôs Point Explanation tool shows how any given model prediction can be attributed to its individual input features.

The following is an example of an explanation for a model predicting the likelihood of customer churn:

![](https://files.readme.io/b8e4f81-Tabular_Explain.png ""Tabular_Explain.png"")

A brief tour of the features above:

- **_Explanation Method_**: The explanation method is selected from the **Explanation Type** dropdown.

- **_Input Vector_**: The far left column contains the input vector. Each input can be adjusted.

- **_Model Prediction_**: The box in the upper-left shows the model‚Äôs prediction for this input vector.

  - If the model produces multiple outputs (e.g. probabilities in a multiclass classifier), you can click on the prediction field to select and explain any of the output components. This can be particularly useful when diagnosing misclassified examples.

- **_Feature Attributions_**: The colored bars on the right represent how the prediction is attributed to the individual feature inputs.

  - A positive value (blue bar) indicates a feature is responsible for driving the prediction in the positive direction.
  - A negative value (red bar) is responsible for driving the prediction in a negative direction.

- **_Baseline Prediction_**: The thin colored line just above the bars shows the difference between the baseline prediction and the model prediction. The specifics of the baseline calculation vary with the explanation method, but usually it's approximately the mean prediction of the training/reference data distribution (i.e. the dataset specified when importing the model into Fiddler). The baseline prediction represents a typical model"
"slug: ""point-explainability""  prediction.

**Two numbers** accompany each feature‚Äôs attribution bar in the UI.

- _The first number_ is the **attribution**. The sum of these values over all features will always equal the difference between the model prediction and a baseline prediction value.

- _The second number_, the percentage in parentheses, is the **feature attribution divided by the sum of the absolute values of all the feature attributions**. This provides an easy to compare, relative measure of feature strength and directionality (notice that negative attributions have negative percentages) and is bounded by ¬±100%.

> üìò Info
> 
> An input box labeled **‚ÄúTop N‚Äù** controls how many attributions are visible at once.  If the values don‚Äôt add up as described above, it‚Äôs likely that weaker attributions are being filtered-out by this control.

Finally, it‚Äôs important to note that **feature attributions combine model behavior with characteristics of the data distribution**.

## Language (NLP) Models

For language models, Fiddler‚Äôs Point Explanation provides the word-level impact on the prediction score when using perturbative methods (SHAP and Fiddler); for the Integrated Gradients method, tokenization can be customized in your model‚Äôs `package.py` wrapper script. The explanations are interactive‚Äîedit the text, and the explanation updates immediately.

Here is an example of an explanation of a prediction from a sentiment analysis model:

![](https://files.readme.io/970a86b-NLP_Explain.png ""NLP_Explain.png"")

## Point Explanation Methods: How to Quantify Prediction Impact of a Feature?

**Introduction**

One strategy for explaining the prediction of a machine learning model is to measure the influence that each of its inputs have on the prediction made. This is called Feature Impact.

To measure Feature Impact, **additive attribution methods** can be quite powerful. Fiddler includes:

- **SHAP** and **Fiddler SHAP**, which require only the ability to ask a model for predictions, and are thus suitable across all types of models; no knowledge of the model implementation is necessary.
- **Integrated Gradients**, a method that takes advantage of the gradient vector of the prediction, which is typically available in deep learning models, to efficiently explain complex models with large input dimensionality.

**Additive Attributions**

To explain a prediction with an additive attribution method, we look at how individual features contribute to the _prediction difference_. The prediction difference is a comparison between the prediction as a point in feature space (we refer to this as the _explain-point_), and a counterfactual baseline position (or a distribution of positions), representing an uninteresting or typical model inference.

Each feature is assigned a fraction of the prediction difference for which it is responsible. This fraction is called the feature attribution, and it‚Äôs what we show in our explanations.

Additive attribution methods have the following characteristics:

- The sum of feature attributions always equals the prediction difference.
- Features that have no effect on a model‚Äôs prediction receive a feature attribution of zero.
- Features that have the identical effect receive the same attribution.
- Features with mutual information share the attribution for any effect that information has on the prediction.

Additionally, each of these methods takes into account interactions between the features (e.g. two features that have no effect individually but in combination change the model output). This is explicitly built into the Shapley value formalism, and is captured in the path integral over gradients in Integrated Gradients.

**Shapley Values and their Approximation**

The Shapley value[<sup>\[1\]</sup>](#references) (proposed by Lloyd Shapley in "
"slug: ""point-explainability"" 1953) is one way to derive feature attributions. Shapley values distribute the total payoff of a collaborative game across a coalition of cooperating players. They are computed by tabulating the average gain in payoff when a particular player is added to the coalition, over all coalition sizes and permutations of players.

In our case, we consider the ‚Äútotal gains‚Äù to be the prediction value, and a ‚Äúplayer‚Äù is a single model feature. The collaborative ‚Äúgame‚Äù is all of the model features cooperating to form a prediction value.

Why do we create ‚Äúcoalitions‚Äù with only a subset of the features? In some scenarios, it may be appropriate to replace a feature with a zero value when removed from the coalition (e.g. text models where no mask token is available). In others (e.g. models with dense tabular inputs), values are swapped in from a reference distribution or baseline example as a zero value may have a specific meaning (like zero income on a credit application).

Shapley values have desirable properties including:

- **_Linearity_**: If two games are combined, then the total gains correspond to the gains derived from a linear combination of the gains of each game.
- **_Efficiency_**: The sum of the values of all players equals the value of the grand coalition, so that all the gain is distributed among the players. In our case, the efficiency property says _the feature attributions should sum to the prediction value_. The attributions can be positive or negative, since a feature can raise or lower a predicted value.

**Approximating Shapley Values**

Computation of exact Shapley values can be extremely computationally expensive‚Äîin fact, exponentially so, in the number of input features. Fiddler makes two approximation methods available:

- **SHAP**[<sup>\[2\]</sup>](#references) (SHapely Additive exPlanations) approximates Shapley values by sampling coalitions according to a combinatorially weighted kernel (compensating for the number of permutations of features in coalitions of different cardinality). It samples the feature space uniformly between baseline-like feature vectors and explain-point-like feature vectors. This has the effect of downsampling behavior in the immediate vicinity of the explain-point, a region where the model may be saturated or uniform in its prediction, and attributions may not be helpful.
- **Fiddler SHAP**[<sup>\[3\]</sup>](#references) builds on the SHAP approach and is optimized for computing distributions of Shapley values for each feature by comparing the explain-point against a distribution of baselines. This makes it possible to compute confidence intervals around the mean attribution for each feature and identify clusters in attribution space where distinct, individually relevant explanations might be important (e.g. ‚Äúyour loan application was rejected for a set of reasons when compared to applications in your region, and for another set of reasons when compared to applications with the same profession‚Äù).

Approximate Shapley value methods can be used to explain nearly any model, since you only need to be able to ask the model for predictions at a variety of positions in the feature space.

**Integrated Gradients**

Another additive attribution method: the Integrated Gradients method.

For models whose prediction is continuous and piecewise differentiable in the feature space, it can be useful to provide additional information through the gradient (slope vector) of a prediction.

Fiddler supports Integrated Gradients (IG)[<sup>\[4\]</sup>](#references). In this method, an approximate integral tabulates components of the slope along a linear path from baseline to explain-point, and attributes them to respective input features. This"
"slug: ""point-explainability""  has several advantages:

1. For models with very high dimensional feature volumes (e.g. images, text), where differentiable deep-learning models typically excel, this method can be very performant (O(N) vs. the O(2^n) of the Shapley methods)
2. Attributions can be computed for intermediate layers within the model, providing fine-grained model diagnostics. This is naturally extensible to models with hybrid and multimodal inputs.
3. In comparison to local gradients and saliency methods, the IG path integral samples the large-scale behavior of the model and is resistant to amplifying noise in the possibly saturated region around the explain-point.

## References

1. <https://en.wikipedia.org/wiki/Shapley_value>
2. S. Lundberg, S Lee. ‚ÄúA Unified Approach to Interpreting Model Predictions.‚Äù NeurIPS, 2017 <http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf>
3. L. Merrick  and A. Taly ‚ÄúThe Explanation Game: Explaining Machine Learning Models Using Shapley Values‚Äù <https://arxiv.org/abs/1909.08128>
4. M. Sundararajan, A. Taly, Q. Yan ‚ÄúAxiomatic Attribution for Deep Networks‚Äù  <http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf>

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Global Explainability""
slug: ""global-explainability""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:25:47 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:48:58 GMT+0000 (Coordinated Universal Time)""
---
Fiddler provides powerful visualizations to describe the impact of features in your model. Feature impact and importance can be found in either the Explain or Analyze tab.

Global explanations are available in the UI for **structured (tabular)** and **natural language (NLP)** models, for both classification and regression. They are also supported via API using the Fiddler Python package. Global explanations are available for both production and baseline queries.

## Tabular Models

For tabular models, Fiddler‚Äôs Global Explanation tool shows the impact/importance of the features in the model.

Two global explanation methods are available:

- **_Feature impact_** ‚Äî Gives the average absolute change in the model prediction when a feature is randomly ablated (removed).
- **_Feature importance_** ‚Äî Gives the average change in loss when a feature is randomly ablated.

Feature impact and importance are displayed as percentages of all attributions.

The following is an example of feature impact for a model predicting the likelihood of successful loan repayment:

![](https://files.readme.io/2548d18-Global-Expln-Tabular.png ""Global-Expln-Tabular.png"")

## Language (NLP) Models

For language models, Fiddler‚Äôs Global Explanation performs ablation feature impact on a collection of text samples, determining which words have the most impact on the prediction.

> üìò Info
> 
> For speed performance, Fiddler uses a random corpus of 200 documents from the dataset. When using the [`get_feature_impact`](ref:clientget_feature_impact) function from the Fiddler API client, the number of input sentences can be changed to use a bigger corpus of texts.

Two types of visualization are available:

- **_Word cloud_** ‚Äî Displays a word cloud of top 150 words from a collection of text for this model. Fiddler provides three options:
  - **Average change**: The average impact of a word in the corpus of documents. This takes into account the impact's directionality.
  - **Average absolute feature impact**:  The average absolute impact of a word in the corpus of documents. This only takes the absolute impact of the word into account, and not its directionality.
  - **Occurrences**: The number of times a word is present in the corpus of text.

- **_Bar chart_** ‚Äî Displays the impact for the **Top N** words. By default, only words with at least 15 occurrences are displayed. This number can be modified in the UI and will be reflected in real time in the bar chart. Fiddler provides two options:
  - **Average change**: The average impact of a word in the corpus of documents. This takes into account the impact's directionality. Since positive and negative directionalities can cancel out, Fiddler provides a histogram of the individual impact, which can be found by clicking on the word.
  - **Average absolute feature impact**: The average absolute impact of a word in the corpus of documents. This only takes the absolute impact of the word into account, and not its directionality.

The following image shows an example of word impact for a sentiment analysis model:

![](https://files.readme.io/f02245d-Screen_Shot_2023-01-20_at_2."
"slug: ""global-explainability"" 39.08_PM.png)

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Useful Queries for Root Cause Analysis""
slug: ""useful-queries-for-root-cause-analysis""
excerpt: ""This page has an examples of queries which one can use in the **Analyze** tab to perform Root Cause Analysis of an issue or look at various aspect of the data.""
hidden: false
createdAt: ""Wed Sep 07 2022 14:46:59 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:49:35 GMT+0000 (Coordinated Universal Time)""
---
## 1. Count of events from the previous day

In order to look at how many events were published from the previous/last publishing date, we can do it in two ways - 

### i. Jump from **Monitor** tab

This can be done in the following steps - 

1. In the monitor tab, click on the 'jump to last event' button to get to the most recent event 
2. Select the appropriate time bin, in this case, we can select **1D bin** to get day-wise aggregated data
3. Once we have the data in the chart, we can select the most recent bin
4. Select 'Export bin and feature to analyze' to jump to analyze tab

![](https://files.readme.io/54545c9-1a.png)

5. In the analyze tab, query will be auto-populated based on the **Monitor** tab selection
6. Modify the query to count the number of events from the selection 

   ```sql
   SELECT
     count(*)
   FROM
     production.""bank_churn""
   WHERE
     fiddler_timestamp BETWEEN '2022-07-20 00:00:00'
     AND '2022-07-20 23:59:59'
   ```

### ii. Using `date` function in Analyze tab

To know how many events were published on the last publishing day, we can use `date` function of SQL  
Use the following query to query number of events

```sql
select
  *
from
  ""production.churn_classifier_test""
where
  date(fiddler_timestamp) = (
    select
      date(max(fiddler_timestamp))
    from
      ""production.churn_classifier_test""
  )
```

![](https://files.readme.io/2676acb-2.png)

## 2. Number of events on last day by output label

If we want to check how many events were published on the last day by the output class, we can use the following query 

```sql SQL
select
  churn,
  count(*)
from
  ""production.churn_classifier_test""
where
  date(fiddler_timestamp) = (
    select
      date(max(fiddler_timestamp))
    from
      ""production.churn_classifier_test""
  )
group by 
  churn
```

![](https://files.readme.io/29e443f-3.png)

## 3. Check events with missing values

If you want to check events where one of the columns is has null values, you can use the `isnull` function. 

```sql
SELECT
  *
FROM
  production.""churn_classifier_test""
WHERE
  isnull(""estimatedsalary"")
LIMIT
  1000
```

![](https://files.readme.io/43c2eac-4.png)

## 4. Frequency by Categorical column

We query w.r.t to a categorical field. For example, we can count the number of events by geography which is a categorical"
"slug: ""useful-queries-for-root-cause-analysis""  column using the following query 

```sql
SELECT
  geography,
  count(*)
FROM
  ""production.churn_classifier_test""
GROUP BY
  geography

```

![](https://files.readme.io/cbc5c25-5.png)

## 5. Frequency by Metadata

We can even query w.r.t to a metadata field. For example, if we consider gender to be a metadata column (specified in ModelInfo object), then we can obtain a frequency of events by the metadata field using the following query 

```sql
SELECT
  gender,
  count(*)
FROM
  ""production.churn_classifier_test""
GROUP BY
  gender

```

![](https://files.readme.io/4e5a79d-6.png)

## Filter Events by Cluster_ID

You can query events with certain cluster_ids if you have custom features defined such as [embedding vectors](doc:vector-monitoring-platform).

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/0d3cfe6-Screenshot_2023-10-19_at_3.26.43_PM.png"",
        """",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Dashboards""
slug: ""dashboards-platform""
excerpt: """"
hidden: false
createdAt: ""Tue Feb 21 2023 22:34:44 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:39:55 GMT+0000 (Coordinated Universal Time)""
---
## Overview

With Fiddler, you can create comprehensive dashboards that bring together all of your monitoring data in one place. This includes monitoring charts for data drift, traffic, data integrity, and performance metrics. Adding monitoring charts to your dashboards lets you create a detailed view of your model's performance. These dashboards can inform your team, management, or stakeholders, and make data-driven decisions that help improve your AI performance. 

View a list of the **[available metrics for monitoring charts here](doc:monitoring-charts-platform#supported-metric-types)**.

## Dashboards Functionality

Dashboards offer a powerful way to analyze the overall health and performance of your models, as well as to compare multiple models. 

### Dashboard Filters

- [Flexible filters](doc:dashboards-ui#dashboard-filters) including date range, time zone, and bin size to customize your view

### Chart Utilities

- [Leverage the chart toolbar ](doc:dashboard-interactions#zoom)to zoom into data and toggle between line and bar chart types

### [Dashboard Basics](doc:dashboard-utilities)

- Easily save, delete, or share your dashboard
- Click on a chart name to edit the base chart
- Remove and add monitoring charts to your dashboard
- Perform model-to-model comparison
- Plot drift or data integrity for multiple columns in one view

![](https://files.readme.io/9bf5fc2-image.png)

Checkout more on the [Dashboards UI Guide](doc:dashboards-ui).

[block:html]
{
  ""html"": ""<div class=\""fiddler-cta\"">\n<a class=\""fiddler-cta-link\"" href=\""https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\""><img src=\""https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\"" alt=\""Fiddler Demo\""></a>\n</div>""
}
[/block]
"
"---
title: ""ML Algorithms In Fiddler""
slug: ""ds""
excerpt: """"
hidden: true
createdAt: ""Fri Nov 18 2022 22:11:48 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
"
"---
title: ""Baselines""
slug: ""fiddler-baselines""
excerpt: ""A baseline is a set of reference data that is used to compare the performance of our model for monitoring purposes.""
hidden: false
createdAt: ""Thu Jan 19 2023 22:47:23 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
A model needs a baseline dataset for comparing its performance and identifying any degradation. A baseline is a set of reference data that is used to compare with our current data. 

The dataset that was used to train the model is often a good starting point for a baseline. For more in-depth analysis, we may want to use a specific time period or a rolling window of production events. 

In Fiddler, **the default baseline for all monitoring metrics is the training dataset **that was associated with the model during registration. Use this default baseline if you do not anticipate any differences between training and production. [New baselines can be added to existing models using the Python client APIs](ref:add_baseline).
"
"---
title: ""Explainability""
slug: ""explainability-platform""
excerpt: ""Platform information""
hidden: false
createdAt: ""Mon Dec 19 2022 19:01:04 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:39:25 GMT+0000 (Coordinated Universal Time)""
---
Fiddler's Explainability offering covers:

- [Point Explainations](doc:point-explainability) 
- [Global Explainations](doc:global-explainability)
- [Surrogate Model](doc:artifacts-and-surrogates#surrogate-model)

[block:html]
{
  ""html"": ""<div class=\""fiddler-cta\"">\n<a class=\""fiddler-cta-link\"" href=\""https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\""><img src=\""https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\"" alt=\""Fiddler Demo\""></a>\n</div>""
}
[/block]
"
"---
title: ""Supported Browsers""
slug: ""supported-browsers""
excerpt: ""Platform Guide""
hidden: false
createdAt: ""Tue Jan 10 2023 22:16:01 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Fiddler Product can be accessed through the following supported web browsers:

- Google Chrome
- Firefox
- Safari
- Microsoft Edge
"
"---
title: ""Project Architecture""
slug: ""project-architecture""
excerpt: """"
hidden: false
createdAt: ""Tue Nov 15 2022 18:06:28 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:58:14 GMT+0000 (Coordinated Universal Time)""
---
Supervised machine learning involves identifying a predictive task, finding data to enable that task, and building a model using that data. 

Fiddler captures this workflow with **project**, **dataset**, and **model** entities.

## Project

In Fiddler, a project is essentially a parent folder that hosts one or more **model** (s) for the ML task (e.g. A Project HousePredict for predicting house prices will LinearRegression-HousePredict, RandomForest-HousePredict).

## Models

A model in Fiddler represents a **placeholder** for a machine-learning model. It's a placeholder because we may not need the **[model artifacts](doc:artifacts-and-surrogates#Model-Artifacts)**. Instead, we may just need adequate [information about the model](ref:fdlmodelinfo) in order to monitor model-specific data. 

> üìò Info
> 
> You can [upload your model artifacts](https://dash.readme.com/project/fiddler/v1.6/docs/uploading-model-artifacts) to Fiddler to unlock high-fidelity explainability for your model. However, it is not required. If you do not wish to upload your artifact but want to explore explainability with Fiddler, we can build a [**surrogate model**](doc:artifacts-and-surrogates#surrogate-model) on the backend to be used in place of your artifact.

## Datasets

A dataset in Fiddler is a data table containing [information about data](ref:fdldatasetinfo) such as **features**, **model outputs**, and a **target** for machine learning models. Optionally, you can also upload **metadata** and ‚Äú**decision**‚Äù columns, which can be used to segment the dataset for analyses, track business decisions, and work as protected attributes in bias-related workflows. 

In order to monitor **production data**, a [dataset must be uploaded](ref:clientupload_dataset) to be used as a **baseline** for making comparisons. This baseline dataset should be sampled from your model's **training data**. The sample should be unbiased and should faithfully capture moments of the parent distribution. Further, values appearing in the baseline dataset's columns should be representative of their entire ranges within the complete training dataset.

**Datasets are used by Fiddler in the following ways:**

1. As a reference for [drift calculations](doc:data-drift-platform) and [data integrity violations ](doc:data-integrity-platform)on the **[Monitor](doc:monitoring-ui)** page
2. To train a model to be used as a [surrogate](doc:artifacts-and-surrogates#surrogate-model) when using [`add_model_surrogate`](/reference/clientadd_model_surrogate)
3. For computing model performance metrics globally on the **[Evaluate](doc:evaluation-ui)** page, or on slices on the **[Analyze](doc:analytics-ui)** page
4. As a reference for explainability algorithms (e.g. partial dependence plots, permutation feature impact, approximate Shapley values, and ICE plots).

Based on the above uses, _datasets with sizes much in excess of 10K rows are often unnecessary_ and can lead to excessive upload, precomputation, and query times"
"slug: ""project-architecture"" . That being said, here are some situations where larger datasets may be desirable:

- **Auto-modeling for tasks with significant class imbalance; or strong and complex feature interactions, possibly with deeply encoded semantics**
  - However, in use cases like these, most users opt to upload carefully-engineered model artifacts tailored to the specific application.
- **Deep segmentation analysis**
  - If it‚Äôs desirable to perform model analyses on very specific subpopulations (e.g. ‚Äú55-year-old Canadian home-owners who have been customers between 18 and 24 months‚Äù), large datasets may be necessary to have sufficient reference representation to drive model analytics.

> üìò Info
> 
> Datasets can be uploaded to Fiddler using the[ Python API client](doc:installation-and-setup).

 [Check the UI Guide to Visualize Project Architecture on our User Interface](doc:project-structure)

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Administration""
slug: ""administration-platform""
excerpt: """"
hidden: false
createdAt: ""Tue Nov 15 2022 18:09:04 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:57:55 GMT+0000 (Coordinated Universal Time)""
---
## Organization Roles

Fiddler access control comes with some preset roles. There are two global roles at the organizational level 

- **_ADMINISTRATOR_** ‚Äî Has complete access to every aspect of the organization.
  - As an administrator, you can [invite users](doc:inviting-users) to the platform.
- **_MEMBER_** ‚Äî Access is assigned at the project and model level.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/0fbfca7-roles.png"",
        ""roles.png"",
        """"
      ],
      ""align"": ""center"",
      ""sizing"": ""550px""
    }
  ]
}
[/block]


## Project Roles

Each project supports its own set of permissions for its users.

There are three roles that can be assigned:

- **_OWNER_** ‚Äî Assigns super-user permissions to the user.
- **_WRITE_** ‚Äî Allows a user to perform write operations (e.g. uploading datasets and/or models, using slice and explain, sending events to Fiddler for monitoring, etc).
- **_READ_** ‚Äî Allows a user to perform read operations (e.g. getting project/dataset/model metadata, accessing pre-existing charts, etc.).

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/3b07b46-project_roles.png"",
        ""project_roles.png"",
        """"
      ],
      ""align"": ""center"",
      ""sizing"": ""550px""
    }
  ]
}
[/block]


**Some notes about these roles:**

- A user who creates a project is assigned the **OWNER** role by default.
- A project **OWNER** or an organization **ADMINISTRATOR** can share/unshare projects with other users or teams.
- Only the **OWNER** only and an organization **ADMINISTRATOR** have access to a project until that project is explicitly shared with others.
- Project roles can be assigned to individual users or teams by the project  
  **OWNER** or by an organization **ADMINISTRATOR**.

## Teams

A team is a group of users.

- Each user can be a member of zero or more teams.
- Team roles are associated with project roles (i.e. teams can be granted  
  **READ**, **WRITE**, and/or **OWNER** permissions for a project).

Click [here](doc:settings#teams) for more information on teams.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Model Task Types""
slug: ""task-types""
excerpt: """"
hidden: false
createdAt: ""Tue Nov 15 2022 18:06:58 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:42:57 GMT+0000 (Coordinated Universal Time)""
---
From 23.5 and onwards, Fiddler supports **six** model tasks. These include:

- Binary Classification
- Multi-class Classification
- Regression
- Ranking
- LLM
- Not set

**Binary classification** is the task of classifying the elements of an outcome set into two groups (each called class) on the basis of a classification rule. 

[Onboarding](doc:onboarding-a-model) a Binary classification task in Fiddler requires the following:

- A single output column of type float (range 0-1) which represents the soft output of the model. This column has to be defined.
- A single target column that represents the true outcome. This column has to be defined.
- A list of input features has to be defined.

Typical binary classification problems include:

- Determining whether a customer will churn or not. Here the outcome set has two outcomes: The customer will churn or the customer will not. Further, the outcome can only belong to either of the two classes.
- Determining whether a patient has a disease or not. Here the outcome set has two outcomes: the patient has the disease or does not.

**Multiclass classification** is the task of classifying the elements of an outcome set into three or more groups (each called class) on the basis of a classification rule. 

[Onboarding](doc:onboarding-a-model) a Multiclass classification task in Fiddler requires the following:

- Multiple output columns (one per class) of type float (range 0-1) which represent the soft outputs of the model. Those columns have to be defined.
- A single target column that represents the true outcome. This column has to be defined.
- A list of input features has to be defined.

Typical multiclass classification problems include:

- Determining whether an image is a cat, a dog, or a bird. Here the outcome set has more than two outcomes. Further, the image can only be determined to be one of the three outcomes and it's thus a multiclass classification problem.

**Regression** is the task of predicting a continuous numeric quantity. 

[Onboarding](doc:onboarding-a-model) a Regression task in Fiddler requires the following:

- A single numeric output column that represents the output of the model. This column has to be defined.
- A single numeric target column that represents the true outcome. This column has to be defined.
- A list of input features has to be defined.

Typical regression problems include:

- Determining the average home price based on a given set of housing-related features such as its square footage, number of beds and baths, its location, etc.
- Determining the income of an individual based on features such as age, work location, job sector, etc.

**Ranking** is the task of constructing a rank-ordered list of items given a particular query that seeks some information. 

[Onboarding](doc:onboarding-a-model) a Ranking task in Fiddler requires the following:

- A single numeric output column that represents the output of the model. This column has to be defined.
- A single target column that represents the true outcome. This column has to be defined.
- A list of input features has to be defined.

Typical ranking problems include:

- Ranking documents in"
"slug: ""task-types""  information retrieval systems.
- Ranking relevancy of advertisements based on user search queries.

**LLM** is the task dedicated for Large Language Models, a type of transformer model. It represents a deep learning model that can process human languages. 

[Onboarding](doc:onboarding-a-model) an LLM task in Fiddler doesn't require any specific format with regards to the targets/outputs/inputs definition. Those can be defined or not, with any type and no minimum or maximum column has to be defined. However, in that setting, Fiddler doesn't offer XAI functionalities or performance metrics.

Typical LLM problems include:

- Chatbots and Virtual assistants that can answer questions.
- Content creation like articles, blog posts, etc.

**Not set** is the task to choose if a use-case is not covered by the previous tasks or the use-case doesn't need performance metrics. In this setting, the user doesn't have to specify the required data as discussed above to onboard their model.

[Onboarding](doc:onboarding-a-model) a `NOT_SET` task in Fiddler doesn't require any specific format with regards to the targets/outputs/inputs definition Those can be defined or not, with any type and no minimum or maximum column has to be defined. However, in that setting, Fiddler doesn't offer XAI functionalities or performance metrics.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Analytics and Evaluation""
slug: ""analytics-eval-platform""
excerpt: ""Platform Guide""
hidden: false
createdAt: ""Wed Feb 01 2023 21:50:06 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:42:46 GMT+0000 (Coordinated Universal Time)""
---
## Analytics

Fiddler‚Äôs industry-first model analytics tool, called Slice and Explain, allows you to perform an exploratory or targeted analysis of model behavior.

1. **_Slice_** ‚Äî Identify a selection, or slice, of data. Or, you can start with the entire dataset for global analysis.
2. **_Explain_** ‚Äî Analyze model behavior on that slice using Fiddler‚Äôs visual explanations and other data insights.

Slice and Explain is designed to help data scientists, model validators, and analysts drill down into a model and dataset to see global, local, or instance-level explanations for the model‚Äôs predictions.

Slice and Explain can help you answer questions like:

- What are the key drivers of my model output in a subsection of the data?
- How are the model inputs correlated to other inputs and to the output?
- Where is my model underperforming?
- How is my model performing across the classes in a protected group?

Access Slice and Explain from the Analyze tab for your model. Slice and Explain currently support all tabular models.

**For details on how to use Fiddler Analytics through our interface check the [Analytics Page](doc:analytics-ui) on our UI Guide**

## Evaluation

Model performance evaluation is one of the key tasks in the ML model lifecycle. A model's performance indicates how successful the model is at making useful predictions on data.

**For details on how to use Fiddler Evaluation through our interface check the [Evaluation Page](doc:evaluation-ui) on our UI Guide**

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert 

[block:html]
{
  ""html"": ""<div class=\""fiddler-cta\"">\n<a class=\""fiddler-cta-link\"" href=\""https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\""><img src=\""https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\"" alt=\""Fiddler Demo\""></a>\n</div>""
}
[/block]
"
"---
title: ""Monitoring""
slug: ""monitoring-platform""
excerpt: """"
hidden: false
createdAt: ""Tue Nov 15 2022 18:06:49 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:34:55 GMT+0000 (Coordinated Universal Time)""
---
Fiddler Monitoring helps you identify issues with the performance of your ML models after deployment. Fiddler Monitoring has five Metric Types which can be monitored and alerted on:

1. **Data Drift**
2. **Performance**
3. **Data Integrity**
4. **Traffic**
5. **Statistic**

## Integrate with Fiddler Monitoring

Integrating Fiddler monitoring is a four-step process:

1. **Upload dataset**

   Fiddler needs a dataset to be used as a baseline for monitoring. A dataset can be uploaded to Fiddler using our UI and Python package. For more information, see:

   - [client.upload_dataset()](ref:clientupload_dataset) 

2. **Onboard model**

   Fiddler needs some specifications about your model in order to help you troubleshoot production issues. Fiddler supports a wide variety of model formats. For more information, see:

   - [client.add_model()](ref:clientadd_model) 

3. **Configure monitoring for this model**

   You will need to configure bins and alerts for your model. These will be discussed in detail below.

4. **Send traffic from your live deployed model to Fiddler**

   Use the Fiddler SDK to send us traffic from your live deployed model.

## Publish events to Fiddler

In order to send traffic to Fiddler, use the [`publish_event`](ref:clientpublish_event) API from the Fiddler SDK.

The `publish_event` API can be called in real-time right after your model inference. 

An event can contain the following:

- Inputs
- Outputs
- Target
- Decisions (categorical only)
- Metadata

These aspects of an event can be monitored on the platform.

> üìò Info
> 
> You can also publish events as part of a batch call after the fact using the `publish_events_batch` API (click [here](ref:clientpublish_events_batch) for more information). In this case, you will need to send Fiddler the original event timestamps as to accurately populate the time series charts.

## Updating events

Fiddler supports [partial updates of events](doc:updating-events) for your **target** column. This can be useful when you don‚Äôt have access to the ground truth for your model at the time the model's prediction is made. Other columns can only be sent at insertion time (with `update_event=False`).

***

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert 

[block:html]
{
  ""html"": ""<div class=\""fiddler-cta\"">\n<a class=\""fiddler-cta-link\"" href=\""https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\""><img src=\""https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\"" alt=\""Fiddler Demo\""></a>\n</div>""
}
[/block]
"
"---
title: ""Model: Artifacts, Package, Surrogate""
slug: ""artifacts-and-surrogates""
excerpt: ""Important terminologies for the ease of use of Fiddler Explainability""
hidden: false
createdAt: ""Tue Nov 15 2022 18:06:36 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Model Artifacts and Model Package

A model in Fiddler is a placeholder that may not need the **model artifacts** for monitoring purposes. However, for explainability, model artifacts are needed. 

_Required_ model artifacts include: 

- The **[model file](doc:artifacts-and-surrogates#model-file) **(e.g. `*.pkl`)
- [`package.py`](doc:artifacts-and-surrogates#packagepy-wrapper-script): A wrapper script containing all of the code needed to standardize the execution of the model.

A collection of model artifacts in a directory is referred to as a **model package**. To start, **place your model artifacts in a new directory**. This directory will be the model package you will upload to Fiddler to add or update model artifacts. 

While the model file and package.py are required artifacts in a model package, you can also _optionally_ add other artifacts such as:

- [`model.yaml`](doc:artifacts-and-surrogates#modelyaml-configuration-file): A YAML file containing all the information about the model as specified in [ModelInfo](ref:fdlmodelinfo). This model metadata is used in Fiddler‚Äôs explanations, analytics, and UI.
- Any serialized [preprocessing objects](#preprocessing-objects) needed to transform data before running predictions or after.

In the following, we discuss the various model artifacts.

### Model File

A model file is a **serialized representation of your model** as a Python object.

Model files can be stored in a variety of formats. Some include

- Pickle (`.pkl`)
- Protocol buffer (`.pb`)
- Hierarchical Data Format/HDF5 (`.h5`)

### package.py wrapper script

Fiddler‚Äôs artifact upload process is **framework-agnostic**. Because of this, a **wrapper script** is needed to let Fiddler know how to interact with your particular model and framework.

The wrapper script should be named `package.py`, and it should be **placed in the same directory as your model artifact**. Below is an example of what `package.py` should look like.

```python
from pathlib import Path
import pandas as pd

PACKAGE_PATH = Path(__file__).parent

class MyModel:

    def __init__(self):
        """"""
        Here we can load in the model and any other necessary
            serialized objects from the PACKAGE_PATH.
        """"""

    def predict(self, input_df):
        """"""
        The predict() function should return a DataFrame of predictions
            whose columns correspond to the outputs of your model.
        """"""

def get_model():
    return MyModel()
```

The only hard requirements for `package.py` are

- The script must be named `package.py`
- The script must implement a function called `get_model`, which returns a model object
- This model object must implement a function called `predict`, which takes in a pandas DataFrame of model inputs and returns a pandas DataFrame of model predictions

### model.yaml configuration file

In case you want to update the custom explanations (`custom_explanation_names`) or the preferred explanation method (`preferred_explanation_method`) in"
"slug: ""artifacts-and-surrogates""  the model info, you will need to construct a YAML file with **specifications for how your model operates**. This can be easily obtained from [fdl.ModelInfo()](ref:fdlmodelinfo) object.

> üìò Info
> 
> For information on constructing a [fdl.ModelInfo()](ref:fdlmodelinfo) object, see [Creating ModelInfo Object](doc:registering-a-model#creating-a-modelinfo-object).

> üöß Warning
> 
> Currently, only the following fields in model info can be updated:
> 
> - `custom_explanation_names`
> - `preferred_explanation_method`
> - `display_name`
> - `description`

Once you have your [fdl.ModelInfo()](ref:fdlmodelinfo), you can call its [fdl.ModelInfo.to_dict()](ref:fdlmodelinfoto_dict) function to **generate a dictionary** that can be used for the YAML configuration file.

```python
import yaml

with open('model.yaml', 'w') as yaml_file:
    yaml.dump({'model': model_info.to_dict()}, yaml_file)
```

Note that we are adding `model` key whose value is the dictionary produced by the [`fdl.ModelInfo`](ref:fdlmodelinfo) object.

Once it‚Äôs been created, you can place it in the directory with your model artifact and `package.py` script.

### Preprocessing objects

Another component of your model package could be any **serialized preprocessing objects** that are used to transform the data before or after making predictions.

You can place these in the model package directory as well.

> üìò Info
> 
> For example, in the case that we have a **categorical feature**, we may need to **encode** it as one or more numeric columns before calling the model‚Äôs prediction function. In that case, we may have a serialized transform object called `encoder.pkl`. This should also be included in the model package directory.

### requirements.txt file

> üìò Info
> 
> This is only used starting at 23.1 version with Model Deployment enabled.

Each base image (see [image_uri](doc:model-deployment) for more information on base images) comes with a few pre-installed libraries and these can be overridden by specifying `requirements.txt` file inside your model artifact directory where `package.py` is defined.

Add the dependencies to requirements.txt file like this:

```python
scikit-learn==1.0.2  
numpy==1.23.0  
pandas==1.5.0
```

## Surrogate Model

A surrogate model is an approximation of your model intended to make qualitative explainability calculations possible for scenarios where model ingestion is impossible or explainability is an occasional nice-to-have, but not a primary component of a model monitoring workflow.  

Fiddler creates a surrogate when you call [`add_model_surrogate`](/reference/clientadd_model_surrogate).  This requires that you've already added a model using [add_model](ref:clientadd_model).

> üöß Surrogates can currently only be created for models with tabular input types.

Fiddler produces a surrogate by training a gradient-boosted decision tree (LightGBM) to the ground-truth labels provided and with a general, predefined set of settings.
"
"---
title: ""Fairness""
slug: ""fairness""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:24:34 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:42:37 GMT+0000 (Coordinated Universal Time)""
---
> üöß Note
> 
> Model Fairness is in preview mode. Contact us for early access.

Fiddler provides powerful visualizations and metrics to detect model bias. Currently, _we support structured (tabular) models for classification tasks_ in both the Fiddler UI and the [API client](ref:about-the-fiddler-client). These visualizations are available for both production and dataset queries.

## Definitions of Fairness

Models are trained on real-world examples to mimic past outcomes on unseen data. The training data could be biased, which means the model will perpetuate the biases in the decisions it makes.

While there is not a universally agreed upon definition of fairness, we define a ‚Äòfair‚Äô ML model as a model that does not favor any group of people based on their characteristics.

Ensuring fairness is key before deploying a model in production. For example, in the US, the government prohibited discrimination in credit and real-estate transactions with fair lending laws like the Equal Credit Opportunity Act (ECOA) and the Fair Housing Act (FHAct).

The Equal Employment Opportunity Commission (EEOC) acknowledges 12 factors of discrimination:[<sup>\[1\]</sup>](#reference) age, disability, equal pay/compensation, genetic information, harassment, national origin, pregnancy, race/color, religion, retaliation, sex, sexual harassment. These are what we call protected attributes.

## Fairness Metrics

Fiddler provides the following fairness metrics:

- Disparate Impact
- Group Benefit
- Equal Opportunity
- Demographic Parity

The choice of the metric is use case-dependent. An important point to make is that it's impossible to optimize all the metrics at the same time. This is something to keep in mind when analyzing fairness metrics.

## Disparate Impact

Disparate impact is a form of **indirect and unintentional discrimination**, in which certain decisions disproportionately affect members of a protected group.

Mathematically, disparate impact compares the pass rate of one group to that of another.

The pass rate is the rate of positive outcomes for a given group. It's defined as follows:

pass rate = passed / (num of ppl in the group)

Disparate impact is calculated by:

`DI = (pass rate of group 1) / (pass rate of group 2)`

Groups 1 and 2 are interchangeable. Therefore, the following formula can be used to calculate disparate impact:

`DI = min{pr_1, pr_2} / max{pr_1, pr_2}.`

The disparate impact value is between 0 and 1. The Four-Fifths rule states that the disparate impact has to be greater than 80%.

For example:

`pass-rate_1 = 0.3, pass-rate_2 = 0.4, DI = 0.3/0.4 = 0.75`

`pass-rate_1 = 0.4, pass-rate_2 = 0.3, DI = 0.3/0.4 = 0.75`

> üìò Info
> 
> Disparate impact is the only legal metric available. The other metrics are not yet codified in US law.

## Demographic Parity

Demographic parity states that the proportion of each segment of a"
"slug: ""fairness""  protected class should receive the positive outcome at equal rates.

Mathematically, demographic parity compares the pass rate of two groups.

The pass rate is the rate of positive outcome for a given group. It is defined as follow:

`pass rate = passed / (num of ppl in the group)`

If the decisions are fair, the pass rates should be the same.

## Group Benefit

Group benefit aims to measure the rate at which a particular event is predicted to occur within a subgroup compared to the rate at which it actually occurs.

Mathematically, group benefit for a given group is defined as follows:

`Group Benefit = (TP+FP) / (TP + FN).`

Group benefit equality compares the group benefit between two groups. If the two groups are treated equally, the group benefit should be the same.

## Equal Opportunity

Equal opportunity means that all people will be treated equally or similarly and not disadvantaged by prejudices or bias.

Mathematically, equal opportunity compares the true positive rate (TPR) between two groups. TPR is the probability that an actual positive will test positive. The true positive rate is defined as follows:

`TPR = TP/(TP+FN)`

If the two groups are treated equally, the TPR should be the same.

## Intersectional Fairness

We believe fairness should be ensured to all subgroups of the population. We extended the classical metrics (which are defined for two classes) to multiple classes. In addition, we allow multiple protected features (e.g. race _and_ gender). By measuring fairness along overlapping dimensions, we introduce the concept of intersectional fairness.

To understand why we decided to go with intersectional fairness, we can take a simple example. In the figure below, we observe that equal numbers of black and white people pass. Similarly, there is an equal number of men and women passing. However, this classification is unfair because we don‚Äôt have any black women and white men that passed, and all black men and white women passed. Here, we observe bias within subgroups when we take race and gender as protected attributes.

![](https://files.readme.io/21f6b94-intersectional_fairness.svg ""intersectional_fairness.svg"")

The EEOC provides examples of past intersectional discrimination/harassment cases.[<sup>\[2\]</sup>](#reference)

## Model Behavior

In addition to the fairness metrics, we provide information about model outcomes and model performance for each subgroup. 

## Dataset Fairness

We also provide a section for dataset fairness, with a mutual information matrix and a label distribution. Note that this is a pre-modeling step.

**Mutual information **gives information about existing dependence in your dataset between the protected attributes and the remaining features. We are displaying Normalized Mutual Information (NMI). This metric is symmetric, and has values between 0 and 1, where 1 means perfect dependency.

For more details about the implementation of the intersectional framework, please refer to this [research paper](https://arxiv.org/pdf/2101.01673.pdf).

## Reference

[^1]\: USEEOC article on [_Discrimination By Type_](https://www.eeoc.gov/discrimination-type)  
[^2]\:  USEEOC article on [_Intersectional Discrimination/Harassment_](https://www.eeoc.gov/initiatives/e-race/significant-eeoc-racecolor-casescovering-private-and-federal-sectors#intersectional)

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert 

[block:html"
"slug: ""fairness"" ]
{
  ""html"": ""<div class=\""fiddler-cta\"">\n<a class=\""fiddler-cta-link\"" href=\""https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\""><img src=\""https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\"" alt=\""Fiddler Demo\""></a>\n</div>""
}
[/block]
"
"---
title: ""Flexible Model Deployment""
slug: ""model-deployment""
excerpt: ""How to define the environment my model needs?""
hidden: false
createdAt: ""Tue Jan 17 2023 20:49:36 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Fiddler Platform supports models with varying dependencies. For example, the Fiddler platform allows you to have two models, running the same library but have incompatible versions, through our flexible model deployment specification.

When you add a model artifact into Fiddler (see [add_model_artifact](ref:clientadd_model_artifact)), you can specify the deployment needed to run the model. 

`add_model_artifact` now takes a `deployment_params` argument where you can specify the following information using [fdl.DeploymentParams](ref:fdldeploymentparams):

- `image_uri`: This is the docker image used to create a new runtime to serve the model. You can choose a base image from the following list, with the matching requirements for your model:

  [block:parameters]{""data"":{""h-0"":""Image uri"",""h-1"":""Dependencies"",""0-0"":""`md-base/python/machine-learning:1.1.0`"",""0-1"":""catboost==1.1.1  \nfiddler-client==1.7.4  \nflask==2.2.2  \ngevent==21.12.0  \ngunicorn==20.1.0  \njoblib==1.2.0  \nlightgbm==3.3.0  \nnltk==3.7  \nnumpy==1.23.4  \npandas==1.5.1  \nprometheus-flask-exporter==0.21.0  \npydantic==1.10.7  \nscikit-learn==1.1.1  \nshap==0.40.0  \nxgboost==1.7.1"",""1-0"":""`md-base/python/deep-learning:1.2.0`"",""1-1"":""fiddler-client==1.7.4  \nflask==2.2.2  \ngevent==21.12.0  \ngunicorn==20.1.0  \njoblib==1.2.0  \nnltk==3.7  \nnumpy==1.23.4  \npandas==1.5.1  \nPillow==9.3.0  \nprometheus-flask-exporter==0.21.0  \npydantic==1.10.7  \ntensorflow==2.9.3  \ntorch==1.13.1  \ntorchvision==0.14.1  \ntransformers==4.24.0"",""2-0"":""`md-base/python/python-38:1.1.0`"",""2-1"":""fiddler-client==1.7.4  \nflask==2.2.2  \ngevent==21.12.0  \ngunicorn==20.1.0  \nprometheus-flask-exporter==0.21.0  \npydantic==1.10.7"",""3-0"":""`md-base/python/python-39:1.1.0`"",""3-1"":""fidd"
"slug: ""model-deployment"" ler-client==1.7.4  \nflask==2.2.2  \ngevent==21.12.0  \ngunicorn==20.1.0  \nprometheus-flask-exporter==0.21.0  \npydantic==1.10.7""},""cols"":2,""rows"":4,""align"":[""left"",""left""]}[/block]

Each base image comes with a few pre-installed libraries and these can be overridden by specifying [requirements.txt](doc:artifacts-and-surrogates#requirementstxt-file) file inside your model artifact directory where [package.py](doc:artifacts-and-surrogates#packagepy-wrapper-script) is defined.  

`md-base/python/python-38` and `md-base/python/python-39` are images with the least pre-installed dependencies, use this if none of the other images matches your requirement. 

> üöß Be aware
> 
> Installing new dependencies at runtime will take time and is prone to network errors.

- `replicas`: The number of replicas running the model.
- `memory`: The amount of memory (mebibytes) reserved per replica. NLP models might need more memory, so ensure to allocate the required amount of resources.

> üöß Be aware
> 
> Your model might need more memory than the default setting. Please ensure you set appropriate amount of resources. If you get a `ModelServeError` error when adding a model, it means you didn't provide enough memory for your model.

- `cpu`: The amount of CPU (milli cpus) reserved per replica.

Both [add_model_artifact](ref:clientadd_model_artifact) and [update_model_artifact](ref:clientupdate_model_artifact) methods support passing `deployment_params`. For example:

```python python
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'

# Specify deployment parameters
deployment_params = fdl.DeploymentParams(
        image_uri=""md-base/python/machine-learning:1.1.0"",
        cpu=250,
        memory=512,
  		  replicas=1,
)

# Add model artifact
client.add_model_artifact(  
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    model_dir='model_dir/',
  	deployment_params=deployment_params,
)
```

Once the model is added in Fiddler, you can fine-tune the model deployment based on the scaling requirements, using [update_model_deployment](ref:clientupdate_model_deployment). This function allows you to:

- **Horizontal scaling**: horizontal scaling via replicas parameter. This will create multiple Kubernetes pods internally to handle requests.
- **Vertical scaling**: Model deployments support vertical scaling via cpu and memory parameters. Some models might need more memory to load the artifacts into memory or process the requests.
- **Scale down**: You may want to scale down the model deployments to avoid allocating the resources when the model is not in use. Use active parameters to scale down the deployment.
- **Scale up**: This will again create the model deployment Kubernetes pods with the resource values available in the database.

> üìò Note
> 
> Supported from server version `23.1` with Flexible Model Deployment feature enabled.
"
"---
title: ""Global Explainability""
slug: ""global-explainability-platform""
excerpt: """"
hidden: false
createdAt: ""Fri Nov 18 2022 22:57:28 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:41:38 GMT+0000 (Coordinated Universal Time)""
---
Fiddler provides powerful visualizations to describe the impact of features in your model. Feature impact and importance can be found in either the Explain or Analyze tab.

Global explanations are available in the UI for **structured (tabular)** and **natural language (NLP)** models, for both classification and regression. They are also supported via API using the Fiddler Python package. Global explanations are available for both production and dataset queries.

## Tabular Models

For tabular models, Fiddler‚Äôs Global Explanation tool shows the impact/importance of the features in the model.

Two global explanation methods are available:

- **_Feature impact_** ‚Äî Gives the average absolute change in the model prediction when a feature is randomly ablated (removed).
- **_Feature importance_** ‚Äî Gives the average change in loss when a feature is randomly ablated.

## Language (NLP) Models

For language models, Fiddler‚Äôs Global Explanation performs ablation feature impact on a collection of text samples, determining which words have the most impact on the prediction.

> üìò Info
> 
> For speed performance, Fiddler uses a random corpus of 200 documents from the dataset. When using the [`get_feature_importance`](ref:clientget_feature_importance) function from the Fiddler API client, the argument `num_refs` can be changed to use a bigger corpus of texts.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Point Explainability""
slug: ""point-explainability-platform""
excerpt: """"
hidden: false
createdAt: ""Fri Nov 18 2022 22:57:20 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:41:30 GMT+0000 (Coordinated Universal Time)""
---
Fiddler provides powerful visualizations that can explain your model's behavior. These explanations can be queried at an individual prediction level in the **Explain** tab, at a model level in the **Analyze** tab or within the monitoring context in the **Monitor** tab.

Explanations are available in the UI for structured (tabular) and natural language (NLP) models. They are also supported via API using the [fiddler-client](https://pypi.org/project/fiddler-client/) Python package. Explanations are available for both production and baseline queries.

Fiddler‚Äôs explanations are interactive ‚Äî you can change feature inputs and immediately view an updated prediction and explanation. We have productized several popular **explanation methods** to work fast and at scale:

- SHAP and Fiddler SHAP, using Kernel SHAP implementations, are game-theory based methods. They work for all models, because they only require the ability to ask a model for predictions.
- Integrated Gradients, which is particularly performant for deep learning models with a large number of inputs. It requires the model‚Äôs prediction to be mathematically differentiable, and a prediction gradient must be made available to Fiddler.
- Tree SHAP, is not enabled by default but can be used for Tree-based model. This is a faster and model-specific method to approximate Shapley values.

These methods are discussed in more detail below.

In addition to the previous out of the box explanation methods, Fiddler allows to bring your own explanation method. This can be customized in your model‚Äôs `package.py` wrapper script.

## Tabular Models

For tabular models, Fiddler‚Äôs Point Explanation tool shows how any given model prediction can be attributed to its individual input features.

The following is an example of an explanation for a model predicting the likelihood of customer churn:

![](https://files.readme.io/b8e4f81-Tabular_Explain.png ""Tabular_Explain.png"")

A brief tour of the features above:

- **_Explanation Method_**: The explanation method is selected from the **Explanation Type** dropdown.

- **_Input Vector_**: The far left column contains the input vector. Each input can be adjusted.

- **_Model Prediction_**: The box in the upper-left shows the model‚Äôs prediction for this input vector.

  - If the model produces multiple outputs (e.g. probabilities in a multiclass classifier), you can click on the prediction field to select and explain any of the output components. This can be particularly useful when diagnosing misclassified examples.

- **_Feature Attributions_**: The colored bars on the right represent how the prediction is attributed to the individual feature inputs.

  - A positive value (blue bar) indicates a feature is responsible for driving the prediction in the positive direction.
  - A negative value (red bar) is responsible for driving the prediction in a negative direction.

- **_Baseline Prediction_**: The thin colored line just above the bars shows the difference between the baseline prediction and the model prediction. The specifics of the baseline calculation vary with the explanation method, but usually it's approximately the mean prediction of the training/reference data distribution (i.e. the dataset specified when importing the model into Fiddler). The baseline prediction represents a typical"
"slug: ""point-explainability-platform""  model prediction.

**Two numbers** accompany each feature‚Äôs attribution bar in the UI.

- _The first number_ is the **attribution**. The sum of these values over all features will always equal the difference between the model prediction and a baseline prediction value.

- _The second number_, the percentage in parentheses, is the **feature attribution divided by the sum of the absolute values of all the feature attributions**. This provides an easy to compare, relative measure of feature strength and directionality (notice that negative attributions have negative percentages) and is bounded by ¬±100%.

> üìò Info
> 
> An input box labeled **‚ÄúTop N‚Äù** controls how many attributions are visible at once.  If the values don‚Äôt add up as described above, it‚Äôs likely that weaker attributions are being filtered-out by this control.

Finally, it‚Äôs important to note that **feature attributions combine model behavior with characteristics of the data distribution**.

## Language (NLP) Models

For language models, Fiddler‚Äôs Point Explanation provides the word-level impact on the prediction score when using perturbative methods (SHAP and Fiddler); for the Integrated Gradients method, tokenization can be customized in your model‚Äôs `package.py` wrapper script. The explanations are interactive‚Äîedit the text, and the explanation updates immediately.

Here is an example of an explanation of a prediction from a sentiment analysis model:

![](https://files.readme.io/970a86b-NLP_Explain.png ""NLP_Explain.png"")

## Point Explanation Methods: How to Quantify Prediction Impact of a Feature?

**Introduction**

One strategy for explaining the prediction of a machine learning model is to measure the influence that each of its inputs have on the prediction made. This is called Feature Impact.

To measure Feature Impact, **additive attribution methods** can be quite powerful. Fiddler includes:

- **SHAP** and **Fiddler SHAP**, which require only the ability to ask a model for predictions, and are thus suitable across all types of models; no knowledge of the model implementation is necessary.
- **Integrated Gradients**, a method that takes advantage of the gradient vector of the prediction, which is typically available in deep learning models, to efficiently explain complex models with large input dimensionality.

**Additive Attributions**

To explain a prediction with an additive attribution method, we look at how individual features contribute to the _prediction difference_. The prediction difference is a comparison between the prediction as a point in feature space (we refer to this as the _explain-point_), and a counterfactual baseline position (or a distribution of positions), representing an uninteresting or typical model inference.

Each feature is assigned a fraction of the prediction difference for which it is responsible. This fraction is called the feature attribution, and it‚Äôs what we show in our explanations.

Additive attribution methods have the following characteristics:

- The sum of feature attributions always equals the prediction difference.
- Features that have no effect on a model‚Äôs prediction receive a feature attribution of zero.
- Features that have the identical effect receive the same attribution.
- Features with mutual information share the attribution for any effect that information has on the prediction.

Additionally, each of these methods takes into account interactions between the features (e.g. two features that have no effect individually but in combination change the model output). This is explicitly built into the Shapley value formalism, and is captured in the path integral over gradients in Integrated Gradients.

**Shapley Values and their Approximation**

The Shapley value[<sup>\[1\]</sup>](#references) (proposed by Lloyd Shapley in"
"slug: ""point-explainability-platform""  1953) is one way to derive feature attributions. Shapley values distribute the total payoff of a collaborative game across a coalition of cooperating players. They are computed by tabulating the average gain in payoff when a particular player is added to the coalition, over all coalition sizes and permutations of players.

In our case, we consider the ‚Äútotal gains‚Äù to be the prediction value, and a ‚Äúplayer‚Äù is a single model feature. The collaborative ‚Äúgame‚Äù is all of the model features cooperating to form a prediction value.

Why do we create ‚Äúcoalitions‚Äù with only a subset of the features? In some scenarios, it may be appropriate to replace a feature with a zero value when removed from the coalition (e.g. text models where no mask token is available). In others (e.g. models with dense tabular inputs), values are swapped in from a reference distribution or baseline example as a zero value may have a specific meaning (like zero income on a credit application).

Shapley values have desirable properties including:

- **_Linearity_**: If two games are combined, then the total gains correspond to the gains derived from a linear combination of the gains of each game.
- **_Efficiency_**: The sum of the values of all players equals the value of the grand coalition, so that all the gain is distributed among the players. In our case, the efficiency property says _the feature attributions should sum to the prediction value_. The attributions can be positive or negative, since a feature can raise or lower a predicted value.

**Approximating Shapley Values**

Computation of exact Shapley values can be extremely computationally expensive‚Äîin fact, exponentially so, in the number of input features. Fiddler makes two approximation methods available:

- **SHAP**[<sup>\[2\]</sup>](#references) (SHapely Additive exPlanations) approximates Shapley values by sampling coalitions according to a combinatorially weighted kernel (compensating for the number of permutations of features in coalitions of different cardinality). It samples the feature space uniformly between baseline-like feature vectors and explain-point-like feature vectors. This has the effect of downsampling behavior in the immediate vicinity of the explain-point, a region where the model may be saturated or uniform in its prediction, and attributions may not be helpful.
- **Fiddler SHAP**[<sup>\[3\]</sup>](#references) builds on the SHAP approach and is optimized for computing distributions of Shapley values for each feature by comparing the explain-point against a distribution of baselines. This makes it possible to compute confidence intervals around the mean attribution for each feature and identify clusters in attribution space where distinct, individually relevant explanations might be important (e.g. ‚Äúyour loan application was rejected for a set of reasons when compared to applications in your region, and for another set of reasons when compared to applications with the same profession‚Äù).

Approximate Shapley value methods can be used to explain nearly any model, since you only need to be able to ask the model for predictions at a variety of positions in the feature space.

**Integrated Gradients**

Another additive attribution method: the Integrated Gradients method.

For models whose prediction is continuous and piecewise differentiable in the feature space, it can be useful to provide additional information through the gradient (slope vector) of a prediction.

Fiddler supports Integrated Gradients (IG)[<sup>\[4\]</sup>](#references). In this method, an approximate integral tabulates components of the slope along a linear path from baseline to explain-point, and attributes them to respective input features."
"slug: ""point-explainability-platform""  This has several advantages:

1. For models with very high dimensional feature volumes (e.g. images, text), where differentiable deep-learning models typically excel, this method can be very performant (O(N) vs. the O(2^n) of the Shapley methods)
2. Attributions can be computed for intermediate layers within the model, providing fine-grained model diagnostics. This is naturally extensible to models with hybrid and multimodal inputs.
3. In comparison to local gradients and saliency methods, the IG path integral samples the large-scale behavior of the model and is resistant to amplifying noise in the possibly saturated region around the explain-point.

## References

1. <https://en.wikipedia.org/wiki/Shapley_value>
2. S. Lundberg, S Lee. ‚ÄúA Unified Approach to Interpreting Model Predictions.‚Äù NeurIPS, 2017 <http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf>
3. L. Merrick  and A. Taly ‚ÄúThe Explanation Game: Explaining Machine Learning Models Using Shapley Values‚Äù <https://arxiv.org/abs/1909.08128>
4. M. Sundararajan, A. Taly, Q. Yan ‚ÄúAxiomatic Attribution for Deep Networks‚Äù  <http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf>

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Point Explanations""
slug: ""point-explanations""
excerpt: """"
hidden: true
createdAt: ""Fri Dec 16 2022 23:19:37 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:41:45 GMT+0000 (Coordinated Universal Time)""
---
[block:parameters]
{
  ""data"": {
    ""h-0"": """",
    ""h-1"": ""Model Input Type"",
    ""h-2"": ""Default Reference Sice"",
    ""h-3"": ""Permutations"",
    ""0-0"": ""Point Explanations  \nSHAP and Fiddler SHAP"",
    ""0-1"": ""Tabular"",
    ""0-2"": ""200"",
    ""0-3"": """",
    ""1-0"": """",
    ""1-1"": ""Text"",
    ""1-2"": ""200"",
    ""1-3"": """",
    ""2-0"": ""Global Explanations  \nRandom Ablation Feature Impact (and Importance for models with tabular inputs)"",
    ""2-1"": ""Tabular"",
    ""2-2"": ""10K"",
    ""2-3"": """",
    ""3-0"": """",
    ""3-1"": ""Text"",
    ""3-2"": ""200"",
    ""3-3"": """"
  },
  ""cols"": 4,
  ""rows"": 4,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


# How to Quantify Prediction Impact of a Feature?

One approach for explaining the prediction of a machine learning model is to measure the influence that each of its inputs has on the prediction made. This is called Feature Impact.

- SHAP and Fiddler SHAP, using Kernel SHAP implementations, are game-theory based methods. They work for all models, because they only require the ability to ask a model for predictions.
- Integrated Gradients, which is particularly performant for deep learning models with a large number of inputs. It requires the model‚Äôs prediction to be mathematically differentiable, and a prediction gradient must be made available to Fiddler.
- Tree SHAP, is not enabled by default but can be used for Tree-based model. This is a faster and model-specific method to approximate Shapley values.

These methods are discussed in more detail below.

In addition to the previous out of the box explanation methods, Fiddler allows to bring your own explanation method. This can be customized in your model‚Äôs `package.py` wrapper script.

## Tabular Models

## Language (NLP) Models

## Point Explanation Methods:

**Introduction**

To measure Feature Impact, **additive attribution methods** can be quite powerful. Fiddler includes:

- **SHAP** and **Fiddler SHAP**, which require only the ability to ask a model for predictions, and are thus suitable across all types of models; no knowledge of the model implementation is necessary.
- **Integrated Gradients**, a method that takes advantage of the gradient vector of the prediction, which is typically available in deep learning models, to efficiently explain complex models with large input dimensionality.

**Additive Attributions**

To explain a prediction with an additive attribution method, we look at how individual features contribute to the _prediction difference_. The prediction difference is a comparison between the prediction as a point in feature space (we refer to this as the _explain-point_), and a counterfactual baseline position (or a distribution of positions),"
"slug: ""point-explanations""  representing an uninteresting or typical model inference.

Each feature is assigned a fraction of the prediction difference for which it is responsible. This fraction is called the feature attribution, and it‚Äôs what we show in our explanations.

Additive attribution methods have the following characteristics:

- The sum of feature attributions always equals the prediction difference.
- Features that have no effect on a model‚Äôs prediction receive a feature attribution of zero.
- Features that have the identical effect receive the same attribution.
- Features with mutual information share the attribution for any effect that information has on the prediction.

Additionally, each of these methods takes into account interactions between the features (e.g. two features that have no effect individually but in combination change the model output). This is explicitly built into the Shapley value formalism, and is captured in the path integral over gradients in Integrated Gradients.

**Shapley Values and their Approximation**

The Shapley value[<sup>\[1\]</sup>](#references) (proposed by Lloyd Shapley in 1953) is one way to derive feature attributions. Shapley values distribute the total payoff of a collaborative game across a coalition of cooperating players. They are computed by tabulating the average gain in payoff when a particular player is added to the coalition, over all coalition sizes and permutations of players.

In our case, we consider the ‚Äútotal gains‚Äù to be the prediction value, and a ‚Äúplayer‚Äù is a single model feature. The collaborative ‚Äúgame‚Äù is all of the model features cooperating to form a prediction value.

Why do we create ‚Äúcoalitions‚Äù with only a subset of the features? In some scenarios, it may be appropriate to replace a feature with a zero value when removed from the coalition (e.g. text models where no mask token is available). In others (e.g. models with dense tabular inputs), values are swapped in from a reference distribution or baseline example as a zero value may have a specific meaning (like zero income on a credit application).

Shapley values have desirable properties including:

- **_Linearity_**: If two games are combined, then the total gains correspond to the gains derived from a linear combination of the gains of each game.
- **_Efficiency_**: The sum of the values of all players equals the value of the grand coalition, so that all the gain is distributed among the players. In our case, the efficiency property says _the feature attributions should sum to the prediction value_. The attributions can be positive or negative, since a feature can raise or lower a predicted value.

**Approximating Shapley Values**

Computation of exact Shapley values can be extremely computationally expensive‚Äîin fact, exponentially so, in the number of input features. Fiddler makes two approximation methods available:

- **SHAP**[<sup>\[2\]</sup>](#references) (SHapely Additive exPlanations) approximates Shapley values by sampling coalitions according to a combinatorially weighted kernel (compensating for the number of permutations of features in coalitions of different cardinality). It samples the feature space uniformly between baseline-like feature vectors and explain-point-like feature vectors. This has the effect of downsampling behavior in the immediate vicinity of the explain-point, a region where the model may be saturated or uniform in its prediction, and attributions may not be helpful.
- **Fiddler SHAP**[<sup>\[3\]</sup>](#references) builds on the SHAP approach and is optimized for computing distributions of Shapley values for each feature by comparing the explain-point against a distribution of baselines. This makes it"
"slug: ""point-explanations""  possible to compute confidence intervals around the mean attribution for each feature and identify clusters in attribution space where distinct, individually relevant explanations might be important (e.g. ‚Äúyour loan application was rejected for a set of reasons when compared to applications in your region, and for another set of reasons when compared to applications with the same profession‚Äù).

Approximate Shapley value methods can be used to explain nearly any model, since you only need to be able to ask the model for predictions at a variety of positions in the feature space.

**Integrated Gradients**

Another additive attribution method: the Integrated Gradients method.

For models whose prediction is continuous and piecewise differentiable in the feature space, it can be useful to provide additional information through the gradient (slope vector) of a prediction.

Fiddler supports Integrated Gradients (IG)[<sup>\[4\]</sup>](#references). In this method, an approximate integral tabulates components of the slope along a linear path from baseline to explain-point, and attributes them to respective input features. This has several advantages:

1. For models with very high dimensional feature volumes (e.g. images, text), where differentiable deep-learning models typically excel, this method can be very performant (O(N) vs. the O(2^n) of the Shapley methods)
2. Attributions can be computed for intermediate layers within the model, providing fine-grained model diagnostics. This is naturally extensible to models with hybrid and multimodal inputs.
3. In comparison to local gradients and saliency methods, the IG path integral samples the large-scale behavior of the model and is resistant to amplifying noise in the possibly saturated region around the explain-point.

## References

1. <https://en.wikipedia.org/wiki/Shapley_value>
2. S. Lundberg, S Lee. ‚ÄúA Unified Approach to Interpreting Model Predictions.‚Äù NeurIPS, 2017 <http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf>
3. L. Merrick  and A. Taly ‚ÄúThe Explanation Game: Explaining Machine Learning Models Using Shapley Values‚Äù <https://arxiv.org/abs/1909.08128>
4. M. Sundararajan, A. Taly, Q. Yan ‚ÄúAxiomatic Attribution for Deep Networks‚Äù  <http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf>

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Embedding Visualization with UMAP""
slug: ""embedding-visualization-with-umap""
excerpt: """"
hidden: false
createdAt: ""Tue Nov 14 2023 04:38:29 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Introduction to embedding visualization

Embedding visualization is a powerful technique used to understand and interpret complex relationships in high-dimensional data. Reducing the dimensionality of custom features into a 2D or 3D space makes it easier to identify patterns, clusters, and outliers.

In Fiddler, high-dimensional data like embeddings and vectors are ingested as a [Custom feature](ref:fdlcustomfeaturetype).

Our goal in this document is to visualize these custom features.

## UMAP Technique for embedding visualization

We utilize the [UMAP](https://umap-learn.readthedocs.io/en/latest/) (Uniform Manifold Approximation and Projection) technique for embedding visualizations. UMAP is a dimension reduction technique that is particularly good at preserving the local structure of the data, making it ideal for visualizing embeddings. We reduce the high-dimensional embeddings to a 3D space.

UMAP is supported for both Text and Image embeddings in [Custom feature](ref:fdlcustomfeaturetype).

> üìò To create an embedding visualization chart
> 
> Follow the UI Guide on [creating the embedding visualization chart here.](doc:embedding-visualization-chart-creation)
"
"---
title: ""Traffic""
slug: ""traffic-platform""
excerpt: ""Platform Guide""
hidden: false
createdAt: ""Mon Dec 19 2022 19:28:11 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:37:50 GMT+0000 (Coordinated Universal Time)""
---
Traffic as a service metric gives you basic insights into the operational health of your ML service in production.

![](https://files.readme.io/d2c1eaa-Screenshot_2023-02-01_at_5.13.34_PM.png)

## What is being tracked?

- **_Traffic_** ‚Äî The volume of traffic received by the model over time.

## Why is it being tracked?

- Traffic is a basic high-level metric that informs us of the overall system's health.

## What steps should I take when I see an outlier?

- A dip or spike in traffic needs to be investigated. For example, a dip could be due to a production model server going down; a spike could be an adversarial attack.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Custom Metrics""
slug: ""custom-metrics""
excerpt: """"
hidden: false
createdAt: ""Thu Oct 26 2023 16:21:43 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Tue Dec 19 2023 15:28:35 GMT+0000 (Coordinated Universal Time)""
---
# Overview

Fiddler offers the ability to customize metrics for your specific use case.

# Language

Fiddler Custom Metrics are constructed using the [Fiddler Query Language (FQL)](doc:fiddler-query-language). 

# Adding a Custom Metric

**Note:** To add a Custom Metric using the Python client, see [client.add_custom_metric](ref:clientadd_custom_metric)

From the Model page, you can access Custom Metrics by clicking the **Custom Metrics** tab at the top of the page.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/e0b5069-Screen_Shot_2023-11-20_at_1.49.03_PM.png"",
        """",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


Then click **Add Custom Metric** to add a new Custom Metric.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/f91ffdb-Screen_Shot_2023-11-20_at_1.51.09_PM.png"",
        """",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


Finally, enter the Name, Description, and Definition for your Custom Metric and click **Save**.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/d1cc8f7-Screen_Shot_2023-11-20_at_1.52.40_PM.png"",
        """",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


# Accessing Custom Metrics in Charts and Alerts

After your Custom Metric is saved, it can be selected from Charts and Alerts.

**Charts:**

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/0563507-Screen_Shot_2023-12-18_at_1.23.49_PM.png"",
        """",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


**Alerts:**

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/8ae37f9-Screen_Shot_2023-12-18_at_1.26.27_PM.png"",
        """",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


# Modifying Custom Metrics

Since alerts can be set on Custom Metrics, making modifications to a metric may introduce inconsistencies in alerts.

Therefore, **Custom Metrics cannot be modified once they are created**.

If you'd like to try out a new metric, you can create a new one with a different Definition.

# Deleting Custom Metrics

**Note:** To delete a Custom Metric using the Python client, see [client.delete_custom_metric](ref:clientdelete_custom_metric)

From the Custom Metrics tab, you can delete a metric by clicking the trash icon next to the metric of interest.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/f646c51"
"slug: ""custom-metrics"" -Screen_Shot_2023-11-21_at_12.16.34_PM.png"",
        """",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]
"
"---
title: ""Alerts""
slug: ""alerts-platform""
excerpt: """"
hidden: false
createdAt: ""Fri Jan 27 2023 19:53:56 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:15:43 GMT+0000 (Coordinated Universal Time)""
---
Fiddler enables users to set up alert rules to track a model's health and performance over time. Fiddler alerts also enable users to dig into triggered alerts and perform root cause analysis to discover what is causing a model to degrade. Users can set up alerts using both the [Fiddler UI](doc:alerts-ui) and the [Fiddler API Client](doc:alerts-client).

## Supported Metric Types

You can get alerts for the following metrics:

- [**Data Drift**](doc:data-drift)  ‚Äî Predictions and all features
  - Model performance can be poor if models trained on a specific dataset encounter different data in production.
- [**Performance**](doc:performance) 
  - Model performance tells us how well a model is doing on its task. A poorly performing model can have significant business implications.
- [**Data Integrity**](doc:data-integrity)  ‚Äî All features
  - There are three types of violations that can occur at model inference: missing feature values, type mismatches (e.g. sending a float input for a categorical feature type) or range mismatches (e.g. sending an unknown US State for a State categorical feature).
- [**Service Metrics**](doc:traffic-platform) 
  - The volume of traffic received by the model over time that informs us of the overall system health.

## Supported Comparison Types

You have two options for deciding when to be alerted:

1. **Absolute** ‚Äî Compare the metric to an absolute value
   1. e.g. if traffic for a given hour is less than 1000, then alert.
2. **Relative** ‚Äî Compare the metric to a previous time period
   1. e.g. if traffic is down 10% or more than it was at the same time one week ago, then alert.

You can set the alert threshold in either case.

## Alert Rule Priority

Whether you're setting up an alert rule to keep tabs on a model in a test environment, or data for production scenarios, Fiddler has you covered. Easily set the Alert Rule Priority to indicate the importance of any given Alert Rule. Users can select from Low, Medium, and High priorities. 

## Alert Rule Severity

For additional flexibility, users can now specify up to two threshold values, **Critical** and **Warning** severities. Critical severity is always required when setting up an Alert Rule, but Warning can be optionally set as well.

## Why do we need alerts?

- It‚Äôs not possible to manually track all metrics 24/7.
- Sensible alerts are your first line of defense, and they are meant to warn about issues in production.

## What should I do when I receive an alert?

- Click on the link in the email to go to the tab where the alert originated (e.g. Data Drift). 
- Under the Monitoring tab, more information can be obtained from the drill down below the main chart.
- You can also examine the data in the Analyze tab. You can use SQL to slice and dice the data, and use custom visualization tools and operators to make sense of the model‚Äôs behavior within the time range under consideration.

## Sample Alert Email

Here's a sample of an email that's sent if an alert is triggered:

![](https://files.read"
"slug: ""alerts-platform"" me.io/9dfc566-Monitor_Alert_Email_0710.png ""Monitor_Alert_Email_0710.png"")

## Integrations

Fiddler supports the following alert notification integrations:

- Email
- Slack
- PagerDuty
"
"---
title: ""Performance Tracking""
slug: ""performance-tracking-platform""
excerpt: ""Platform Guide""
hidden: false
createdAt: ""Mon Dec 19 2022 19:27:22 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:38:06 GMT+0000 (Coordinated Universal Time)""
---
## What is being tracked?

![](https://files.readme.io/4a646d4-qs_monitoring.png ""qs_monitoring.png"")

- **_Decisions_** - The post-prediction business decisions made as a result of the model output. Decisions are calculated before [client.publish_event()](ref:clientpublish_event) (they're not inferred by Fiddler). For binary classification models, a decision is usually determined using a threshold. For multi-class classification models, it's usually determined using the argmax value of the model outputs.

- **_Performance metrics_**

| Model Task Type       | Metric                                                         | Description                                                                                                                                        |
| :-------------------- | :------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------- |
| Binary Classification | Accuracy                                                       | (TP + TN) / (TP + TN + FP + FN)                                                                                                                    |
| Binary Classification | True Positive Rate/Recall                                      | TP / (TP + FN)                                                                                                                                     |
| Binary Classification | False Positive Rate                                            | FP / (FP + TN)                                                                                                                                     |
| Binary Classification | Precision                                                      | TP / (TP + FP)                                                                                                                                     |
| Binary Classification | F1 Score                                                       | 2  \* ( Precision \*  Recall ) / ( Precision + Recall )                                                                                            |
| Binary Classification | AUROC                                                          | Area Under the Receiver Operating Characteristic (ROC) curve, which plots the true positive rate against the false positive rate                   |
| Binary Classification | Binary Cross Entropy                                           | Measures the difference between the predicted probability distribution and the true distribution                                                   |
| Binary Classification | Geometric Mean                                                 | Square Root of ( Precision \* Recall )                                                                                                             |
| Binary Classification | Calibrated Threshold                                           | A threshold that balances precision and recall at a particular operating point                                                                     |
| Binary Classification | Data Count                                                     | The number of events where target and output are both not NULL. **_This will be used as the denominator when calculating accuracy_**.              |
| Binary Classification | Expected Calibration Error                                     | Measures the difference between predicted probabilities and empirical probabilities                                                                |
| Multi Classification  | Accuracy                                                       | (Number of correctly classified samples) / ( Data Count ). Data Count refers to the number of events where the target and output are both not NULL |
| Multi Classification  | Log Loss                                                       | Measures the difference between the predicted probability distribution and the true distribution, in a logarithmic scale                           |
| Regression            | Coefficient of determination (R-squared)                       | Measures the proportion of variance in the dependent variable that is explained by the independent variables                                       |
| Regression            | Mean Squared Error (MSE)                                       | Average of the squared differences between the predicted and true values                                                                           |
| Regression            | Mean Absolute Error (MAE)                                      | Average of the absolute differences between the predicted and true values                                                                          |
| Regression            | Mean Absolute Percentage Error (MAPE)                          | Average of the absolute percentage differences between the predicted and true values                                                               |
| Regression            | Weighted Mean Absolute Percentage Error (WMAPE)                | The weighted average of the absolute percentage differences between the predicted and true values                                                  |
| Ranking               | Mean Average Precision (MAP)‚Äîfor binary relevance ranking only | Measures the average precision of the relevant items in the top-k results                                                                          |
| Ranking               | Normalized Discounted"
"slug: ""performance-tracking-platform""  Cumulative Gain (NDCG)                   | Measures the quality of the ranking of the retrieved items, by discounting the relevance scores of items at lower ranks                            |

## Why is it being tracked?

- Model performance tells us how well a model is doing on its task. A poorly performing model can have significant business implications.
- The volume of decisions made on the basis of the predictions give visibility into the business impact of the model.

## What steps should I take based on this information?

- For decisions, if there is an increase or decrease in approvals, we can cross-check with the average prediction and prediction drift trendlines on the [Data Drift Tab](doc:data-drift). In general, the average prediction value should increase with an increase in the number of approvals, and vice-versa.
- For changes in model performance‚Äîagain, the best way to cross-verify the results is by checking the [Data Drift Tab](doc:data-drift) ). Once you confirm that the performance issue is not due to the data, you need to assess if the change in performance is due to temporary factors, or due to longer-lasting issues.
- You can check if there are any lightweight changes you can make to help recover performance‚Äîfor example, you could try modifying the decision threshold.
- Retraining the model with the latest data and redeploying it is usually the solution that yields the best results, although it may be time-consuming and expensive.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Monitoring with Statistics""
slug: ""statistics""
excerpt: """"
hidden: false
createdAt: ""Thu Oct 05 2023 13:28:07 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Wed Dec 20 2023 17:18:45 GMT+0000 (Coordinated Universal Time)""
---
Fiddler supports some simple statistic metrics which can be used to monitor basic aggregations over Columns.

These can be particularly useful when you have a custom metadata field which you would like to monitor over time in addition to Fiddler's other out-of-the-box metrics.

Specifically, we support

- Average (takes the arithmetic mean of a numeric column)
- Sum (calculates the sum of a numeric column)
- Frequency (shows the count of occurrences for each value in a categorical or boolean column)

These metrics can be accessed in Charts and Alerts by selecting the Statistic Metric Type.

**Monitoring a Statistic Metric in Charts:**

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/453a99d-Screen_Shot_2023-10-26_at_1.37.08_PM.png"",
        """",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


**Creating an Alert on a Statistic Metric:**

Alert rules can be established based on statistics too.  Like an alert rule, these can be setup using the Fiddler UI, the Fiddler python client or using Fiddler's REST-ful API.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/2b19cf0-Screen_Shot_2023-12-19_at_2.31.43_PM.png"",
        """",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]
"
"---
title: ""Vector Monitoring""
slug: ""vector-monitoring-platform""
excerpt: ""\""Patented Fiddler Technology\""""
hidden: false
createdAt: ""Mon Dec 19 2022 19:22:52 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
# Vector Monitoring for Unstructured Data

While Fiddler calculates data drift at deployment time for numerical features that are stored in columns of the baseline dataset, many modern machine learning systems use input features that cannot be represented as a single number (e.g., text or image data). Such complex features are usually rather represented by high-dimensional vectors which are obtained by applying a vectorization method (e.g., text embeddings generated by NLP models). Furthermore, Fiddler users might be interested in monitoring a group of univariate features together and detecting data drift in multi-dimensional feature spaces.

In order to address the above needs, Fiddler provides vector monitoring capability which involves enabling users to define custom features, and a novel method for monitoring data drift in multi-dimensional spaces.

### Defining Custom Features

Users can use the Fiddler client to define one or more custom features. Each custom feature is specified by a group of dataset columns that need to be monitored together as a vector. Once a list of custom features is defined and passed to Fiddler (the details of how to use the Fiddler client to define custom features are provided in the following.), Fiddler runs a clustering-based data drift detection algorithm for each custom feature and calculates a corresponding drift value between the baseline and the published events at the selected time period.

```Text Python
CF1 = fdl.CustomFeature.from_columns(['f1','f2','f3'], custom_name = 'vector1')
CF2 = fdl.CustomFeature.from_columns(['f1','f2','f3'], n_clusters=5, custom_name = 'vector2')
CF3 = fdl.TextEmbedding(name='text_embedding',column='embedding',source_column='text')
CF4 = fdl.ImageEmbedding(name='image_embedding',column='embedding',source_column='image_url')
```

### Passing Custom Features List to Model Info

```python
model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id = DATASET_ID,
    features = data_cols,
    target='target',
    outputs='predicted_score',
    custom_features = [CF1,CF2,CF3,CF4]
)
```

> üìò Quick Start for NLP Monitoring
> 
> Check out our [Quick Start guide for NLP monitoring](doc:simple-nlp-monitoring-quick-start) for a fully functional notebook example.
"
"---
title: ""Class-Imbalanced Data""
slug: ""class-imbalanced-data""
excerpt: """"
hidden: false
createdAt: ""Tue Jul 05 2022 17:20:48 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:38:26 GMT+0000 (Coordinated Universal Time)""
---
## Monitoring class-imbalanced models

Drift is a measure of how different the production distribution is from the baseline distribution on which the model was trained. In practice, the distributions are approximated using histograms and then compared using divergence metrics like Jensen‚ÄìShannon divergence or Population Stability Index. Generally, when constructing the histograms, every event contributes equally to the bin counts.

However, for scenarios with large class imbalance the minority class‚Äô contribution to the histograms would be minimal. Hence, any change in production distribution with respect to the minority class would not lead to a significant change in the production histograms. Consequently, even if there is a significant change in distribution with respect to the minority class, the drift value would not change significantly.

To solve this issue, Fiddler monitoring provides a way for events to be weighted based on the class distribution. For such models, when computing the histograms, events belonging to the minority class would be up-weighted whereas those belonging to the majority class would be down-weighted.

Fiddler has implemented two solutions for class imbalance use cases.

**Workflow 1: User provided global class weights**  
The user computes the class distribution on baseline data and then provides the class weights via the Model-Info object.  
Class weights can either be manually entered by the user or computed from their dataset

- Please refer to the API docs on how to [specify global class-weights](/reference/fdlweightingparams)

- To tease out drift in a class-imbalanced fraud usecase checkout out the [class-imbalanced-fraud-notebook](https://colab.research.google.com/github/fiddler-labs/fiddler-examples/blob/main/quickstart/Fiddler_Quickstart_Imbalanced_Data.ipynb)

**Workflow 2: User provided event level weights**  
User provides event level weights as a metadata column in baseline data and provides them while publishing events  
Details:

- Users will add an ""\_\_weight"" column in their model_info (must be a metadata type column, and must be nullable=True).

- The reference data needs to have an ""\_\_weight"" column, which may never be all null/missing/NaN  weights; all rows must contain valid float values. We expect the user to enforce this assumption.

- Note that the use of weighting parameters requires the presence of model outputs for both workflows in the baseline dataset.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Data Integrity""
slug: ""data-integrity-platform""
excerpt: ""platform guide""
hidden: false
createdAt: ""Mon Dec 19 2022 18:33:03 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:37:58 GMT+0000 (Coordinated Universal Time)""
---
ML models are increasingly driven by complex feature pipelines and automated workflows that involve dynamic data. Data is transformed from source to model input, which can result in data inconsistencies and errors.

There are three types of violations that can occur at model inference: **missing feature values**, **type mismatches** (e.g. sending a float input for a categorical feature type) or **range mismatches** (e.g. sending an unknown US State for a State categorical feature).

You can track all these violations in the Data Integrity tab. The time series shown above tracks the violations of data integrity constraints set up for this model.

## What is being tracked?

![](https://files.readme.io/8a59eb0-Monitoring_DataIntegrity.png ""Monitoring_DataIntegrity.png"")

The time series above tracks the violations of data integrity constraints set up for this model.

- **_Missing value violations_** ‚Äî The percentage of missing value violations over all features for a given period of time.
- **_Type violations_** ‚Äî The percentage of data type mismatch violations over all features for a given period of time.
- **_Range violations_** ‚Äî The percentage of range mismatch violations over all features for a given period of time.
- **_All violating events_** ‚Äî An aggregation of all the data integrity violations above for a given period of time.

## Why is it being tracked?

- Data integrity issues can cause incorrect data to flow into the model, which can lead to poor model performance and have a negative impact on the business or end-user experience. 

## How does it work?

It can be tedious to set up constraints for individual features when they number in the tens or hundreds. To avoid this, you can provide Fiddler with a baseline dataset that's representative of the data you expect your model to infer on in production. This should be sampled from your model's training set, and can be [uploaded to Fiddler using the Python API client](ref:clientupload_dataset).

Fiddler will automatically generate constraints based on the distribution of data in this dataset.

- **Missing values**: If a feature has no missing values, then the data integrity violation will be set up to trigger when any missing values are seen. Similarly, if the feature has 50% of its values missing, then the data integrity violation will be set up to trigger when more than 50% of the values encountered are missing in a specified time range.
- **Type mismatch**: A data integrity violation will be triggered when the type of a feature value differs from what was specified for that feature in the baseline dataset.
- **Range mismatch**: 
  - For categorical features, a data integrity violation will be triggered when it sees any value other than the ones specified in the baseline. 
  - For continuous variables, the violation will be triggered if the values are outside the range specified in the baseline. 
  - For [vector datatype](ref:fdldatatype), a range mismatch will be triggered when a dimension mismatch occurs compared to the expected dimension from the baseline.

## What steps should I take with this information?

- The visualization above informs us of the feature-wise breakdown of the violations. The raw counts of the violations are shown in parentheses.
- If there is a spike in violations, or an unexpected violation occurs (such as missing values"
"slug: ""data-integrity-platform""  for a feature that doesn‚Äôt accept a missing value), then a deeper examination of the feature pipeline may be required.
- You can also drill down deeper into the data by examining it in the **Analyze** tab. We can use SQL to slice and dice the data and try to find the root cause of the issues.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Data Drift""
slug: ""data-drift-platform""
excerpt: ""Platform Guide""
hidden: false
createdAt: ""Mon Dec 19 2022 19:26:33 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:37:27 GMT+0000 (Coordinated Universal Time)""
---
Model performance can be poor if models trained on a specific dataset encounter different data in production. This is called data drift. 

## What is being tracked?

Fiddler supports the following:

- **_Drift Metrics_**
  - **Jensen‚ÄìShannon distance (JSD)**
    - A distance metric calculated between the distribution of a field in the baseline dataset and that same distribution for the time period of interest.
    - For more information on JSD, click [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html).
  - **Population Stability Index (PSI)**
    - A drift metric based on the multinomial classification of a variable into bins or categories. The differences in each bin between the baseline and the time period of interest are then utilized to calculate it as follows:

> üöß Note
> 
> There is a possibility that PSI can shoot to infinity. To avoid this, PSI calculation in Fiddler is done such that each bin count is incremented with a base_count=1. Thus, there might be a slight difference in the PSI values obtained from manual calculations.

- **_Average Values_** ‚Äì The mean of a field (feature or prediction) over time. This can be thought of as an intuitive drift score.
- **_Drift Analytics_** ‚Äì You can drill down into the features responsible for the prediction drift using the table at the bottom.
  - **_Feature Impact_**: The contribution of a feature to the model‚Äôs predictions, averaged over the baseline dataset. The contribution is calculated using random ablation feature impact.
  - **_Feature Drift_**: Drift of the feature, calculated using the drift metric of choice.
  - **_Prediction Drift Impact_**: A heuristic calculated using the product of the feature impact and the feature drift. The higher the score, the more this feature is likely to have contributed to the prediction drift.

## Why is it being tracked?

- Data drift is a great proxy metric for **performance decline**, especially if there is delay in getting labels for production events. (e.g. In a credit lending use case, an actual default may happen after months or years.)
- Monitoring data drift also helps you stay informed about **distributional shifts in the data for features of interest**, which could have business implications even if there is no decline in model performance. 

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Fiddler Query Language (FQL)""
slug: ""fiddler-query-language""
excerpt: """"
hidden: false
createdAt: ""Mon Nov 20 2023 18:46:44 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Tue Dec 19 2023 15:22:08 GMT+0000 (Coordinated Universal Time)""
---
# Overview

[Custom Metrics](doc:custom-metrics) are defined using the **Fiddler Query Language (FQL)**, a flexible set of constants, operators, and functions which can accommodate a large variety of metrics.

# Definitions

| Term               | Definition                                                                             |
| :----------------- | :------------------------------------------------------------------------------------- |
| Row-level function | A function which executes row-wise for a set of data. Returns a value for each row.    |
| Aggregate function | A function which executes across rows. Returns a single value for a given set of rows. |

# FQL Rules

- Every metric must return either an aggregate or a combination of aggregates (see Aggregate functions below). To clarify, you may not define a metric using purely row-level functions.
- [Column](ref:fdlcolumn) names can be referenced by name either with double quotes (""my_column"") or with no quotes (my_column).
- Single quotes (') are used to represent string values.

# Examples

## Simple Example

Let‚Äôs say you wanted to create a Custom Metric for the following metric definition:

- If an event is a false negative, assign a value of -40. If the event is a false positive, assign a value of -400.  If the event is a true positive or true negative, then assign a value of 250.

We can formulate this metric using FQL with the following code:

`average(if(Prediction < 0.5 and Target == 1, -40, if(Prediction >= 0.5 and Target == 0, -400, 250)))`

(Here, we assume `Prediction` is the name of the output column for a binary classifier and `Target` is the name of our label column.)

## Null Violation Flag

This metric returns `1` if a given time bin has at least one null value in the `column1` column. Otherwise, it returns `0`.

`if(null_violation_count(column1) > 0, 1, 0)`

## Tweedie Loss

An implementation of the Tweedie Loss Function. Here, `Target` is the name of the target column and `Prediction` is the name of the prediction/output column.

`average((Target * Prediction ^ (1 - 0.5)) / (1 - 0.5) + Prediction ^ (2 - 0.5) / (2 - 0.5))`

# Data Types

FQL distinguishes between three data types:

[block:parameters]
{
  ""data"": {
    ""h-0"": ""Data type"",
    ""h-1"": ""Supported values"",
    ""h-2"": ""Examples"",
    ""h-3"": ""Supported Model Schema Data Types"",
    ""0-0"": ""Number"",
    ""0-1"": ""Any numeric value (integers and floats are both included)"",
    ""0-2"": ""`10`  \n`2.34`"",
    ""0-3"": ""[`Data.Type.INTEGER`](ref:fdldatatype)  \n[`DataType.FLOAT`](ref:fdldatatype)"",
    ""1-0"": ""Boolean"",
    ""1-1"": ""Only `true` and `false"
"slug: ""fiddler-query-language"" `"",
    ""1-2"": ""`true`  \n`false`"",
    ""1-3"": ""[`DataType.BOOLEAN`](ref:fdldatatype)"",
    ""2-0"": ""String"",
    ""2-1"": ""Any value wrapped in single quotes (`'`)"",
    ""2-2"": ""`'This is a string.'`  \n`'200.0'`"",
    ""2-3"": ""[`DataType.CATEGORY`](ref:fdldatatype)  \n[`DataType.STRING`](ref:fdldatatype)""
  },
  ""cols"": 4,
  ""rows"": 3,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


# Constants

| Symbol  | Description                            |
| :------ | :------------------------------------- |
| `true`  | Boolean constant for true expressions  |
| `false` | Boolean constant for false expressions |

# Operators

[block:parameters]
{
  ""data"": {
    ""h-0"": ""Symbol"",
    ""h-1"": ""Description"",
    ""h-2"": ""Syntax"",
    ""h-3"": ""Returns"",
    ""h-4"": ""Examples"",
    ""0-0"": ""`^`"",
    ""0-1"": ""Exponentiation"",
    ""0-2"": ""`Number ^ Number`"",
    ""0-3"": ""`Number`"",
    ""0-4"": ""`2.5 ^ 4`  \n`(column1 - column2)^2`"",
    ""1-0"": ""`-`"",
    ""1-1"": ""Unary negation"",
    ""1-2"": ""`-Number`"",
    ""1-3"": ""`Number`"",
    ""1-4"": ""`-column1`"",
    ""2-0"": ""`*`"",
    ""2-1"": ""Multiplication"",
    ""2-2"": ""`Number * Number`"",
    ""2-3"": ""`Number`"",
    ""2-4"": ""`2 * 10`  \n`2 * column1`  \n`column1 * column2`  \n`sum(column1) * 10`"",
    ""3-0"": ""`/`"",
    ""3-1"": ""Division"",
    ""3-2"": ""`Number / Number`"",
    ""3-3"": ""`Number`"",
    ""3-4"": ""`2 / 10`  \n`2 / column1`  \n`column1 / column2`  \n`sum(column1) / 10`"",
    ""4-0"": ""`%`"",
    ""4-1"": ""Modulo"",
    ""4-2"": ""`Number % Number`"",
    ""4-3"": ""`Number`"",
    ""4-4"": ""`2 % 10`  \n`2 % column1`  \n`column1 % column2`  \n`sum(column1) % 10`"",
    ""5-0"": ""`+`"",
    ""5-1"": ""Addition"",
    ""5-2"": ""`Number + Number`"",
    ""5-3"": ""`Number`"",
    ""5-4"": ""`2 + 2`  \n`2 + column1`  \n`column1 + column2`  \n`average(column1) + 2`"",
    ""6-0"": ""`-`"",
    ""6-1"": ""Subtraction"",
    """
"slug: ""fiddler-query-language"" 6-2"": ""`Number - Number`"",
    ""6-3"": ""`Number`"",
    ""6-4"": ""`2 - 2`  \n`2 - column1`  \n`column1 - column2`  \n`average(column1) - 2`"",
    ""7-0"": ""`<`"",
    ""7-1"": ""Less than"",
    ""7-2"": ""`Number < Number`"",
    ""7-3"": ""`Boolean`"",
    ""7-4"": ""`10 < 20`  \n`column1 < 10`  \n`column1 < column2`  \n`average(column2) < 5`"",
    ""8-0"": ""`<=`"",
    ""8-1"": ""Less than or equal to"",
    ""8-2"": ""`Number <= Number`"",
    ""8-3"": ""`Boolean`"",
    ""8-4"": ""`10 <= 20`  \n`column1 <= 10`  \n`column1 <= column2`  \n`average(column2) <= 5`"",
    ""9-0"": ""`>`"",
    ""9-1"": ""Greater than"",
    ""9-2"": ""`Number > Number`"",
    ""9-3"": ""`Boolean`"",
    ""9-4"": ""`10 > 20`  \n`column1 > 10`  \n`column1 > column2`  \n`average(column2) > 5`"",
    ""10-0"": ""`>=`"",
    ""10-1"": ""Greater than or equal to"",
    ""10-2"": ""`Number >= Number`"",
    ""10-3"": ""`Boolean`"",
    ""10-4"": ""`10 >= 20`  \n`column1 >= 10`  \n`column1 >= column2`  \n`average(column2) >= 5`"",
    ""11-0"": ""`==`"",
    ""11-1"": ""Equals"",
    ""11-2"": ""`Number == Number`"",
    ""11-3"": ""`Boolean`"",
    ""11-4"": ""`10 == 20`  \n`column1 == 10`  \n`column1 == column2`  \n`average(column2) == 5`"",
    ""12-0"": ""`!=`"",
    ""12-1"": ""Does not equal"",
    ""12-2"": ""`Number != Number`"",
    ""12-3"": ""`Boolean`"",
    ""12-4"": ""`10 != 20`  \n`column1 != 10`  \n`column1 != column2`  \n`average(column2) != 5`"",
    ""13-0"": ""`not`"",
    ""13-1"": ""Logical NOT"",
    ""13-2"": ""`not Boolean`"",
    ""13-3"": ""`Boolean`"",
    ""13-4"": ""`not true`  \n`not column1`"",
    ""14-0"": ""`and`"",
    ""14-1"": ""Logical AND"",
    ""14-2"": ""`Boolean and Boolean`"",
    ""14-3"": ""`Boolean`"",
    ""14-4"": ""`true and false`  \n`column1 and column2`"",
    ""15-0"": ""`or`"",
    ""15-1"": ""Logical OR"",
    ""15-2"": ""`Boolean or Boolean`"",
    ""15"
"slug: ""fiddler-query-language"" -3"": ""`Boolean`"",
    ""15-4"": ""`true or false`  \n`column1 or column2`""
  },
  ""cols"": 5,
  ""rows"": 16,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


# Constant functions

| Symbol | Description                                           | Syntax | Returns  | Examples                    |
| :----- | :---------------------------------------------------- | :----- | :------- | :-------------------------- |
| `e()`  | Base of the natural logarithm                         | `e()`  | `Number` | `e() == 2.718281828459045`  |
| `pi()` | The ratio of a circle's circumference to its diameter | `pi()` | `Number` | `pi() == 3.141592653589793` |

# Row-level functions

Row-level functions can be applied either to a single value or to a column/row expression (in which case they are mapped element-wise to each value in the column/row expression).

[block:parameters]
{
  ""data"": {
    ""h-0"": ""Symbol"",
    ""h-1"": ""Description"",
    ""h-2"": ""Syntax"",
    ""h-3"": ""Returns"",
    ""h-4"": ""Examples"",
    ""0-0"": ""`if(condition, value1, value2)`"",
    ""0-1"": ""Evaluates `condition` and returns `value1` if true, otherwise returns `value2`.  \n`value1` and `value2` must have the same type."",
    ""0-2"": ""`if(Boolean, Any, Any)`"",
    ""0-3"": ""`Any`"",
    ""0-4"": ""`if(false, 'yes', 'no') == 'no'`  \n`if(column1 == 1, 'yes', 'no')`"",
    ""1-0"": ""`length(x)`"",
    ""1-1"": ""Returns the length of string `x`."",
    ""1-2"": ""`length(String)`"",
    ""1-3"": ""`Number`"",
    ""1-4"": ""`length('Hello world') == 11`"",
    ""2-0"": ""`to_string(x)`"",
    ""2-1"": ""Converts a value `x` to a string."",
    ""2-2"": ""`to_string(Any)`"",
    ""2-3"": ""`String`"",
    ""2-4"": ""`to_string(42) == '42'`  \n`to_string(true) == 'true'`"",
    ""3-0"": ""`is_null(x)`"",
    ""3-1"": ""Returns `true` if `x` is null, otherwise returns `false`."",
    ""3-2"": ""`is_null(Any)`"",
    ""3-3"": ""`Boolean`"",
    ""3-4"": ""`is_null('') == true`  \n`is_null(\""column1\"")`"",
    ""4-0"": ""`abs(x)`"",
    ""4-1"": ""Returns the absolute value of number `x`."",
    ""4-2"": ""`abs(Number)`"",
    ""4-3"": ""`Number`"",
    ""4-4"": ""`abs(-3) == 3`"",
    ""5-0"": ""`exp(x)`"",
    ""5-1"": ""Returns `e^x`, where `e` is the base of the natural logarithm."",
   "
"slug: ""fiddler-query-language""  ""5-2"": ""`exp(Number)`"",
    ""5-3"": ""`Number`"",
    ""5-4"": ""`exp(1) == 2.718281828459045`"",
    ""6-0"": ""`log(x)`"",
    ""6-1"": ""Returns the natural logarithm (base `e`) of number `x`."",
    ""6-2"": ""`log(Number)`"",
    ""6-3"": ""`Number`"",
    ""6-4"": ""`log(e) == 1`"",
    ""7-0"": ""`log2(x)`"",
    ""7-1"": ""Returns the binary logarithm (base `2`) of number `x`."",
    ""7-2"": ""`log2(Number)`"",
    ""7-3"": ""`Number`"",
    ""7-4"": ""`log2(16) == 4`"",
    ""8-0"": ""`log10(x)`"",
    ""8-1"": ""Returns the binary logarithm (base `10`) of number `x`."",
    ""8-2"": ""`log10(Number)`"",
    ""8-3"": ""`Number`"",
    ""8-4"": ""`log10(1000) == 3`"",
    ""9-0"": ""`sqrt(x)`"",
    ""9-1"": ""Returns the positive square root of number `x`."",
    ""9-2"": ""`sqrt(Number)`"",
    ""9-3"": ""`Number`"",
    ""9-4"": ""`sqrt(144) == 12`""
  },
  ""cols"": 5,
  ""rows"": 10,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


# Aggregate functions

Every Custom Metric must be wrapped in an aggregate function or be a combination of aggregate functions.

| Symbol       | Description                                                                          | Syntax            | Returns  | Examples                 |
| :----------- | :----------------------------------------------------------------------------------- | :---------------- | :------- | :----------------------- |
| `sum(x)`     | Returns the sum of a numeric column or row expression `x`.                           | `sum(Number)`     | `Number` | `sum(column1 + column2)` |
| `average(x)` | Returns the arithmetic mean/average value of a numeric column or row expression `x`. | `average(Number)` | `Number` | `average(2 * column1)`   |
| `count(x)`   | Returns the number of non-null rows of a column or row expression `x`.               | `count(Any)`      | `Number` | `count(column1)`         |

# Built-in metric functions

[block:parameters]
{
  ""data"": {
    ""h-0"": ""Symbol"",
    ""h-1"": ""Description"",
    ""h-2"": ""Syntax"",
    ""h-3"": ""Returns"",
    ""h-4"": ""Examples"",
    ""0-0"": ""`jsd(column, baseline)`"",
    ""0-1"": ""The Jensen-Shannon distance of column `column` with respect to baseline `baseline`."",
    ""0-2"": ""`jsd(Any, String)`"",
    ""0-3"": ""`Number`"",
    ""0-4"": ""`jsd(column1, 'my_baseline')`"",
    ""1-0"": ""`psi(column, baseline)`"",
    ""1-1"": ""The population stability index of column `column` with respect to baseline `baseline`."",
    ""1-2"": ""`psi(Any, String)`"
"slug: ""fiddler-query-language"" "",
    ""1-3"": ""`Number`"",
    ""1-4"": ""`psi(column1, 'my_baseline')`"",
    ""2-0"": ""`null_violation_count(column)`"",
    ""2-1"": ""Number of rows with null values in column `column`."",
    ""2-2"": ""`null_violation_count(Any)`"",
    ""2-3"": ""`Number`"",
    ""2-4"": ""`null_violation_count(column1)`"",
    ""3-0"": ""`range_violation_count(column)`"",
    ""3-1"": ""Number of rows with out-of-range values in column `column`."",
    ""3-2"": ""`range_violation_count(Any)`"",
    ""3-3"": ""`Number`"",
    ""3-4"": ""`range_violation_count(column1)`"",
    ""4-0"": ""`type_violation_count(column)`"",
    ""4-1"": ""Number of rows with invalid data types in column `column`."",
    ""4-2"": ""`type_violation_count(Any)`"",
    ""4-3"": ""`Number`"",
    ""4-4"": ""`type_violation_count(column1)`"",
    ""5-0"": ""`any_violation_count(column)`"",
    ""5-1"": ""Number of rows with at least one Data Integrity violation in `column`."",
    ""5-2"": ""`any_violation_count(Any)`"",
    ""5-3"": ""`Number`"",
    ""5-4"": ""`any_violation_count(column1)`"",
    ""6-0"": ""`traffic()`"",
    ""6-1"": ""Total row count. Includes null rows."",
    ""6-2"": ""`traffic()`"",
    ""6-3"": ""`Number`"",
    ""6-4"": ""`traffic()`"",
    ""7-0"": ""`tp(class)`"",
    ""7-1"": ""True positive count. Available for binary classification and multiclass classification models. For multiclass, `class` is used to specify the positive class."",
    ""7-2"": ""`tp(class=Optional[String])`"",
    ""7-3"": ""`Number`"",
    ""7-4"": ""`tp()`  \n`tp(class='class1')`"",
    ""8-0"": ""`tn(class)`"",
    ""8-1"": ""True negative count. Available for binary classification and multiclass classification models. For multiclass, `class` is used to specify the positive class."",
    ""8-2"": ""`tn(class=Optional[String])`"",
    ""8-3"": ""`Number`"",
    ""8-4"": ""`tn()`  \n`tn(class='class1')`"",
    ""9-0"": ""`fp(class)`"",
    ""9-1"": ""False positive count. Available for binary classification and multiclass classification models. For multiclass, `class` is used to specify the positive class."",
    ""9-2"": ""`fp(class=Optional[String])`"",
    ""9-3"": ""`Number`"",
    ""9-4"": ""`fp()`  \n`fp(class='class1')`"",
    ""10-0"": ""`fn(class)`"",
    ""10-1"": ""False negative count. Available for binary classification and multiclass classification models. For multiclass, `class` is used to specify the positive class."",
    ""10-2"": ""`fn(class=Optional[String])`"",
    ""10-3"": ""`Number`"",
    ""10-4"":"
"slug: ""fiddler-query-language""  ""`fn()`  \n`fn(class='class1')`"",
    ""11-0"": ""`precision(target, threshold)`"",
    ""11-1"": ""Precision between target and output. Available for binary classification model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""11-2"": ""`precision(target=Optional[Any], threshold=Optional[Number])`"",
    ""11-3"": ""`Number`"",
    ""11-4"": ""`precision()`  \n`precision(target=column1)`  \n`precision(threshold=0.5)`  \n`precision(target=column1, threshold=0.5)`"",
    ""12-0"": ""`recall(target, threshold)`"",
    ""12-1"": ""Recall between target and output. Available for binary classification model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""12-2"": ""`recall(target=Optional[Any], threshold=Optional[Number])`"",
    ""12-3"": ""`Number`"",
    ""12-4"": ""`recall()`  \n`recall(target=column1)`  \n`recall(threshold=0.5)`  \n`recall(target=column1, threshold=0.5)`"",
    ""13-0"": ""`f1_score(target, threshold)`"",
    ""13-1"": ""F1 score between target and output. Available for binary classification model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""13-2"": ""`f1_score(target=Optional[Any], threshold=Optional[Number])`"",
    ""13-3"": ""`Number`"",
    ""13-4"": ""`f1_score()`  \n`f1_score(target=column1)`  \n`f1_score(threshold=0.5)`  \n`f1_score(target=column1, threshold=0.5)`"",
    ""14-0"": ""`fpr(target, threshold)`"",
    ""14-1"": ""False positive rate between target and output. Available for binary classification model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""14-2"": ""`fpr(target=Optional[Any], threshold=Optional[Number])`"",
    ""14-3"": ""`Number`"",
    ""14-4"": ""`fpr()`  \n`fpr(target=column1)`  \n`fpr(threshold=0.5)`  \n`fpr(target=column1, threshold=0.5)`"",
    ""15-0"": ""`auroc(target)`"",
    ""15-1"": ""Area under the ROC curve between target and output. Available for binary classification model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""15-2"": ""`auroc(target=Optional[Any])`"",
    ""15-3"": ""`Number`"",
    ""15-4"": ""`auroc()`  \n`auroc(target=column1)`"",
    ""16-0"": ""`geometric_mean(target, threshold)`"",
    ""16-1"": ""Geometric mean score between target and output. Available for binary classification model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""16-2"":"
"slug: ""fiddler-query-language""  ""`geometric_mean(target=Optional[Any], threshold=Optional[Number])`"",
    ""16-3"": ""`Number`"",
    ""16-4"": ""`geometric_mean()`  \n`geometric_mean(target=column1)`  \n`geometric_mean(threshold=0.5)`  \n`geometric_mean(target=column1, threshold=0.5)`"",
    ""17-0"": ""`expected_calibration_error(target)`"",
    ""17-1"": ""Expected calibration error between target and output. Available for binary classification model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""17-2"": ""`expected_calibration_error(target=Optional[Any])`"",
    ""17-3"": ""`Number`"",
    ""17-4"": ""`expected_calibration_error()`  \n`expected_calibration_error(target=column1)`"",
    ""18-0"": ""`log_loss(target)`"",
    ""18-1"": ""Log loss (binary cross entropy) between target and output. Available for binary classification model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""18-2"": ""`log_loss(target=Optional[Any])`"",
    ""18-3"": ""`Number`"",
    ""18-4"": ""`log_loss()`  \n`log_loss(target=column1)`"",
    ""19-0"": ""`calibrated_threshold(target)`"",
    ""19-1"": ""Optimal threshold value for a high TPR and a low FPR. Available for binary classification model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""19-2"": ""`calibrated_threshold(target=Optional[Any])`"",
    ""19-3"": ""`Number`"",
    ""19-4"": ""`calibrated_threshold()`  \n`calibrated_threshold(target=column1)`"",
    ""20-0"": ""`accuracy(target, threshold)`"",
    ""20-1"": ""Accuracy score between target and outputs. Available for multiclass classification model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""20-2"": ""`accuracy(target=Optional[Any], threshold=Optional[Number])`"",
    ""20-3"": ""`Number`"",
    ""20-4"": ""`accuracy()`  \n`accuracy(target=column1)`  \n`accuracy(threshold=0.5)`  \n`accuracy(target=column1, threshold=0.5)`"",
    ""21-0"": ""`log_loss(target)`"",
    ""21-1"": ""Log loss score between target and outputs. Available for multiclass classification model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""21-2"": ""`log_loss(target=Optional[Any])`"",
    ""21-3"": ""`Number`"",
    ""21-4"": ""`log_loss()`  \n`log_loss(target=column1)`"",
    ""22-0"": ""`r2(target)`"",
    ""22-1"": ""R-squared score between target and output. Available for regression model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""22-2"": ""`r2(target=Optional[Any])`"",
    ""22-3"": ""`Number`"",
    ""22-4"": ""`r2()`  \n"
"slug: ""fiddler-query-language"" `r2(target=column1)`"",
    ""23-0"": ""`mse(target)`"",
    ""23-1"": ""Mean squared error between target and output. Available for regression model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""23-2"": ""`mse(target=Optional[Any])`"",
    ""23-3"": ""`Number`"",
    ""23-4"": ""`mse()`  \n`mse(target=column1)`"",
    ""24-0"": ""`mae(target)`"",
    ""24-1"": ""Mean absolute error between target and output. Available for regression model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""24-2"": ""`mae(target=Optional[Any])`"",
    ""24-3"": ""`Number`"",
    ""24-4"": ""`mae()`  \n`mae(target=column1)`"",
    ""25-0"": ""`mape(target)`"",
    ""25-1"": ""Mean absolute percentage error between target and output. Available for regression model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""25-2"": ""`mape(target=Optional[Any])`"",
    ""25-3"": ""`Number`"",
    ""25-4"": ""`mape()`  \n`mape(target=column1)`"",
    ""26-0"": ""`wmape(target)`"",
    ""26-1"": ""Weighted mean absolute percentage error between target and output. Available for regression model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""26-2"": ""`wmape(target=Optional[Any])`"",
    ""26-3"": ""`Number`"",
    ""26-4"": ""`wmape()`  \n`wmape(target=column1)`"",
    ""27-0"": ""`map(target)`"",
    ""27-1"": ""Mean average precision score. Available for ranking model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""27-2"": ""`map(target=Optional[Any])`"",
    ""27-3"": ""`Number`"",
    ""27-4"": ""`map()`  \n`map(target=column1)`"",
    ""28-0"": ""`ndcg_mean(target)`"",
    ""28-1"": ""Mean normalized discounted cumulative gain score. Available for ranking model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""28-2"": ""`ndcg_mean(target=Optional[Any])`"",
    ""28-3"": ""`Number`"",
    ""28-4"": ""`ndcg_mean()`  \n`ndcg_mean(target=column1)`"",
    ""29-0"": ""`query_count(target)`"",
    ""29-1"": ""Count of ranking queries. Available for ranking model tasks.  \nIf `target` is specified, it will be used in place of the default target column."",
    ""29-2"": ""`query_count(target=Optional[Any])`"",
    ""29-3"": ""`Number`"",
    ""29-4"": ""`query_count()`  \n`query_count(target=column1)`""
  },
  ""cols"": 5,
  ""rows"": 30,
  ""align"": [
    """
"slug: ""fiddler-query-language"" left"",
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]
"
"---
title: ""Monitoring Charts""
slug: ""monitoring-charts-platform""
excerpt: """"
hidden: false
createdAt: ""Thu Feb 23 2023 22:56:27 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Fiddler AI‚Äôs monitoring charts allow you to easily track your models and ensure that they are performing optimally. For any of your models, monitoring charts for data drift, performance, data integrity, or traffic metrics can be displayed using Fiddler Dashboards.

## Supported Metric Types

Monitoring charts enable you to plot one of the following metric types for a given model:

- [**Data Drift**](doc:data-drift-platform#what-is-being-tracked)
  - Plot drift for up to 20 columns at once and track it using your choice of Jensen‚ÄìShannon distance (JSD) or Population Stability Index (PSI).
- [**Performance**](doc:performance-tracking-platform#what-is-being-tracked)
  - Available metrics are model dependent.
- [**Data Integrity Violations**](doc:data-integrity-platform#what-is-being-tracked)
  - Plot data integrity violations for up to 20 columns and track one of the three violations at once.
- [**Traffic **](doc:traffic-platform#what-is-being-tracked)

## Key Features:

### Multiple Charting Options

You can [plot up to 20 columns](doc:monitoring-charts-ui#chart-metric-queries--filters) and 6 metric queries for a model enabling you to easily perform model-to-model comparisons and plot multiple metrics in a single chart view.

### Downloadable CSV Data

You can [easily download a CSV of the raw chart data](doc:monitoring-charts-ui#breakdown-summary). This feature allows you to analyze your data further.

### Advanced Chart Functionality

The monitoring charts feature offers [advanced chart functionalities ](doc:monitoring-charts-ui#chart-metric-queries--filters)  to provide you with the flexibility to customize your charts and view your data in a way that is most useful to you. Features include:

- Zoom
- Dragging of time ranges
- Toggling between bar and line chart types
- Adjusting the scale between linear and log options
- Adjusting the range of the y-axis

![](https://files.readme.io/9ad4867-image.png)

Check out more on the [Monitoring Charts UI Guide](doc:monitoring-charts-ui).
"
"---
title: ""client.delete_webhook""
slug: ""clientdelete_webhook""
excerpt: ""To delete a webhook.""
hidden: false
createdAt: ""Tue Sep 19 2023 10:49:10 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters | Type | Default | Description                                            |
| :--------------- | :--- | :------ | :----------------------------------------------------- |
| uuid             | str  | None    | The unique system generated identifier for the webook. |

```python Usage

client.delete_webhook(
    uuid = ""ffcc2ddf-f896-41f0-bc50-4e7b76bb9ace"",
)
```

| Return Type | Description |
| :---------- | :---------- |
| None        |             |
"
"---
title: ""client.get_custom_metric""
slug: ""clientget_custom_metric""
excerpt: ""Gets details about a user-defined Custom Metric.""
hidden: false
createdAt: ""Thu Oct 26 2023 17:14:40 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type   | Required | Description                                 |
| :-------------- | :----- | :------- | :------------------------------------------ |
| metric_id       | string | Yes      | The unique identifier for the custom metric |

```python Usage
METRIC_ID = '7d06f905-80b1-4a41-9711-a153cbdda16c'

custom_metric = client.get_custom_metric(
  metric_id=METRIC_ID
)
```

| Return Type                                 | Description                                        |
| :------------------------------------------ | :------------------------------------------------- |
| `fiddler.schema.custom_metric.CustomMetric` | Custom metric object with details about the metric |
"
"---
title: ""client.get_triggered_alerts""
slug: ""clientget_triggered_alerts""
excerpt: ""To get a list of all triggered alerts for given alert rule and time period""
hidden: false
createdAt: ""Tue Nov 01 2022 07:26:54 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters | Type                 | Default    | Description                                                                                                       |
| :--------------- | :------------------- | :--------- | :---------------------------------------------------------------------------------------------------------------- |
| alert_rule_uuid  | str                  | None       | The unique system generated identifier for the alert rule.                                                        |
| start_time       | Optional[datetime]   | 7 days ago | Start time to filter trigger alerts in yyyy-MM-dd format, inclusive.                                              |
| end_time         | Optional[datetime]   | today      | End time to filter trigger alerts in yyyy-MM-dd format, inclusive.                                                |
| offset           | Optional[int]        | None       | Pointer to the starting of the page index                                                                         |
| limit            | Optional[int]        | None       | Number of records to be retrieved per page, also referred as page_size                                            |
| ordering         | Optional\[List[str]] | None       | List of Alert Rule fields to order by. Eg. [‚Äòalert_time_bucket‚Äô] or [‚Äò- alert_time_bucket‚Äô] for descending order. |

> üìò Info
> 
> The Fiddler client can be used to get a list of triggered alerts for given alert rule and time duration.

```python Usage

trigerred_alerts = client.get_triggered_alerts(
    alert_rule_uuid = ""588744b2-5757-4ae9-9849-1f4e076a58de"",
    start_time = ""2022-05-01"",
    end_time = ""2022-09-30"",
  	ordering = ['alert_time_bucket'], #['-alert_time_bucket'] for descending
    limit= 4, ## to set number of rules to show in one go
    offset = 0, # page offset
)
```

| Return Type           | Description                                                      |
| :-------------------- | :--------------------------------------------------------------- |
| List[TriggeredAlerts] | A List containing TriggeredAlerts objects returned by the query. |
"
"---
title: ""client.get_alert_rules""
slug: ""clientget_alert_rules""
excerpt: ""To get a list of all alert rules for project, model, and other filtering parameters""
hidden: false
createdAt: ""Tue Nov 01 2022 06:38:59 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
[block:parameters]
{
  ""data"": {
    ""h-0"": ""Input Parameters"",
    ""h-1"": ""Type"",
    ""h-2"": ""Default"",
    ""h-3"": ""Description"",
    ""0-0"": ""project_id"",
    ""0-1"": ""Optional [str]"",
    ""0-2"": ""None"",
    ""0-3"": ""A unique identifier for the project."",
    ""1-0"": ""model_id"",
    ""1-1"": ""Optional [str]"",
    ""1-2"": ""None"",
    ""1-3"": ""A unique identifier for the model."",
    ""2-0"": ""alert_type"",
    ""2-1"": ""Optional\\[[fdl.AlertType](ref:fdlalerttype)]"",
    ""2-2"": ""None"",
    ""2-3"": ""Alert type. One of:  `AlertType.PERFORMANCE`, `AlertType.DATA_DRIFT`, `AlertType.DATA_INTEGRITY`, or `AlertType.SERVICE_METRICS`"",
    ""3-0"": ""metric"",
    ""3-1"": ""Optional\\[[fdl.Metric](ref:fdlmetric)]"",
    ""3-2"": ""None"",
    ""3-3"": ""When alert_type is SERVICE_METRICS:  `Metric.TRAFFIC`.  \n  \nWhen alert_type is PERFORMANCE, choose one of the following based on the machine learning model.  \n1)  For binary_classfication: One of  \n`Metric.ACCURACY`, `Metric.TPR`, `Metric.FPR`, `Metric.PRECISION`, `Metric.RECALL`, `Metric.F1_SCORE`, `Metric.ECE`, `Metric.AUC`  \n2) For Regression: One of  \n `Metric.R2`, `Metric.MSE`, `Metric.MAE`, `Metric.MAPE`, `Metric.WMAPE`  \n3)  For Multi-class:  \n`Metric.ACCURACY`, `Metric.LOG_LOSS`  \n4) For Ranking:  \n`Metric.MAP`, `Metric.MEAN_NDCG`  \n  \nWhen alert_type is DRIFT:  \n`Metric.PSI` or `Metric.JSD`  \n  \nWhen alert_type is DATA_INTEGRITY:  \nOne of  \n`Metric.RANGE_VIOLATION`,  \n`Metric.MISSING_VALUE`,  \n`Metric.TYPE_VIOLATION`"",
    ""4-0"": ""columns"",
    ""4-1"": ""Optional\\[List[str]]"",
    ""4-2"": ""None"",
    ""4-3"": "" [Optional] List of column names on which alert rule was created. Please note, Alert Rule matching any columns from this list will be returned. "",
    ""5-0"": ""offset"",
    ""5-1"": ""Optional[int]"",
    ""5-2"": ""None"",
    ""5-3"": ""Pointer to the starting of the page index"",
    """
"slug: ""clientget_alert_rules"" 6-0"": ""limit"",
    ""6-1"": ""Optional[int]"",
    ""6-2"": ""None"",
    ""6-3"": ""Number of records to be retrieved per page, also referred as page_size"",
    ""7-0"": ""ordering"",
    ""7-1"": ""Optional\\[List[str]]"",
    ""7-2"": ""None"",
    ""7-3"": ""List of Alert Rule fields to order by. Eg. [‚Äòcritical_threshold‚Äô] or [‚Äò- critical_threshold‚Äô] for descending order.""
  },
  ""cols"": 4,
  ""rows"": 8,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


> üìò Info
> 
> The Fiddler client can be used to get a list of alert rules with respect to the filtering parameters.

```python Usage

import fiddler as fdl

alert_rules = client.get_alert_rules(
    project_id = 'project-a',
    model_id = 'model-a', 
    alert_type = fdl.AlertType.DATA_INTEGRITY, 
    metric = fdl.Metric.MISSING_VALUE,
    columns = [""age"", ""gender""], 
    ordering = ['critical_threshold'], #['-critical_threshold'] for descending
    limit= 4, ## to set number of rules to show in one go
    offset = 0, # page offset (multiple of limit)
)
```

| Return Type     | Description                                                |
| :-------------- | :--------------------------------------------------------- |
| List[AlertRule] | A List containing AlertRule objects returned by the query. |
"
"---
title: ""client.get_custom_metrics""
slug: ""clientget_custom_metrics""
excerpt: ""Gets details about all user-defined Custom Metrics for a given model.""
hidden: false
createdAt: ""Thu Oct 26 2023 17:18:33 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type          | Default | Required | Description                              |
| :-------------- | :------------ | :------ | :------- | :--------------------------------------- |
| project_id      | string        |         | Yes      | The unique identifier for the project    |
| model_id        | string        |         | Yes      | The unique identifier for the model      |
| limit           | Optional[int] | 300     | No       | Maximum number of items to return        |
| offset          | Optional[int] | 0       | No       | Number of items to skip before returning |

```python Usage
PROJECT_ID = 'my_project'
MODEL_ID = 'my_model'

custom_metrics = client.get_custom_metrics(
  project_id=PROJECT_ID,
  model_id=MODEL_ID
)
```

| Return Type                                       | Description                                       |
| :------------------------------------------------ | :------------------------------------------------ |
| `List[fiddler.schema.custom_metric.CustomMetric]` | List of custom metric objects for the given model |
"
"---
title: ""client.add_webhook""
slug: ""clientadd_webhook""
excerpt: ""To create a webhook.""
hidden: false
createdAt: ""Tue Sep 19 2023 10:08:44 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters | Type | Default | Description                                                                   |
| :--------------- | :--- | :------ | :---------------------------------------------------------------------------- |
| name             | str  | None    | A unique name for the webhook.                                                |
| url              | str  | None    | The webhook url used for sending notification messages.                       |
| provider         | str  | None    | The platform that provides webhooks functionality. Only ‚ÄòSLACK‚Äô is supported. |

```python Usage

client.add_webhook(
        name='range_violation_channel',
        url='https://hooks.slack.com/services/T9EAVLUQ5/P982J/G8ISUczk37hxQ15C28d',
        provider='SLACK')
)
```

| Return Type                   | Description                     |
| :---------------------------- | :------------------------------ |
| [fdl.Webhook](ref:fdlwebhook) | Details of the webhook created. |

Example responses:

```python Response
Webhook(uuid='df2397d3-23a8-4eb3-987a-2fe43b758b08',
        name='range_violation_channel', organization_name='some_org_name',
        url='https://hooks.slack.com/services/T9EAVLUQ5/P982J/G8ISUczk37hxQ15C28d',
        provider='SLACK')
```
"
"---
title: ""client.update_webhook""
slug: ""clientupdate_webhook""
excerpt: ""To update a webhook, by changing name, url or provider.""
hidden: false
createdAt: ""Tue Sep 19 2023 20:47:55 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters | Type | Default | Description                                                                   |
| :--------------- | :--- | :------ | :---------------------------------------------------------------------------- |
| name             | str  | None    | A unique name for the webhook.                                                |
| url              | str  | None    | The webhook url used for sending notification messages.                       |
| provider         | str  | None    | The platform that provides webhooks functionality. Only ‚ÄòSLACK‚Äô is supported. |
| uuid             | str  | None    | The unique system generated identifier for the webook.                        |

```python Usage
client.update_webhook(uuid='e20bf4cc-d2cf-4540-baef-d96913b14f1b',
                      name='drift_violation',
                      url='https://hooks.slack.com/services/T9EAVLUQ5/P982J/G8ISUczk37hxQ15C28d',
                      provider='SLACK')
```

| Return Type                   | Description                            |
| :---------------------------- | :------------------------------------- |
| [fdl.Webhook](ref:fdlwebhook) | Details of Webhook after modification. |

Example Response:

```Text Response
Webhook(uuid='e20bf4cc-d2cf-4540-baef-d96913b14f1b',
        name='drift_violation', organization_name='some_org_name',
        url='https://hooks.slack.com/services/T9EAVLUQ5/P982J/G8ISUczk37hxQ15C28d',
        provider='SLACK')
```
"
"---
title: ""client.get_webhook""
slug: ""clientget_webhook""
excerpt: ""To get details of a webhook.""
hidden: false
createdAt: ""Tue Sep 19 2023 10:58:51 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters | Type | Default | Description                                            |
| :--------------- | :--- | :------ | :----------------------------------------------------- |
| uuid             | str  | None    | The unique system generated identifier for the webook. |

```python Usage

client.get_webhook(
    alert_rule_uuid = ""a5f085bc-6772-4eff-813a-bfc20ff71002"",
)
```

| Return Type                   | Description         |
| :---------------------------- | :------------------ |
| [fdl.Webhook](ref:fdlwebhook) | Details of Webhook. |

Example responses:

```python Response
Webhook(uuid='a5f085bc-6772-4eff-813a-bfc20ff71002',
        name='binary_classification_alerts_channel',
        organization_name='some_org',
        url='https://hooks.slack.com/services/T9EAVLUQ5/P982J/G8ISUczk37hxQ15C28d,
        provider='SLACK')
```
"
"---
title: ""client.get_webhooks""
slug: ""clientget_webhooks""
excerpt: ""To get a list of all webhooks for an organization.""
hidden: false
createdAt: ""Tue Sep 19 2023 20:02:50 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters | Type          | Default | Description                                 |
| :--------------- | :------------ | :------ | :------------------------------------------ |
| limit            | Optional[int] | 300     | Number of records to be retrieved per page. |
| offset           | Optional[int] | 0       | Pointer to the starting of the page index.  |

```python Usage
response = client.get_webhooks()
```

| Return Type                          | Description                 |
| :----------------------------------- | :-------------------------- |
| List\[[fdl.Webhook](ref:fdlwebhook)] | A List containing webhooks. |

Example Response

```python Response
[
  Webhook(uuid='e20bf4cc-d2cf-4540-baef-d96913b14f1b', name='model_1_alerts', organization_name='some_org', url='https://hooks.slack.com/services/T9EAVLUQ5/P982J/G8ISUczk37hxQ15C28d', provider='SLACK'),
 	Webhook(uuid='bd4d02d7-d1da-44d7-b194-272b4351cff7', name='drift_alerts_channel', organization_name='some_org', url='https://hooks.slack.com/services/T9EAVLUQ5/P982J/G8ISUczk37hxQ15C28d', provider='SLACK'),
 	Webhook(uuid='761da93b-bde2-4c1f-bb17-bae501abd511', name='project_1_alerts', organization_name='some_org', url='https://hooks.slack.com/services/T9EAVLUQ5/P982J/G8ISUczk37hxQ15C28d', provider='SLACK')
]
```
"
"---
title: ""client.delete_alert_rule""
slug: ""clientdelete_alert_rule""
excerpt: ""To delete an alert rule""
hidden: false
createdAt: ""Tue Nov 01 2022 07:31:30 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters | Type | Default | Description                                                |
| :--------------- | :--- | :------ | :--------------------------------------------------------- |
| alert_rule_uuid  | str  | None    | The unique system generated identifier for the alert rule. |

> üìò Info
> 
> The Fiddler client can be used to get a list of triggered alerts for given alert rule and time duration.

```python Usage

client.delete_alert_rule(
    alert_rule_uuid = ""588744b2-5757-4ae9-9849-1f4e076a58de"",
)
```

| Return Type | Description |
| :---------- | :---------- |
| None        |             |
"
"---
title: ""client.add_custom_metric""
slug: ""clientadd_custom_metric""
excerpt: ""Adds a user-defined Custom Metric to a model.""
hidden: false
createdAt: ""Thu Oct 26 2023 17:22:50 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
For details on supported constants, operators, and functions, see [Fiddler Query Language](doc:fiddler-query-language).

| Input Parameter | Type   | Required | Description                                     |
| :-------------- | :----- | :------- | :---------------------------------------------- |
| name            | string | Yes      | Name of the custom metric                       |
| project_id      | string | Yes      | The unique identifier for the project           |
| model_id        | string | Yes      | The unique identifier for the model             |
| definition      | string | Yes      | The FQL metric definition for the custom metric |
| description     | string | No       | A description of the custom metric              |

```python Usage
PROJECT_ID = 'my_project'
MODEL_ID = 'my_model'

definition = """"""
    average(if(Prediction < 0.5 and Target == 1, -40, if(Prediction >= 0.5 and Target == 0, -400, 250)))
""""""

client.add_custom_metric(
    name='Loan Value',
    description='A custom value score assigned to a loan',
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    definition=definition
)
```
"
"---
title: ""client.build_notifications_config""
slug: ""clientbuild_notifications_config""
excerpt: ""To build notification configuration to be used while creating alert rules.""
hidden: false
createdAt: ""Tue Nov 01 2022 07:37:44 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters   | Type                 | Default | Description                                       |
| :----------------- | :------------------- | :------ | :------------------------------------------------ |
| emails             | Optional[str]        | None    | Comma separated emails list                       |
| pagerduty_services | Optional[str]        | None    | Comma separated pagerduty services list           |
| pagerduty_severity | Optional[str]        | None    | Severity for the alerts triggered by pagerduty    |
| webhooks           | Optional\[List[str]] | None    | Comma separated valid uuids of webhooks available |

> üìò Info
> 
> The Fiddler client  can be used to build notification configuration to be used while creating alert rules.

```python Usage

notifications_config = client.build_notifications_config(
    emails = ""name@abc.com"",
)

```
```python Usage with pagerduty
notifications_config = client.build_notifications_config(
  emails = ""name1@abc.com,name2@email.com"",
  pagetduty_services = 'pd_service_1',
  pagerduty_severity = 'critical'
)

```
```python Usage with webhooks
notifications_config = client.build_notifications_config(
    webhooks = [""894d76e8-2268-4c2e-b1c7-5561da6f84ae"", ""3814b0ac-b8fe-4509-afc9-ae86c176ef13""]
)
```

| Return Type                 | Description                                                                                   |
| :-------------------------- | :-------------------------------------------------------------------------------------------- |
| Dict\[str, Dict[str, Any]]: | dict with emails and pagerduty dict. If left unused, will store empty string for these values |

Example Response:

```python Response
{'emails': {'email': 'name@abc.com'}, 'pagerduty': {'service': '', 'severity': ''}, 'webhooks': []}
```
"
"---
title: ""client.add_monitoring_config""
slug: ""clientadd_monitoring_config""
excerpt: ""Adds a custom configuration for monitoring.""
hidden: false
createdAt: ""Mon May 23 2022 20:41:07 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameters | Type           | Default | Description                                                       |
| :--------------- | :------------- | :------ | :---------------------------------------------------------------- |
| config_info      | dict           | None    | Monitoring config info for an entire org or a project or a model. |
| project_id       | Optional [str] | None    | The unique identifier for the project.                            |
| model_id         | Optional [str] | None    | The unique identifier for the model.                              |

> üìò Info
> 
> _add_monitoring_config_ can be applied at the model, project, or organization level.
> 
> - If _project_id_ and _model_id_ are specified, the configuration will be applied at the **model** level.
> - If _project_id_ is specified but model_id is not, the configuration will be applied at the **project** level.
> - If neither _project_id_ nor _model_id_ are specified, the configuration will be applied at the **organization** level.

```python Usage
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'

monitoring_config = {
    'min_bin_value': 3600,
    'time_ranges': ['Day', 'Week', 'Month', 'Quarter', 'Year'],
    'default_time_range': 7200
}

client.add_monitoring_config(
    config_info=monitoring_config,
    project_id=PROJECT_ID,
    model_id=MODEL_ID
)
```
"
"---
title: ""client.delete_custom_metric""
slug: ""clientdelete_custom_metric""
excerpt: ""Deletes a user-defined Custom Metric from a model.""
hidden: false
createdAt: ""Thu Oct 26 2023 17:22:55 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter | Type   | Required | Description                                 |
| :-------------- | :----- | :------- | :------------------------------------------ |
| uuid            | string | Yes      | The unique identifier for the custom metric |

```python Usage
METRIC_ID = '7d06f905-80b1-4a41-9711-a153cbdda16c'

client.delete_custom_metric(
  metric_id=METRIC_ID
)
```
"
"---
title: ""client.add_alert_rule""
slug: ""clientadd_alert_rule""
excerpt: ""To add an alert rule""
hidden: false
createdAt: ""Tue Nov 01 2022 05:06:57 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Tue Dec 19 2023 21:18:57 GMT+0000 (Coordinated Universal Time)""
---
[block:parameters]
{
  ""data"": {
    ""h-0"": ""Input Parameters"",
    ""h-1"": ""Type"",
    ""h-2"": ""Default"",
    ""h-3"": ""Description"",
    ""0-0"": ""name"",
    ""0-1"": ""str"",
    ""0-2"": ""None"",
    ""0-3"": ""A name for the alert rule"",
    ""1-0"": ""project_id"",
    ""1-1"": ""str"",
    ""1-2"": ""None"",
    ""1-3"": ""The unique identifier for the project."",
    ""2-0"": ""model_id"",
    ""2-1"": ""str"",
    ""2-2"": ""None"",
    ""2-3"": ""The unique identifier for the model."",
    ""3-0"": ""alert_type"",
    ""3-1"": ""[fdl.AlertType](ref:fdlalerttype)"",
    ""3-2"": ""None"",
    ""3-3"": ""One of `AlertType.PERFORMANCE`,  \n`AlertType.DATA_DRIFT`,  \n`AlertType.DATA_INTEGRITY`, `AlertType.SERVICE_METRICS`, or  \n`AlertType.STATISTIC`"",
    ""4-0"": ""metric"",
    ""4-1"": ""[fdl.Metric](ref:fdlmetric)"",
    ""4-2"": ""None"",
    ""4-3"": ""When alert_type is `AlertType.SERVICE_METRICS` this should be `Metric.TRAFFIC`.  \n  \nWhen alert_type is `AlertType.PERFORMANCE`, choose one of the following based on the ML model task:  \n  \nFor binary_classfication:  \n`Metric.ACCURACY`  \n`Metric.TPR`  \n`Metric.FPR`  \n`Metric.PRECISION`  \n`Metric.RECALL`  \n`Metric.F1_SCORE`  \n`Metric.ECE`  \n`Metric.AUC`  \n  \nFor regression:  \n`Metric.R2`  \n`Metric.MSE`  \n`Metric.MAE`  \n`Metric.MAPE`  \n`Metric.WMAPE`  \n  \nFor multi-class classification:  \n`Metric.ACCURACY`  \n`Metric.LOG_LOSS`  \n  \nFor ranking:  \n`Metric.MAP`  \n`Metric.MEAN_NDCG`  \n  \nWhen alert_type is `AlertType.DATA_DRIFT` choose one of the following:  \n`Metric.PSI`  \n`Metric.JSD`  \n  \nWhen alert_type is `AlertType.DATA_INTEGRITY` choose one of the following:  \n`Metric.RANGE_VIOLATION`  \n`Metric.MISSING_VALUE`  \n`Metric.TYPE_VIOLATION`  \n  \nWhen alert_type is `AlertType.STATISTIC` choose one of the following: "
"slug: ""clientadd_alert_rule""  \n`Metric.AVERAGE`  \n`Metric.SUM`  \n`Metric.FREQUENCY`"",
    ""5-0"": ""bin_size"",
    ""5-1"": ""[fdl.BinSize](ref:fdlbinsize)"",
    ""5-2"": ""ONE_DAY"",
    ""5-3"": ""Duration for which the metric value is calculated. Choose one of the following:  \n`BinSize.ONE_HOUR`  \n`BinSize.ONE_DAY` `BinSize.SEVEN_DAYS`"",
    ""6-0"": ""compare_to"",
    ""6-1"": ""[fdl.CompareTo](ref:fdlcompareto)"",
    ""6-2"": ""None"",
    ""6-3"": ""Whether the metric value compared against a static value or the same bin from a previous time period.  \n`CompareTo.RAW_VALUE` `CompareTo.TIME_PERIOD`."",
    ""7-0"": ""compare_period"",
    ""7-1"": ""[fdl.ComparePeriod](ref:fdlcompareperiod)"",
    ""7-2"": ""None"",
    ""7-3"": ""Required only when `CompareTo` is `TIME_PERIOD`. Choose one of the following: `ComparePeriod.ONE_DAY `  \n`ComparePeriod.SEVEN_DAYS` `ComparePeriod.ONE_MONTH`  \n`ComparePeriod.THREE_MONTHS`"",
    ""8-0"": ""priority"",
    ""8-1"": ""[fdl.Priority](ref:fdlpriority)"",
    ""8-2"": ""None"",
    ""8-3"": ""`Priority.LOW`  \n`Priority.MEDIUM`  \n`Priority.HIGH`"",
    ""9-0"": ""warning_threshold"",
    ""9-1"": ""float"",
    ""9-2"": ""None"",
    ""9-3"": ""[Optional] Threshold value to crossing which a warning level severity alert will be triggered.  This should be a decimal which represents a percentage (e.g. 0.45)."",
    ""10-0"": ""critical_threshold"",
    ""10-1"": ""float "",
    ""10-2"": ""None"",
    ""10-3"": ""Threshold value to crossing which a critical level severity alert will be triggered.  This should be a decimal which represents a percentage (e.g. 0.45)."",
    ""11-0"": ""condition"",
    ""11-1"": ""[fdl.AlertCondition](ref:fdlalertcondition)"",
    ""11-2"": ""None"",
    ""11-3"": ""Specifies if the rule should trigger if the metric is greater than or less than the thresholds. `AlertCondition.LESSER `  \n`AlertCondition.GREATER`"",
    ""12-0"": ""notifications_config"",
    ""12-1"": ""Dict\\[str, Dict[str, Any]]"",
    ""12-2"": ""None"",
    ""12-3"": ""[Optional] notifications config object created using helper method [build_notifications_config()](ref:clientbuild_notifications_config)"",
    ""13-0"": ""columns"",
    ""13-1"": ""List[str]"",
    ""13-2"": ""None"",
    ""13-3"": ""Column names on which alert rule is to be created.  \nApplicable only when alert_type is AlertType.DATA_INTEGRITY or AlertType.DRIFT. When alert type is AlertType.DATA_INTEGRITY, it can take \\*[**\\*ANY\\*\\**]\\* to check for all columns."",
    ""14-0"": ""baseline_id"",
"
"slug: ""clientadd_alert_rule""     ""14-1"": ""str"",
    ""14-2"": ""None"",
    ""14-3"": ""Name of the baseline whose histogram is compared against the same derived from current data. When no baseline_id is specified then the [default baseline](doc:fiddler-baselines) is used.  \n  \nUsed only when alert type is `AlertType.DATA_DRIFT`.""
  },
  ""cols"": 4,
  ""rows"": 15,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


> üìò Info
> 
> The Fiddler client can be used to create a variety of alert rules. Rules can be of **Data Drift**, **Performance**, **Data Integrity**, and **Service Metrics ** types and they can be compared to absolute (compare_to = RAW_VALUE) or to relative values (compare_to = TIME_PERIOD).

```python Usage - time_period
# To add a Performance type alert rule which triggers an email notification 
# when precision metric is 5% higher than that from 1 hr bin one day ago.

import fiddler as fdl

notifications_config = client.build_notifications_config(
    emails = ""user_1@abc.com, user_2@abc.com"",
)
client.add_alert_rule(
    name = ""perf-gt-5prec-1hr-1d-ago"",
    project_id = 'project-a',
    model_id = 'model-a',
    alert_type = fdl.AlertType.PERFORMANCE,
    metric = fdl.Metric.PRECISION,
    bin_size = fdl.BinSize.ONE_HOUR, 
    compare_to = fdl.CompareTo.TIME_PERIOD,
    compare_period = fdl.ComparePeriod.ONE_DAY,
    warning_threshold = 0.05,
    critical_threshold = 0.1,
    condition = fdl.AlertCondition.GREATER,
    priority = fdl.Priority.HIGH,
    notifications_config = notifications_config
)
```
```python Usage - raw_value

# To add Data Integrity alert rule which triggers an email notification when 
# published events have more than 5 null values in any 1 hour bin for the _age_ column. 
# Notice compare_to = fdl.CompareTo.RAW_VALUE.

import fiddler as fdl

client.add_alert_rule(
    name = ""age-null-1hr"",
    project_id = 'project-a',
    model_id = 'model-a',
    alert_type = fdl.AlertType.DATA_INTEGRITY,
    metric = fdl.Metric.MISSING_VALUE,
    bin_size = fdl.BinSize.ONE_HOUR, 
    compare_to = fdl.CompareTo.RAW_VALUE,
    priority = fdl.Priority.HIGH,
    warning_threshold = 5,
    critical_threshold = 10,
    condition = fdl.AlertCondition.GREATER,
    column = ""age"",
    notifications_config = notifications_config
)
```
```python Usage - baseline
# To add a Data Drift type alert rule which triggers an email notification 
# when PSI metric for 'age' column from an hr is 5% higher than that from 'baseline_name' dataset.

import fiddler as fdl

client.add_baseline(project_id='project-a', 
                    model_id='model-a', 
                    baseline_name='baseline_name', 
                    type=fdl.BaselineType.PRE_PRODUCTION, 
                    dataset_id='dataset-a')

notifications_config = client.build_notifications_config(
    emails = ""user_1@abc.com, user_2@abc.com"",
)

client.add_alert_rule(
    name"
"slug: ""clientadd_alert_rule""  = ""psi-gt-5prec-age-baseline_name"",
    project_id = 'project-a',
    model_id = 'model-a',
    alert_type = fdl.AlertType.DATA_DRIFT,
    metric = fdl.Metric.PSI,
    bin_size = fdl.BinSize.ONE_HOUR, 
    compare_to = fdl.CompareTo.RAW_VALUE,
    warning_threshold = 0.05,
    critical_threshold = 0.1,
    condition = fdl.AlertCondition.GREATER,
    priority = fdl.Priority.HIGH,
    notifications_config = notifications_config,
    columns = [""age""],
    baseline_id = 'baseline_name'
)
```
```python Usage - multiple columns
# To add Drift type alert rule which triggers an email notification when 
# value of JSD metric is more than 0.5 for one hour bin for  _age_ or _gender_ columns. 
# Notice compare_to = fdl.CompareTo.RAW_VALUE.

import fiddler as fdl
notifications_config = client.build_notifications_config(
    emails = ""user_1@abc.com, user_2@abc.com"",
)

client.add_alert_rule(
    name = ""jsd_multi_col_1hr"",
    project_id = 'project-a',
    model_id = 'model-a',
    alert_type = fdl.AlertType.DATA_DRIFT,
    metric = fdl.Metric.JSD,
    bin_size = fdl.BinSize.ONE_HOUR, 
    compare_to = fdl.CompareTo.RAW_VALUE,
    warning_threshold = 0.4,
    critical_threshold = 0.5,
    condition = fdl.AlertCondition.GREATER,
    priority = fdl.Priority.HIGH,
    notifications_config = notifications_config,
    columns = [""age"", ""gender""],
)
```

| Return Type | Description               |
| :---------- | :------------------------ |
| Alert Rule  | Created Alert Rule object |

Example responses:

```python Response for time_period rule
[AlertRule(alert_rule_uuid='9b8711fa-735e-4a72-977c-c4c8b16543ae',
           organization_name='some_org_name',
           project_id='project-a',
           model_id='model-a',
           name='perf-gt-5prec-1hr-1d-ago',
           alert_type=AlertType.PERFORMANCE,
           metric=Metric.PRECISION,
           priority=Priority.HIGH,
           compare_to='CompareTo.TIME_PERIOD,
           compare_period=ComparePeriod.ONE_DAY,
           compare_threshold=None,
           raw_threshold=None,
           warning_threshold=0.05,
           critical_threshold=0.1,
           condition=AlertCondition.GREATER,
           bin_size=BinSize.ONE_HOUR)]
```
```python Response for raw_value rule
AlertRule(alert_rule_uuid='e1aefdd5-ef22-4e81-b869-3964eff8b5cd', 
organization_name='some_org_name', 
project_id='project-a', 
model_id='model-a', 
name='age-null-1hr', 
alert_type=AlertType.DATA_INTEGRITY, 
metric=Metric.MISSING_VALUE, 
column='age', 
priority=Priority.HIGH, 
compare_to=CompareTo.RAW_VALUE, 
compare_period=None, 
warning_threshold=5, 
critical_threshold=10, 
condition=AlertCondition.GREATER,
bin_size=BinSize.ONE_HOUR)

```
```python Response for baseline rule
AlertRule(alert_rule_uuid='e1aefdd5-ef22-4e81-b869-3964eff8b5cd',"
"slug: ""clientadd_alert_rule""  
organization_name='some_org_name', 
project_id='project-a', 
model_id='model-a', 
name='psi-gt-5prec-age-baseline_name', 
alert_type=AlertType.DATA_DRIFT, 
metric=Metric.PSI, 
priority=Priority.HIGH, 
compare_to=CompareTo.RAW_VALUE, 
compare_period=None, 
warning_threshold=5, 
critical_threshold=10, 
condition=AlertCondition.GREATER,
bin_size=BinSize.ONE_HOUR,
columns=['age'],
baseline_id='baseline_name')
```
```python Response for multiple column rule
[AlertRule(alert_rule_uuid='9b8711fa-735e-4a72-977c-c4c8b16543ae',
           organization_name='some_org_name',
           project_id='project-a',
           model_id='model-a',
           name='perf-gt-5prec-1hr-1d-ago',
           alert_type=AlertType.DRIFT,
           metric=Metric.JSD,
           priority=Priority.HIGH,
           compare_to='CompareTo.RAW_VALUE,
           compare_period=ComparePeriod.ONE_HOUR,
           compare_threshold=None,
           raw_threshold=None,
           warning_threshold=0.4,
           critical_threshold=0.5,
           condition=AlertCondition.GREATER,
           bin_size=BinSize.ONE_HOUR,
           columns=['age', 'gender'])]
```
"
"---
title: ""client.get_baseline""
slug: ""get_baseline""
excerpt: """"
hidden: false
createdAt: ""Thu Nov 03 2022 16:48:00 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
`get_baseline` helps get the configuration parameters of the existing baseline

| Input Parameter | Type   | Required | Description                            |
| :-------------- | :----- | :------- | :------------------------------------- |
| project_id      | string | Yes      | The unique identifier for the project  |
| model_id        | string | Yes      | The unique identifier for the model    |
| baseline_id     | string | Yes      | The unique identifier for the baseline |

```python Usage
PROJECT_NAME = 'example_project'
MODEL_NAME = 'example_model'
BASELINE_NAME = 'example_preconfigured'


baseline = client.get_baseline(
  project_id=PROJECT_NAME,
  model_id=MODEL_NAME,
  baseline_id=BASELINE_NAME,
)
```

| Return Type                     | Description                                                  |
| :------------------------------ | :----------------------------------------------------------- |
| [fdl.Baseline](ref:fdlbaseline) | Baseline schema object with all the configuration parameters |
"
"---
title: ""client.list_baselines""
slug: ""list_baselines""
excerpt: """"
hidden: false
createdAt: ""Thu Nov 03 2022 16:51:18 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Gets all the baselines in a project or attached to a single model within a project

| Input Parameter | Type   | Required | Description                           |
| :-------------- | :----- | :------- | :------------------------------------ |
| project_id      | string | Yes      | The unique identifier for the project |
| model_id        | string | No       | The unique identifier for the model   |

```python Usage
PROJECT_NAME = 'example_project'
MODEL_NAME = 'example_model'

# list baselines across all models within a project
client.list_baselines(
  project_id=ROJECT_NAME
)

# list baselines within a model
client.list_baselines(
  project_id=PROJECT_NAME,
  model_id=MODEL_NAME,
)
```

| Return Type                             | Description                     |
| :-------------------------------------- | :------------------------------ |
| [List\[fdl.Baseline\]](ref:fdlbaseline) | List of baseline config objects |
"
"---
title: ""client.add_baseline""
slug: ""add_baseline""
excerpt: """"
hidden: false
createdAt: ""Fri Oct 21 2022 23:22:31 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
[block:parameters]
{
  ""data"": {
    ""h-0"": ""Input Parameter"",
    ""h-1"": ""Type"",
    ""h-2"": ""Required"",
    ""h-3"": ""Description"",
    ""0-0"": ""project_id"",
    ""0-1"": ""string"",
    ""0-2"": ""Yes"",
    ""0-3"": ""The unique identifier for the project"",
    ""1-0"": ""model_id"",
    ""1-1"": ""string"",
    ""1-2"": ""Yes"",
    ""1-3"": ""The unique identifier for the model"",
    ""2-0"": ""baseline_id"",
    ""2-1"": ""string"",
    ""2-2"": ""Yes"",
    ""2-3"": ""The unique identifier for the baseline"",
    ""3-0"": ""type"",
    ""3-1"": ""[fdl.BaselineType](ref:fdlbaselinetype)"",
    ""3-2"": ""Yes"",
    ""3-3"": ""one of :  \n  \nPRE_PRODUCTION  \nSTATIC_PRODUCTION  \nROLLING_PRODUCTION"",
    ""4-0"": ""dataset_id"",
    ""4-1"": ""string"",
    ""4-2"": ""No"",
    ""4-3"": ""Training or validation dataset uploaded to Fiddler for a PRE_PRODUCTION baseline"",
    ""5-0"": ""start_time"",
    ""5-1"": ""int"",
    ""5-2"": ""No"",
    ""5-3"": ""seconds since epoch to be used as the start time for STATIC_PRODUCTION baseline"",
    ""6-0"": ""end_time"",
    ""6-1"": ""int"",
    ""6-2"": ""No"",
    ""6-3"": ""seconds since epoch to be used as the end time for STATIC_PRODUCTION baseline"",
    ""7-0"": ""offset"",
    ""7-1"": ""[fdl.WindowSize](ref:fdlwindowsize)"",
    ""7-2"": ""No"",
    ""7-3"": ""offset in seconds relative to the current time to be used for ROLLING_PRODUCTION baseline"",
    ""8-0"": ""window_size"",
    ""8-1"": ""[fdl.WindowSize](ref:fdlwindowsize)"",
    ""8-2"": ""No"",
    ""8-3"": ""width of the window in seconds to be used for ROLLING_PRODUCTION baseline""
  },
  ""cols"": 4,
  ""rows"": 9,
  ""align"": [
    ""left"",
    ""left"",
    ""left"",
    ""left""
  ]
}
[/block]


### Add a pre-production baseline

```c Usage
PROJECT_NAME = 'example_project'
BASELINE_NAME = 'example_pre'
DATASET_NAME = 'example_validation'
MODEL_NAME = 'example_model'


client.add_baseline(
  project_id=PROJECT_NAME,
  model_id=MODEL_NAME,
  baseline_id=BASELINE_NAME,
  type=BaselineType.PRE_PRODUCTION, 
  dataset_id=DATASET_NAME, 
)
```

### Add a static production baseline

```c Usage
from datetime import datetime
from fiddler"
"slug: ""add_baseline""  import BaselineType, WindowSize

start = datetime(2023, 1, 1, 0, 0) # 12 am, 1st Jan 2023
end = datetime(2023, 1, 2, 0, 0) # 12 am, 2nd Jan 2023

PROJECT_NAME = 'example_project'
BASELINE_NAME = 'example_static'
DATASET_NAME = 'example_dataset'
MODEL_NAME = 'example_model'
START_TIME = start.timestamp()
END_TIME = end.timestamp()


client.add_baseline(
  project_id=PROJECT_NAME,
  model_id=MODEL_NAME,
  baseline_id=BASELINE_NAME,
  type=BaselineType.STATIC_PRODUCTION,
  start_time=START_TIME,
  end_time=END_TIME,
)
```

### Add a rolling time window baseline

```c Usage
from fiddler import BaselineType, WindowSize

PROJECT_NAME = 'example_project'
BASELINE_NAME = 'example_rolling'
DATASET_NAME = 'example_validation'
MODEL_NAME = 'example_model'

client.add_baseline(
  project_id=PROJECT_NAME,
  model_id=MODEL_NAME,
  baseline_id=BASELINE_NAME,
  type=BaselineType.ROLLING_PRODUCTION,
  offset=WindowSize.ONE_MONTH, # How far back to set our window
  window_size=WindowSize.ONE_WEEK, # Size of the sliding window
)
```

| Return Type                     | Description                                                  |
| :------------------------------ | :----------------------------------------------------------- |
| [fdl.Baseline](ref:fdlbaseline) | Baseline schema object with all the configuration parameters |
"
"---
title: ""client.delete_baseline""
slug: ""delete_baseline""
excerpt: """"
hidden: false
createdAt: ""Thu Nov 03 2022 16:49:42 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Deletes an existing baseline from a project

| Input Parameter | Type   | Required | Description                            |
| :-------------- | :----- | :------- | :------------------------------------- |
| project_id      | string | Yes      | The unique identifier for the project  |
| model_id        | string | Yes      | The unique identifier for the model    |
| baseline_id     | string | Yes      | The unique identifier for the baseline |

```python Usage
PROJECT_NAME = 'example_project'
MODEL_NAME = 'example_model'
BASELINE_NAME = 'example_preconfigured'


client.delete_baseline(
  project_id=PROJECT_NAME,
  model_id=MODEL_NAME,
  baseline_id=BASELINE_NAME,
)
```
"
"---
title: ""Welcome to Fiddler's Documentation!""
slug: ""welcome""
excerpt: ""This is Fiddler‚Äôs AI Observability Platform Documentation. Fiddler is a pioneer in AI Observability for responsible AI. Data Science, MLOps, and LOB teams use Fiddler to monitor, explain, analyze, and improve ML models, generative AI models, and AI applications.""
hidden: false
metadata: 
  title: ""Welcome | Fiddler Docs""
  description: ""This document provides links to helpful guides for onboarding, including a platform guide, user interface guide, client guide, and deployment guide.""
  image: []
  robots: ""index""
createdAt: ""Mon Feb 27 2023 18:08:02 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 23:23:29 GMT+0000 (Coordinated Universal Time)""
---
Here you can find a number of helpful guides to aid with onboarding. These include:

[block:html]
{
  ""html"": ""<style>\n  .index-container {\n      display: grid;\n      grid: auto / 50% 50%;\n      grid-gap: 20px;\n      max-width: 97.5%;\n  }\n  .index-container .index-item {\n    padding: 20px;\n    border: 1px solid #CCCCCC;\n    border-radius: 5px;\n    grid-gap: 15px;\n    \n}\n.index-item{\n  text-decoration: none !important;\n  color: #000000;\n }\n.index-item:hover{\n  color: #000000;\n  border-color: #1A5EF3;\n  -webkit-box-shadow: 0 2px 4px rgb(0 0 0 / 10%);\n  -moz-box-shadown: 0 2px 4px rgb(0 0 0 / 10%);\n  box-shadow: 0 2px 4px rgb(0 0 0 / 10%);\n } \n  \n.index-title {\n    font-size: 20px !important;\n    color: #111111;\n    margin-top: 0px !important;\n    margin-bottom: 20px;\n}\n@media only screen and (max-width: 420px){\n  .index-container {\n    grid: auto / 100%;\n  }\n}\n  </style>\n<div class=\""index-container\"">\n  <a class=\""index-item\"" href=\""https://docs.fiddler.ai/v23.6/docs/administration-platform\"">\n    <div>\n\t\t\t<h2 class=\""index-title\"">Platform Guide</h2>\n    \t<p>How Fiddler does AI Observability and Fiddler-specific terminologies.</p>\n \t\t</div>\n  </a>\n\n  <a class=\""index-item\"" href=\""https://docs.fiddler.ai/v23.6/docs/administration-ui\"">\n    <div>\n      <h2 class=\""index-title\"">User Interface (UI) Guide</h2>\n      <p>An introduction to the product UI with screenshots that illustrate how to interface with the product.</p>\n    </div>\n  </a>\n\n  <a class=\""index-item\"" href=\""https://docs.fiddler.ai/v23.6/docs/installation-and-setup\"">\"
"slug: ""welcome"" n    <div>\n      <h2 class=\""index-title\"">Client Guide</h2>\n      <p>For using Fiddler client including API references and code examples.</p>\n    </div>\n  </a>\n\n  <a class=\""index-item\"" href=\""https://docs.fiddler.ai/v23.6/docs/deploying-fiddler\"">\n  \t<div>\n      <h2 class=\""index-title\"">Deployment Guide</h2>\n      <p>For technical details on how to deploy Fiddler on your premises or ours.</p>\n  \t</div>\n  </a>\n</div>\n""
}
[/block]


‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert

[block:html]
{
  ""html"": ""<div class=\""fiddler-cta\"">\n<a class=\""fiddler-cta-link\"" href=\""https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\""><img src=\""https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\"" alt=\""Fiddler Demo\""></a>\n</div>""
}
[/block]
"
"---
title: ""Product Tour""
slug: ""product-tour""
excerpt: ""Here's a tour of our product UI!""
hidden: false
metadata: 
  title: ""Product Tour | Fiddler Docs""
  description: ""Take a tour of Fiddler AI Observability platform.""
  image: []
  robots: ""index""
createdAt: ""Tue Apr 19 2022 20:09:29 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 23:23:38 GMT+0000 (Coordinated Universal Time)""
---
# Video Demo

Watch the video to learn how Fiddler AI Observability provides data science and MLOps teams with a unified platform to monitor, analyze, explain, and improve machine learning models at scale, and build trust in AI.

[block:embed]
{
  ""html"": ""<iframe class=\""embedly-embed\"" src=\""//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FPENnn3YUAcg&display_name=YouTube&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DPENnn3YUAcg&image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FPENnn3YUAcg%2Fhqdefault.jpg&key=7788cb384c9f4d5dbbdbeffd9fe4b92f&type=text%2Fhtml&schema=youtube\"" width=\""854\"" height=\""480\"" scrolling=\""no\"" title=\""YouTube embed\"" frameborder=\""0\"" allow=\""autoplay; fullscreen\"" allowfullscreen=\""true\""></iframe>"",
  ""url"": ""https://www.youtube.com/watch?v=PENnn3YUAcg"",
  ""favicon"": ""https://www.google.com/favicon.ico"",
  ""image"": ""http://i.ytimg.com/vi/PENnn3YUAcg/hqdefault.jpg"",
  ""provider"": ""youtube.com"",
  ""href"": ""https://www.youtube.com/watch?v=PENnn3YUAcg"",
  ""typeOfEmbed"": ""youtube""
}
[/block]


# Documented UI Tour

When you log in to Fiddler, you are on the Home page and you can visualize monitoring information for your models across all your projects. 

- At the top of the page, you will see donut charts for the number of triggered alerts for [Performance](doc:performance-tracking-platform), [Data Drift](doc:data-drift-platform), and [Data Integrity](doc:data-integrity-platform). 
- To the right of the donut charts, you will find the Bookmarks as well as a Recent Job Status card that lets you keep track of long-running async jobs and whether they have failed, are in progress, or successfully completed. 
- The [Monitoring](doc:monitoring-ui) summary table displays your models across different [projects](doc:project-architecture) along with information on their traffic, drift, and the number of triggered alerts.

![](https://files.readme.io/e959fe5-image.png)

View all of your bookmarked, Projects, Models, Datasets, Charts, and Dashboards by clicking ""View All"" on the Bookmarks card on the homepage or navigating directly to Bookmarks via the navigation bar.

![](https://files.readme.io/aad0a68-image.png)

Track all of your ongoing and completed model, dataset, and event publish jobs by clicking ""View All"
"slug: ""product-tour"" "" on the Jobs card on the homepage or navigating directly to the Jobs via the navigation bar.

![](https://files.readme.io/f914df5-image.png)

On the side navigation bar, below charts, is the [Projects](doc:project-structure) Tab. You can click on the Projects tab and it lands on a page that lists all your projects contained within Fiddler. See the [Fiddler Samples](doc:product-tour#fiddler-samples)  section below for more information on these projects. You can create new projects within the UI (by clicking the ‚ÄúAdd Project‚Äù button) or via the [Fiddler Client](ref:about-the-fiddler-client).

![](https://files.readme.io/8a47f0a-image.png)

**Projects** represent your organization's distinct AI applications or use cases. Within Fiddler, Projects house all the **Models** specific to a given application, and thus serve as a jumping-off point for the majority of Fiddler‚Äôs model monitoring and explainability features.

Go ahead and click on the _fraud_detection_ to navigate to the Project Overview page.

![](https://files.readme.io/a4e5021-image.png)

Here you can see a list of the models contained within the fraud detection project, as well as a project dashboard to which analyze charts can be pinned. Go ahead and click the ‚Äúfraud_detection_model_v1‚Äù model.

![](https://files.readme.io/1a7fa3e-image.png)

From the Model Overview page, you can view details about the model: its metadata (schema), the files in its model directory, and its features, which are sorted by impact (the degree to which each feature influences the model‚Äôs prediction score).

You can then navigate to the platform's core monitoring and explainability capabilities. These include:

- **_Monitor_** ‚Äî Track and configure alerts on your model‚Äôs performance, data drift, data integrity, and overall service metrics. Read the [Monitoring](doc:monitoring-platform) documentation for more details.
- **_Analyze_** ‚Äî Analyze the behavior of your model in aggregate or with respect to specific segments of your population. Read the [Analytics](doc:analytics-ui) documentation for more details.
- **_Explain_** ‚Äî Generate ‚Äúpoint‚Äù or prediction-level explanations on your training or production data for insight into how each model decision was made. Read the [Explainability](doc:explainability-platform) documentation for more details.
- **_Evaluate_** ‚Äî View your model‚Äôs performance on its training and test sets for quick validation prior to deployment. Read the [Evaluation](doc:evaluation-ui) documentation for more details.

## Fiddler Samples

Fiddler Samples is a set of datasets and models that are preloaded into Fiddler. They represent different data types, model frameworks, and machine learning techniques. See the table below for more details.

| **Project**   | **Model**                       | **Dataset** | **Model Framework** | **Algorithm**       | **Model Task**             | **Explanation Algos** |
| ------------- | ------------------------------- | ----------- | ------------------- | ------------------- | -------------------------- | --------------------- |
| Bank Churn    | Bank Churn                      | Tabular     | scikit-learn        | Random Forest       | Binary Classification      | Fiddler Shapley       |
| Heart Disease | Heart Disease                   | Tabular     | Tensorflow          |                     | Binary Classification      | Fiddler Shapley, IG   |
| IMDB          | Imdb Rnn                        | Text        | Tensorflow"
"slug: ""product-tour""           | BiLSTM              | Binary Classfication       | Fiddler Shapley, IG   |
| Iris          | Iris                            | Tabular     | scikit-learn        | Logistic Regression | Multi-class Classification | Fiddler Shapley       |
| Lending       | Logreg-all                      | Tabular     | scikit-learn        | Logistic Regression | Binary Classification      | Fiddler Shapley       |
|               | Logreg-simple                   | Tabular     | scikit-learn        | Logistic Regression | Binary Classification      | Fiddler Shapley       |
|               | Xgboost-simple-sagemaker        | Tabular     | scikit-learn        | XGboost             | Binary Classification      | Fiddler Shapley       |
| Newsgroup     | Christianity Atheism Classifier | Text        | scikit-learn        | Random Forest       | Binary Classification      | Fiddler Shapley       |
| Wine Quality  | Linear Model Wine Regressor     | Tabular     | scikit-learn        | Elastic Net         | Regression                 | Fiddler Shapley       |
|               | DNN Wine Regressor              | Tabular     | Tensorflow          |                     | Regression                 | Fiddler Shapley       |

See the [README](https://github.com/fiddler-labs/fiddler-examples) for more information.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert

[block:html]
{
  ""html"": ""<div class=\""fiddler-cta\"">\n<a class=\""fiddler-cta-link\"" href=\""https://www.fiddler.ai/demo?utm_source=fiddler_docs&utm_medium=referral\""><img src=\""https://files.readme.io/4d190fd-fiddler-docs-cta-demo.png\"" alt=\""Fiddler Demo\""></a>\n</div>""
}
[/block]
"
"---
title: ""Updating model artifacts""
slug: ""updating-model-artifacts""
excerpt: ""Update a model already in Fiddler (surrogate or user artifact model)""
hidden: false
createdAt: ""Wed Feb 01 2023 15:55:08 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
If you need to update a model artifact already uploaded in Fiddler, you can use the `client.update_model_artifact` function. This allows you to replace a surrogate model or your own uploaded model.

Once you have prepared the [model artifacts directory](doc:artifacts-and-surrogates), you can update your model using [client.update_model_artifact](ref:clientupdate_model_artifact)

```python
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'
MODEL_ARTIFACTS_DIR = Path('model/')

client.update_model_artifact(
    artifact_dir=MODEL_ARTIFACTS_DIR,
    project_id=PROJECT_ID,
    model_id=MODEL_ID
)
```
"
"---
title: ""Alerts with Fiddler Client""
slug: ""alerts-client""
excerpt: """"
hidden: false
createdAt: ""Tue Oct 25 2022 16:49:32 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:53:43 GMT+0000 (Coordinated Universal Time)""
---
The complete user guide for alerts and setting up alert rules in the Fiddler UI is provided [here](doc:alerts-ui). In addition to using the Fiddler UI, users have the flexibility to set up alert rules using the Fiddler API client. In particular, the Fiddler client enables the following workflows:

- Add alert rules
- Delete alert rules
- Get the list of all alert rules
- Get the list of triggered alerts

In this document we present examples of how to use the Fiddler client for different alert rule tasks.

## Add an Alert Rule

The Fiddler client can be used to create a variety of alert rules. Rules can be of **Data Drift**, **Performance**, **Data Integrity**, and **Service Metrics ** types and they can be compared to absolute or to relative values.

### Notifications

Before creating a new alert rule, users choose the type of the notification that will be leveraged by Fiddler when an alert is raised. Currently Fiddler client supports email and PagerDuty services as notifications. To create a notification configuration we call the [build_notifications_config()](ref:clientbuild_notifications_config) API. For example, the following code snippet creates a notification configuration using a comma separated list of email addresses.

```python python
notifications_config_emails = client.build_notifications_config(
  emails = ""username_1@email.com,username_2@email.com""
)
```

To create a notification configuration using both email addresses and pager duty.

```python python
notifications_config = client.build_notifications_config(
  emails = ""username_1@email.com,username_2@email.com""
  pagerduty_services = 'pagerduty_service_1,""pagerduty_service_2"",
  pagerduty_severity = 'critical'
)
```

### Example 1: Data Integrity Alert Rule to compare against a raw value

Now let's sets up a Data Integrity alert rule which triggers an email notification when published events have 5% null values in any 1 hour bin for the _age_ column. Notice compare_to = 'raw_value'. The [add_alert_rule()](ref:clientadd_alert_rule) API is used to create alert rules.

```python
client.add_alert_rule(
    name = ""age-null-1hr"",
    project_id = PROJECT_ID,
    model_id = MODEL_ID,
    alert_type = fdl.AlertType.DATA_INTEGRITY,
    metric = fdl.Metric.MISSING_VALUE,
    bin_size = fdl.BinSize.ONE_HOUR, 
    compare_to = fdl.CompareTo.RAW_VALUE,
    warning_threshold = 5,
    critical_threshold = 10,
    condition = fdl.AlertCondition.GREATER,
    column = ""age"",
    priority = fdl.Priority.HIGH,
    notifications_config = notifications_config
)
```

Please note, the possible values for bin_size are 'one_hour', 'one_day', and 'seven_days'. When  alert_type is 'data_integrity', use one of 'missing_value', 'range_violation', or 'type_violation' for metric type. 

### Example 2: Performance Alert Rule to compare against a previous time window

And the following API call sets up a Performance alert rule which triggers an email notification"
"slug: ""alerts-client""  when precision metric is 5% higher than that from 1 hr bin one day ago. Notice compare_to = 'time_period' and compare_period = '1 day'.

```python
client.add_alert_rule(
    name = ""perf-gt-5prec-1hr-1d-ago"",
    project_id = 'project-a',
    model_id = 'model-a',
    alert_type = fdl.AlertType.PERFORMANCE,
    metric = fdl.Metric.PRECISION,
    bin_size = fdl.BinSize.ONE_HOUR, 
    compare_to = fdl.CompareTo.TIME_PERIOD,
    compare_period = fdl.ComparePeriod.ONE_DAY,
    warning_threshold = 0.05,
    critical_threshold = 0.1,
    condition = fdl.AlertCondition.GREATER,
    priority = fdl.Priority.HIGH,
    notifications_config = notifications_config
)
```

Please note, the possible values for compare_period are 'one_day', 'seven_days', 'one_month', and 'three_months'.

## Get Alert Rules

The [get_alert_rules()](ref:clientget_alert_rules) API can be used to get a list of all alert rules with respect to the filtering parameters and it returns a paginated list of alert rules.

```python
import fiddler as fdl

alert_rules = client.get_alert_rules(
    project_id = 'project-a',
    model_id = 'model-a', 
    alert_type = fdl.AlertType.DATA_INTEGRITY, 
    metric = fdl.Metric.MISSING_VALUE,
    column = ""age"", 
    ordering = ['critical_threshold'], #['-critical_threshold'] for descending
    limit= 4, ## to set the number of rules to show in one go
    offset = 0, # page offset (multiple of limit)
)
```

Here is an example output of get_alert_rules() API:

```
[AlertRule(alert_rule_uuid='9b8711fa-735e-4a72-977c-c4c8b16543ae',
           organization_name='some_org_name',
           project_id='some_project_id',
           model_id='some_model_id',
           name='age-null-1hr',
           alert_type=AlertType.DATA_INTEGRITY,
           metric=Metric.MISSING_VALUE,
           column='age',
           priority=Priority.HIGH,
           compare_to=CompareTo.RAW_VALUE,
           compare_period=None,
           warning_threshold=0.05,
           critical_threshold=0.1,
           condition=AlertCondition.GREATER,
           bin_size=BinSize.ONE_HOUR)]
```

## Delete an Alert Rule

To delete an alert rule we need the corresponding unique **alert_rule_uuid** which is part of the output we get from  [get_alert_rules()](ref:clientget_alert_rules). Then we can delete a rule by calling the [delete_alert_rule()](ref:clientdelete_alert_rule)  API as shown below:

```python
client.delete_alert_rule(alert_rule_uuid = ""some_alert_rule_uuid"")
```

## Get Triggered Alerts

Finally, to get a paginated list of triggered alerts for a given alert rule in a given time range we can call the [get_triggered_alerts()](ref:clientget_triggered_alerts) API as the following:

```python
triggered_alerts = client.get_triggered_alerts(
    alert_rule_uuid = ""some_alert_rule_uuid"",
    start_time = ""2022-05-01"",
    end_time = ""2022-09-30""
    ordering = ['alert_time_bucket'], #['-alert_time_bucket'] for descending, optional.
"
"slug: ""alerts-client""     limit= 4, ## to set number of rules to show in one go, optional.
    offset = 0, # optional, page offset.
)
```

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Specifying Custom Features""
slug: ""vector-monitoring-copy""
excerpt: ""\""Patented Fiddler Technology\""""
hidden: false
createdAt: ""Thu Oct 19 2023 19:24:35 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
# Vector Monitoring for Unstructured Data

```python pyth
CF1 = fdl.CustomFeature.from_columns(['f1','f2','f3'], custom_name = 'vector1')
CF2 = fdl.CustomFeature.from_columns(['f1','f2','f3'], n_clusters=5, custom_name = 'vector2')
CF3 = fdl.TextEmbedding(name='text_embedding',column='embedding',source_column='text')
CF4 = fdl.ImageEmbedding(name='image_embedding',column='embedding',source_column='image_url')
```

### Passing Custom Features List to Model Info

```python
model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id = DATASET_ID,
    features = data_cols,
    target='target',
    outputs='predicted_score',
    custom_features = [CF1,CF2,CF3,CF4]
)
```

> üìò Quick Start for NLP Monitoring
> 
> Check out our [Quick Start guide for NLP monitoring](doc:simple-nlp-monitoring-quick-start) for a fully functional notebook example.
"
"---
title: ""Publishing Production Data""
slug: ""publishing-production-data""
excerpt: """"
hidden: false
createdAt: ""Fri Nov 18 2022 23:28:25 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
This Section guides you on the various ways you can provide event data to Fiddler and update and retrieve them.
"
"---
title: ""Designing a Baseline Dataset""
slug: ""designing-a-baseline-dataset""
excerpt: """"
hidden: false
createdAt: ""Mon May 23 2022 16:30:17 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
In order for Fiddler to monitor drift or data integrity issues in incoming production data, it needs something to compare this data to.

A baseline dataset is a **representative sample** of the kind of data you expect to see in production. It represents the ideal form of data that your model works best on.

_For this reason,_ **_it should be sampled from your model‚Äôs training set._**

***

**A few things to keep in mind when designing a baseline dataset:**

- It‚Äôs important to include **enough data** to ensure you have a representative sample of the training set.
- You may want to consider **including extreme values (min/max)** of each column in your training set so you can properly monitor range violations in production data. However, if you choose not to, you can manually specify these ranges before upload (see [Customizing Your Dataset Schema])(doc:customizing-your-dataset-schema).
"
"---
title: ""Customizing Your Dataset Schema""
slug: ""customizing-your-dataset-schema""
excerpt: """"
hidden: false
createdAt: ""Mon May 23 2022 16:36:05 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
It's common to want to modify your [`fdl.DatasetInfo`](ref:fdldatasetinfo) object in the case where **something was inferred incorrectly** by [`fdl.DatasetInfo.from_dataframe`](ref:fdldatasetinfo).

Let's walk through an example of how to do this.

***

Suppose you've loaded in a dataset as a pandas DataFrame.

```python
import pandas as pd

df = pd.read_csv('example_dataset.csv')
```

Below is an example of what is displayed upon inspection.

![](https://files.readme.io/3ffd956-example_df_1.png ""example_df (1).png"")

***

Suppose you create a [`fdl.DatasetInfo`](ref:fdldatasetinfo) object by inferring the details from this DataFrame.

```python
dataset_info = fdl.DatasetInfo.from_dataframe(df)
```

Below is an example of what is displayed upon inspection.

![](https://files.readme.io/571f9e4-example_datasetinfo.png ""example_datasetinfo.png"")

But upon inspection, you notice **a few things are wrong**.

1. The [value range](doc:customizing-your-dataset-schema#modifying-a-columns-value-range) of `output_column` is set to `[0.01, 0.99]`, when it should really be `[0.0, 1.0]`.
2. There are no [possible values](doc:customizing-your-dataset-schema#modifying-a-columns-possible-values) set for `feature_3`.
3. The [data type](#modifying-a-columns-data-type) of `feature_3` is set to [`fdl.DataType.STRING`](ref:fdldatatype), when it should really be [`fdl.DataType.CATEGORY`](ref:fdldatatype).

Let's see how we can address these issues.

## Modifying a column‚Äôs value range

Let's say we want to modify the range of `output_column` in the above [`fdl.DatasetInfo`](ref:fdldatasetinfo) object to be `[0.0, 1.0]`.

You can do this by setting the `value_range_min` and `value_range_max` of the `output_column` column.

```python
dataset_info['output_column'].value_range_min = 0.0
dataset_info['output_column'].value_range_max = 1.0
```

## Modifying a column‚Äôs possible values

Let's say we want to modify the possible values of `feature_3` to be `['Yes', 'No']`.

You can do this by setting the `possible_values` of the `feature_3` column.

```python
dataset_info['feature_3'].possible_values = ['Yes', 'No']
```

## Modifying a column‚Äôs data type

Let's say we want to modify the data type of `feature_3` to be [`fdl.DataType.CATEGORY`](ref:fdldatatype).

You can do this by setting the `data_type` of the `feature_3` column.

```python
dataset_info['feature_3'].data_type = fdl.DataType.CATEGORY
```

> üöß Note when modifying a column's data type to Category
> 
> Note that"
"slug: ""customizing-your-dataset-schema""  it is also required when modifying a column's data type to Category to also set the column's possible_values to the list of unique values for that column.
> 
> dataset_info['feature_3'].data_type = fdl.DataType.CATEGORY  
> dataset_info['feature_3'].possible_values = ['Yes', 'No']
"
"---
title: ""Package.py Examples""
slug: ""model-task-examples""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:14:39 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
"
"---
title: ""Authorizing the Client""
slug: ""authorizing-the-client""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 17:18:59 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
In order to use the client, you‚Äôll need to provide some **authorization details**.

Specifically, there are three pieces of information that are required:

- The [URL](#finding-your-url) you are connecting to
- Your [organization ID](#finding-your-organization-id)
- An [authorization token](#finding-your-authorization-token) for your user

This information can be provided in **two ways**:

1. As arguments to the client when it's instantiated (see [`fdl.FiddlerApi`](ref:client-setup))
2. In a configuration file (see [`fiddler.ini`](#authorizing-via-configuration-file))

## Finding your URL

The URL should point to **wherever Fiddler has been deployed** for your organization.

If using Fiddler‚Äôs managed cloud service, it should be of the form  

```
https://app.fiddler.ai
```

## Finding your organization ID

To find your organization ID, navigate to the **Settings** page. Your organization ID will be immediately available on the **General** tab.

![](https://files.readme.io/2c7de6e-finding_your_org_id.png ""finding_your_org_id.png"")

## Finding your authorization token

To find your authorization token, first navigate to the **Settings** page. Then click **Credentials** and **Create Key**.

![](https://files.readme.io/ea51e6a-finding_your_auth_token.png ""finding_your_auth_token.png"")

## Connecting the Client

Once you've located the URL, the org_id and the authorization token, you can connect the Fiddler client to your environment.

```python Connect the Client
URL = 'https://app.fiddler.ai'
ORG_ID = 'my_org'
AUTH_TOKEN = '9AYWiqwxe2hnCAePxg-uEWJUDYRZIZKBSBpx0TvItnw' # not a valid token

# Connect to the Fiddler client
client = fdl.FiddlerApi(
    url=URL,
    org_id=ORG_ID,
    auth_token=AUTH_TOKEN
)
```

## Authorizing via configuration file

If you would prefer not to send authorization details as arguments to [`fdl.FiddlerApi`](ref:client-setup), you can specify them in a **configuration file** called `fiddler.ini`.

The file should be **located in the same directory as the script or notebook** that initializes the [`fdl.FiddlerApi`](ref:client-setup) object.

***

The syntax should follow the below example:

```python fiddler.ini
[FIDDLER]
url = https://app.fiddler.ai
org_id = my_org
auth_token = xtu4g_lReHyEisNg23xJ8IEex0YZEZeeEbTwAsupT0U
```

Then you can initialize the [`fdl.FiddlerApi`](ref:client-setup)object without any arguments, and Fiddler will automatically detect the `fiddler.ini` file:

```python Instantiate with fiddler.ini
client = fdl.FiddlerApi()
```
"
"---
title: ""Uploading a Baseline Dataset""
slug: ""uploading-a-baseline-dataset""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:07:03 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
To upload a baseline dataset to Fiddler, you can use the [`client.upload_dataset`](ref:clientupload_dataset) API. Let's walk through a simple example of how this can be done.

***

The first step is to load your baseline dataset into a pandas DataFrame.

```python
import pandas as pd

df = pd.read_csv('example_dataset.csv')
```

## Creating a DatasetInfo object

Then, you'll need to create a [fdl.DatasetInfo()](ref:fdldatasetinfo) object that can be used to **define the schema for your dataset**.

This schema can be inferred from your DataFrame using the [fdl.DatasetInfo.from_dataframe()](ref:fdldatasetinfofrom_dataframe) function.

```python
dataset_info = fdl.DatasetInfo.from_dataframe(df)
```

> üìò Info
> 
> In the case that you have **categorical columns in your dataset that are encoded as strings**, you can use the `max_inferred_cardinality` argument.
> 
> This argument specifies a threshold for unique values in a column. Any column with fewer than `max_inferred_cardinality` unique values will be converted to [fdl.DataType.CATEGORY](ref:fdldatatype)  type.

```python
dataset_info = fdl.DatasetInfo.from_dataframe(
        df=df,
        max_inferred_cardinality=1000
    )
```

## Uploading your dataset

Once you have your [fdl.DatasetInfo()](ref:fdldatasetinfo) object, you can make any **necessary adjustments** before upload (see [Customizing Your Dataset Schema](doc:customizing-your-dataset-schema) ).

When you're ready, the dataset can be uploaded using [client.upload_dataset()](ref:clientupload_dataset).

```python
PROJECT_ID = 'example_project'
DATASET_ID = 'example_dataset'

client.upload_dataset(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    dataset={
        'baseline': df
    },
    info=dataset_info
)
```
"
"---
title: ""Surrogate Models - Client Guide""
slug: ""surrogate-models-client-guide""
excerpt: """"
hidden: false
createdAt: ""Tue Dec 13 2022 22:22:39 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:52:38 GMT+0000 (Coordinated Universal Time)""
---
Fiddler‚Äôs explainability features require a model on the backend that can generate explanations for you.

> üìò If you don't want to or cannot upload your actual model file, Surrogate Models serve as a way for Fiddler to generate approximate explanations.

A surrogate model **will be built automatically** for you when you call  [`add_model_surrogate`](/reference/clientadd_model_surrogate).  
You just need to provide a few pieces of information about how your model operates.

## What you need to specify

- Your model‚Äôs task (regression, binary classification, etc.)
- Your model‚Äôs target column (ground truth labels)
- Your model‚Äôs output column (model predictions)
- Your model‚Äôs feature columns

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""ML Framework Examples""
slug: ""ml-framework-examples""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:13:20 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
"
"---
title: ""Installation and Setup""
slug: ""installation-and-setup""
excerpt: """"
hidden: false
createdAt: ""Tue May 10 2022 17:14:02 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:51:51 GMT+0000 (Coordinated Universal Time)""
---
Fiddler offers a **Python SDK client** that allows you to connect to Fiddler directly from a Jupyter notebook or automated pipeline.

***

The client is available for download from PyPI via `pip`:

```
pip install fiddler-client
```

<br>

Once you've installed the client, you can import the `fiddler` package into any Python script:

```python
import fiddler as fdl
```

***

> üìò Info
> 
> For detailed documentation on the client‚Äôs many features, check out the [API reference](ref:client-setup) section.

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Uploading model artifacts""
slug: ""uploading-model-artifacts""
excerpt: ""Upload a model artifact in Fiddler""
hidden: false
createdAt: ""Wed Feb 01 2023 16:04:40 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Before uploading your model artifact into Fiddler, you need to add the model with [client.add_model](ref:clientadd_model).

Once you have prepared the [model artifacts directory](doc:artifacts-and-surrogates), you can upload your model using [client.add_model_artifact](ref:clientadd_model_artifact)

```python
PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'
MODEL_ARTIFACTS_DIR = Path('model/')

client.add_model_artifact(
    model_dir=MODEL_ARTIFACTS_DIR,
    project_id=PROJECT_ID,
    model_id=MODEL_ID
)
```
"
"---
title: ""Onboarding a Model""
slug: ""onboarding-a-model""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:07:09 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Fri Dec 08 2023 22:52:10 GMT+0000 (Coordinated Universal Time)""
---
To onboard a model **without uploading your model artifact**, you can use the [client.add_model()](ref:clientadd_model) Python client. Let's walk through a simple example of how this can be done.

***

> üìò Note
> 
> Using [client.add_model()](ref:clientadd_model) does not provide Fiddler with a model artifact.  Onboarding a model in this fashion is a good start for model monitoring, but Fiddler will not be able to offer model explainability features without a model artifact.  You can subsequently call [client.add_model_surrogate()](ref:clientadd_model_surrogate) or [client.add_model_artifact()](ref:clientadd_model_artifact) to provide Fiddler with a model artifact.  Please see [Uploading a Model Artifact](doc:uploading-model-artifacts) for more information.

Suppose you have uploaded the following baseline dataset, and you‚Äôve created a [fdl.DatasetInfo()](ref:fdldatasetinfo)  object for it called `dataset_info` (See [Uploading a Baseline Dataset](doc:uploading-a-baseline-dataset)).

![](https://files.readme.io/82cf758-example_df.png ""example_df.png"")

```python
PROJECT_ID = 'example_project'
DATASET_ID = 'example_dataset'

dataset_info = client.get_dataset_info(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID
)
```

Although the data has been uploaded to Fiddler, there is still **no specification** for which columns to use for which purpose.

## Creating a ModelInfo object

To **provide this specification**, you can create a [fdl.ModelInfo()](ref:fdlmodelinfo) object.

In this case, we‚Äôd like to tell Fiddler to use

- `feature_1`, `feature_2`, and `feature_3` as features
- `output_column` as the model output
- `target_column` as the model's target/ground truth

Further you want to specify the [model task type](doc:task-types). To save time, Fiddler provides a function to add this specification to an existing [fdl.DatasetInfo()](ref:fdldatasetinfo) object.

```python
model_task = fdl.ModelTask.BINARY_CLASSIFICATION
model_target = 'target_column'
model_outputs = ['output_column']
model_features = [
    'feature_1',
    'feature_2',
    'feature_3'
]

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id=DATASET_ID,
    features=model_features,
    target=model_target,
    outputs=model_outputs,
    model_task=model_task
)
```

The [fdl.ModelInfo.from_dataset_info()](ref:fdlmodelinfofrom_dataset_info) function allows you to specify a [fdl.DatasetInfo()](ref:fdldatasetinfo) object along with some extra specification and it will **automatically generate** your [fdl.ModelInfo()](ref:fdlmodelinfo) object for you.

## Onboarding your model

Once you have your [fdl.ModelInfo()](ref:fdlmodelinfo)"
"slug: ""onboarding-a-model""  object, you can call [client.add_model()](ref:clientadd_model) to onboard your model with Fiddler.

```python
MODEL_ID = 'example_model'

client.add_model(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    model_id=MODEL_ID,
    model_info=model_info
)
```

‚Ü™ Questions? [Join our community Slack](https://www.fiddler.ai/slackinvite) to talk to a product expert
"
"---
title: ""Specifying Custom Missing Value Encodings""
slug: ""specifying-custom-missing-value-encodings""
excerpt: """"
hidden: false
createdAt: ""Tue Aug 30 2022 18:19:30 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
There may be cases in which you have missing values in your data, but you encode these values in a special way (other than the standard `NaN`).

In such cases, Fiddler offers a way to specify **your own missing value encodings for each column**.

***

You can create a ""fall back"" dictionary, which holds the values you would like to have treated as missing for each column. Then just pass that dictionary into your [`fdl.ModelInfo`](/reference/fdlmodelinfo)  object before onboarding your model.

```python
fall_back = {
  'column_1': [-999, 'missing'],
  'column_2': [-1, '?', 'na']
}

model_info = fdl.ModelInfo.from_dataset_info(
  ...
  fall_back=fall_back
)
```
"
"---
title: ""Multi-class Classification Model Package.py""
slug: ""multiclass-classification-1""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:12:40 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üöß Note
> 
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](doc:uploading-a-model-artifact).

Suppose you would like to upload a model artifact for a **multiclass classification model**.

Following is an example of what the `package.py` script may look like.

```python
import pickle
from pathlib import Path
import pandas as pd

PACKAGE_PATH = Path(__file__).parent

OUTPUT_COLUMNS = ['probability_0', 'probability_1', 'probability_2']

class MyModel:

    def __init__(self):
        
        # Load the model
        with open(PACKAGE_PATH / 'model.pkl', 'rb') as pkl_file:
            self.model = pickle.load(pkl_file)

    def predict(self, input_df):
        
        # Store predictions in a DataFrame
        return pd.DataFrame(self.model.predict_proba(input_df), columns=OUTPUT_COLUMNS)

def get_model():
    return MyModel()
```

Here, we are assuming that the model prediction columns that have been specified in the [`fdl.ModelInfo`](ref:fdlmodelinfo) object are called `probability_0`, `probability_1`, and `probability_2`.
"
"---
title: ""Ranking Model Package.py""
slug: ""uploading-a-ranking-model-artifact""
excerpt: """"
hidden: false
createdAt: ""Mon Oct 31 2022 21:23:47 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üöß Note
> 
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](doc:uploading-model-artifacts).

Suppose you would like to upload a model artifact for a **ranking model**.

Following is an example of what the `package.py` script may look like.

```python
import pickle
from pathlib import Path
import pandas as pd

PACKAGE_PATH = Path(__file__).parent

class ModelPackage:

    def __init__(self):
        self.output_columns = ['score']
        with open(PACKAGE_PATH / 'model.pkl', 'rb') as infile:
            self.model = pickle.load(infile)
    
    def predict(self, input_df):
        pred = self.model.predict(input_df)
        return pd.DataFrame(pred, columns=self.output_columns)
    
def get_model():
    return ModelPackage()
```

Here, we are assuming that the model prediction column that has been specified in the [`fdl.ModelInfo`](ref:fdlmodelinfo) object is called `score`.

Please checkout this [quickstart notebook](doc:ranking-model) to work through an example of onboarding a ranking model on to Fiddler.
"
"---
title: ""Regression Model Package.py""
slug: ""regression""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:12:29 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üöß Note
> 
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](doc:uploading-model-artifacts).

Suppose you would like to upload a model artifact for a **regression model**.

Following is an example of what the `package.py` script may look like.

```python
import pickle
from pathlib import Path
import pandas as pd

PACKAGE_PATH = Path(__file__).parent

OUTPUT_COLUMN = ['predicted_quality']

class MyModel:

    def __init__(self):
        
        # Load the model
        with open(PACKAGE_PATH / 'model.pkl', 'rb') as pkl_file:
            self.model = pickle.load(pkl_file)

    def predict(self, input_df):
        
        # Store predictions in a DataFrame
        return pd.DataFrame(self.model.predict(input_df), columns=OUTPUT_COLUMN)

def get_model():
    return MyModel()
```

Here, we are assuming that the model prediction column that has been specified in the [`fdl.ModelInfo`](ref:fdlmodelinfo) object is called `predicted_quality`.
"
"---
title: ""Binary Classification Model Package.py""
slug: ""binary-classification-1""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:12:34 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üöß Note
> 
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](doc:uploading-model-artifacts).

Suppose you would like to upload a model artifact for a **binary classification model**.

Following is an example of what the `package.py` script may look like.

```python
import pickle
from pathlib import Path
import pandas as pd

PACKAGE_PATH = Path(__file__).parent

OUTPUT_COLUMN = ['probability_over_50k']

class MyModel:

    def __init__(self):
        
        # Load the model
        with open(PACKAGE_PATH / 'model.pkl', 'rb') as pkl_file:
            self.model = pickle.load(pkl_file)

    def predict(self, input_df):
        
        # Store predictions in a DataFrame
        return pd.DataFrame(self.model.predict_proba(input_df)[:, 1], columns=OUTPUT_COLUMN)

def get_model():
    return MyModel()
```

Here, we are assuming that the model prediction column that has been specified in the [`fdl.ModelInfo`](ref:fdlmodelinfo) object is called `probability_over_50k`.
"
"---
title: ""Ranking""
slug: ""ranking""
excerpt: """"
hidden: false
createdAt: ""Mon May 02 2022 15:39:22 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Onboarding a Ranking Model

Suppose you would like to onboard a ranking model for the following dataset.

![](https://files.readme.io/1d6eb09-expedia_df.png ""expedia_df.png"")

Following is an example of how you would construct a [`fdl.ModelInfo`](ref:fdlmodelinfo) object for a ranking model.

```python
PROJECT_ID = 'example_project'
DATASET_ID = 'expedia_data'
MODEL_ID = 'ranking_model'

model_task = fdl.ModelTask.RANKING
model_group_by = 'srch_id'
model_target = 'click_bool'
model_outputs = ['score']
raning_top_k = 20
model_features = [
    'price_usd',
    'promotion_flag',
    'weekday',
    'week_of_year',
    'hour_time',
    'minute_time'

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id=DATASET_ID,
    features=model_features,
    group_by=model_group_by,
    ranking_top_k=ranking_top_k,
    target=model_target,
    outputs=model_outputs,
    model_task=model_task,
)

client.add_model(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    model_id=MODEL_ID,
    model_info=model_info
)
```

> üìò Note
> 
> Using [client.add_model()](ref:clientadd_model) does not provide Fiddler with a model artifact.  Onboarding a model in this fashion is a good start for model monitoring, but Fiddler will not be able to offer model explainability features without a model artifact.  You can subsequently call [client.add_model_surrogate()](ref:clientadd_model_surrogate) or [client.add_model_artifact()](ref:clientadd_model_artifact) to provide Fiddler with a model artifact.  Please see [Uploading a Model Artifact](doc:uploading-model-artifacts) for more information.

> üöß Note
> 
> `group_by`: when onboarding a ranking model, you **must specify** a `group_by` argument to the [`fdl.ModelInfo`](ref:fdlmodelinfo) object. It will tell Fiddler which column should be used for **grouping items** so that they may be ranked within a group.
> 
> `ranking_top_k`: an optional parameter unique to ranking model. Default to `50`. It's an int representing the top k outputs to take into consideration when computing performance metrics MAP and NDCG.

> üìò Tips
> 
> When onboarding a **graded ranking model** with **categorical target**, `categorical_target_class_detail` is a required argument for [`fdl.ModelInfo`](ref:fdlmodelinfo) object. For example: `categorical_target_class_details=['booked','click_no_booking','no_click']`
"
"---
title: ""Multiclass Classification""
slug: ""multiclass-classification""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:12:22 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Onboarding a Multiclass Classification Model

Suppose you would like to onboard a multiclass classification model for the following dataset.

![](https://files.readme.io/5eabe9a-iris_df.png ""iris_df.png"")

Following is an example of how you would construct a [`fdl.ModelInfo`](ref:fdlmodelinfo) object and onboard such a model.

[block:tutorial-tile]
{
  ""backgroundColor"": ""#018FF4"",
  ""emoji"": ""ü¶â"",
  ""id"": ""657247e2c34d980010a87870"",
  ""link"": ""https://docs.fiddler.ai/v1.5/recipes/add-a-multi-class-classification-model"",
  ""slug"": ""add-a-multi-class-classification-model"",
  ""title"": ""Add a Multi-class Classification Model""
}
[/block]


> üìò categorical_target_class_details
> 
> For multiclass models, the `categorical_target_class_details` argument is required.
> 
> This argument should be a **list of your target classes** in the order that your model outputs predictions for them.

```python
PROJECT_ID = 'example_project'
DATASET_ID = 'iris_data'
MODEL_ID = 'multiclass_model'

dataset_info = client.get_dataset_info(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID
)

model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION
model_target = 'species'
model_outputs = [
    'probability_0',
    'probability_1',
    'probability_2'
]
model_features = [
    'sepal_length',
    'sepal_width',
    'petal_length',
    'petal_width'
]

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id=DATASET_ID,
    target=model_target,
    outputs=model_outputs,
    model_task=model_task,
    categorical_target_class_details=[0, 1, 2]
)

client.add_model(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    model_id=MODEL_ID,
    model_info=model_info
)
```

> üìò Note
> 
> Using [client.add_model()](ref:clientadd_model) does not provide Fiddler with a model artifact.  Onboarding a model in this fashion is a good start for model monitoring, but Fiddler will not be able to offer model explainability features without a model artifact.  You can subsequently call [client.add_model_surrogate()](ref:clientadd_model_surrogate) or [client.add_model_artifact()](ref:clientadd_model_artifact) to provide Fiddler with a model artifact.  Please see [Uploading a Model Artifact](doc:uploading-model-artifacts) for more information.
"
"---
title: ""No Model Task Specified""
slug: ""no-model-task-specified""
excerpt: """"
hidden: false
createdAt: ""Tue Oct 10 2023 18:52:44 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Onboarding a model without specifying a model task

The model task `NOT_SET` can be used if Fiddler doesn't provide the model task needed or if XAI and scoring functionalities are not necessary. This model task doesn't have any restrictions for the outputs and targets field, meaning those can be omitted or specified for any columns (no restriction on the number or type of columns).

Suppose you would like to onboard a model for the following dataset.

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/235babe-f17fd5e-wine_df.png"",
        """",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


Following is an example of how you could construct a [`fdl.ModelInfo`](ref:fdlmodelinfo) object and onboard such a model.

```python
PROJECT_ID = 'example_project'
DATASET_ID = 'wine_data'
MODEL_ID = 'example_model'

dataset_info = client.get_dataset_info(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID
)

model_task = fdl.ModelTask.NOT_SET

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id=DATASET_ID,
    model_task=model_task
)

client.add_model(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    model_id=MODEL_ID,
    model_info=model_info
)
```
"
"---
title: ""LLM""
slug: ""llm""
excerpt: ""Large Language Model""
hidden: false
createdAt: ""Tue Oct 10 2023 18:52:05 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Onboarding an LLM task

Suppose you would like to onboard an LLM model for the following dataset:

[block:image]
{
  ""images"": [
    {
      ""image"": [
        ""https://files.readme.io/f781abd-Screen_Shot_2023-10-10_at_4.24.17_PM.png"",
        """",
        """"
      ],
      ""align"": ""center""
    }
  ]
}
[/block]


Following is an example of how you could construct a [`fdl.ModelInfo`](ref:fdlmodelinfo) object and onboard such a model.

```python
PROJECT_ID = 'example_project'
DATASET_ID = 'dialogue_data'
MODEL_ID = 'llm_model'

dataset_info = client.get_dataset_info(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID
)

model_task = fdl.ModelTask.LLM

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id=DATASET_ID,
    model_task=model_task
)

client.add_model(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    model_id=MODEL_ID,
    model_info=model_info
)
```
"
"---
title: ""Binary Classification""
slug: ""binary-classification""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:12:12 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Onboarding a Binary Classification Model

Suppose you would like to onboard a binary classification model for the following dataset.

![](https://files.readme.io/138d2f2-adult_df.png ""adult_df.png"")

Following is an example of how you would construct a [`fdl.ModelInfo`](ref:fdlmodelinfo) object and onboard such a model.

```python
PROJECT_ID = 'example_project'
DATASET_ID = 'adult_data'
MODEL_ID = 'binary_model'

dataset_info = client.get_dataset_info(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID
)

model_task = fdl.ModelTask.BINARY_CLASSIFICATION
model_target = 'income'
model_outputs = ['probability_over_50k']
model_features = [
    'age',
    'fnlwgt',
    'education_num',
    'capital_gain',
    'capital_loss',
    'hours_per_week'
]

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id=DATASET_ID,
    target=model_target,
    outputs=model_outputs,
    model_task=model_task
)

client.add_model(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    model_id=MODEL_ID,
    model_info=model_info
)
```
"
"---
title: ""Regression""
slug: ""regression-models""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:11:30 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Onboarding a Regression Model

Suppose you would like to onboard a regression model for the following dataset.

![](https://files.readme.io/f17fd5e-wine_df.png ""wine_df.png"")

Following is an example of how you would construct a [`fdl.ModelInfo`](ref:fdlmodelinfo) object and onboard such a model.

```python
PROJECT_ID = 'example_project'
DATASET_ID = 'wine_data'
MODEL_ID = 'regression_model'

dataset_info = client.get_dataset_info(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID
)

model_task = fdl.ModelTask.REGRESSION
model_target = 'quality'
model_outputs = ['predicted_quality']
model_features = [
    'fixed_acidity',
    'volatile_acidity',
    'citric_acid',
    'residual_sugar',
    'chlorides',
    'free_sulfur_dioxide',
    'total_sulfur_dioxide',
    'density',
    'ph',
    'sulphates',
    'alcohol'
]

model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id=DATASET_ID,
    target=model_target,
    outputs=model_outputs,
    model_task=model_task
)

client.add_model(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    model_id=MODEL_ID,
    model_info=model_info
)
```

> üöß Note
> 
> If you **do not provide model predictions** in the DataFrame used to infer the [`fdl.DatasetInfo`](ref:fdldatasetinfo) object, you‚Äôll need to pass a dictionary into the `outputs` argument of [`fdl.ModelInfo.from_dict`](ref:fdlmodelinfofrom_dict) that contains the **min and max values** for the model output.
> 
> ```python
> model_outputs = {
>     'predicted_quality': (0.0, 1.0)
> }
> ```
"
"---
title: ""Publishing Events With Complex Data Formats""
slug: ""publishing-events-with-complex-data-formats""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:15:51 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üìò Info
> 
> See [`client.publish_events_batch_schema`](ref:clientpublish_events_batch_schema) for detailed information on function usage.

## Using a mapping to transform event data

Fiddler supports publishing batches of events that are stored in unconventional formats.

These formats include:

- [CSVs with unlabeled columns](#unlabeled-tabular-data)
- [Nested structures (JSON/Avro)](#nested-data-formats)
- [Files with events for multiple models](#publishing-to-multiple-models-from-the-same-file)

To handle complex data formats, Fiddler offers the ability to transform event data prior to ingestion.  
The function [`client.publish_events_batch_schema`](ref:clientpublish_events_batch_schema) accepts a ""schema"" containing a mapping which will be used to transform production data according to your needs.

Here's an example of what one of these schemas may look like:

```python
publish_schema = {
    ""__static"": {
        ""__project"": ""example_project"",
        ""__model"": ""example_model""
    },
    ""__dynamic"": {
        ""__timestamp"": ""column0"",
        ""feature0"": ""column1"",
        ""feature1"": ""column2"",
        ""feature2"": ""column3"",
        ""model_output"": ""column4"",
        ""model_target"": ""column5""
    }
}
```

The above schema allows us to take unlabeled columns (named `column0` through `column4`) and map them to the names that Fiddler expects (specified in [`fdl.ModelInfo`](ref:fdlmodelinfo)).

Some notes about the above schema:

- `__static` fields are hard-coded. They do not reference anything within the structure.
- `__dynamic` fields point to a location within the structure. We can use forward slashes (/) to indicate traversal of the nested structure.
- `__project` refers to the project ID for the project to which we would like to publish events.
- `__model` refers to the model ID for the model to which we would like to publish events.
- `__timestamp` must refer to the timestamp field within the file structure. If it is not specified, you‚Äôll need to include `__default_timestamp` in the `__static` section.
- You can set the default timestamp to the current time by setting `""__default_timestamp"": ""CURRENT_TIME""` in the `__static` section.

Once you have a schema, you can publish a batch of events using the [`client.publish_events_batch_schema`](ref:clientpublish_events_batch_schema) function:

```python
client.publish_events_batch_schema(
    publish_schema=publish_schema,
    batch_source='example_batch.csv'
)
```

## Unlabeled tabular data

You can use one of these schemas in the case where you have a CSV file with no column headers.  
The mapping will allow you to reference columns by index rather than name.

Suppose we took the above example, except this time the columns had no headers.  
We could use the following schema to map the columns to the necessary names.

```python
publish_schema = {
    ""__static"": {
        ""__project"": ""example_project"",
        ""__model"": ""example_model""
    },
"
"slug: ""publishing-events-with-complex-data-formats""     ""__dynamic"": {
        ""__timestamp"": ""[0]"",
        ""feature0"": ""[1]"",
        ""feature1"": ""[2]"",
        ""feature2"": ""[3]"",
        ""model_output"": ""[4]"",
        ""model_target"": ""[5]""
    }
}
```

## Nested data formats

Fiddler supports publishing batches of events that are stored in non-tabular formats.  
For JSON/Avro nested structures, you can provide a mapping dictionary that will extract the fields you want to monitor and flatten them prior to upload.f

Suppose you have some nested data that‚Äôs structured as follows.  
Here, `value0` through `value7` are the fields we want to monitor.

```json
{
    ""data"": {
        ""value0"": 0,
        ""value1"": 1,
        ""more_data"": {
            ""value2"": 2,
            ""value3"": 3,
            ""even_more_data"": [
                {
                    ""value4"": 4,
                    ""value5"": 5
                },
                {
                    ""value6"": 6,
                    ""value7"": 7
                }
            ]
        }
    }
}
```

For Fiddler to extract these six inputs, we can use the following mapping to flatten the data.

```python
publish_schema = {
    ""__static"": {
        ""__project"": ""example_project"",
        ""__model"": ""example_model"",
        ""__default_timestamp"": ""CURRENT_TIME""
    },
    ""__dynamic"": {
        ""value0"": ""data/value0"",
        ""value1"": ""data/value1"",
        ""value2"": ""data/more_data/value2"",
        ""value3"": ""data/more_data/value3"",
        ""value4"": ""data/more_data/even_more_data[0]/value4"",
        ""value5"": ""data/more_data/even_more_data[0]/value5"",
        ""value6"": ""data/more_data/even_more_data[-1]/value6"",
        ""value7"": ""data/more_data/even_more_data[-1]/value7""
    }
}
```

## Using iterators for multiple events stored in the same row

We can also use mappings to extract multiple events contained in a single JSON/Avro row.  
For this, we will look for ""iterators"" in the row, which are just lists/arrays containing the multiple events we would like to extract.

- `__iterator` fields point to the location of a list of subtrees we would like to iterate over to obtain multiple records. For each tree within the iterator, additional dynamic fields can be specified. Those fields will be joined with the fields outside of the iterator.
  - Note that an `__iterator_key` must be specified for iterators. This should contain the path to the list containing items to be iterated over.

Suppose you have some nested data that‚Äôs structured as follows.  
Here, `value0` through `value5` are the fields we want to monitor.

```json
{
    ""data"": {
        ""value0"": 0,
        ""value1"": 1,
        ""more_data"": [
            {
                ""value2"": 2,
                ""value3"": 3,
                ""even_more_data"": [
                    {
                        ""value4"": 4,
                        ""value5"": 5
                    },
                    {
                        ""value4"": 6,
                        ""value5"": 7
                    }
                ]
            },
            {
                ""value2"": 8,
                ""value3"": 9,
                ""even_more_data"": [
"
"slug: ""publishing-events-with-complex-data-formats""                     {
                        ""value4"": 10,
                        ""value5"": 11
                    },
                    {
                        ""value4"": 12,
                        ""value5"": 13
                    }
                ]
            }
        ]
    }
}
```

Notice that we have four records contained within identical subtrees of the structure. Fiddler will perform a join on the values within the subtrees and the values outside of the subtrees.

```python
publish_schema = {
    ""__static"": {
        ""__project"": ""example_project"",
        ""__model"": ""example_model"",
        ""__default_timestamp"": ""CURRENT_TIME""
    },
    ""__dynamic"": {
        ""value0"": ""data/value0"",
        ""value1"": ""data/value1""
    },
    ""__iterator"": {
        ""__iterator_key"": ""more_data"",
        ""__dynamic"": {
            ""value2"": ""value2"",
            ""value3"": ""value3""
        },
        ""__iterator"": {
            ""__iterator_key"": ""even_more_data"",
            ""__dynamic"": {
                ""value4"": ""value4"",
                ""value5"": ""value5""
            }
        }
    }
}
```

To clarify, this is the output we will see once the values from above example are flattened.  
Note that the outermost fields have been duplicated across all the records.

![](https://files.readme.io/bba09c8-publish_schema_df.png ""publish_schema_df.png"")

## Publishing to multiple models from the same file

Fiddler allows you to publish a single file containing events for multiple models using one API call.

To do this, you can include conditional keys in the schema, which can be used to tell Fiddler which project/model to publish to.

Here's an example of what these conditionals looks like within a schema:

```python
publish_schema = {
    ""__static"": {
        ""__default_timestamp"": ""CURRENT_TIME""
    },
    ""__dynamic"": {
        ""__timestamp"": ""column0"",
        ""__project"": ""column1""
        ""__model"": ""column2"",

        ""!example_project_1,example_model_1"": {
            ""feature0"": ""column3"",
            ""feature1"": ""column4"",
            ""model_output"": ""column6"",
            ""model_target"": ""column7""
        },

        ""!example_project_1,example_model_2"": {
            ""feature0"": ""column4"",
            ""feature1"": ""column5"",
            ""model_output"": ""column6"",
            ""model_target"": ""column7""
        },

        ""!example_project_2,example_model_3"": {
            ""feature0"": ""column3"",
            ""feature1"": ""column5"",
            ""model_output"": ""column6"",
            ""model_target"": ""column7""
        }
    }
}
```

In the above schema, we use the `""!example_project_1,example_model_1""` conditional to tell Fiddler to publish the events to the `example_project_1` project and `example_model_1` model, using the schema defined for that conditional.

> üöß Note
> 
> In order to use this conditional functionality, you'll need to specify the `__project` and `__model` **outside** of the conditional.
"
"---
title: ""Using Custom Timestamps""
slug: ""using-custom-timestamps""
excerpt: """"
hidden: false
createdAt: ""Wed Jul 06 2022 16:25:21 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Fiddler supports **custom timestamp formats when publishing events**.

By default, Fiddler will try to infer your timestamp format, but if you would like to manually specify it, you can do so as well.

When calling [`client.publish_event`](ref:clientpublish_event), there is a `timestamp_format` argument that can be specified to tell Fiddler which format you are using in the event timestamp (specified by `event_timestamp`).

Fiddler supports the following timestamp formats:

## Unix/epoch time in milliseconds

These timestamps take the form of `1637344470000`.

We can specify this timestamp format by passing `fdl.FiddlerTimestamp.EPOCH_MILLISECONDS` into the `timestamp_format` argument.

```python
client.publish_event(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    event=example_event,
    event_id='event_001',
    event_timestamp=1637344470000,
    timestamp_format=fdl.FiddlerTimestamp.EPOCH_MILLISECONDS
)
```

## Unix/epoch time in seconds

These timestamps take the form of `1637344470`.

We can specify this timestamp format by passing `fdl.FiddlerTimestamp.EPOCH_SECONDS` into the `timestamp_format` argument.

```python
client.publish_event(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    event=example_event,
    event_id='event_001',
    event_timestamp=1637344470,
    timestamp_format=fdl.FiddlerTimestamp.EPOCH_SECONDS
)
```

## ISO 8601

These timestamps take the form of `2021-11-19 17:54:30`.

We can specify this timestamp format by passing `fdl.FiddlerTimestamp.ISO_8601` into the `timestamp_format` argument.

```python
client.publish_event(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    event=example_event,
    event_id='event_001',
    event_timestamp='2021-11-19 17:54:30',
    timestamp_format=fdl.FiddlerTimestamp.ISO_8601
)
```

## What if I'm using batch publishing?

The same argument (`timestamp_format`) is available in both the [`client.publish_events_batch`](ref:clientpublish_events_batch) and [`client.publish_events_batch_schema`](ref:clientpublish_events_batch_schema) functions.
"
"---
title: ""Publishing Batches of Events""
slug: ""publishing-batches-of-events""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:15:35 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üìò Info
> 
> See [client.publish_events_batch()](ref:clientpublish_events_batch) for detailed information on function usage.

Fiddler has a flexible ETL framework for retrieving and publishing batches of production data, either from local storage or from the cloud. This provides maximum flexibility in how you are required to store your data when publishing events to Fiddler.  

***

**The following data formats are currently supported:**

- pandas DataFrame objects (`pd.DataFrame`)
- CSV files (`.csv`),
- Parquet files (`.pq`)
- Pickled pandas DataFrame objects (`.pkl`),
- gzipped CSV files (`.csv.gz`),

***

**The following data locations are supported:**

- In memory (for DataFrames)
- Local disk
- AWS S3

***

Once you have a batch of events stored somewhere, all you need to do to publish the batch to Fiddler is call the Fiddler client's `publish_events_batch` function.

```python
client.publish_events_batch(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    batch_source=""my_batch.csv""
)
```

_After calling the function, please allow 3-5 minutes for events to populate the_ **_Monitor_** _page._
"
"---
title: ""Publishing Ranking Events""
slug: ""ranking-events""
excerpt: """"
hidden: false
createdAt: ""Thu Jun 30 2022 22:05:47 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
## Publish ranking events

### The grouped format

Before publishing ranking model events into Fiddler, we need to make sure they are in **grouped format** (i.e. the listing returned within the same **query id**‚Äîwhich is usually the `group_by` argument passed to [`fdl.ModelInfo`](/reference/fdlmodelinfo)‚Äîis in the same row with other cells as lists). The first row in the example below indicates there are 3 items returned by **query id**(`srch_id'` in the table) 1. 

Below is an example of what this might look like.

| srch_id | price_usd                 | review_score      | ...   | prediction              | target    |
| :------ | :------------------------ | :---------------- | :---- | :---------------------- | --------- |
| 101     | [134.77,180.74,159.80]    | [5.0,2.5,4.5]     | [...] | [1.97, 0.84,-0.69]      | [1,0,0]   |
| ...     | ...                       |                   | ...   | ...                     | ...       |
| 112     | [26.00,51.00,205.11,73.2] | [3.0,4.5,2.0,1.0] | [...] | [10.75,8.41,-0.23,-3.2] | [0,1,0,0] |

In the above example, `srch_id` is the name of our `group_by` column, and the other columns all contain lists corresponding to the given group.

### How can I convert a flat CSV file into this format?

If you're storing your data in a flat CSV file (i.e. each row contains a single item), Fiddler provides a utility function that can be used to convert the flat CSV file into the grouped format specified above. 

```python
from fiddler.utils.pandas_helper import convert_flat_csv_data_to_grouped
import pandas as pd

grouped_df = convert_flat_csv_data_to_grouped(input_data=pd.read_csv('path/to/ranking_events.csv'), group_by_col='srch_id')
```

### Call `publish_events_batch`

```python
client.publish_events_batch(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    batch_source=grouped_df,
  	id_field='event_id',
)
```

In the above example, the `group_by_col` argument should refer to the same column that was specified in the `group_by` argument passed to [`fdl.ModelInfo`](/reference/fdlmodelinfo).

## Update ranking events

### Prepare the updating dataframe

We also support updating events for ranking model. You can use `publish_events_batch` and `publish_event` APIs with `update_event` flag to `True` and keep the grouped format unchanged.

For example, you might want to alter the exisiting `target` after events are published. You can create a dataframe in the format below where `group_by_col`,`id_col` and `target_col` are required fields. You can either upload the complete group of events within one `query"
"slug: ""ranking-events"" _id` or the subset contains the changed events.

`Complete format`

| srch_id | event_id                  | target    |
| :------ | :------------------------ | :-------- |
| 101     | ['001','002','003']       | [0,1,0]   |
| ...     | ...                       | ...       |
| 112     | ['367','368','369','370'] | [0,0,0,1] |

`Partial format`

| srch_id | event_id      | target |
| :------ | :------------ | :----- |
| 101     | ['001','002'] | [0,1]  |
| ...     | ...           | ...    |
| 112     | ['367','370'] | [0,1]  |

### Call `publish_events_batch` with `update_event` flag set to True

```python Python
client.publish_events_batch(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    batch_source=grouped_df_update,
  	id_field='event_id',
  	update_event=True,
)
```

### Or call `publish_event` with `update_event` flag

```python Python
events_dict = grouped_df_graded.to_dict('index')
for i, group_id in enumerate(events_dict):
    e= events_dict[group_id]
    '''
    first event:
    {'srch_id':101,'event_id':['001','002'],'target':[0,1]}
    '''
    client_v2.publish_event(project_id=project_id, model_id=model_id, event=e, update_event=True, event_id=str(e['event_id']))
```
"
"---
title: ""Retrieving Events""
slug: ""retrieving-events""
excerpt: """"
hidden: false
createdAt: ""Wed Jul 06 2022 16:22:23 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
After publishing events to Fiddler, you may want to retrieve them for further analysis.

## Querying production data

You can query production data from the **Analyze** tab by issuing the following SQL query to Fiddler.

```sql
SELECT
    *
FROM
    ""production.MODEL_ID""
```

The above query will return the entire production table (all published events) for a model with a model ID of `MODEL_ID`.

## Querying a baseline dataset

You can query a baseline dataset that has been uploaded to Fiddler with the following SQL query.

```sql
SELECT
    *
FROM
    ""DATASET_ID.MODEL_ID""
```

Here, this will return the entire baseline dataset that has been uploaded with an ID of `DATASET_ID` to a model with an ID of `MODEL_ID`.
"
"---
title: ""Streaming Live Events""
slug: ""streaming-live-events""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:07:23 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üìò Info
> 
> See [`client.publish_event`](ref:clientpublish_event) for detailed information on function usage.

One way to publish production data to Fiddler is by streaming data asynchronously.

This process is very simple, but it requires that each event is structured as a Python dictionary that maps field names (as they are [onboarded](ref:fdlmodelinfo) with Fiddler) to values.

***

## Example 1: A simple three-input fraud model.

```python
my_event = {
    ""age"": 30,
    ""gender"": ""Male"",
    ""salary"": 80000.0,
    ""predicted_fraud"": 0.89,
    ""is_fraud"": 1
}
```

> üöß Note
> 
> If you have a pandas DataFrame, you can easily **convert it into a list of event dictionaries** in the above form by using its `to_dict` function.

```python Python
my_events = my_df.to_dict(orient=""records"")
```

***

Then to upload the event to Fiddler, all you have to do is call the Fiddler client's `publish_event` method.

```python
client.publish_event(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    event=my_event,
    event_timestamp=1635862057000
)
```

_After calling the function, please allow 3-5 minutes for events to populate the_ **_Monitor_** _page._

> üìò Info
> 
> The `event_timestamp` field should contain the **Unix timestamp in milliseconds** for the time the event occurred. This timestamp will be used to plot the event on time series charts for monitoring.
> 
> If you do not specify an event timestamp, the current time will be used.

## Example 2: Bank churn event

In order to send traffic to Fiddler, use the [`publish_event`](ref:clientpublish_event) API from the Fiddler SDK. Here is a sample of the API call:

```python Publish Event
import fiddler as fdl
	fiddler_api = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=token)
	# Publish an event
	fiddler_api.publish_event(
		project_id='bank_churn',
		model_id='bank_churn',
		event={
			""CreditScore"": 650,      # data type: int
			""Geography"": ""France"",   # data type: category
			""Gender"": ""Female"",
			""Age"": 45,
			""Tenure"": 2,
			""Balance"": 10000.0,      # data type: float
			""NumOfProducts"": 1,
			""HasCrCard"": ""Yes"",
			""isActiveMember"": ""Yes"",
			""EstimatedSalary"": 120000,
			""probability_churned"": 0.105,
      ""churn"": 1
		},
		event_id=‚Äôsome_unique_id‚Äô, #optional
		update_event=False, #optional
		event_timestamp=1511253040519 #optional
	)
```

The `publish_event`"
"slug: ""streaming-live-events""  API can be called in real-time right after your model inference. 

> üìò Info
> 
> You can also publish events as part of a batch call after the fact using the `publish_events_batch` API (click [here](ref:clientpublish_events_batch) for more information). In this case, you will need to send Fiddler the original event timestamps as to accurately populate the time series charts.

Following is a description of all the parameters for `publish_event`:

- `project_id`: Project ID for the project this event belongs to.

- `model_id`: Model ID for the model this event belongs to.

- `event`: The actual event as an array. The event can contain:

  - Inputs
  - Outputs
  - Target
  - Decisions (categorical only)
  - Metadata

- `event_id`: A user-generated unique event ID that Fiddler can use to join inputs/outputs to targets/decisions/metadata sent later as an update.

- `update_event`: A flag indicating if the event is a new event (insertion) or an update to an existing event. When updating an existing event, it's required that the user sends an `event_id`.

- `event_timestamp`: The timestamp at which the event (or update) occurred, represented as a UTC timestamp in milliseconds. When updating an existing event, use the time of the update, i.e., the time the target/decision were generated and not when the model predictions were made.
"
"---
title: ""Updating Events""
slug: ""updating-events""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:16:43 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
Fiddler supports _partial_ updates of events. Specifically, for your **[target](ref:fdlmodelinfo)** column. 

The most common use case for this functionality is updating ground truth labels. Existing events that lack ground truth labels can be updated once the actual values are discovered. Or you might find that the initially uploaded labels are wrong and wish to correct them. Other columns can only be sent at insertion time (with `update_event=False`).

Set `update_event=True` to indicate that you are updating an existing event. You only need to provide the decision, metadata, and/or target fields that you want to change‚Äîany fields you leave out will remain as they were before the update.

***

For [`client.publish_event`](ref:clientpublish_event):

```python
client.publish_event(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    event=my_event,
    event_id=my_id,
    update_event=True
)
```

For [`client.publish_events_batch`](ref:clientpublish_events_batch):

```python
client.publish_events_batch(
    project_id=PROJECT_ID,
    model_id=MODEL_ID,
    batch_source=""my_batch.csv"",
    id_field=my_id_field,
    update_event=True
)
```

For [`client.publish_events_batch_schema`](ref:clientpublish_events_batch_schema):

```python
client.publish_events_batch_schema(
    batch_source=""my_batch.csv"",
    publish_schema=my_schema,
    update_event=True
)
```

***

> üìò **There are a few points to be aware of:**
> 
> - [Performance](doc:performance) metrics (available from the **Performance** tab of the **Monitor** page) will be computed as events are updated.
>   - For example, if the ground truth values are originally missing from events in a given time range, there will be **no performance metrics available** for that time range. Once the events are updated, performance metrics will be computed and will populate the monitoring charts.
>   - Events that do not originally have ground truth labels should be **uploaded with empty values**‚Äînot dummy values. If dummy values are used, you will have improper performance metrics, and once the new values come in, the old, incorrect values will still be present.
> - In order to update existing events, you will need access to the event IDs used at the time of upload. If you do not have access to those event IDs, you can find them by using the [`client.get_slice`](ref:clientget_slice) API and checking the `__event_id` column from the resulting DataFrame.
> - If you pass an updated timestamp for an existing event, **this timestamp will be used** for plotting decisions and computed performance metrics on the **[Monitor](doc:monitoring-ui)** page. That is, the bin for which data will appear will depend on the new timestamp, not the old one.
"
"---
title: ""Uploading a TensorFlow HDF5 Model Artifact""
slug: ""tensorflow-hdf5""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:14:00 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üöß Note
> 
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](doc:uploading-model-artifacts).

Suppose you would like to upload a model artifact for a **TensorFlow (HDF5) model**.

Following is an example of what the `package.py` script may look like.

```python
import pickle
from pathlib import Path
import pandas as pd
import tensorflow as tf

PACKAGE_PATH = Path(__file__).parent

OUTPUT_COLUMN = ['probability_over_50k']

class MyModel:

    def __init__(self):
        
        # Load the model
        self.model = tf.keras.models.load_model(PACKAGE_PATH / 'model.h5')

    def predict(self, input_df):
        
        # Store predictions in a DataFrame
        return pd.DataFrame(self.model.predict(input_df), columns=OUTPUT_COLUMN)

def get_model():
    return MyModel()
```
"
"---
title: ""Uploading an XGBoost Model Artifact""
slug: ""xgboost""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:13:35 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üöß Note
> 
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](doc:uploading-model-artifacts).

Suppose you would like to upload a model artifact for a **XGBoost model**.

Following is an example of what the `package.py` script may look like.

```python
import pickle
from pathlib import Path
import pandas as pd
import xgboost as xgb

PACKAGE_PATH = Path(__file__).parent

OUTPUT_COLUMN = ['probability_over_50k']

class MyModel:

    def __init__(self):
        
        # Load the model
        with open(PACKAGE_PATH / 'model.pkl', 'rb') as pkl_file:
            self.model = pickle.load(pkl_file)

    def transform_input(self, input_df):
        
        # Convert DataFrame to XGBoost DMatrix
        return xgb.DMatrix(input_df)

    def predict(self, input_df):
        
        # Apply data transformation
        transformed_input = self.transform_input(input_df)
        
        # Store predictions in a DataFrame
        return pd.DataFrame(self.model.predict(transformed_input), columns=OUTPUT_COLUMN)

def get_model():
    return MyModel()
```
"
"---
title: ""Uploading a TensorFlow SavedModel Model Artifact""
slug: ""tensorflow-savedmodel""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:13:41 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üöß Note
> 
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](doc:uploading-model-artifacts).

Suppose you would like to upload a model artifact for a **TensorFlow (SavedModel) model**.

Following is an example of what the `package.py` script may look like.

```python
import pickle
from pathlib import Path
import pandas as pd
import tensorflow as tf

PACKAGE_PATH = Path(__file__).parent

OUTPUT_COLUMN = ['probability_over_50k']

class MyModel:

    def __init__(self):
        
        # Load the model
        self.model = tf.keras.models.load_model(PACKAGE_PATH / 'saved_model')

    def predict(self, input_df):
        
        # Store predictions in a DataFrame
        return pd.DataFrame(self.model.predict(input_df), columns=OUTPUT_COLUMN)

def get_model():
    return MyModel()
```
"
"---
title: ""Uploading a scikit-learn Model Artifact""
slug: ""scikit-learn""
excerpt: """"
hidden: false
createdAt: ""Tue Apr 19 2022 20:13:31 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
> üöß Note
> 
> For more information on uploading a model artifact to Fiddler, see [Uploading a Model Artifact](doc:uploading-model-artifacts).

Suppose you would like to upload a model artifact for a **scikit-learn model**.

Following is an example of what the `package.py` script may look like.

```python
import pickle
from pathlib import Path
import pandas as pd
from sklearn.linear_model import LogisticRegression

PACKAGE_PATH = Path(__file__).parent

OUTPUT_COLUMN = ['probability_over_50k']

class MyModel:

    def __init__(self):
        
        # Load the model
        with open(PACKAGE_PATH / 'model.pkl', 'rb') as pkl_file:
            self.model = pickle.load(pkl_file)

    def predict(self, input_df):
        
        # Store predictions in a DataFrame
        return pd.DataFrame(self.model.predict_proba(input_df)[:, 1], columns=OUTPUT_COLUMN)

def get_model():
    return MyModel()
```
"
"---
title: ""client.get_slice""
slug: ""clientget_slice""
excerpt: ""Retrieve a slice of data as a pandas DataFrame.""
hidden: false
createdAt: ""Thu Oct 05 2023 16:33:27 GMT+0000 (Coordinated Universal Time)""
updatedAt: ""Thu Dec 07 2023 22:32:06 GMT+0000 (Coordinated Universal Time)""
---
| Input Parameter  | Type            | Default | Description                                                                                                                                     |
| :--------------- | :-------------- | :------ | :---------------------------------------------------------------------------------------------------------------------------------------------- |
| sql_query        | str             | None    | The SQL query used to retrieve the slice.                                                                                                       |
| project_id       | str             | None    | The unique identifier for the project.  The model and/or the dataset to be queried within the project are designated in the _sql_query_ itself. |
| columns_override | Optional [list] | None    | A list of columns to include in the slice, even if they aren't specified in the query.                                                          |

```python Usage - Query a dataset
import pandas as pd

PROJECT_ID = 'example_project'
DATASET_ID = 'example_dataset'
MODEL_ID = 'example_model'

query = f"""""" SELECT * FROM ""{DATASET_ID}.{MODEL_ID}"" """"""

slice_df = client.get_slice(
    sql_query=query,
    project_id=PROJECT_ID
)
```
```python Usage - Query published events
import pandas as pd

PROJECT_ID = 'example_project'
MODEL_ID = 'example_model'

query = f"""""" SELECT * FROM ""production.{MODEL_ID}"" """"""

slice_df = client.get_slice(
    sql_query=query,
    project_id=PROJECT_ID
)
```

| Return Type  | Description                                                    |
| :----------- | :------------------------------------------------------------- |
| pd.DataFrame | A pandas DataFrame containing the slice returned by the query. |

> üìò Info
> 
> Only read-only SQL operations are supported. Certain SQL operations like aggregations and joins might not result in a valid slice.
"
"package.py for R based models```python
import fiddler as fdl
```


```python
print(fdl.__version__)
```

    1.6.2



```python
url = ''
token = ''
org_id = ''

client = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=token, version=2)
```


```python
project_id = 'test_r3'
model_id = 'iris'
dataset_id = 'iris'
```


```python
# client.create_project(project_id=project_id)
```


```python
import pandas as pd
from pathlib import Path
import yaml
```


```python
df = pd.read_csv('test_R/data_r.csv')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>Sepal.Length</th>
      <th>Sepal.Width</th>
      <th>Petal.Length</th>
      <th>Petal.Width</th>
      <th>Species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
  </tbody>
</table>
</div>




```python
dataset_info = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=100)
dataset_info
```




<div style=""border: thin solid rgb(41, 57, 141); padding: 10px;""><h3 style=""text-align: center; margin: auto;"">DatasetInfo
</h3><pre>display_name: 
files: []
</pre><hr>Columns:<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>column</th>
      <th>dtype</th>
      <th>count(possible_values)</th>
      <th>is_nullable</th>
      <th>value_range</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Sepal.Length</td>
      <td>FLOAT</td>
      <td></td>
      <td>False</td>
      <td>4.3 - 7.9</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Sepal.Width</td>
      <td>FLOAT</td>
      <td></td>
      <td>False</td>
      <td>2.0 - 4.4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Petal.Length</td>
      <td>FLOAT</td>
      <td></td>
      <td>False</td>
      <td>1.0 - 6.9</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Petal.Width</td>
      <td>FLOAT</td>
      <td></td>
      <td>False</td>
      <td>0.1 - 2.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Species</td>
      <td>CATEGORY</td>
      <td>3</td>
      <td>False</td>
      <td></td>
    </tr>
  </tbody>
</table>
</div></div>




```python
client.upload_dataset(project_id=project_id, dataset={'baseline': df},
                      dataset_id=dataset_id, info=dataset_info)
```


```python
target = 'Species'
outputs = ['proba_setosa', 'proba_versicolor', 'proba_virginica']
features = list(df.drop(columns=[target]).columns)
    
# Generate ModelInfo
model_info = fdl.ModelInfo.from_dataset_info(
    dataset_info=dataset_info,
    dataset_id=dataset_id,
    target=target,
    outputs=outputs,
    features=features,
    categorical_target_class_details=['setosa', 'versicolor', 'virginica'],
    model_task=fdl.ModelTask.MULTICLASS_CLASSIFICATION,
)
model_info
```


```python
model_dir = Path('test_R/iris_r')
```


```python
# save model schema
with open(model_dir / 'model.yaml', 'w') as yaml_file:
    yaml.dump({'model': model_info.to_dict()}, yaml_file)
```


```python
%%writefile test_R/iris_r/package.py

from pathlib import Path

import numpy as np
import pandas as pd
import rpy2.robjects as robjects
from rpy2.robjects import numpy2ri, pandas2ri
from rpy2.robjects.packages import importr

pandas2ri.activate()
numpy2ri.activate()
r = robjects.r


class Model:
    """"""
    R Model Loader

    Attributes
    ----------
    model : R object
    """"""

    def __init__(self):
        self.model = None

    def load(self, path):
        """"""
        load the model at `path`
        """"""
        model_rds_path = f'{path}.rds'

        self.model = r.readRDS(model_rds_path)
        
        _ = [importr(dep.strip()) for dep in ['randomForest'] if dep.strip() != '']


        return self

    def predict(self, input_df):
        """"""
        Perform classification on samples in X.

        Parameters
        ----------
        input_df : pandas dataframe, shape (n_samples, n_features)
        Returns
        -------
        pred : array, shape (n_samples)
        """"""

        if self.model is None:
            raise Exception('There is no Model')


        pred = r.predict(self.model, [input_df], type='prob')
        df = pd.DataFrame(np.array(pred), columns=['proba_setosa', 'proba_versicolor', 'proba_virginica'])

        return df


MODEL_PATH = 'iris'
PACKAGE_PATH = Path(__file__).parent


def get_model():
    return Model().load(str(PACKAGE_PATH / MODEL_PATH))
```


```python
client.upload_model_package(artifact_path=model_dir, project_id=project_id, model_id=model_id)
```


```python
client.run_model(project_id=project_id, model_id=model_id, df=df.head())
```


```python
client.run_feature_importance()
```
"
"Once you have added a model on the Fiddler platform using a specific model info object, that is fdl.ModelInfo, you cannot modify aspects such as features, inputs, outputs, model task etc. specified in the model info object. Currently, if you want to change fundamental details about a modelinfo object, then it is advised to create/add a new model with a new modelinfo object."
Custom metrics is an upcoming feature and it is currently not supported.
"Re-uploading in Fiddler essentially means having to delete what was uploaded or ingested earlier, making the updates you want to make, and then following the same steps as before for the specific thing you are looking to upload. So for example, if you want make changes to the baseline dataset you uploaded, you will have to delete the dataset and then make modifications for the datasetinfo object or the dataset itself and then upload the dataset again. As for events, currently there isn't a way for the user to directly delete events. Please contact Fiddler personnell for the same. "
"Currently, only the following fields in [fdl.ModelInfo()](ref:fdlmodelinfo) can be updated:
> 
> - `custom_explanation_names`
> - `preferred_explanation_method`
> - `display_name`
> - `description` "
"AI has been in the limelight thanks to ‚Äårecent AI products like ChatGPT, DALLE- 2, and Stable Diffusion. These breakthroughs reinforce the notion that companies need to double down on their AI strategy and execute on their roadmap to stay ahead of the competition. However, Large Language Models (LLMs) and other generative AI models pose the risk of providing users with inaccurate or biased results, generating adversarial output that‚Äôs harmful to users, and exposing private information used in training. This makes it critical for companies to implement LLMOps practices to ensure generative AI models and LLMs are continuously high-performing, correct, and safe.The Fiddler AI Observability platform helps standardize LLMOps by streamlining LLM workflows from pre-production to production, and creating a continuous feedback loop for improved prompt engineering and LLM fine-tuning.Figure 1: Fiddler AI Observability optimizes LLMs and generative AI for better outcomesPre-production Workflow:Robust evaluation of prompts and models with Fiddler AuditorWe are thrilled to launch Fiddler Auditor today to ensure LLMs perform in a safe and correct fashion.¬†Fiddler Auditor is the first robustness library that leverages LLMs to evaluate robustness of other LLMs. Testing the robustness of LLMs in pre-production is a critical step in LLMOps. It helps identify weaknesses that can result in hallucinations, generate harmful or biased responses, and expose private information. ML and software application teams can now utilize the Auditor to test model robustness by applying perturbations, including adversarial examples, out-of-distribution inputs, and linguistic variations, and obtain a report to analyze the outputs generated by the LLM.A practitioner can evaluate LLMs from OpenAI, Anthropic, and Cohere using the Fiddler Auditor and find areas to improve correctness and performance while minimizing hallucinations. In the example below, we tested OpenAI‚Äôs test-davinci-003 model with the following prompt and the best output it should generate when prompted: Then, we entered five perturbations with linguistic variations, and only one of them generated the desired output as seen in the report below. If the LLM were released for public use as is, users would lose trust in it as the model generates hallucinations for simple paraphrasing, and users could potentially be harmed had they acted on the output generated.Figure 2: Evaluate the robustness of LLMs in a reportThe Fiddler Auditor is on GitHub. Don‚Äôt forget to give us a star if you enjoy using it! ‚≠ê‚ÄçProduction Workflow:Continuous monitoring to ensure optimal experienceTransitioning into production requires continuous monitoring to ensure optimal performance. Earlier this year, we announced how vector monitoring in the Fiddler AI Observability platform can monitor LLM-based embeddings generated by OpenAI, Anthropic, Cohere, and embeddings from other LLMs with a minimal integration effort. Our clustering-based multivariate drift detection algorithm is a novel method for measuring data drift in natural language processing (NLP) and computer vision (CV) models.ML teams can track and share LLM metrics like model performance, latency, toxicity, costs, and other LLM-specific metrics in real-time using custom dashboards and charts. Metrics like toxicity are calculated by using methods from HuggingFace. Early warnings from flexible model monitoring alerts cut through the noise and help teams prioritize on business-critical¬† issues.¬†Figure 3: Track metrics like toxicity in real-time to improve prompt engineering and LLM fine-tuningImproving LLM performance using root cause analysisOrganizations need in-depth visibility into their AI solutions to help improve user satisfaction. Through slice & explain, ML teams can get a 360¬∞ view into the performance of their AI solutions, helping them refine prompt context, and gain valuable inputs for fine-tuning models.Fiddler AI Observability: A Unified Platform for ML and Generative AI¬†Figure 4: The Fiddler AI Observability platformWith these new product enhancements, the Fiddler AI Observability platform is a full stack platform for predictive and generative AI models. ML/AI and engineering teams can standardize their practices for both LLMOps and MLOps through model monitoring, explainable AI, analytics, fairness, and safety.¬†We continue our unwavering mission to partner with companies in their AI journey to build trust into AI. Our product and data science teams have been working with companies that are defining ways to operationalize AI beyond predictive models and successfully implement generative AI models to deliver high performance AI, reduce costs, and be responsible with model governance.We look forward to building more capabilities to help companies standardize their LLMOps and MLOps. "
"LLM means large language model.  A large language model (LLM) is a type of artificial intelligence (AI) algorithm that uses deep learning techniques and massively large data sets to understand, summarize, generate and predict new content."
"The term generative AI, or GenAI, also is closely connected with LLMs, which are, in fact, a type of generative AI that has been specifically architected to help generate text-based content."
"FM, or FMs, means Foundation Models.  Foundation Models are the same as large language models."
