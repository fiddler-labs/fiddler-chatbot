{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3d3af9-e777-4e44-adff-da030fecfb4b",
   "metadata": {},
   "source": [
    "# Installs and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ac1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "from cassandra.query import named_tuple_factory\n",
    "\n",
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d4a0d-10d5-4491-aa06-019378450ebe",
   "metadata": {},
   "source": [
    "### Connect to DataStax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b3d41-2153-45c4-b37b-8567ca48c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This secure connect bundle is autogenerated when you donwload your SCB, \n",
    "# if yours is different update the file name below\n",
    "cloud_config= {'secure_connect_bundle': 'datastax_auth/secure-connect-fiddlerai.zip'}\n",
    "\n",
    "ASTRA_DB_APPLICATION_TOKEN = os.environ.get('ASTRA_DB_APPLICATION_TOKEN')\n",
    "print(ASTRA_DB_APPLICATION_TOKEN)\n",
    "\n",
    "auth_provider=PlainTextAuthProvider(\"token\", ASTRA_DB_APPLICATION_TOKEN)\n",
    "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
    "\n",
    "session = cluster.connect()\n",
    "\n",
    "session.set_keyspace('fiddlerai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed5b8a-e8e0-48cf-aa49-eed9ec6c1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llmProvider = 'openai'  # 'GCP_VertexAI', 'Azure_OpenAI'\n",
    "table_name = 'fiddler_doc_snippets_' + llmProvider\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"open_ai\"\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Explicitly use the desired model and configure its dimension size\n",
    "myEmbedding = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    model_kwargs={\"dimensions\": 1536}\n",
    ")\n",
    "print(f\"LLM from OpenAI, Embeddings from OpenAI using model '{myEmbedding.model}' with 1536 dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bc517-6674-4c23-a3d9-c43df13e1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed data \n",
    "file_path = 'documentation_data/vector_index_feed_v25.10.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "# df.filter(items=['row_id', 'vector', 'body_blob', 'metadata_s'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a854840-1349-4784-8632-adf4cb0b4543",
   "metadata": {},
   "source": [
    "#### Please double-check your dataframe before running the next cell. \n",
    "##### Running the next cell will delete the existing snippets from the last version of the docs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8d361-8bee-48bd-abdd-98b9a5305b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the database table prior to refill\n",
    "\n",
    "session.execute(f\"TRUNCATE TABLE {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b25b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the pre-chunked documents from the DataFrame\n",
    "# Each row in the CSV is already a perfectly sized chunk from the preprocessing notebook.\n",
    "print(\"Loading pre-chunked documents from the CSV...\")\n",
    "loader = DataFrameLoader(df, page_content_column=\"text\")\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} document chunks.\")\n",
    "\n",
    "# 2. Initialize the vector store directly\n",
    "# We don't need a text_splitter because the documents are already chunked.\n",
    "print(f\"Initializing Cassandra vector store with table '{table_name}'...\")\n",
    "vector_store = Cassandra(\n",
    "    embedding=myEmbedding,\n",
    "    session=session,\n",
    "    keyspace=\"fiddlerai\",\n",
    "    table_name=table_name,\n",
    ")\n",
    "\n",
    "# 3. Add the documents to the vector store\n",
    "# This will create embeddings for each document chunk and store them in Cassandra.\n",
    "print(f\"Adding {len(documents)} chunks to the vector store. This may take a few minutes...\")\n",
    "vector_store.add_documents(documents)\n",
    "\n",
    "print(\"\\n✅ Success! All document chunks have been embedded and stored in Cassandra.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56576c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-liner test\n",
    "def quick_test():\n",
    "    query = \"What is Fiddler?\"\n",
    "    docs = vector_store.similarity_search(query, k=2)\n",
    "    return f\"Retrieved {len(docs)} docs. First doc preview: {docs[0].page_content[:100] if docs else 'None'}\"\n",
    "\n",
    "print(quick_test())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e0c5b9-faa9-46fb-8ba9-81c7b4313198",
   "metadata": {},
   "source": [
    "### You are done. Please check the Chatbot to ensure it is working and returning answers from the lastest docs we just uploaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d516fed6-e6cd-48ca-a169-6ac55940791c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sample code for querying the datastax Vector DB to ensure we have the right data there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f753cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY: Check table structure using system schema queries\n",
    "print(\"=== TABLE INFORMATION ===\")\n",
    "\n",
    "# Temporarily set row factory to get normal Cassandra Row objects\n",
    "original_row_factory = session.row_factory\n",
    "session.row_factory = named_tuple_factory\n",
    "\n",
    "try:\n",
    "    # Get table info\n",
    "    table_info_query = \"\"\"\n",
    "    SELECT table_name, bloom_filter_fp_chance, caching, comment, compaction, compression \n",
    "    FROM system_schema.tables \n",
    "    WHERE keyspace_name = 'fiddlerai' AND table_name = %s\n",
    "    \"\"\"\n",
    "\n",
    "    rows = session.execute(table_info_query, [table_name])\n",
    "    for row in rows:\n",
    "        print(f\"Table: {row.table_name}\")\n",
    "        print(f\"Compaction: {row.compaction}\")\n",
    "        print(f\"Compression: {row.compression}\")\n",
    "        print(f\"Caching: {row.caching}\")\n",
    "\n",
    "    print(\"\\n=== COLUMN INFORMATION ===\")\n",
    "\n",
    "    # Get column info  \n",
    "    columns_query = \"\"\"\n",
    "    SELECT column_name, type, kind, position \n",
    "    FROM system_schema.columns \n",
    "    WHERE keyspace_name = 'fiddlerai' AND table_name = %s\n",
    "    --ORDER BY position\n",
    "    \"\"\"\n",
    "\n",
    "    rows = session.execute(columns_query, [table_name])\n",
    "    for row in rows:\n",
    "        print(f\"Column: {row.column_name}, Type: {row.type}, Kind: {row.kind}\")\n",
    "\n",
    "    print(\"\\n=== ROW COUNT (APPROXIMATE) ===\")\n",
    "    # Get approximate row count\n",
    "    count_query = f\"SELECT COUNT(*) FROM fiddlerai.{table_name}\"\n",
    "    try:\n",
    "        result = session.execute(count_query)\n",
    "        for row in result:\n",
    "            print(f\"Approximate row count: {row.count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: COUNT(*) may timeout on large tables. Error: {e}\")\n",
    "        \n",
    "    print(\"\\n=== SAMPLE DATA ===\")\n",
    "    # Show sample data\n",
    "    sample_query = f\"SELECT * FROM fiddlerai.{table_name} LIMIT 3\"\n",
    "    rows = session.execute(sample_query)\n",
    "    for i, row in enumerate(rows):\n",
    "        print(f\"\\nSample row {i+1}:\")\n",
    "        print(f\"  Row ID: {getattr(row, 'row_id', 'N/A')}\")\n",
    "        if hasattr(row, 'body_blob'):\n",
    "            print(f\"  Content preview: {str(row.body_blob)[:100]}...\")\n",
    "        if hasattr(row, 'vector') and row.vector:\n",
    "            print(f\"  Vector dimensions: {len(row.vector)}\")\n",
    "        if hasattr(row, 'metadata_s'):\n",
    "            print(f\"  Metadata: {row.metadata_s}\")\n",
    "\n",
    "finally:\n",
    "    # Restore the original pandas factory\n",
    "    session.row_factory = original_row_factory\n",
    "    print(\"\\n=== Row factory restored ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a75294-e010-4999-9385-8924d975db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cqlSelect = f'SELECT * FROM fiddlerai.{table_name} LIMIT 10;'  # (Not a production-optimized query ...)\n",
    "\n",
    "rows = session.execute(cqlSelect)\n",
    "print(rows)\n",
    "for row_i, row in enumerate(rows):\n",
    "    print(f'\\nRow {row_i}:')\n",
    "    print(row)\n",
    "    # depending on the cassIO version, the underlying Cassandra table can have different structure ...\n",
    "    #try:\n",
    "        # you are using the new cassIO 0.1.0+ : congratulations :)\n",
    "       # print(f'    row_id:            {row.row_id}')\n",
    "        #print(f'    vector:            {str(row.vector)[:64]} ...')\n",
    "       # print(f'    body_blob:         {row.body_blob} ...')\n",
    "       # print(f'    metadata_s:        {row.metadata_s}')        \n",
    "    #except AttributeError:\n",
    "        # Please upgrade your cassIO to the latest version ...\n",
    "    print(f'    document_id:      {row.row_id}')\n",
    "    print(f'    embedding_vector: {str(row.vector)[:64]} ...')\n",
    "    print(f'    document:         {row.body_blob[:64]} ...')\n",
    "    print(f'    metadata_blob:    {row.metadata_s}')\n",
    "\n",
    "print('\\n...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ea12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cql_query = f'SELECT * FROM fiddlerai.{table_name};'\n",
    "\n",
    "try:\n",
    "    print(\"Executing query and loading data into pandas DataFrame...\")\n",
    "    rows = session.execute(cql_query)\n",
    "\n",
    "    # 2. Convert the Cassandra ResultSet to a pandas DataFrame\n",
    "    df = pd.DataFrame(list(rows))\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Query returned no data. CSV file will not be created.\")\n",
    "    else:\n",
    "        # 3. Save the DataFrame to a CSV file\n",
    "        csv_file_path = f'{table_name}_output_pandas.csv'\n",
    "        \n",
    "        # The index=False argument prevents pandas from writing the DataFrame index as a column\n",
    "        df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"✅ Success! Data has been saved to '{csv_file_path}'\")\n",
    "\n",
    "finally:\n",
    "    if 'cluster' in locals():\n",
    "        print('in locals')\n",
    "    if cluster.is_shutdown:\n",
    "        print('is shutdown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd67ae-2260-4566-a23e-f981f69cc1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "createTableSQL = \"\"\"CREATE TABLE fiddler_chatbot_history (\n",
    "    row_id text PRIMARY KEY,\n",
    "    response text,\n",
    "    response_vector vector<float, 1536>,\n",
    "    source_docs text,\n",
    "    source_docs_vector vector<float, 1536>,\n",
    "    question text,\n",
    "    question_vector vector<float, 1536>,\n",
    "    comment text,\n",
    "    feedback int,\n",
    "    metadata_s map<text, text>,\n",
    "    ts timestamp)\"\"\"\n",
    "# ) WITH additional_write_policy = '99p'\n",
    "#     AND bloom_filter_fp_chance = 0.01\n",
    "#     AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n",
    "#     AND comment = ''\n",
    "#     AND compaction = {'class': 'org.apache.cassandra.db.compaction.UnifiedCompactionStrategy'}\n",
    "#     AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n",
    "#     AND crc_check_chance = 1.0\n",
    "#     AND default_time_to_live = 0\n",
    "#     AND gc_grace_seconds = 864000\n",
    "#     AND max_index_interval = 2048\n",
    "#     AND memtable_flush_period_in_ms = 0\n",
    "#     AND min_index_interval = 128\n",
    "#     AND read_repair = 'BLOCKING'\n",
    "#     AND speculative_retry = '99p';\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a1bad-c048-40a9-b9b6-c070a28a4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.execute(createTableSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c2668f-2d7a-454a-9024-8aefbe645ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "response = openai.Embedding.create(\n",
    "    model=EMBEDDING_MODEL, input=\"How are you doing\", dimensions=1536\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e377fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTED: Check table structure using system schema queries\n",
    "from cassandra.query import named_tuple_factory\n",
    "print(\"=== TABLE INFORMATION ===\")\n",
    "\n",
    "# Temporarily set row factory to get normal Cassandra Row objects\n",
    "original_row_factory = session.row_factory\n",
    "session.row_factory = named_tuple_factory\n",
    "\n",
    "try:\n",
    "    # Get table info\n",
    "    table_info_query = \"\"\"\n",
    "    SELECT table_name, bloom_filter_fp_chance, caching, comment, compaction, compression \n",
    "    FROM system_schema.tables \n",
    "    WHERE keyspace_name = 'fiddlerai' AND table_name = %s\n",
    "    \"\"\"\n",
    "\n",
    "    rows = session.execute(table_info_query, [table_name])\n",
    "    for row in rows:\n",
    "        print(f\"Table: {row.table_name}\")\n",
    "        print(f\"Compaction: {row.compaction}\")\n",
    "        print(f\"Compression: {row.compression}\")\n",
    "        print(f\"Caching: {row.caching}\")\n",
    "\n",
    "    print(\"\\n=== COLUMN INFORMATION ===\")\n",
    "\n",
    "    # Get column info  \n",
    "    columns_query = \"\"\"\n",
    "    SELECT column_name, type, kind, position \n",
    "    FROM system_schema.columns \n",
    "    WHERE keyspace_name = 'fiddlerai' AND table_name = %s\n",
    "    --ORDER BY position\n",
    "    \"\"\"\n",
    "\n",
    "    rows = session.execute(columns_query, [table_name])\n",
    "    for row in rows:\n",
    "        print(f\"Column: {row.column_name}, Type: {row.type}, Kind: {row.kind}\")\n",
    "\n",
    "    print(\"\\n=== ROW COUNT (APPROXIMATE) ===\")\n",
    "    # Get approximate row count\n",
    "    count_query = f\"SELECT COUNT(*) FROM fiddlerai.{table_name}\"\n",
    "    try:\n",
    "        result = session.execute(count_query)\n",
    "        for row in result:\n",
    "            print(f\"Approximate row count: {row.count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: COUNT(*) may timeout on large tables. Error: {e}\")\n",
    "        \n",
    "    print(\"\\n=== SAMPLE DATA ===\")\n",
    "    # Show sample data\n",
    "    sample_query = f\"SELECT * FROM fiddlerai.{table_name} LIMIT 3\"\n",
    "    rows = session.execute(sample_query)\n",
    "    for i, row in enumerate(rows):\n",
    "        print(f\"\\nSample row {i+1}:\")\n",
    "        print(f\"  Row ID: {getattr(row, 'row_id', 'N/A')}\")\n",
    "        if hasattr(row, 'body_blob'):\n",
    "            print(f\"  Content preview: {str(row.body_blob)[:100]}...\")\n",
    "        if hasattr(row, 'vector') and row.vector:\n",
    "            print(f\"  Vector dimensions: {len(row.vector)}\")\n",
    "        if hasattr(row, 'metadata_s'):\n",
    "            print(f\"  Metadata: {row.metadata_s}\")\n",
    "\n",
    "finally:\n",
    "    # Restore the original pandas factory\n",
    "    session.row_factory = original_row_factory\n",
    "    print(\"\\n=== Row factory restored ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b3082-7539-4b3d-8beb-1f171345d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_factory(colnames, rows):\n",
    "    return pd.DataFrame(rows, columns=colnames)\n",
    "\n",
    "session.row_factory = pandas_factory\n",
    "session.default_fetch_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4758a9-fa0e-494a-bda5-0868b2d0ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = session.execute('SELECT * from squad')\n",
    "    \n",
    "df_baseline = rows._current_rows\n",
    "df_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c4d5b-1cf4-49da-b9cd-b7d53eb8e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = df_baseline.dtypes\n",
    "\n",
    "print(column_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517937fc-1fe1-49cd-b7c5-a3040d2dc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline['answers'] = df_baseline['answers'].apply(lambda x : str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20625513-14d6-421d-a026-7ada970746cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f03af-9702-4065-8c6d-4227df192646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-secure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
