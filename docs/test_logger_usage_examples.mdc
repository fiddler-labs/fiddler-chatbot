---
description: 
globs: 
alwaysApply: true
---
# Test Logger Usage Examples

This document provides comprehensive examples of how to use the test logger wrapper to ensure all command executions are properly logged and prevent hallucinated test results.

## How Test Logger Works

```mermaid
flowchart TD
    A["Command Request"] --> B["Test Logger Wrapper"]
    B --> C["Create Log File"]
    C --> D["Execute Command"]
    D --> E["Capture All Output"]
    E --> F["Write to Log"]
    F --> G["Return Results + Log Path"]
    G --> H["Reference Log in Response"]
    
    style B fill:#e1f5fe
    style F fill:#c8e6c9
    style H fill:#fff3e0
```

## Basic Usage Pattern

### Old Way (Prohibited)

```bash
# DON'T DO THIS - No logging, potential for hallucinated results
python -m pytest tests/
python src/app.py
uv run pipeline
```

### New Way (Required)

```bash
# DO THIS - All commands wrapped with logging
python src/utils/test_logger.py "python -m pytest tests/" "tests/"
python src/utils/test_logger.py "python src/app.py" "src/app.py"  
python src/utils/test_logger.py "uv run pipeline" "pipeline.py"
```

## Common Usage Examples

```bash
# Basic pytest run
python src/utils/test_logger.py "python -m pytest tests/ -v" "tests/"

# Specific test file
python src/utils/test_logger.py "python -m pytest tests/test_chatbot.py -v" "tests/test_chatbot.py"

# With coverage
python src/utils/test_logger.py "python -m pytest tests/ --cov=src" "tests/"

# Main application
python src/utils/test_logger.py "python src/app.py" "src/app.py"

# Data generation workflow
python src/utils/test_logger.py "python src/data_generation.py" "src/data_generation.py"

# Chatbot script
python src/utils/test_logger.py "python src/chatbot.py" "src/chatbot.py"

# UV run with script
python src/utils/test_logger.py "uv run --script build" "build_script.py"

# UV install dependencies
python src/utils/test_logger.py "uv pip install -r requirements.txt" "requirements.txt"

# UV run pipeline
python src/utils/test_logger.py "uv run pipeline --config config.yaml" "pipeline_config.yaml"

# Command with multiple arguments
python src/utils/test_logger.py "python src/app.py --config config.yaml --verbose" "src/app.py"

# Command with environment variables
python src/utils/test_logger.py "API_KEY='' python src/chatbot.py" "src/chatbot.py"

# Command with pipes (note: wrap entire command in quotes)
python src/utils/test_logger.py "python src/data_generation.py | tee output.log" "src/data_generation.py"
```

## VSCode Integration Examples

The launch.json has been updated with these new configurations:

### Available Launch Configurations

- **Test Logger: Run Fiddler-Chatbot (Logged)** - Runs main app with logging
- **Test Logger: Run Data-Generation (Logged)** - Runs data generation with logging
- **Test Logger: Run Pytest (Logged)** - Runs pytest with logging
- **Test Logger: Custom Command** - Prompts for custom command to run with logging

### Using Custom Command Configuration

1. Select "Test Logger: Custom Command" from the debug dropdown
2. Enter your command when prompted (e.g., `uv run pipeline`)
3. Enter the test file path when prompted (e.g., `pipeline.py`)
4. The command will run with full logging

## Programmatic Usage

### Basic Function Call

```python
from src.utils.test_logger import log_test_execution

# Execute command with logging
log_file, success = log_test_execution(
    "python -m pytest tests/test_example.py -v",
    "tests/test_example.py"
)

print(f"Command succeeded: {success}")
print(f"Log file: {log_file}")
```

### Advanced Usage with Working Directory

```python
from src.utils.test_logger import log_test_execution

# Execute in specific directory
log_file, success = log_test_execution(
    "make test",
    "Makefile",
    working_directory="/path/to/project"
)
```

### Getting Latest Log

```python
from src.utils.test_logger import get_latest_test_log, summarize_test_results

# Get most recent log file
latest_log = get_latest_test_log()
if latest_log:
    summary = summarize_test_results(latest_log)
    print(f"Latest test results: {summary}")
```

## Log File Structure

Each execution creates a timestamped log file in `logs/cursor_logs/`:
`test_run_2024-07-02_22-12-51.log`

### Log File Contents

```code-output
Test Execution Log - 2024-07-02 22:12:51.123456
Command: python -m pytest tests/test_example.py -v
Test File: tests/test_example.py
Working Directory: /Users/saifraja/Github/fiddler-chatbot
==================================================
Exit Code: 0
STDOUT:
========================= test session starts =========================
platform darwin -- Python 3.11.0, pytest-7.4.0
collected 2 items

tests/test_example.py::test_function_works PASSED [50%]
tests/test_example.py::test_edge_case PASSED [100%]

========================= 2 passed in 0.12s =========================

STDERR:

==================================================
Test Summary:
- Success: True
- Log file: logs/cursor_logs/test_run_2024-07-02_22-12-51.log
- Timestamp: 2024-07-02 22:12:51.123456
```

## AI Assistant Requirements

When using the test logger, AI assistants must:

### ✅ Required Behaviors

- Always use test logger wrapper for command execution
- Reference the specific log file path in responses
- Include actual output snippets from log files
- Verify tests contain real assertions, not print statements
- Provide complete failure information when tests fail

### ❌ Prohibited Behaviors

- Running commands directly without logging
- Creating fake test outputs with print statements
- Summarizing results without showing actual log content
- Claiming tests passed without referencing log files
- Creating tests that cannot actually fail

## Example AI Response Format

```code-output
I executed the test using the test logger wrapper:

Command: python src/utils/test_logger.py "python -m pytest tests/test_chatbot.py -v" "tests/test_chatbot.py"

Results logged to: logs/cursor_logs/test_run_2024-07-02_22-15-30.log

Output from log file:

Exit Code: 0
STDOUT:
========================= test session starts =========================
collected 3 items

tests/test_chatbot.py::test_chatbot_response PASSED [33%]
tests/test_chatbot.py::test_invalid_input PASSED [66%]  
tests/test_chatbot.py::test_error_handling PASSED [100%]

========================= 3 passed in 0.45s =========================

All 3 tests passed successfully. The tests contain real assertions checking:
- Chatbot response accuracy
- Invalid input handling  
- Error condition management

No hallucinated print statements detected - all tests use proper pytest assertions.
```

## Troubleshooting

### Common Issues

#### Command Not Found

```bash
# Issue: python command not found
python src/utils/test_logger.py "python script.py" "script.py"

# Solution: Use python3 or full path
python3 src/utils/test_logger.py "python3 script.py" "script.py"
```

#### Permission Errors

```bash
# Issue: Permission denied on log directory
# Solution: Ensure logs/cursor_logs directory is writable
chmod 755 logs/cursor_logs
```

#### Complex Commands with Quotes

```bash
# Issue: Nested quotes in command
# Solution: Escape inner quotes or use different quote types
python src/utils/test_logger.py "python -c 'print(\"Hello\")'" "test.py"
```

### Validation Checklist

Before reporting test results, verify:

- Command was executed through test logger wrapper
- Test file contains real assertions (not just print statements)
- Log file path is referenced in response
- Actual output is included in response
