"""
code documentation for cassandra vector index loader at ./docs/srcdoc--loader_cassandra_vector_index.py.md
"""

import os
import sys
from datetime import datetime
from typing import Optional, Dict, Any, List

from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider
from cassandra.query import named_tuple_factory

import pandas as pd
from openai import OpenAI as OpenAIClient

from langchain.document_loaders import DataFrameLoader
from langchain.vectorstores.cassandra import Cassandra
from langchain_openai import OpenAI
from langchain_openai import OpenAIEmbeddings

import logging
logger = logging.getLogger(__name__)


# ==================== CONFIGURATION ====================
# Centralized configuration to avoid hardcoding throughout the file
CONFIG = { # todo - follow this pattern in the chatbot.py file too
    "secure_bundle_path": "datastax_auth/secure-connect-fiddlerai.zip",
    "keyspace": "fiddlerai",
    "llm_provider": "openai", # 'GCP_VertexAI', 'Azure_OpenAI'
    "embedding_model": "text-embedding-3-large",
    "embedding_dimensions": 1536,
    "temperature": 0,
    "default_csv_path": "documentation_data/vector_index_feed_v25.10.csv",
    "squad_table": "squad",
    "chatbot_history_table": "fiddler_chatbot_history"
    }

# Computed values
TABLE_NAME = f'fiddler_doc_snippets_{CONFIG["llm_provider"]}'

# ==================== DATABASE CONNECTION ====================

def connect_to_cassandra() -> tuple:
    """
    Establish connection to DataStax Cassandra
    Returns: (cluster, session) tuple
    """
    try:
        # This secure connect bundle is autogenerated when you download your SCB
        cloud_config = {'secure_connect_bundle': CONFIG["secure_bundle_path"]}
        
        ASTRA_DB_APPLICATION_TOKEN = os.environ.get('ASTRA_DB_APPLICATION_TOKEN')
        if not ASTRA_DB_APPLICATION_TOKEN:
            raise ValueError("ASTRA_DB_APPLICATION_TOKEN environment variable not set")
            
        auth_provider = PlainTextAuthProvider("token", ASTRA_DB_APPLICATION_TOKEN)
        cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)
        
        session = cluster.connect()
        session.set_keyspace(CONFIG["keyspace"])
        
        logger.info(f"✅ Connected to Cassandra keyspace: {CONFIG['keyspace']}")
        return cluster, session
        
    except Exception as e:
        logger.error(f"❌ Failed to connect to Cassandra: {e}")
        raise

def setup_llm_and_embeddings():
    """
    Configure OpenAI LLM and embeddings
    Returns: (llm, embedding) tuple
    """
    try:
        os.environ["OPENAI_API_TYPE"] = "open_ai"
        
        llm = OpenAI(temperature=CONFIG["temperature"])
        
        # Explicitly use the desired model and configure its dimension size
        embedding = OpenAIEmbeddings(
            model=CONFIG["embedding_model"],
            model_kwargs={"dimensions": CONFIG["embedding_dimensions"]}
        )
        
        logger.info(f"✅ LLM and Embeddings configured: {embedding.model} with {CONFIG['embedding_dimensions']} dimensions")
        return llm, embedding
        
    except Exception as e:
        logger.error(f"❌ Failed to setup LLM/Embeddings: {e}")
        raise

# Legacy variables for compatibility
llmProvider = CONFIG["llm_provider"]
table_name = TABLE_NAME

# ==================== DATA LOADING AND VECTOR STORE ====================

def load_documentation_data() -> pd.DataFrame:
    """
    Load documentation data from CSV file
    """
    try:
        df = pd.read_csv(CONFIG["default_csv_path"])
        logger.info(f"✅ Loaded {len(df)} rows from {CONFIG['default_csv_path']}")
        df.info()
        return df
    except Exception as e:
        logger.error(f"❌ Failed to load CSV file: {e}")
        raise

def safe_truncate_table(session, table_name: str, force: bool = False) -> bool:
    """
    Safely truncate table with confirmation and optional backup
    Returns True if truncated, False if skipped
    """
    if not force:
        # First, check if table has data
        try:
            count_result = session.execute(f"SELECT COUNT(*) FROM {CONFIG['keyspace']}.{table_name}")
            current_count = list(count_result)[0].count
            
            if current_count > 0:
                logger.warning(f"\n⚠️  WARNING: Table '{table_name}' contains {current_count} rows.")
                logger.warning("This operation will DELETE ALL existing data.")
                
                # In production, you might want to create a backup table here
                backup_table = f"{table_name}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                logger.warning(f"Backing up to: {backup_table}")
                
                # todo - add a backup table code here

        except Exception as e:
            logger.error(f"Warning: Could not check table count: {e}")
    
    try:
        session.execute(f"TRUNCATE TABLE {table_name}")
        logger.info(f"✅ Table '{table_name}' truncated successfully.")
        return True
    except Exception as e:
        logger.error(f"❌ Failed to truncate table: {e}")
        raise

def populate_vector_store(df: pd.DataFrame, session, embedding, table_name: str, truncate_first: bool = True) -> Optional[Cassandra]:
    """
    Load documents into Cassandra vector store
    """
    try:
        # Optional: Clear the database table prior to refill
        if truncate_first:
            safe_truncate_table(session, table_name)
            logger.info("Truncated table.")
        
        # 1. Load the pre-chunked documents from the DataFrame
        logger.info("Loading pre-chunked documents from the DataFrame...")
        loader = DataFrameLoader(df, page_content_column="text")
        documents = loader.load()
        logger.info(f"Loaded {len(documents)} document chunks.")
        
        # 2. Initialize the vector store
        logger.info(f"Initializing Cassandra vector store with table '{table_name}'...")
        vector_store = Cassandra(
            embedding=embedding,
            session=session,
            keyspace=CONFIG["keyspace"],
            table_name=table_name,
        )
        
        # 3. Add the documents to the vector store
        logger.info(f"Adding {len(documents)} chunks to the vector store. This may take a few minutes...")
        vector_store.add_documents(documents)
        
        logger.info("\n✅ Success! All document chunks have been embedded and stored in Cassandra.")
        return vector_store
        
    except Exception as e:
        logger.error(f"❌ Failed to populate vector store: {e}")
        raise

def test_vector_store(vector_store, query: str = "What is Fiddler?", k: int = 2):
    """
    Quick test of the vector store functionality
    """
    try:
        docs = vector_store.similarity_search(query, k=k)
        result = f"Retrieved {len(docs)} docs for query '{query}'. "
        if docs:
            result += f"First doc preview: {docs[0].page_content[:100]}..."
        else:
            result += "No documents found."
        return result
    except Exception as e:
        return f"Test failed: {e}"

# ==================== TABLE INSPECTION UTILITIES ====================

def inspect_table_structure(session, table_name: str, keyspace: Optional[str] = None) -> None:
    """
    Utility to inspect Cassandra table structure and sample data
    """
    if keyspace is None:
        keyspace = CONFIG["keyspace"]
        
    logger.info(f"\n=== INSPECTING TABLE: {keyspace}.{table_name} ===")
    
    # Temporarily set row factory to get normal Cassandra Row objects
    original_row_factory = session.row_factory
    session.row_factory = named_tuple_factory
    
    try:
        # Check table structure using system schema queries
        logger.info("\n=== TABLE INFORMATION ===")
        table_info_query = """
        SELECT table_name, bloom_filter_fp_chance, caching, comment, compaction, compression 
        FROM system_schema.tables 
        WHERE keyspace_name = %s AND table_name = %s
        """
        
        rows = session.execute(table_info_query, (keyspace, table_name))
        for row in rows:
            logger.info(f"Table: {row.table_name} ; \nCompaction: {row.compaction} ; \nCompression: {row.compression} ; \nCaching: {row.caching}")
        
        # Get column info
        logger.info("\n=== COLUMN INFORMATION ===")
        columns_query = """
        SELECT column_name, type, kind, position 
        FROM system_schema.columns 
        WHERE keyspace_name = %s AND table_name = %s
        """
        
        rows = session.execute(columns_query, (keyspace, table_name))
        for row in rows:
            logger.info(f"Column: {row.column_name}, Type: {row.type}, Kind: {row.kind}")
        
        # Get approximate row count
        logger.info("\n=== ROW COUNT (APPROXIMATE) ===")
        count_query = f"SELECT COUNT(*) FROM {keyspace}.{table_name}"
        try:
            result = session.execute(count_query)
            for row in result:
                logger.info(f"Approximate row count: {row.count}")
        except Exception as e:
            logger.error(f"Note: COUNT(*) may timeout on large tables. Error: {e}")
        
        # Show sample data
        logger.info("\n=== SAMPLE DATA ===")
        sample_query = f"SELECT * FROM {keyspace}.{table_name} LIMIT 3"
        rows = session.execute(sample_query)
        for i, row in enumerate(rows):
            logger.info(f"\nSample row {i+1}:")
            logger.info(f"  Row ID: {getattr(row, 'row_id', 'N/A')}")
            if hasattr(row, 'body_blob'):
                logger.info(f"  Content preview: {str(row.body_blob)[:100]}...")
            if hasattr(row, 'vector') and row.vector:
                logger.info(f"  Vector dimensions: {len(row.vector)}")
            if hasattr(row, 'metadata_s'):
                logger.info(f"  Metadata: {row.metadata_s}")
                
    except Exception as e:
        logger.error(f"❌ Error inspecting table: {e}")
        
    finally:
        # Restore the original row factory
        session.row_factory = original_row_factory
        logger.info("\n=== Row factory restored ===")

def query_and_display_rows(session, table_name: str, limit: int = 10) -> None:
    """
    Query and display rows from a table with proper formatting
    """
    try:
        cql_select = f'SELECT * FROM {CONFIG["keyspace"]}.{table_name} LIMIT {limit};'
        rows = session.execute(cql_select)
        
        logger.info(f"\n=== DISPLAYING {limit} ROWS FROM {table_name} ===")
        for row_i, row in enumerate(rows):
            logger.info(f'\nRow {row_i}:')
            # Handle different cassIO versions
            if hasattr(row, 'row_id'):
                logger.info(f'    row_id:      {row.row_id}')
                logger.info(f'    vector:      {str(row.vector)[:64]}...')
                logger.info(f'    body_blob:   {row.body_blob[:64]}...')
                logger.info(f'    metadata_s:  {row.metadata_s}')
            else:
                # Legacy format
                logger.info(f'    document_id:      {getattr(row, "document_id", "N/A")}')
                logger.info(f'    embedding_vector: {str(getattr(row, "embedding_vector", ""))[:64]}...')
                logger.info(f'    document:         {getattr(row, "document", "")[:64]}...')
                logger.info(f'    metadata_blob:    {getattr(row, "metadata_blob", "N/A")}')
                
        logger.info('\n...')
        
    except Exception as e:
        logger.error(f"❌ Error querying rows: {e}")

# ==================== DATA EXPORT UTILITIES ====================

def export_table_to_csv(session, table_name: str, output_file: Optional[str] = None) -> Optional[str]:
    """
    Export Cassandra table data to CSV file
    Returns the path to the created CSV file
    """
    if output_file is None:
        output_file = f'{table_name}_output_pandas.csv'
        
    try:
        cql_query = f'SELECT * FROM {CONFIG["keyspace"]}.{table_name};'
        logger.info(f"Executing query and loading data into pandas DataFrame...")
        rows = session.execute(cql_query)
        
        # Convert the Cassandra ResultSet to a pandas DataFrame
        df = pd.DataFrame(list(rows))
        
        if df.empty:
            logger.info("Query returned no data. CSV file will not be created.")
            return None
        else:
            # Save the DataFrame to a CSV file
            df.to_csv(output_file, index=False, encoding='utf-8')
            logger.info(f"✅ Success! Data has been saved to '{output_file}'")
            return output_file
            
    except Exception as e:
        logger.error(f"❌ Failed to export data to CSV: {e}")
        raise

# ==================== TABLE CREATION UTILITIES ====================

def create_chatbot_history_table(session) -> None:
    """
    Create table for storing chatbot interaction history
    """
    try:
        create_table_sql = f"""--sql
        CREATE TABLE IF NOT EXISTS {CONFIG["chatbot_history_table"]}
        (
            row_id text PRIMARY KEY,
            response text,
            response_vector vector<float, {CONFIG['embedding_dimensions']}>,
            source_docs text,
            source_docs_vector vector<float, {CONFIG['embedding_dimensions']}>,
            question text,
            question_vector vector<float, {CONFIG['embedding_dimensions']}>,
            comment text,
            feedback int,
            metadata_s map<text, text>,
            ts timestamp
        )
        -- WITH additional_write_policy = '99p'
            --     AND bloom_filter_fp_chance = 0.01
            --     AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
            --     AND comment = ''
            --     AND compaction = {'class': 'org.apache.cassandra.db.compaction.UnifiedCompactionStrategy'}
            --     AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
            --     AND crc_check_chance = 1.0
            --     AND default_time_to_live = 0
            --     AND gc_grace_seconds = 864000
            --     AND max_index_interval = 2048
            --     AND memtable_flush_period_in_ms = 0
            --     AND min_index_interval = 128
            --     AND read_repair = 'BLOCKING'
            --     AND speculative_retry = '99p';
        """
        
        session.execute(create_table_sql)
        logger.info(f"✅ Created table '{CONFIG['chatbot_history_table']}' successfully.")
        
    except Exception as e:
        logger.error(f"❌ Failed to create chatbot history table: {e}")
        raise

# ==================== OPENAI EMBEDDING UTILITIES ====================

def test_openai_embeddings(text: str = "Test text: Tell me about yourself") -> None:
    """
    Test OpenAI embeddings using the new API
    """
    try:
        # Using the new OpenAI client API
        client = OpenAIClient()
        response = client.embeddings.create(
            model=CONFIG["embedding_model"],
            input=text,
            dimensions=CONFIG["embedding_dimensions"]
        )
        logger.info(f"✅ Embedding retrieval successful. Model: {CONFIG['embedding_model']}")
        logger.info(f"   Embedding dimensions: {len(response.data[0].embedding)}")
        logger.info(f"   Generated embedding for: '{text}'")
        logger.info(f"   Embedding: {response.data[0].embedding[:80]}...")
        
    except Exception as e:
        logger.error(f"❌ Failed to test OpenAI embeddings: {e}")
        raise

# ==================== SQUAD TABLE UTILITIES ====================

def query_squad_table(session) -> pd.DataFrame:
    """
    Query and process data from the squad table
    """
    try:
        def pandas_factory(colnames, rows):
            return pd.DataFrame(rows, columns=colnames)
        
        session.row_factory = pandas_factory
        
        rows = session.execute(f'SELECT * from {CONFIG["squad_table"]}')
        df_baseline = rows._current_rows
        
        logger.info(f"✅ Retrieved {len(df_baseline)} rows from squad table")
        logger.info("Column types:")
        logger.info(df_baseline.dtypes)
        
        # Convert answers column to string
        df_baseline['answers'] = df_baseline['answers'].apply(lambda x: str(x))
        
        return df_baseline
        
    except Exception as e:
        logger.error(f"❌ Failed to query squad table: {e}")
        raise
    finally:
        # Reset row factory to default
        session.row_factory = None



# ==================== MAIN EXECUTION ====================

def main():
    """
    Main execution function for the vector index loader
    
    This function demonstrates the typical workflow:
    1. Connect to Cassandra
    2. Setup LLM and embeddings
    3. Load documentation data
    4. Populate vector store
    5. Test the vector store
    6. Optionally inspect tables and export data
    """
    logger.info("=== Cassandra Vector Index Loader ===")
    logger.info(f"Target table: {TABLE_NAME} \nKeyspace: {CONFIG['keyspace']} \nEmbedding model: {CONFIG['embedding_model']}")
    
    # Initialize connections and models
    cluster, session = connect_to_cassandra()
    llm, myEmbedding = setup_llm_and_embeddings()


    # 1. Load and populate vector store
    logger.info("\n1. Loading documentation data...")
    df = load_documentation_data()
    
    logger.info("\n2. Populating vector store...")
    vector_store = populate_vector_store(df, session, myEmbedding, TABLE_NAME, truncate_first=True)
    
    if vector_store:
        logger.info("\n3. Testing vector store...")
        test_result = test_vector_store(vector_store)
        logger.info(test_result)

    # ### You are done. Please check the Chatbot to ensure it is working and returning answers from the lastest docs we just uploaded.


    # 2. Inspect table (optional)
    logger.info("\n4. Inspecting table structure...")
    inspect_table_structure(session, TABLE_NAME)
    
    # 3. Query and display sample rows (optional)
    logger.info("\n5. Displaying sample rows...")
    query_and_display_rows(session, TABLE_NAME, limit=5)
    
    # 4. Export to CSV (optional)
    logger.info("\n6. Exporting data to CSV...")
    csv_file = export_table_to_csv(session, TABLE_NAME)
    
    # 5. Create chatbot history table (optional)
    logger.info("\n7. Creating chatbot history table...")
    create_chatbot_history_table(session)
    
    # 6. Test OpenAI embeddings (optional)
    logger.info("\n8. Testing OpenAI embeddings...")
    test_openai_embeddings()
    
    # 7. Query squad table:
    logger.info("\n9. Querying squad table...")
    df_squad = query_squad_table(session)
    logger.info(f"✅ Retrieved {len(df_squad)} rows from squad table")
    logger.info(f"Column types: {df_squad.dtypes}")

    
    
    # Close connections when done
    if cluster and not cluster.is_shutdown:
        cluster.shutdown()
        logger.info("✅ Cassandra connection closed.")
        
        logger.info("\n✅ Script completed. Use the functions above for specific operations.")


if __name__ == "__main__":
    main()

