{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9086acc",
   "metadata": {},
   "source": [
    "\n",
    "# Fiddler Chatbot Database Administration Notebook\n",
    "\n",
    "This notebook provides administrative tools for managing the Fiddler chatbot database systems.\n",
    "- **Vector Store Table**: `fiddler_doc_snippets_openai`\n",
    "- **Chatbot Ledger Table**: `fiddler_chatbot_ledger` \n",
    "- **Keyspace**: `fiddlerai`\n",
    "- **Current LLM Model**: `gpt-4-turbo`\n",
    "- **Embedding Model**: `text-embedding-3-large`\n",
    "\n",
    "### 1. **Data Inspection**\n",
    "   - Views contents of vector store and ledger tables\n",
    "   - Checks table structures and row counts\n",
    "   - Analyzes data quality and completeness\n",
    "\n",
    "### 2. **Data Analysis**\n",
    "   - Examines chatbot interaction patterns\n",
    "   - Reviews user feedback and ratings\n",
    "   - Analyzes token usage and costs\n",
    "\n",
    "### 3. **Data Export**\n",
    "   - Exports data for Fiddler AI monitoring\n",
    "   - Creates CSV files for analysis\n",
    "   - Prepares baseline and event datasets\n",
    "\n",
    "### 4. **Maintenance Operations**\n",
    "   - Data cleanup and validation\n",
    "   - Performance monitoring\n",
    "   - Schema validation\n",
    "\n",
    "**‚ö†Ô∏è Important**: This notebook has been updated to align with the current system configuration as of 2025. Legacy migration code has been preserved but marked as historical reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4096ec-29eb-4cfa-8346-7b283cbd23b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import cassandra\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print( os.getcwd() )\n",
    "os.chdir('../')\n",
    "print( os.getcwd() )\n",
    "\n",
    "from config import CONFIG_CHATBOT_OLD as config  # noqa: E402\n",
    "\n",
    "print(f\"Cassandra version: {cassandra.__version__}\")\n",
    "print(\"Current system configuration loaded:\")\n",
    "print(f\"  - Keyspace:        {config['ASTRA_DB_KEYSPACE']}\")\n",
    "print(f\"  - Vector table:    {config['ASTRA_DB_TABLE_NAME']}\")\n",
    "print(f\"  - Ledger table:    {config['ASTRA_DB_LEDGER_TABLE_NAME']}\")\n",
    "print(f\"  - LLM Model:       {config['OPENAI_LLM_MODEL']}\")\n",
    "print(f\"  - Embedding Model: {config['OPENAI_EMBEDDING_MODEL']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b3d41-2153-45c4-b37b-8567ca48c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This secure connect bundle is autogenerated when you donwload your SCB, if yours is different update the file name below\n",
    "# Database connection setup using current configuration\n",
    "print(\"üîÑ Setting up database connection...\")\n",
    "\n",
    "cloud_config = {\n",
    "    'secure_connect_bundle': '../'+config[\"ASTRA_DB_SECURE_BUNDLE_PATH\"]\n",
    "}\n",
    "\n",
    "# Get authentication token from environment\n",
    "ASTRA_DB_APPLICATION_TOKEN = os.environ.get('ASTRA_DB_APPLICATION_TOKEN')\n",
    "if not ASTRA_DB_APPLICATION_TOKEN:\n",
    "    raise ValueError(\"ASTRA_DB_APPLICATION_TOKEN environment variable is required\")\n",
    "\n",
    "# Create connection\n",
    "auth_provider = PlainTextAuthProvider(\"token\", ASTRA_DB_APPLICATION_TOKEN)\n",
    "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
    "session = cluster.connect()\n",
    "\n",
    "# Set keyspace using configuration\n",
    "keyspace = config[\"ASTRA_DB_KEYSPACE\"]\n",
    "session.set_keyspace(keyspace)\n",
    "\n",
    "print(\"‚úÖ Connected to DataStax Astra DB using keyspace: {keyspace}\")\n",
    "\n",
    "def pandas_factory(colnames, rows):\n",
    "    return pd.DataFrame(rows, columns=colnames)\n",
    "\n",
    "session.row_factory = pandas_factory\n",
    "# session.default_fetch_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc9d69-43c3-4cf6-88a2-f243a3a8d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rid = \"<class 'uuid.UUID'>\"\n",
    "# rows = session.execute(\"DELETE row_id FROM fiddler_chatbot_ledger WHERE model_name= '' \")\n",
    "# df = rows._current_rows\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c18fb6-71ca-4195-98fb-4363a1f6c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the main vector store table\n",
    "vector_table = config[\"ASTRA_DB_TABLE_NAME\"]\n",
    "print(f\"üìä Querying vector store table: {vector_table}\")\n",
    "\n",
    "query = f'SELECT * FROM {vector_table}'\n",
    "rows = session.execute(query)\n",
    "df_docs = rows._current_rows\n",
    "\n",
    "print(f\"üìà Retrieved {len(df_docs)} documents from vector store\")\n",
    "print(f\"üìã Table schema: {list(df_docs.columns)}\")\n",
    "\n",
    "# Display basic statistics\n",
    "if len(df_docs) > 0:\n",
    "    print(f\"üìè Vector dimensions: {len(df_docs.iloc[0]['vector']) if 'vector' in df_docs.columns else 'N/A'}\")\n",
    "    print(f\"üìù Sample document preview: {str(df_docs.iloc[0]['body_blob'])[:100]}...\" if 'body_blob' in df_docs.columns else \"\")\n",
    "\n",
    "df_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdec1b8-6a78-47e4-9ef7-5678baa56de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the current chatbot interaction ledger\n",
    "ledger_table = config[\"ASTRA_DB_LEDGER_TABLE_NAME\"]\n",
    "print(f\"üìä Querying chatbot ledger table: {ledger_table}\")\n",
    "\n",
    "query = f\"SELECT * FROM {ledger_table} LIMIT 100\"\n",
    "rows = session.execute(query)\n",
    "df_ledger = rows._current_rows\n",
    "\n",
    "if len(df_ledger) > 0:\n",
    "    df_ledger = df_ledger.sort_values(by=['ts'], ascending=False)\n",
    "    print(f\"üìà Retrieved {len(df_ledger)} chatbot interactions\")\n",
    "    print(f\"üìã Current schema: {list(df_ledger.columns)}\")\n",
    "    \n",
    "    # Check for any missing or extra fields\n",
    "    actual_fields = set(df_ledger.columns)\n",
    "            \n",
    "    print(f\"üöÄ Latest interaction timestamp: {df_ledger.iloc[0]['ts'] if 'ts' in df_ledger.columns else 'N/A'}\")\n",
    "    print(f\"ü§ñ Current model in use: {df_ledger.iloc[0]['model_name'] if 'model_name' in df_ledger.columns else 'N/A'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data found in ledger table\")\n",
    "    df_ledger = pd.DataFrame()\n",
    "\n",
    "# Display the latest 5 interactions\n",
    "df_ledger.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90201333-1163-4382-aa17-9f3e1e07cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA EXPORT FOR FIDDLER AI MONITORING\n",
    "# =============================================================================\n",
    "# Export chatbot interaction data for analysis and monitoring\n",
    "\n",
    "ledger_table = config[\"ASTRA_DB_LEDGER_TABLE_NAME\"]\n",
    "print(f\"üìä Exporting FIRST 100 rows of data from {ledger_table}\")\n",
    "\n",
    "query = f'SELECT * FROM {ledger_table} LIMIT 100'\n",
    "rows = session.execute(query)\n",
    "df_export = rows._current_rows\n",
    "\n",
    "if len(df_export) > 0:\n",
    "    # Sort by timestamp\n",
    "    df_export = df_export.sort_values(by=['ts'])\n",
    "    \n",
    "    # Create baseline dataset (first 50 interactions for model baseline)\n",
    "    baseline_count = min(50, len(df_export))\n",
    "    df_baseline = df_export.iloc[:baseline_count].copy()\n",
    "    df_baseline = df_baseline.drop(columns=['ts'], errors='ignore')  # Remove timestamp for baseline\n",
    "    \n",
    "    # Create events dataset (remaining interactions for monitoring)\n",
    "    df_events = df_export.iloc[baseline_count:].copy()\n",
    "    \n",
    "    # Export to CSV files with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    baseline_filename = f'chatbot_baseline_{timestamp}.csv'\n",
    "    events_filename = f'chatbot_events_{timestamp}.csv'\n",
    "    \n",
    "    df_baseline.to_csv(baseline_filename, index=False)\n",
    "    df_events.to_csv(events_filename, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Exported baseline data: {baseline_filename} ({len(df_baseline)} rows)\")\n",
    "    print(f\"‚úÖ Exported events data: {events_filename} ({len(df_events)} rows)\")\n",
    "    print(f\"üìä Total interactions: {len(df_export)}\")\n",
    "    \n",
    "    # Display export summary\n",
    "    if len(df_export) > 0:\n",
    "        print(f\"üìÖ Date range: {df_export['ts'].min()} to {df_export['ts'].max()}\")\n",
    "        print(f\"ü§ñ Models used: {df_export['model_name'].unique().tolist() if 'model_name' in df_export.columns else 'N/A'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data available for export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd4c0c-3e8a-4ac1-8595-4e4a4789c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAINTENANCE AND CLEANUP OPERATIONS (USE WITH CAUTION)\n",
    "# =============================================================================\n",
    "# These operations can modify or delete data - use only when necessary\n",
    "# All operations are commented out for safety\n",
    "\n",
    "# WARNING: The following operations are DESTRUCTIVE and should only be used\n",
    "# by administrators who understand the consequences\n",
    "\n",
    "# Clean up specific problematic records:\n",
    "# session.execute(\"DELETE FROM fiddler_chatbot_conversation WHERE row_id='-1'\")\n",
    "\n",
    "# Remove legacy tables (ONLY if migration is complete and verified):\n",
    "# session.execute(\"DROP TABLE fiddler_chatbot_history\")\n",
    "\n",
    "# Clear vector store (ONLY for complete rebuild):\n",
    "# vector_table = config[\"ASTRA_DB_TABLE_NAME\"]\n",
    "# session.execute(f\"TRUNCATE TABLE {vector_table}\")\n",
    "\n",
    "# Clear chatbot history (ONLY for fresh start):\n",
    "# ledger_table = config[\"ASTRA_DB_LEDGER_TABLE_NAME\"]\n",
    "# session.execute(f\"TRUNCATE TABLE {ledger_table}\")\n",
    "\n",
    "# Current system status check\n",
    "print(\"üìã Current system status:\")\n",
    "print(f\"   - Vector table:  {config['ASTRA_DB_TABLE_NAME']}\")\n",
    "print(f\"   - Ledger table:  {config['ASTRA_DB_LEDGER_TABLE_NAME']}\")\n",
    "print(f\"   - Current model: {config['OPENAI_LLM_MODEL']}\")\n",
    "print(\"üîê All maintenance operations are commented out for safety\")\n",
    "print(\"‚ö†Ô∏è  Uncomment and execute maintenance operations only if you understand the consequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4b5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS FOR CURRENT SYSTEM ANALYSIS\n",
    "# =============================================================================\n",
    "# Helper functions for analyzing the current chatbot system\n",
    "\n",
    "def get_system_health_summary():\n",
    "    \"\"\"Get a comprehensive health summary of the current system\"\"\"\n",
    "    print(\"üè• SYSTEM HEALTH SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check vector store\n",
    "    vector_table = config[\"ASTRA_DB_TABLE_NAME\"]\n",
    "    rows = session.execute(f'SELECT COUNT(*) as count FROM {vector_table}')\n",
    "    vector_count : int= len( list(rows)[0] )\n",
    "    print(f\"üìö Vector Store ({vector_table}): {vector_count} documents\")\n",
    "    \n",
    "    # Check chatbot ledger\n",
    "    ledger_table = config[\"ASTRA_DB_LEDGER_TABLE_NAME\"]\n",
    "    rows = session.execute(f'SELECT COUNT(*) as count FROM {ledger_table}')\n",
    "    ledger_count : int = len( list(rows)[0] )\n",
    "    print(f\"üí¨ Chatbot Interactions ({ledger_table}): {ledger_count} records\")\n",
    "    \n",
    "    if ledger_count > 0:\n",
    "        # Get recent activity - fetch all data and sort in pandas since Cassandra can't ORDER BY non-clustering columns\n",
    "        recent_query = f'SELECT model_name, ts FROM {ledger_table}'\n",
    "        rows = session.execute(recent_query)\n",
    "        df_recent = rows._current_rows\n",
    "        \n",
    "        if len(df_recent) > 0:\n",
    "            # Sort by timestamp in pandas and get the latest\n",
    "            df_recent = df_recent.sort_values(by=['ts'], ascending=False)\n",
    "            latest_row = df_recent.iloc[0]\n",
    "            latest_ts = latest_row['ts']\n",
    "            latest_model = latest_row['model_name']\n",
    "            print(f\"üïê Latest interaction: {latest_ts}\")\n",
    "            print(f\"ü§ñ Latest model used: {latest_model}\")\n",
    "    \n",
    "    print(f\"‚öôÔ∏è  Configured model: {config['OPENAI_LLM_MODEL']}\")\n",
    "    print(f\"üß† Embedding model: {config['OPENAI_EMBEDDING_MODEL']}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "def get_recent_interactions(limit=5):\n",
    "    \"\"\"Get the most recent chatbot interactions\"\"\"\n",
    "    ledger_table = config[\"ASTRA_DB_LEDGER_TABLE_NAME\"]\n",
    "    query = f'SELECT row_id, prompt, response, model_name, ts FROM {ledger_table}'\n",
    "    \n",
    "    try:\n",
    "        rows = session.execute(query)\n",
    "        df_recent = rows._current_rows\n",
    "        \n",
    "        if len(df_recent) > 0:\n",
    "            # Sort by timestamp in pandas and get the most recent records\n",
    "            df_recent = df_recent.sort_values(by=['ts'], ascending=False)\n",
    "            df_recent = df_recent.head(limit)\n",
    "            \n",
    "            print(f\"üí¨ RECENT INTERACTIONS (Last {len(df_recent)})\")\n",
    "            print(\"=\" * 60)\n",
    "            for i, (_, row) in enumerate(df_recent.iterrows(), 1):\n",
    "                print(f\"{i}. {row['ts']} - {row['model_name']}\")\n",
    "                prompt = str(row['prompt']) if row['prompt'] is not None else \"No prompt\"\n",
    "                response = str(row['response']) if row['response'] is not None else \"No response\"\n",
    "                print(f\"   Q: {prompt[:100]}...\" if len(prompt) > 100 else f\"   Q: {prompt}\")\n",
    "                print(f\"   A: {response[:100]}...\" if len(response) > 100 else f\"   A: {response}\")\n",
    "                print()\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            return df_recent.to_dict('records')\n",
    "        else:\n",
    "            print(\"üí¨ No interactions found\")\n",
    "            return []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not retrieve recent interactions: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run health summary by default\n",
    "get_system_health_summary()\n",
    "get_recent_interactions(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fad584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Cassandra as CassandraVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "from cassandra.cluster import Cluster\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# VECTOR SIMILARITY SEARCH UTILITY\n",
    "# =============================================================================\n",
    "# This cell replicates the vector search functionality from the main chatbot\n",
    "# to allow debugging and testing of retrieval queries\n",
    "\n",
    "\"\"\"\n",
    "üöÄ Vector Search Utility Ready!\n",
    "üìù Example usage:\n",
    "    documents = test_vector_search('What is Fiddler?')\n",
    "    documents = test_vector_search('How to monitor models?', k=5)\n",
    "    documents = test_vector_search('API documentation', k=3, show_details=False)\n",
    "\"\"\"\n",
    "\n",
    "def test_vector_search(query: str, k: int = 3, show_details: bool = True):\n",
    "    \"\"\"\n",
    "    Perform vector similarity search using the same logic as the main chatbot.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query (e.g., \"What is Fiddler?\")\n",
    "        k (int): Number of documents to retrieve (default: 3, same as chatbot)\n",
    "        show_details (bool): Whether to show detailed document content\n",
    "    \n",
    "    Returns:\n",
    "        List of retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"üîç Performing vector search for: '{query}'\")\n",
    "    print(f\"üìä Retrieving top {k} most relevant documents...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Set up embeddings using the same configuration as chatbot\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=config[\"OPENAI_EMBEDDING_MODEL\"], \n",
    "        dimensions=1536\n",
    "    )\n",
    "    \n",
    "    # Create a new session specifically for vector search (without pandas_factory)\n",
    "    # The existing session has pandas_factory which conflicts with LangChain's expectations\n",
    "    cloud_config = {\n",
    "        'secure_connect_bundle': '../'+config[\"ASTRA_DB_SECURE_BUNDLE_PATH\"]\n",
    "    }\n",
    "    \n",
    "    ASTRA_DB_APPLICATION_TOKEN = os.environ.get('ASTRA_DB_APPLICATION_TOKEN')\n",
    "    if not ASTRA_DB_APPLICATION_TOKEN:\n",
    "        raise ValueError(\"ASTRA_DB_APPLICATION_TOKEN environment variable is required\")\n",
    "    \n",
    "    # Create a separate connection for vector operations (without custom row factory)\n",
    "    auth_provider = PlainTextAuthProvider(\"token\", ASTRA_DB_APPLICATION_TOKEN)\n",
    "    vector_cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
    "    vector_session = vector_cluster.connect()\n",
    "    vector_session.set_keyspace(config[\"ASTRA_DB_KEYSPACE\"])\n",
    "    \n",
    "    # Create Cassandra vector store using the dedicated session\n",
    "    vector_store = CassandraVectorStore(\n",
    "        embedding=embeddings,\n",
    "        session=vector_session,\n",
    "        keyspace=config[\"ASTRA_DB_KEYSPACE\"],\n",
    "        table_name=config[\"ASTRA_DB_TABLE_NAME\"]\n",
    "    )\n",
    "    \n",
    "    # Perform similarity search with timing\n",
    "    start_time = time.time()\n",
    "    documents = vector_store.similarity_search(query, k=k)\n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  Search completed in {search_time:.3f} seconds\")\n",
    "    print(f\"üìÑ Retrieved {len(documents)} documents\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"‚ö†Ô∏è  No documents found for the query\")\n",
    "        return []\n",
    "    \n",
    "    # Display results\n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        print(f\"\\nüìã Document {i}:\")\n",
    "        print(f\"   Content length: {len(doc.page_content)} characters\")\n",
    "        \n",
    "        if show_details:\n",
    "            # Show first 200 characters of content\n",
    "            content_preview = doc.page_content[:500]#.replace('\\n', ' ')\n",
    "            print(f\"   Preview: {content_preview}...\")\n",
    "            \n",
    "            # Show metadata if available\n",
    "            if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                print(f\"   Metadata: {doc.metadata}\")\n",
    "        else:\n",
    "            # Just show first 100 characters\n",
    "            content_preview = doc.page_content[:500]#.replace('\\n', ' ')\n",
    "            print(f\"   Preview: {content_preview}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "    vector_cluster.shutdown()\n",
    "    return documents\n",
    "\n",
    "\n",
    "test_documents = test_vector_search(\"Tell me about release 25.13 and 25.9\", k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de01ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
