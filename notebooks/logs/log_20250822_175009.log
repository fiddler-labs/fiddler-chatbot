2025-08-22 17:50:09,479 -     INFO -   67 - chatbot_chainlit - chatbot_chainlit - <module> - Initializing Fiddler monitoring...
2025-08-22 17:50:09,490 -     INFO -   78 - chatbot_chainlit - chatbot_chainlit - <module> - ‚úì Fiddler monitoring initialized successfully
2025-08-22 17:50:09,572 -     INFO -   91 - chatbot_chainlit - chatbot_chainlit - <module> - ‚úì language model initialized successfully
2025-08-22 17:50:09,576 -     INFO -  106 - chatbot_chainlit - chatbot_chainlit - <module> - ‚úì Tools bound to language model successfully
2025-08-22 17:50:09,619 -     INFO -  210 - chatbot_chainlit - chatbot_chainlit - build_chatbot_graph - Building LangGraph workflow...
2025-08-22 17:50:09,622 -     INFO -  249 - chatbot_chainlit - chatbot_chainlit - build_chatbot_graph - ‚úì Workflow compiled successfully
2025-08-22 17:50:09,624 -    DEBUG - 1049 - urllib3.connectionpool - connectionpool - _new_conn - Starting new HTTPS connection (1): mermaid.ink:443
2025-08-22 17:50:10,129 -    DEBUG -  544 - urllib3.connectionpool - connectionpool - _make_request - https://mermaid.ink:443 "GET /img/LS0tCmNvbmZpZzoKICBmbG93Y2hhcnQ6CiAgICBjdXJ2ZTogbGluZWFyCi0tLQpncmFwaCBURDsKCV9fc3RhcnRfXyg8cD5fX3N0YXJ0X188L3A+KQoJY2hhdGJvdChjaGF0Ym90KQoJdG9vbF9leGVjdXRpb24odG9vbF9leGVjdXRpb24pCglfX2VuZF9fKDxwPl9fZW5kX188L3A+KQoJX19zdGFydF9fIC0tPiBjaGF0Ym90OwoJY2hhdGJvdCAtLT4gX19lbmRfXzsKCWNsYXNzRGVmIGRlZmF1bHQgZmlsbDojZjJmMGZmLGxpbmUtaGVpZ2h0OjEuMgoJY2xhc3NEZWYgZmlyc3QgZmlsbC1vcGFjaXR5OjAKCWNsYXNzRGVmIGxhc3QgZmlsbDojYmZiNmZjCg==?type=png&bgColor=!white HTTP/1.1" 200 7152
2025-08-22 17:50:10,130 -     INFO -  256 - chatbot_chainlit - chatbot_chainlit - build_chatbot_graph - Workflow graph saved to workflow_graph.png
2025-08-22 17:50:10,165 -    DEBUG - 1049 - urllib3.connectionpool - connectionpool - _new_conn - Starting new HTTPS connection (1): mermaid.ink:443
2025-08-22 17:50:10,782 -    DEBUG -  544 - urllib3.connectionpool - connectionpool - _make_request - https://mermaid.ink:443 "GET /img/LS0tCmNvbmZpZzoKICBmbG93Y2hhcnQ6CiAgICBjdXJ2ZTogbGluZWFyCi0tLQpncmFwaCBURDsKCV9fc3RhcnRfXyhbPHA+X19zdGFydF9fPC9wPl0pOjo6Zmlyc3QKCXNpbV91c2VyX3Byb21wdChzaW1fdXNlcl9wcm9tcHQpCglzaW1fdXNlcl9xdWVzdGlvbihzaW1fdXNlcl9xdWVzdGlvbikKCWNoYXRib3QoY2hhdGJvdCkKCV9fZW5kX18oWzxwPl9fZW5kX188L3A+XSk6OjpsYXN0CglfX3N0YXJ0X18gLS0+IHNpbV91c2VyX3Byb21wdDsKCWNoYXRib3QgLS0+IHNpbV91c2VyX3Byb21wdDsKCXNpbV91c2VyX3Byb21wdCAtLT4gc2ltX3VzZXJfcXVlc3Rpb247CglzaW1fdXNlcl9xdWVzdGlvbiAtLi0+IF9fZW5kX187CglzaW1fdXNlcl9xdWVzdGlvbiAtLiAmbmJzcDtjb250aW51ZSZuYnNwOyAuLT4gY2hhdGJvdDsKCWNsYXNzRGVmIGRlZmF1bHQgZmlsbDojZjJmMGZmLGxpbmUtaGVpZ2h0OjEuMgoJY2xhc3NEZWYgZmlyc3QgZmlsbC1vcGFjaXR5OjAKCWNsYXNzRGVmIGxhc3QgZmlsbDojYmZiNmZjCg==?type=png&bgColor=!white HTTP/1.1" 200 15056
2025-08-22 17:50:11,505 -    DEBUG - 1049 - urllib3.connectionpool - connectionpool - _new_conn - Starting new HTTPS connection (1): preprod.cloud.fiddler.ai:443
2025-08-22 17:50:12,761 -    DEBUG -  544 - urllib3.connectionpool - connectionpool - _make_request - https://preprod.cloud.fiddler.ai:443 "POST /v1/traces HTTP/1.1" 200 2
2025-08-22 17:50:28,375 -    DEBUG -  148 - chatbot_chainlit - chatbot_chainlit - chatbot_node - CHATBOT_NODE: Debug - All Messages in State: list(2) [
  SystemMessage {
      content: """
        # System Instructions
        
        You are Fiddler Chatbot, an expert assistant for Fiddler AI's product documentation with integrated security and quality guardrails.
        Your task is to provide detailed, accurate answers based on retrieved documentation while maintaining strict safety and faithfulness standards.
        
        ---
        
        ## Answer Generation Rules
        
        - Provide clear, informative answers (min 800 characters) using retrieved context
        ... (101 more lines)
      """
    }
  HumanMessage {
      content: """
        I‚Äôm a data scientist running an LLM app on Amazon SageMaker. How do I integrate Fiddler‚Äôs native SageMaker AI (Unified Studio) integration to monitor prompts/responses and add Guardrails? Specifically:
        - What fields should I log for LLM monitoring (prompt, response, user_id, latency, token counts, model/version, metadata)?
        - How do I configure hallucination, toxicity, and prompt-injection detection with Fiddler Guardrails, and what‚Äôs the typical latency overhead?
        - Can you share a minimal Python example for onboarding a project/model and sending LLM events from a SageMaker endpoint?
        - What are deployment options to keep data private (VPC/air-gapped) and typical throughput limits (we‚Äôre ~3‚Äì5M requests/day)?
        
        EXIT NOW
      """
    }
]
2025-08-22 17:50:29,210 -    DEBUG -  544 - urllib3.connectionpool - connectionpool - _make_request - https://preprod.cloud.fiddler.ai:443 "POST /v1/traces HTTP/1.1" 200 2
2025-08-22 17:50:30,155 -    DEBUG -  153 - chatbot_chainlit - chatbot_chainlit - chatbot_node - CHATBOT_NODE: Debug - Response: 
	""
2025-08-22 17:50:30,156 -    DEBUG -  160 - chatbot_chainlit - chatbot_chainlit - chatbot_node - Tool calls detected - transferring to tool_execution node
2025-08-22 17:50:30,158 -    DEBUG -  180 - chatbot_chainlit - chatbot_chainlit - tool_execution_node - Executing tool: rag_over_fiddler_knowledge_base with args: {'query': 'SageMaker integration LLM monitoring fields Guardrails hallucination toxicity prompt-injection latency Python onboarding example deployment privacy VPC air-gapped throughput'}
2025-08-22 17:50:30,184 -  WARNING -  143 - vector_index_mgmt - vector_index_mgmt - cassandra_connection - ‚ö†Ô∏è  Connection attempt 1/3 failed: Secure bundle file not found: datastax_auth/secure-connect-fiddlerai.zip
2025-08-22 17:50:30,185 -     INFO -  144 - vector_index_mgmt - vector_index_mgmt - cassandra_connection - üîÑ Retrying connection in 2.0 seconds...
2025-08-22 17:50:30,549 -    DEBUG -  544 - urllib3.connectionpool - connectionpool - _make_request - https://preprod.cloud.fiddler.ai:443 "POST /v1/traces HTTP/1.1" 200 2
2025-08-22 17:50:32,190 -  WARNING -  143 - vector_index_mgmt - vector_index_mgmt - cassandra_connection - ‚ö†Ô∏è  Connection attempt 2/3 failed: Secure bundle file not found: datastax_auth/secure-connect-fiddlerai.zip
2025-08-22 17:50:32,192 -     INFO -  144 - vector_index_mgmt - vector_index_mgmt - cassandra_connection - üîÑ Retrying connection in 4.0 seconds...
2025-08-22 17:50:36,194 -    ERROR -  140 - vector_index_mgmt - vector_index_mgmt - cassandra_connection - ‚ùå Failed to connect to Cassandra after 3 attempts
2025-08-22 17:50:36,196 -    ERROR -   63 - agentic_tools.rag - rag - rag_over_fiddler_knowledge_base - Error in Cassandra search: Secure bundle file not found: datastax_auth/secure-connect-fiddlerai.zip
2025-08-22 17:50:36,198 -    DEBUG -  203 - chatbot_chainlit - chatbot_chainlit - tool_execution_node - Created tool message for rag_over_fiddler_knowledge_base: content='"Error: Secure bundle file not found: datastax_auth/secure-connect-fiddlerai.zip\\n Please fix your mistakes."' name='rag_over_fiddler_knowledge_base' tool_call_id='call_CSOxGqcGIhLnDvBgDjVWi1Oj'
2025-08-22 17:50:36,200 -    DEBUG -  148 - chatbot_chainlit - chatbot_chainlit - chatbot_node - CHATBOT_NODE: Debug - All Messages in State: list(4) [
  SystemMessage {
      content: """
        # System Instructions
        
        You are Fiddler Chatbot, an expert assistant for Fiddler AI's product documentation with integrated security and quality guardrails.
        Your task is to provide detailed, accurate answers based on retrieved documentation while maintaining strict safety and faithfulness standards.
        
        ---
        
        ## Answer Generation Rules
        
        - Provide clear, informative answers (min 800 characters) using retrieved context
        ... (101 more lines)
      """
    }
  HumanMessage {
      content: """
        I‚Äôm a data scientist running an LLM app on Amazon SageMaker. How do I integrate Fiddler‚Äôs native SageMaker AI (Unified Studio) integration to monitor prompts/responses and add Guardrails? Specifically:
        - What fields should I log for LLM monitoring (prompt, response, user_id, latency, token counts, model/version, metadata)?
        - How do I configure hallucination, toxicity, and prompt-injection detection with Fiddler Guardrails, and what‚Äôs the typical latency overhead?
        - Can you share a minimal Python example for onboarding a project/model and sending LLM events from a SageMaker endpoint?
        - What are deployment options to keep data private (VPC/air-gapped) and typical throughput limits (we‚Äôre ~3‚Äì5M requests/day)?
        
        EXIT NOW
      """
    }
  AIMessage {
      content: ""
      additional_kwargs: dict(1) {
        "tool_calls": list(1) [
                dict(4) {
                          "index": 0
                          "id": "call_CSOxGqcGIhLnDvBgDjVWi1Oj"
                          "function": dict(2) {
                                                  ... (max depth 6 reached):             ... (max depth 6 reached)
                                                  ... (max depth 6 reached):             ... (max depth 6 reached)
                                    }
                          "type": "function"
                        }
              ]
      }
      tool_calls: list(1) [
        dict(4) {
                "name": "rag_over_fiddler_knowledge_base"
                "args": dict(1) {
                          "query": "SageMaker integration LLM monitoring fields Guardrails hallucination toxicity prompt-injection laten..."
                        }
                "id": "call_CSOxGqcGIhLnDvBgDjVWi1Oj"
                "type": "tool_call"
              }
      ]
    }
  ToolMessage {
      content: ""Error: Secure bundle file not found: datastax_auth/secure-connect-fiddlerai.zip\n Please fix your m..."
      name: rag_over_fiddler_knowledge_base
    }
]
2025-08-22 17:50:36,915 -    DEBUG -  544 - urllib3.connectionpool - connectionpool - _make_request - https://preprod.cloud.fiddler.ai:443 "POST /v1/traces HTTP/1.1" 200 2
2025-08-22 17:50:57,266 -    DEBUG -  153 - chatbot_chainlit - chatbot_chainlit - chatbot_node - CHATBOT_NODE: Debug - Response: 
	"""
  It appears there was a technical issue retrieving the latest documentation details for your query. However, I can provide a high-level overview based on standard Fiddler AI integration practices for Amazon SageMaker and LLM monitoring. For the most accurate and up-to-date information, please consult Fiddler‚Äôs official documentation or support.
  
  ---
  
  ## 1. Recommended Fields for LLM Monitoring
  
  When integrating Fiddler with SageMaker for LLM monitoring, you should log the following fields for each inference event:
  
  - `prompt`: The input prompt sent to the LLM.
  - `response`: The LLM‚Äôs generated output.
  ... (89 more lines)
"""
2025-08-22 17:50:57,266 -    DEBUG -  163 - chatbot_chainlit - chatbot_chainlit - chatbot_node - No tool calls - ending conversation turn
2025-08-22 17:50:57,267 -  WARNING -  291 - langgraph - algo - apply_writes - Task chatbot with path ('__pregel_pull', 'chatbot') wrote to unknown channel branch:to:__end__, ignoring it.
2025-08-22 17:50:58,369 -    DEBUG -  544 - urllib3.connectionpool - connectionpool - _make_request - https://preprod.cloud.fiddler.ai:443 "POST /v1/traces HTTP/1.1" 200 2
2025-08-22 17:51:29,087 -    DEBUG -  148 - chatbot_chainlit - chatbot_chainlit - chatbot_node - CHATBOT_NODE: Debug - All Messages in State: list(6) [
  SystemMessage {
      content: """
        # System Instructions
        
        You are Fiddler Chatbot, an expert assistant for Fiddler AI's product documentation with integrated security and quality guardrails.
        Your task is to provide detailed, accurate answers based on retrieved documentation while maintaining strict safety and faithfulness standards.
        
        ---
        
        ## Answer Generation Rules
        
        - Provide clear, informative answers (min 800 characters) using retrieved context
        ... (101 more lines)
      """
    }
  HumanMessage {
      content: """
        I‚Äôm a data scientist running an LLM app on Amazon SageMaker. How do I integrate Fiddler‚Äôs native SageMaker AI (Unified Studio) integration to monitor prompts/responses and add Guardrails? Specifically:
        - What fields should I log for LLM monitoring (prompt, response, user_id, latency, token counts, model/version, metadata)?
        - How do I configure hallucination, toxicity, and prompt-injection detection with Fiddler Guardrails, and what‚Äôs the typical latency overhead?
        - Can you share a minimal Python example for onboarding a project/model and sending LLM events from a SageMaker endpoint?
        - What are deployment options to keep data private (VPC/air-gapped) and typical throughput limits (we‚Äôre ~3‚Äì5M requests/day)?
        
        EXIT NOW
      """
    }
  AIMessage {
      content: ""
      additional_kwargs: dict(1) {
        "tool_calls": list(1) [
                dict(4) {
                          "index": 0
                          "id": "call_CSOxGqcGIhLnDvBgDjVWi1Oj"
                          "function": dict(2) {
                                                  ... (max depth 6 reached):             ... (max depth 6 reached)
                                                  ... (max depth 6 reached):             ... (max depth 6 reached)
                                    }
                          "type": "function"
                        }
              ]
      }
      tool_calls: list(1) [
        dict(4) {
                "name": "rag_over_fiddler_knowledge_base"
                "args": dict(1) {
                          "query": "SageMaker integration LLM monitoring fields Guardrails hallucination toxicity prompt-injection laten..."
                        }
                "id": "call_CSOxGqcGIhLnDvBgDjVWi1Oj"
                "type": "tool_call"
              }
      ]
    }
  ToolMessage {
      content: ""Error: Secure bundle file not found: datastax_auth/secure-connect-fiddlerai.zip\n Please fix your m..."
      name: rag_over_fiddler_knowledge_base
    }
  AIMessage {
      content: """
        It appears there was a technical issue retrieving the latest documentation details for your query. However, I can provide a high-level overview based on standard Fiddler AI integration practices for Amazon SageMaker and LLM monitoring. For the most accurate and up-to-date information, please consult Fiddler‚Äôs official documentation or support.
        
        ---
        
        ## 1. Recommended Fields for LLM Monitoring
        
        When integrating Fiddler with SageMaker for LLM monitoring, you should log the following fields for each inference event:
        
        - `prompt`: The input prompt sent to the LLM.
        - `response`: The LLM‚Äôs generated output.
        ... (89 more lines)
      """
    }
  HumanMessage {
      content: """
        Thanks for the overview‚Äîsuper helpful. A few follow-ups to get this production-ready in SageMaker AI (Unified Studio):
        
        - Latency: You mentioned 100‚Äì500ms overhead, but Fiddler Guardrails advertises sub-100ms. What are the expected p50/p95 latencies for inline moderation, and what config (Trust Models, instance sizing, batching) is required to consistently hit sub-100ms?
        - Native SageMaker AI integration: What are the exact steps in Studio to enable Fiddler? Where do I find it in the UI, what IAM roles/permissions are required, and how do I connect an existing SageMaker endpoint or Model Registry model to Fiddler without leaving Studio?
        - Streaming: Do you support streaming token capture (SSE/WebSocket)? How should I log partial tokens vs final response so monitoring and Guardrails decisions are accurate?
        - Deployment patterns: Can Guardrails run inline inside my VPC as a SageMaker real-time endpoint or sidecar, with block/replace/redact actions? Any sample policy configuration I can start from?
        - Hallucination grounding: How do I plug in our internal vector store/KB so hallucination checks cite evidence, and have those snippets logged to Fiddler for review?
        - Example: Could you share a minimal Python example for Bedrock (Anthropic Claude) that logs user_id/session_id/trace_id, token usage, latency, and calls Guardrails inline?
        
        EXIT NOW
      """
    }
]
2025-08-22 17:51:30,073 -    DEBUG -  544 - urllib3.connectionpool - connectionpool - _make_request - https://preprod.cloud.fiddler.ai:443 "POST /v1/traces HTTP/1.1" 200 2
2025-08-22 17:51:43,633 -    DEBUG -  153 - chatbot_chainlit - chatbot_chainlit - chatbot_node - CHATBOT_NODE: Debug - Response: 
	""
2025-08-22 17:51:43,633 -    DEBUG -  160 - chatbot_chainlit - chatbot_chainlit - chatbot_node - Tool calls detected - transferring to tool_execution node
2025-08-22 17:51:43,634 -    DEBUG -  180 - chatbot_chainlit - chatbot_chainlit - tool_execution_node - Executing tool: rag_over_fiddler_knowledge_base with args: {'query': 'SageMaker AI Studio Fiddler integration Guardrails latency inline moderation Trust Models instance sizing batching p50 p95 IAM roles connect endpoint Model Registry streaming SSE WebSocket token capture logging partial tokens Guardrails inline VPC SageMaker endpoint sidecar block replace redact policy hallucination grounding vector store evidence logging Bedrock Anthropic Claude Python example user_id session_id trace_id token usage latency Guardrails inline'}
2025-08-22 17:51:43,653 -  WARNING -  143 - vector_index_mgmt - vector_index_mgmt - cassandra_connection - ‚ö†Ô∏è  Connection attempt 1/3 failed: Secure bundle file not found: datastax_auth/secure-connect-fiddlerai.zip
2025-08-22 17:51:43,653 -     INFO -  144 - vector_index_mgmt - vector_index_mgmt - cassandra_connection - üîÑ Retrying connection in 2.0 seconds...
2025-08-22 17:51:44,460 -    DEBUG -  544 - urllib3.connectionpool - connectionpool - _make_request - https://preprod.cloud.fiddler.ai:443 "POST /v1/traces HTTP/1.1" 200 2
2025-08-22 17:51:45,657 -  WARNING -  143 - vector_index_mgmt - vector_index_mgmt - cassandra_connection - ‚ö†Ô∏è  Connection attempt 2/3 failed: Secure bundle file not found: datastax_auth/secure-connect-fiddlerai.zip
2025-08-22 17:51:45,658 -     INFO -  144 - vector_index_mgmt - vector_index_mgmt - cassandra_connection - üîÑ Retrying connection in 4.0 seconds...
2025-08-22 17:51:49,663 -    ERROR -  140 - vector_index_mgmt - vector_index_mgmt - cassandra_connection - ‚ùå Failed to connect to Cassandra after 3 attempts
2025-08-22 17:51:49,664 -    ERROR -   63 - agentic_tools.rag - rag - rag_over_fiddler_knowledge_base - Error in Cassandra search: Secure bundle file not found: datastax_auth/secure-connect-fiddlerai.zip
2025-08-22 17:51:49,665 -    DEBUG -  203 - chatbot_chainlit - chatbot_chainlit - tool_execution_node - Created tool message for rag_over_fiddler_knowledge_base: content='"Error: Secure bundle file not found: datastax_auth/secure-connect-fiddlerai.zip\\n Please fix your mistakes."' name='rag_over_fiddler_knowledge_base' tool_call_id='call_VmMBGHQGzcAliOWs9Qngqg3T'
2025-08-22 17:51:49,666 -    DEBUG -  148 - chatbot_chainlit - chatbot_chainlit - chatbot_node - CHATBOT_NODE: Debug - All Messages in State: list(8) [
  SystemMessage {
      content: """
        # System Instructions
        
        You are Fiddler Chatbot, an expert assistant for Fiddler AI's product documentation with integrated security and quality guardrails.
        Your task is to provide detailed, accurate answers based on retrieved documentation while maintaining strict safety and faithfulness standards.
        
        ---
        
        ## Answer Generation Rules
        
        - Provide clear, informative answers (min 800 characters) using retrieved context
        ... (101 more lines)
      """
    }
  HumanMessage {
      content: """
        I‚Äôm a data scientist running an LLM app on Amazon SageMaker. How do I integrate Fiddler‚Äôs native SageMaker AI (Unified Studio) integration to monitor prompts/responses and add Guardrails? Specifically:
        - What fields should I log for LLM monitoring (prompt, response, user_id, latency, token counts, model/version, metadata)?
        - How do I configure hallucination, toxicity, and prompt-injection detection with Fiddler Guardrails, and what‚Äôs the typical latency overhead?
        - Can you share a minimal Python example for onboarding a project/model and sending LLM events from a SageMaker endpoint?
        - What are deployment options to keep data private (VPC/air-gapped) and typical throughput limits (we‚Äôre ~3‚Äì5M requests/day)?
        
        EXIT NOW
      """
    }
  AIMessage {
      content: ""
      additional_kwargs: dict(1) {
        "tool_calls": list(1) [
                dict(4) {
                          "index": 0
                          "id": "call_CSOxGqcGIhLnDvBgDjVWi1Oj"
                          "function": dict(2) {
                                                  ... (max depth 6 reached):             ... (max depth 6 reached)
                                                  ... (max depth 6 reached):             ... (max depth 6 reached)
                                    }
                          "type": "function"
                        }
              ]
      }
      tool_calls: list(1) [
        dict(4) {
                "name": "rag_over_fiddler_knowledge_base"
                "args": dict(1) {
                          "query": "SageMaker integration LLM monitoring fields Guardrails hallucination toxicity prompt-injection laten..."
                        }
                "id": "call_CSOxGqcGIhLnDvBgDjVWi1Oj"
                "type": "tool_call"
              }
      ]
    }
  ToolMessage {
      content: ""Error: Secure bundle file not found: datastax_auth/secure-connect-fiddlerai.zip\n Please fix your m..."
      name: rag_over_fiddler_knowledge_base
    }
  AIMessage {
      content: """
        It appears there was a technical issue retrieving the latest documentation details for your query. However, I can provide a high-level overview based on standard Fiddler AI integration practices for Amazon SageMaker and LLM monitoring. For the most accurate and up-to-date information, please consult Fiddler‚Äôs official documentation or support.
        
        ---
        
        ## 1. Recommended Fields for LLM Monitoring
        
        When integrating Fiddler with SageMaker for LLM monitoring, you should log the following fields for each inference event:
        
        - `prompt`: The input prompt sent to the LLM.
        - `response`: The LLM‚Äôs generated output.
        ... (89 more lines)
      """
    }
  HumanMessage {
      content: """
        Thanks for the overview‚Äîsuper helpful. A few follow-ups to get this production-ready in SageMaker AI (Unified Studio):
        
        - Latency: You mentioned 100‚Äì500ms overhead, but Fiddler Guardrails advertises sub-100ms. What are the expected p50/p95 latencies for inline moderation, and what config (Trust Models, instance sizing, batching) is required to consistently hit sub-100ms?
        - Native SageMaker AI integration: What are the exact steps in Studio to enable Fiddler? Where do I find it in the UI, what IAM roles/permissions are required, and how do I connect an existing SageMaker endpoint or Model Registry model to Fiddler without leaving Studio?
        - Streaming: Do you support streaming token capture (SSE/WebSocket)? How should I log partial tokens vs final response so monitoring and Guardrails decisions are accurate?
        - Deployment patterns: Can Guardrails run inline inside my VPC as a SageMaker real-time endpoint or sidecar, with block/replace/redact actions? Any sample policy configuration I can start from?
        - Hallucination grounding: How do I plug in our internal vector store/KB so hallucination checks cite evidence, and have those snippets logged to Fiddler for review?
        - Example: Could you share a minimal Python example for Bedrock (Anthropic Claude) that logs user_id/session_id/trace_id, token usage, latency, and calls Guardrails inline?
        
        EXIT NOW
      """
    }
  AIMessage {
      content: ""
      additional_kwargs: dict(1) {
        "tool_calls": list(1) [
                dict(4) {
                          "index": 0
                          "id": "call_VmMBGHQGzcAliOWs9Qngqg3T"
                          "function": dict(2) {
                                                  ... (max depth 6 reached):             ... (max depth 6 reached)
                                                  ... (max depth 6 reached):             ... (max depth 6 reached)
                                    }
                          "type": "function"
                        }
              ]
      }
      tool_calls: list(1) [
        dict(4) {
                "name": "rag_over_fiddler_knowledge_base"
                "args": dict(1) {
                          "query": "SageMaker AI Studio Fiddler integration Guardrails latency inline moderation Trust Models instance s..."
                        }
                "id": "call_VmMBGHQGzcAliOWs9Qngqg3T"
                "type": "tool_call"
              }
      ]
    }
  ToolMessage {
      content: ""Error: Secure bundle file not found: datastax_auth/secure-connect-fiddlerai.zip\n Please fix your m..."
      name: rag_over_fiddler_knowledge_base
    }
]
2025-08-22 17:51:50,873 -    DEBUG -  544 - urllib3.connectionpool - connectionpool - _make_request - https://preprod.cloud.fiddler.ai:443 "POST /v1/traces HTTP/1.1" 200 2
2025-08-22 17:52:19,384 -    DEBUG -  544 - urllib3.connectionpool - connectionpool - _make_request - https://preprod.cloud.fiddler.ai:443 "POST /v1/traces HTTP/1.1" 200 2
